{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Търсене и извличане на информация. Приложение на дълбоко машинно обучение\n",
    "### Стоян Михов\n",
    "### Зимен семестър 2025/2026\n",
    "\n",
    "## Упражнение 9\n",
    "\n",
    "За да работи програмата трябва корпуса от публицистични текстове за Югоизточна Европа,\n",
    "да се намира разархивиран в наддиректорията, в която е програмата (виж упражнение 2).\n",
    "\n",
    "Преди да се стартира програмата е необходимо да се активира съответното обкръжение с командата:\n",
    "conda activate tii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import pprint\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class progressBar:\n",
    "    def __init__(self ,barWidth = 50):\n",
    "        self.barWidth = barWidth\n",
    "        self.period = None\n",
    "    def start(self, count):\n",
    "        self.item=0\n",
    "        self.period = int(count / self.barWidth)\n",
    "        sys.stdout.write(\"[\"+(\" \" * self.barWidth)+\"]\")\n",
    "        sys.stdout.flush()\n",
    "    def tick(self):\n",
    "        if self.item>0 and self.item % self.period == 0:\n",
    "            progress = self.item // self.period\n",
    "            sys.stdout.write(f\"\\r[{'-'* progress}{' '*(self.barWidth - progress)}]\")\n",
    "            sys.stdout.flush()\n",
    "        self.item += 1\n",
    "    def stop(self):\n",
    "        sys.stdout.write(\"\\r[\"+(\"-\" * self.barWidth)+\"]\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Разбиване на корпус на тестов и тренинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitSentCorpus(fullSentCorpus, testFraction = 0.1):\n",
    "    random.seed(42)\n",
    "    random.shuffle(fullSentCorpus)\n",
    "    testCount = int(len(fullSentCorpus) * testFraction)\n",
    "    testSentCorpus = fullSentCorpus[:testCount]\n",
    "    trainSentCorpus = fullSentCorpus[testCount:]\n",
    "    return testSentCorpus, trainSentCorpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####   Мултиномен Бейсов класификатор от упражнение 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainMultinomialNB(trainClassCorpus):\n",
    "    N = sum(len(classList) for classList in trainClassCorpus)\n",
    "    classesCount = len(trainClassCorpus)\n",
    "    pb = progressBar(50)\n",
    "    pb.start(N)\n",
    "    V = {}\n",
    "    for c in range(classesCount):\n",
    "        for text in trainClassCorpus[c]:\n",
    "            pb.tick()\n",
    "            terms = [ token.lower() for token in text if token.isalpha() ]\n",
    "            for term in terms:\n",
    "                if term not in V:\n",
    "                    V[term] = [0] * classesCount\n",
    "                V[term][c] += 1\n",
    "    pb.stop()\n",
    "\n",
    "    Nc = [ (len(classList)) for classList in trainClassCorpus ]\n",
    "    prior = [ Nc[c] / N for c in range(classesCount) ]\n",
    "    T = [0] * classesCount\n",
    "    for t in V:\n",
    "        for c in range(classesCount):\n",
    "            T[c] += V[t][c]\n",
    "    condProb = {}\n",
    "    for t in V:\n",
    "        condProb[t] = [ (V[t][c] +1) / (T[c] + len(V)) for c in range(classesCount)]\n",
    "    return condProb, prior, V\n",
    "\n",
    "def applyMultinomialNB(prior, condProb, text, features = None ):\n",
    "    terms = [ token.lower() for token in text if token.isalpha() ]\n",
    "    for c in range(len(prior)):\n",
    "        score = math.log(prior[c])\n",
    "        for t in terms:\n",
    "            if t not in condProb: continue\n",
    "            if features and t not in features: continue\n",
    "            score += math.log(condProb[t][c])\n",
    "        if c == 0 or score > maxScore:\n",
    "            maxScore = score\n",
    "            answer = c\n",
    "    return answer\n",
    "\n",
    "def testClassifier(testClassCorpus, gamma):\n",
    "    L = [ len(c) for c in testClassCorpus ]\n",
    "    pb = progressBar(50)\n",
    "    pb.start(sum(L))\n",
    "    classesCount = len(testClassCorpus)\n",
    "    confusionMatrix = [ [0] * classesCount for _ in range(classesCount) ]\n",
    "    for c in range(classesCount):\n",
    "        for text in testClassCorpus[c]:\n",
    "            pb.tick()\n",
    "            c_MAP = gamma(text)\n",
    "            confusionMatrix[c][c_MAP] += 1\n",
    "    pb.stop()\n",
    "    precision = []\n",
    "    recall = []\n",
    "    Fscore = []\n",
    "    for c in range(classesCount):\n",
    "        extracted = sum(confusionMatrix[x][c] for x in range(classesCount))\n",
    "        if confusionMatrix[c][c] == 0:\n",
    "            precision.append(0.0)\n",
    "            recall.append(0.0)\n",
    "            Fscore.append(0.0)\n",
    "        else:\n",
    "            precision.append( confusionMatrix[c][c] / extracted )\n",
    "            recall.append( confusionMatrix[c][c] / L[c] )\n",
    "            Fscore.append((2.0 * precision[c] * recall[c]) / (precision[c] + recall[c]))\n",
    "    P = sum( L[c] * precision[c] / sum(L) for c in range(classesCount) )\n",
    "    R = sum( L[c] * recall[c] / sum(L) for c in range(classesCount) )\n",
    "    F1 = (2*P*R) / (P + R)\n",
    "    print('=================================================================')\n",
    "    print('Матрица на обърквания: ')\n",
    "    for row in confusionMatrix:\n",
    "        for val in row:\n",
    "            print('{:4}'.format(val), end = '')\n",
    "        print()\n",
    "    print('Прецизност: '+str(precision))\n",
    "    print('Обхват: '+str(recall))\n",
    "    print('F-оценка: '+str(Fscore))\n",
    "    print('Обща презизност: '+str(P))\n",
    "    print('Общ обхват: '+str(R))\n",
    "    print('Обща F-оценка: '+str(F1))\n",
    "    print('=================================================================')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####   Влагане на думи в нискомерно гъсто векторно пространство от упражнение 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractDictionary(corpus, limit=20000):\n",
    "    pb = progressBar()\n",
    "    pb.start(len(corpus))\n",
    "    dictionary = {}\n",
    "    for doc in corpus:\n",
    "        pb.tick()\n",
    "        for w in doc:\n",
    "            if w not in dictionary: dictionary[w] = 0\n",
    "        dictionary[w] += 1\n",
    "    L = sorted([(w,dictionary[w]) for w in dictionary], key = lambda x: x[1] , reverse=True)\n",
    "    if limit > len(L): limit = len(L)\n",
    "    words = [ w for w,_ in L[:limit] ]\n",
    "    word2ind = { w:i for i,w in enumerate(words)}\n",
    "    pb.stop()\n",
    "    return words, word2ind\n",
    "\n",
    "\n",
    "def co_occurrence_matrix(corpus, window_size=4, limit=20000):\n",
    "    words, word2ind = extractDictionary(corpus,limit=limit)\n",
    "    num_words = len(words)\n",
    "    X=np.zeros((num_words,num_words))\n",
    "    pb = progressBar()\n",
    "    pb.start(len(corpus))\n",
    "    for doc in corpus:\n",
    "        pb.tick()\n",
    "        for wi in range(len(doc)):\n",
    "            if doc[wi] not in word2ind: continue\n",
    "            i=word2ind[doc[wi]]\n",
    "            for k in range(1,window_size+1):\n",
    "                if wi-k>=0 and doc[wi-k] in word2ind:\n",
    "                    j=word2ind[doc[wi-k]]\n",
    "                    X[i,j] += 1\n",
    "                if wi+k<len(doc) and doc[wi+k] in word2ind:\n",
    "                    j=word2ind[doc[wi+k]]\n",
    "                    X[i,j] += 1\n",
    "    pb.stop()\n",
    "    return X, words, word2ind\n",
    "\n",
    "def PMI_matrix(C):\n",
    "    rowSums = np.sum(C,axis=1)\n",
    "    colSums = np.sum(C,axis=0)\n",
    "    D = np.sum(rowSums)\n",
    "    Z = np.outer(rowSums,colSums)\n",
    "    X = np.maximum(np.log( D * C / Z),0)\n",
    "    return X\n",
    "\n",
    "def SVD_k_dim(X, k=100, n_iters = 10):\n",
    "    # Документация на метода има на https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html\n",
    "    \n",
    "    print(\"Running Truncated SVD over %i words...\" % (X.shape[0]))\n",
    "    svd = TruncatedSVD(n_components=k, n_iter=n_iters)\n",
    "    svd.fit(X)\n",
    "    X_reduced = svd.transform(X)\n",
    "    print(\"Done.\")\n",
    "    return X_reduced\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Влагане на документи в нискомерно гъсто векторно пространство\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docVector(document, Embedding, word2ind):\n",
    "    tf = np.zeros(len(word2ind))\n",
    "    for w in document:\n",
    "        if w in word2ind:\n",
    "            tf[word2ind[w]] += 1\n",
    "    d=np.dot(tf,Embedding)\n",
    "    return d / np.linalg.norm(d)\n",
    "\n",
    "def corpusEmbedding(corpus, Embedding, word2ind):\n",
    "    return np.stack([ docVector(doc, Embedding, word2ind) for doc in corpus ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####    Логистична регресия -- Бинарен класификатор\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def crossEntropyS(X, Y, w, b):\n",
    "\t# X.shape = (S,N) Y.shape = (S), W.shape = (N) \n",
    "    v = sigmoid(np.dot(X,w)+b)\n",
    "    p = (1-Y) + (2*Y-1)*v\n",
    "    ce = -np.mean(np.log(p))\n",
    "    return ce\n",
    "\n",
    "def gradCrossEntropyS(X,Y,w,b):\n",
    "    g = Y - sigmoid(np.dot(X,w)+b)\n",
    "    db = -np.mean(g)\n",
    "    dw = -np.mean( g[:,np.newaxis] * X,axis=0)\n",
    "    return dw, db\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "####    Логистична регресия -- класификатор при много класове\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmaxV(u):\n",
    "    ### u следва да бъде вектор с резмер N\n",
    "    e = np.exp(u)\n",
    "    return e / np.sum(e)\n",
    "\n",
    "def softmaxM(U):\n",
    "    ### U следва да бъде матрица с размерност: (S,N)\n",
    "    e = np.exp(U)\n",
    "    return e / np.sum(e,axis=1)[:,np.newaxis]\n",
    "\n",
    "def crossEntropyM(X, Y, W, b):\n",
    "    ### класовете са k\n",
    "    ### X е с размерност: (S,N)\n",
    "    ### Y е с размерност: (S)\n",
    "    ### W е с размерност: (N,K)\n",
    "    ### b е с размерност: (K)\n",
    "\n",
    "    S = X.shape[0]\n",
    "    v = softmaxM(np.dot(X,W)+b[np.newaxis,:])\n",
    "    p = v[np.arange(S),Y]\n",
    "    ce = -np.mean(np.log(p))\n",
    "    return ce\n",
    "\n",
    "def gradCrossEntropyM(X,Y,W,b):\n",
    "    S = X.shape[0]\n",
    "    v = softmaxM(np.dot(X,W)+b[np.newaxis,:])\n",
    "    v = -v\n",
    "    v[np.arange(S),Y] += 1.\n",
    "    db = -np.mean(v, axis=0)\n",
    "    dW = -(1/S) * np.dot(X.transpose(), v)\n",
    "    return dW, db\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####    спускане по градиента\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescend(X,Y,tX,tY,w0,b0,crossEntropy,gradCrossEntropy,alpha=1.):\n",
    "    epoch=0\n",
    "    w=w0\n",
    "    b=b0\n",
    "    ceList = []\n",
    "    tceList = []\n",
    "    while epoch<100000:\n",
    "        if epoch % 1000 == 0:\n",
    "            ce = crossEntropy(X, Y, w, b)\n",
    "            tce = crossEntropy(tX, tY, w, b)\n",
    "            print(epoch,ce,tce)\n",
    "            ceList.append(ce)\n",
    "            tceList.append(tce)\n",
    "        epoch += 1\n",
    "        dw, db = gradCrossEntropy(X,Y,w,b)\n",
    "        b -= alpha * db\n",
    "        w -= alpha * dw\n",
    "    return w,b,ceList,tceList\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####   Зареждане на корпуса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:2: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:2: SyntaxWarning: invalid escape sequence '\\.'\n",
      "C:\\Users\\Rumen\\AppData\\Local\\Temp\\ipykernel_18580\\1111099134.py:2: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  myCorpus = PlaintextCorpusReader(corpus_root, '.*\\.txt')\n"
     ]
    }
   ],
   "source": [
    "corpus_root = '../JOURNALISM.BG/C-MassMedia'\n",
    "myCorpus = PlaintextCorpusReader(corpus_root, '.*\\.txt')\n",
    "startToken = '<START>'\n",
    "endToken = '<END>'\n",
    "fileNames = myCorpus.fileids()\n",
    "\n",
    "ecoCorpus = [ [startToken] + [w.lower() for w in myCorpus.words(f)] + [endToken] for f in fileNames if f.find('E-Economy'+'/')==0 ]\n",
    "milCorpus = [ [startToken] + [w.lower() for w in myCorpus.words(f)] + [endToken] for f in fileNames if f.find('S-Military'+'/')==0 ]\n",
    "polCorpus = [ [startToken] + [w.lower() for w in myCorpus.words(f)] + [endToken] for f in fileNames if f.find('J-Politics'+'/')==0 ]\n",
    "culCorpus = [ [startToken] + [w.lower() for w in myCorpus.words(f)] + [endToken] for f in fileNames if f.find('C-Culture'+'/')==0 ]\n",
    "socCorpus = [ [startToken] + [w.lower() for w in myCorpus.words(f)] + [endToken] for f in fileNames if f.find('D-Society'+'/')==0 ]\n",
    "zCorpus = [ [startToken] + [w.lower() for w in myCorpus.words(f)] + [endToken] for f in fileNames if f.find('Z'+'/')==0 ]\n",
    "\n",
    "testEcoCorpus, trainEcoCorpus = splitSentCorpus(ecoCorpus)\n",
    "testMilCorpus, trainMilCorpus = splitSentCorpus(milCorpus)\n",
    "testPolCorpus, trainPolCorpus = splitSentCorpus(polCorpus)\n",
    "testCulCorpus, trainCulCorpus = splitSentCorpus(culCorpus)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####   Тестване на Бейсов класификатор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[--------------------------------------------------]\n",
      "[--------------------------------------------------]\n",
      "=================================================================\n",
      "Матрица на обърквания: \n",
      "  45   0  20   0\n",
      "   2 127  28   1\n",
      "  10  36 688   1\n",
      "   1   0   2  42\n",
      "Прецизност: [0.7758620689655172, 0.7791411042944786, 0.9322493224932249, 0.9545454545454546]\n",
      "Обхват: [0.6923076923076923, 0.8037974683544303, 0.9360544217687075, 0.9333333333333333]\n",
      "F-оценка: [0.7317073170731708, 0.7912772585669782, 0.9341479972844534, 0.9438202247191012]\n",
      "Обща презизност: 0.8989961380342492\n",
      "Общ обхват: 0.8993020937188435\n",
      "Обща F-оценка: 0.8991490898494726\n",
      "=================================================================\n",
      "\n",
      "[--------------------------------------------------]\n",
      "[--------------------------------------------------]----]\n",
      "=================================================================\n",
      "Матрица на обърквания: \n",
      "  60   5\n",
      "   5 153\n",
      "Прецизност: [0.9230769230769231, 0.9683544303797469]\n",
      "Обхват: [0.9230769230769231, 0.9683544303797469]\n",
      "F-оценка: [0.9230769230769231, 0.9683544303797469]\n",
      "Обща презизност: 0.9551569506726457\n",
      "Общ обхват: 0.9551569506726457\n",
      "Обща F-оценка: 0.9551569506726457\n",
      "=================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "condProbM, priorM, VM = trainMultinomialNB([trainEcoCorpus,trainMilCorpus,trainPolCorpus,trainCulCorpus])\n",
    "\n",
    "gamma = lambda text : applyMultinomialNB(priorM, condProbM, text)\n",
    "testClassifier([testEcoCorpus,testMilCorpus,testPolCorpus,testCulCorpus], gamma)\n",
    "\n",
    "condProbM, priorM, VM = trainMultinomialNB([trainEcoCorpus,trainMilCorpus])\n",
    "\n",
    "gamma = lambda text : applyMultinomialNB(priorM, condProbM, text)\n",
    "testClassifier([testEcoCorpus,testMilCorpus], gamma)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Влагане на думите"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[--------------------------------------------------]\n",
      "[--------------------------------------------------]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rumen\\AppData\\Local\\Temp\\ipykernel_18580\\2891412210.py:44: RuntimeWarning: divide by zero encountered in log\n",
      "  X = np.maximum(np.log( D * C / Z),0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Truncated SVD over 20000 words...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "C, words, word2ind = co_occurrence_matrix(ecoCorpus+milCorpus+polCorpus+culCorpus+socCorpus+zCorpus)\n",
    "X = PMI_matrix(C)\n",
    "X_reduced = SVD_k_dim(X)\n",
    "\n",
    "X_lengths_100d = np.linalg.norm(X_reduced, axis=1)\n",
    "X_normalized_100d = X_reduced / X_lengths_100d[:, np.newaxis]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[--------------------------------------------------]----]\n",
      "=================================================================\n",
      "Матрица на обърквания: \n",
      "  60   5\n",
      "   5 153\n",
      "Прецизност: [0.9230769230769231, 0.9683544303797469]\n",
      "Обхват: [0.9230769230769231, 0.9683544303797469]\n",
      "F-оценка: [0.9230769230769231, 0.9683544303797469]\n",
      "Обща презизност: 0.9551569506726457\n",
      "Общ обхват: 0.9551569506726457\n",
      "Обща F-оценка: 0.9551569506726457\n",
      "=================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testClassifier([testEcoCorpus,testMilCorpus], gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.3901413524794382 1.4011360895321865\n",
      "1000 0.4649768498017681 0.4756690448428008\n",
      "2000 0.3833587524198124 0.39348892234674915\n",
      "3000 0.34411613898621013 0.3545695614048132\n",
      "4000 0.3199767134250455 0.3313897825779903\n",
      "5000 0.30316867108374673 0.31584698577207776\n",
      "6000 0.29057015048530216 0.3046205249633559\n",
      "7000 0.28065939135748036 0.29608356106026745\n",
      "8000 0.2725937872042222 0.2893413024523509\n",
      "9000 0.2658624729922241 0.2838592514627351\n",
      "10000 0.2601341576758092 0.2792976643357455\n",
      "11000 0.25518270602272775 0.2754298675817083\n",
      "12000 0.2508474716739653 0.27209863861448114\n",
      "13000 0.2470106396463101 0.26919139983179247\n",
      "14000 0.2435835353122102 0.26662536945868137\n",
      "15000 0.24049796049721742 0.2643382865067469\n",
      "16000 0.23770049724494344 0.26228240625058663\n",
      "17000 0.23514864110723843 0.2604204923592349\n",
      "18000 0.23280810439451693 0.25872307014514206\n",
      "19000 0.23065089130135535 0.25716650014789266\n",
      "20000 0.2286538960977859 0.2557315992487718\n",
      "21000 0.226797864110631 0.2544026356396182\n",
      "22000 0.22506660950286383 0.2531665842847675\n",
      "23000 0.22344641812441737 0.25201256722166504\n",
      "24000 0.2219255858975486 0.25093142719366\n",
      "25000 0.22049405789865723 0.249915398911073\n",
      "26000 0.21914314323376866 0.24895785278167418\n",
      "27000 0.21786528764327073 0.24805309311421941\n",
      "28000 0.21665389055596632 0.2471961977431004\n",
      "29000 0.21550315671008974 0.24638288948655057\n",
      "30000 0.21440797490459573 0.24560943231183327\n",
      "31000 0.21336381822649364 0.24487254685150056\n",
      "32000 0.2123666614140325 0.24416934120404052\n",
      "33000 0.21141291199454446 0.24349725390144908\n",
      "34000 0.21049935257230415 0.2428540066324786\n",
      "35000 0.2096230922009826 0.24223756484093817\n",
      "36000 0.20878152520348137 0.24164610472081172\n",
      "37000 0.20797229613244195 0.24107798543778783\n",
      "38000 0.2071932698217527 0.24053172564421463\n",
      "39000 0.20644250568067088 0.24000598353903843\n",
      "40000 0.2057182355408857 0.23949953986876552\n",
      "41000 0.2050188444927801 0.23901128337938438\n",
      "42000 0.2043428542476705 0.2385401983195418\n",
      "43000 0.20368890864351163 0.2380853536673793\n",
      "44000 0.20305576097670036 0.23764589381131745\n",
      "45000 0.20244226289548428 0.23722103046177337\n",
      "46000 0.20184735463359088 0.23681003560865696\n",
      "47000 0.20127005639802573 0.23641223537034042\n",
      "48000 0.20070946075406612 0.23602700460502382\n",
      "49000 0.20016472587452092 0.2356537621761532\n",
      "50000 0.19963506954028348 0.23529196678064423\n",
      "51000 0.19911976379584262 0.23494111326281436\n",
      "52000 0.19861813017733312 0.23460072934868528\n",
      "53000 0.19812953544239517 0.234270372745115\n",
      "54000 0.1976533877409612 0.23394962855640938\n",
      "55000 0.19718913317441536 0.23363810697793932\n",
      "56000 0.1967362526976329 0.23333544123206706\n",
      "57000 0.19629425932441943 0.23304128571656127\n",
      "58000 0.195862695601995 0.23275531433981128\n",
      "59000 0.19544113132455967 0.232477219020642\n",
      "60000 0.19502916145974308 0.23220670833351614\n",
      "61000 0.1946264042649817 0.23194350628244134\n",
      "62000 0.19423249957366384 0.23168735118907077\n",
      "63000 0.19384710723329895 0.2314379946823446\n",
      "64000 0.19346990568006567 0.231195200778613\n",
      "65000 0.19310059063591256 0.23095874504255712\n",
      "66000 0.19273887391597075 0.23072841382040785\n",
      "67000 0.1923844823354228 0.23050400353798467\n",
      "68000 0.19203715670618027 0.23028532005696745\n",
      "69000 0.19169665091478288 0.2300721780835814\n",
      "70000 0.19136273107386317 0.22986440062454608\n",
      "71000 0.19103517474033813 0.22966181848572442\n",
      "72000 0.1907137701942104 0.22946426980941592\n",
      "73000 0.19039831577249902 0.22927159964668573\n",
      "74000 0.1900886192533811 0.22908365956151197\n",
      "75000 0.1897844972861248 0.22890030726387756\n",
      "76000 0.18948577486283655 0.22872140626923518\n",
      "77000 0.18919228482843764 0.2285468255820402\n",
      "78000 0.18890386742563522 0.22837643940128194\n",
      "79000 0.18862036987196404 0.22821012684615175\n",
      "80000 0.1883416459662539 0.22804777170017204\n",
      "81000 0.1880675557221258 0.22788926217227112\n",
      "82000 0.18779796502634255 0.22773449067343784\n",
      "83000 0.18753274532003888 0.2275833536077183\n",
      "84000 0.18727177330103517 0.22743575117643058\n",
      "85000 0.18701493064559985 0.22729158719458115\n",
      "86000 0.18676210374817004 0.22715076891855357\n",
      "87000 0.18651318347767032 0.22701320688422874\n",
      "88000 0.18626806494918763 0.22687881475476587\n",
      "89000 0.18602664730986637 0.22674750917734196\n",
      "90000 0.18578883353798303 0.22661920964820778\n",
      "91000 0.185554530254248 0.22649383838547352\n",
      "92000 0.1853236475444603 0.226371320209083\n",
      "93000 0.18509609879271188 0.22625158242748417\n",
      "94000 0.1848718005244045 0.22613455473054064\n",
      "95000 0.18465067225840012 0.22602016908826525\n",
      "96000 0.18443263636767973 0.22590835965499229\n",
      "97000 0.18421761794793465 0.22579906267863176\n",
      "98000 0.18400554469355954 0.2256922164146795\n",
      "99000 0.18379634678055531 0.22558776104467895\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainX = np.concatenate((\n",
    "                         corpusEmbedding(trainEcoCorpus,X_normalized_100d,word2ind),\n",
    "                         corpusEmbedding(trainMilCorpus,X_normalized_100d,word2ind),\n",
    "                         corpusEmbedding(trainPolCorpus,X_normalized_100d,word2ind),\n",
    "                         corpusEmbedding(trainCulCorpus,X_normalized_100d,word2ind)\n",
    "                         ))\n",
    "trainY = np.concatenate((\n",
    "                         np.ones(len(trainEcoCorpus),dtype='int32')*0,\n",
    "                         np.ones(len(trainMilCorpus),dtype='int32')*1,\n",
    "                         np.ones(len(trainPolCorpus),dtype='int32')*2,\n",
    "                         np.ones(len(trainCulCorpus),dtype='int32')*3\n",
    "                         ))\n",
    "\n",
    "testX = np.concatenate((\n",
    "                         corpusEmbedding(testEcoCorpus,X_normalized_100d,word2ind),\n",
    "                         corpusEmbedding(testMilCorpus,X_normalized_100d,word2ind),\n",
    "                         corpusEmbedding(testPolCorpus,X_normalized_100d,word2ind),\n",
    "                         corpusEmbedding(testCulCorpus,X_normalized_100d,word2ind)\n",
    "                         ))\n",
    "testY = np.concatenate((\n",
    "                         np.ones(len(testEcoCorpus),dtype='int32')*0,\n",
    "                         np.ones(len(testMilCorpus),dtype='int32')*1,\n",
    "                         np.ones(len(testPolCorpus),dtype='int32')*2,\n",
    "                         np.ones(len(testCulCorpus),dtype='int32')*3\n",
    "                         ))\n",
    "\n",
    "W0 = np.random.normal(0.,1.,size=(100,4))\n",
    "b0 = np.random.normal(0., 1., 4)\n",
    "\n",
    "W,b,ceList,tceList = gradientDescend(trainX,trainY,testX,testY,np.copy(W0),np.copy(b0),crossEntropyM,gradCrossEntropyM,alpha=1.)\n",
    "\n",
    "plt.plot([*range(len(ceList))],ceList)\n",
    "plt.plot([*range(len(tceList))],tceList)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[--------------------------------------------------]\n",
      "=================================================================\n",
      "Матрица на обърквания: \n",
      "  51   0  13   1\n",
      "   1 132  24   1\n",
      "   6  23 704   2\n",
      "   1   0   2  42\n",
      "Прецизност: [0.864406779661017, 0.8516129032258064, 0.9475100942126514, 0.9130434782608695]\n",
      "Обхват: [0.7846153846153846, 0.8354430379746836, 0.9578231292517007, 0.9333333333333333]\n",
      "F-оценка: [0.8225806451612904, 0.8434504792332268, 0.952638700947226, 0.9230769230769231]\n",
      "Обща презизност: 0.9254717399358738\n",
      "Общ обхват: 0.9262213359920239\n",
      "Обща F-оценка: 0.9258463862394933\n",
      "=================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gamma = lambda text : np.argmax(softmaxV(np.dot(docVector(text, X_normalized_100d, word2ind),W)+b))\n",
    "testClassifier([testEcoCorpus,testMilCorpus,testPolCorpus,testCulCorpus], gamma)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Упражнение 9\n",
    "\n",
    "Обектна имплементация на Backpropagation с Numpy операции\n",
    "\n",
    "Първи вариант"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class compNode:\n",
    "    ################################################################################\n",
    "    #### Базов обект за връх в изчислителния граф -- първи вариант\n",
    "    ################################################################################\n",
    "\n",
    "    def __init__(self, predecessors, trainable = True):\n",
    "        self.predecessors = predecessors\n",
    "        self.trainable = trainable\n",
    "        self.value = None\n",
    "        self.grad = None\n",
    "\n",
    "    def calcValue(self): ## трябва да се дефинира за конкретния връх като се извика setValue\n",
    "        return\n",
    "\n",
    "    def propagateGrad(self, grad):\n",
    "        if not self.grad:\n",
    "            self.grad = grad\n",
    "        else:\n",
    "            self.grad += grad\n",
    "\n",
    "    def derivative(self,i): ## трябва да се дефинира за конкретния връх\n",
    "        return\n",
    "    \n",
    "    def propagateBack(self):\n",
    "        if not self.predecessors: return\n",
    "        for i,p in enumerate(self.predecessors):\n",
    "            if p.trainable:\n",
    "                partialGrad = np.dot(self.grad,self.derivative(i))\n",
    "                p.propagateGrad(partialGrad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Конкретни инстанции на обекти за върхове в изчислителния граф "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class logNode(compNode):\n",
    "    def calcValue(self):\n",
    "        x = self.predecessors[0].value\n",
    "        self.value = np.log(x)\n",
    "    def derivative(self,i):\n",
    "        x = self.predecessors[0].value\n",
    "        return np.diag(1/x)\n",
    "\n",
    "class sigmoidNode(compNode):\n",
    "    def calcValue(self):\n",
    "        x = self.predecessors[0].value\n",
    "        self.value = sigmoid(x)\n",
    "    def derivative(self,i):\n",
    "        t = self.predecessors[0].value\n",
    "        s = sigmoid(t)\n",
    "        return np.diag(s*(1-s))\n",
    "\n",
    "class minusMeanNode(compNode):\n",
    "    def calcValue(self):\n",
    "        x = self.predecessors[0].value\n",
    "        self.value = -np.mean(x)\n",
    "    def derivative(self,i):\n",
    "        x = self.predecessors[0].value\n",
    "        S=x.shape[0]\n",
    "        return -1/S * np.ones(S)\n",
    "\n",
    "class probNode(compNode):\n",
    "    def calcValue(self):\n",
    "        v = self.predecessors[0].value\n",
    "        y = self.predecessors[1].value\n",
    "        self.value = (1-y) + (2*y-1)*v\n",
    "    def derivative(self,i):\n",
    "        assert i==0\n",
    "        y = self.predecessors[1].value\n",
    "        return np.diag(2*y-1)\n",
    "\n",
    "class plusVectorsNode(compNode):\n",
    "    def calcValue(self):\n",
    "        x = self.predecessors[0].value\n",
    "        y = self.predecessors[1].value\n",
    "        self.value = x+y\n",
    "    def derivative(self,i):\n",
    "        S = self.value.shape[0]\n",
    "        return np.eye(S)\n",
    "\n",
    "class mulMatrixVectorNode(compNode):\n",
    "    def calcValue(self):\n",
    "        x = self.predecessors[0].value\n",
    "        y = self.predecessors[1].value\n",
    "        self.value = np.dot(x,y)\n",
    "    def derivative(self,i):\n",
    "        assert i==1\n",
    "        u = self.predecessors[0].value\n",
    "        return u\n",
    "\n",
    "class copyNode(compNode):\n",
    "    def calcValue(self):\n",
    "        self.value = self.predecessors[0].value\n",
    "    def derivative(self,i):\n",
    "        S = self.grad.shape\n",
    "        return np.ones(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Създаване на изчислителен граф за логистичната регресия -- първи вариант"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = compNode(None,trainable=False)\n",
    "y = compNode(None,trainable=False)\n",
    "w = compNode(None)\n",
    "b = compNode(None)\n",
    "u = mulMatrixVectorNode([x,w])\n",
    "bS = copyNode([b])\n",
    "t = plusVectorsNode([u,bS])\n",
    "v = sigmoidNode([t])\n",
    "p = probNode([v,y])\n",
    "l = logNode([p])\n",
    "h = minusMeanNode([l])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Топологично сортиране на върховете на изчислителен граф"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSortedNodes(t,L):\n",
    "    if t in L: return L\n",
    "    if t.predecessors:\n",
    "        for p in t.predecessors:\n",
    "            L = getSortedNodes(p,L)\n",
    "    L.append(t)\n",
    "    return L\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Базов обект за модел на невронна мрежа\n",
    "\n",
    "Съдържа имплементация на Backpropagation\n",
    "\n",
    "и стохастично спускане по градиента"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model:\n",
    "    def __init__(self, topNode):\n",
    "        self.topNode = topNode\n",
    "        self.sortedNodes = getSortedNodes(topNode,[])\n",
    "        self.paramNodes = [ v for v in self.sortedNodes if v.trainable and not v.predecessors ]\n",
    "        self.dataNodes = [ v for v in self.sortedNodes if not v.trainable and not v.predecessors ]\n",
    "\n",
    "    def setParameters(self, params):\n",
    "        for i, p in enumerate(params):\n",
    "            self.paramNodes[i].value = np.copy(p)\n",
    "\n",
    "    def setData(self, data):\n",
    "        for i, d in enumerate(data):\n",
    "            self.dataNodes[i].value = d\n",
    "    \n",
    "    def forward(self):\n",
    "        for v in self.sortedNodes:\n",
    "            v.calcValue()\n",
    "\n",
    "    def backwards(self):\n",
    "        for v in self.sortedNodes:\n",
    "            v.grad = None\n",
    "        self.topNode.propagateGrad(1)\n",
    "        for v in reversed(self.sortedNodes):\n",
    "            v.propagateBack()\n",
    "\n",
    "    def updateModel(self,alpha):\n",
    "        for p in self.paramNodes:\n",
    "            p.value -= alpha * p.grad\n",
    "\n",
    "    def calcLoss(self,testData):\n",
    "        self.setData(testData)\n",
    "        self.forward()\n",
    "        return self.topNode.value\n",
    "\n",
    "    def batchedStochasticGradient(self, initialParams, trainData, testData, batchSize, alpha = 1., maxStep = 100000, printInterval = 1000):\n",
    "        self.setParameters(initialParams)\n",
    "        self.topNode.value = None\n",
    "        ceList = []\n",
    "        tceList = []\n",
    "        epoch = 0\n",
    "        step = 0\n",
    "        samples = np.arange(trainData[0].shape[0], dtype='int32')\n",
    "        while True:\n",
    "            np.random.shuffle(samples)\n",
    "            for i in range(0,len(samples),batchSize):\n",
    "                if step % printInterval == 0:\n",
    "                    ce = self.topNode.value\n",
    "                    tce = self.calcLoss(testData)\n",
    "                    print(\"Epoch: \",epoch, \", step: \",step, \", train loss: \", ce, \", test loss: \",tce)\n",
    "                    ceList.append(ce)\n",
    "                    tceList.append(tce)\n",
    "                idx = samples[i:min(i+batchSize, len(samples))]\n",
    "                batchData = [d[idx] for d in trainData ]\n",
    "                self.setData(batchData)\n",
    "                self.forward()\n",
    "                self.backwards()\n",
    "                self.updateModel(alpha)\n",
    "                step += 1\n",
    "\n",
    "                    \n",
    "                if step >= maxStep: return ceList, tceList\n",
    "            epoch += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = compNode(None,trainable=False)\n",
    "y = compNode(None,trainable=False)\n",
    "w = compNode(None)\n",
    "b = compNode(None)\n",
    "u = mulMatrixVectorNode([x,w])\n",
    "bS = copyNode([b])\n",
    "t = plusVectorsNode([u,bS])\n",
    "v = sigmoidNode([t])\n",
    "p = probNode([v,y])\n",
    "l = logNode([p])\n",
    "h = minusMeanNode([l])\n",
    "logistic = model(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 , step:  0 , train loss:  None , test loss:  1.4377669906382167\n",
      "Epoch:  10 , step:  10 , train loss:  0.562404039460713 , test loss:  0.560197990072616\n",
      "Epoch:  20 , step:  20 , train loss:  0.5474942031673484 , test loss:  0.5462940529534209\n",
      "Epoch:  30 , step:  30 , train loss:  0.5336081508173457 , test loss:  0.5333783543488309\n",
      "Epoch:  40 , step:  40 , train loss:  0.5206293378786021 , test loss:  0.5213016300644494\n",
      "Epoch:  50 , step:  50 , train loss:  0.5084908140080819 , test loss:  0.5100005703765378\n",
      "Epoch:  60 , step:  60 , train loss:  0.4971293173204371 , test loss:  0.49941599598254954\n",
      "Epoch:  70 , step:  70 , train loss:  0.4864854666795172 , test loss:  0.4894925463231892\n",
      "Epoch:  80 , step:  80 , train loss:  0.47650383445795386 , test loss:  0.4801786973631408\n",
      "Epoch:  90 , step:  90 , train loss:  0.4671329246672317 , test loss:  0.47142670183791413\n",
      "Epoch:  100 , step:  100 , train loss:  0.4583250789345215 , test loss:  0.46319246935713937\n",
      "Epoch:  110 , step:  110 , train loss:  0.45003632996723436 , test loss:  0.4554354038426703\n",
      "Epoch:  120 , step:  120 , train loss:  0.44222621904225634 , test loss:  0.44811821284796216\n",
      "Epoch:  130 , step:  130 , train loss:  0.4348575909902714 , test loss:  0.44120670046474625\n",
      "Epoch:  140 , step:  140 , train loss:  0.42789637731578645 , test loss:  0.43466955294870097\n",
      "Epoch:  150 , step:  150 , train loss:  0.42131137560596865 , test loss:  0.42847812396385054\n",
      "Epoch:  160 , step:  160 , train loss:  0.4150740312750546 , test loss:  0.422606224476904\n",
      "Epoch:  170 , step:  170 , train loss:  0.4091582259612261 , test loss:  0.4170299208138056\n",
      "Epoch:  180 , step:  180 , train loss:  0.40354007550930115 , test loss:  0.41172734318777143\n",
      "Epoch:  190 , step:  190 , train loss:  0.39819773939246206 , test loss:  0.40667850607832917\n",
      "Epoch:  200 , step:  200 , train loss:  0.3931112426025991 , test loss:  0.4018651411397099\n",
      "Epoch:  210 , step:  210 , train loss:  0.38826231042594733 , test loss:  0.3972705428018944\n",
      "Epoch:  220 , step:  220 , train loss:  0.3836342160769361 , test loss:  0.3928794263607288\n",
      "Epoch:  230 , step:  230 , train loss:  0.37921164085240633 , test loss:  0.3886777981021784\n",
      "Epoch:  240 , step:  240 , train loss:  0.37498054626022925 , test loss:  0.384652836842866\n",
      "Epoch:  250 , step:  250 , train loss:  0.37092805744599044 , test loss:  0.3807927861724817\n",
      "Epoch:  260 , step:  260 , train loss:  0.3670423571688682 , test loss:  0.377086856635916\n",
      "Epoch:  270 , step:  270 , train loss:  0.3633125895473967 , test loss:  0.3735251370803325\n",
      "Epoch:  280 , step:  280 , train loss:  0.35972877279529347 , test loss:  0.370098514404268\n",
      "Epoch:  290 , step:  290 , train loss:  0.35628172018762033 , test loss:  0.3667986009742177\n",
      "Epoch:  300 , step:  300 , train loss:  0.3529629685312223 , test loss:  0.36361766901303005\n",
      "Epoch:  310 , step:  310 , train loss:  0.3497647134554346 , test loss:  0.36054859130938044\n",
      "Epoch:  320 , step:  320 , train loss:  0.3466797508856635 , test loss:  0.35758478764543905\n",
      "Epoch:  330 , step:  330 , train loss:  0.34370142411089355 , test loss:  0.3547201763883408\n",
      "Epoch:  340 , step:  340 , train loss:  0.34082357590455487 , test loss:  0.3519491307386545\n",
      "Epoch:  350 , step:  350 , train loss:  0.33804050520519563 , test loss:  0.3492664391747134\n",
      "Epoch:  360 , step:  360 , train loss:  0.33534692790820886 , test loss:  0.3466672696747804\n",
      "Epoch:  370 , step:  370 , train loss:  0.3327379413619559 , test loss:  0.3441471373392095\n",
      "Epoch:  380 , step:  380 , train loss:  0.33020899220074085 , test loss:  0.3417018750718853\n",
      "Epoch:  390 , step:  390 , train loss:  0.3277558471831332 , test loss:  0.3393276070142536\n",
      "Epoch:  400 , step:  400 , train loss:  0.3253745667371149 , test loss:  0.33702072445626435\n",
      "Epoch:  410 , step:  410 , train loss:  0.32306148094356907 , test loss:  0.33477786397668796\n",
      "Epoch:  420 , step:  420 , train loss:  0.32081316771685225 , test loss:  0.3325958875906905\n",
      "Epoch:  430 , step:  430 , train loss:  0.31862643296580107 , test loss:  0.33047186470547174\n",
      "Epoch:  440 , step:  440 , train loss:  0.316498292540703 , test loss:  0.328403055705375\n",
      "Epoch:  450 , step:  450 , train loss:  0.3144259557917043 , test loss:  0.32638689700636425\n",
      "Epoch:  460 , step:  460 , train loss:  0.312406810582039 , test loss:  0.324420987436341\n",
      "Epoch:  470 , step:  470 , train loss:  0.310438409615523 , test loss:  0.32250307581260473\n",
      "Epoch:  480 , step:  480 , train loss:  0.3085184579521458 , test loss:  0.32063104960103317\n",
      "Epoch:  490 , step:  490 , train loss:  0.3066448015984748 , test loss:  0.3188029245534224\n",
      "Epoch:  500 , step:  500 , train loss:  0.30481541707111376 , test loss:  0.3170168352300305\n",
      "Epoch:  510 , step:  510 , train loss:  0.30302840184177154 , test loss:  0.3152710263238462\n",
      "Epoch:  520 , step:  520 , train loss:  0.3012819655817211 , test loss:  0.3135638447115654\n",
      "Epoch:  530 , step:  530 , train loss:  0.2995744221316824 , test loss:  0.3118937321638332\n",
      "Epoch:  540 , step:  540 , train loss:  0.29790418213054365 , test loss:  0.31025921865406453\n",
      "Epoch:  550 , step:  550 , train loss:  0.29626974624294566 , test loss:  0.30865891621121094\n",
      "Epoch:  560 , step:  560 , train loss:  0.29466969893166367 , test loss:  0.30709151326724526\n",
      "Epoch:  570 , step:  570 , train loss:  0.29310270272602224 , test loss:  0.3055557694549808\n",
      "Epoch:  580 , step:  580 , train loss:  0.2915674929423192 , test loss:  0.3040505108161728\n",
      "Epoch:  590 , step:  590 , train loss:  0.2900628728164922 , test loss:  0.30257462538373625\n",
      "Epoch:  600 , step:  600 , train loss:  0.28858770901307235 , test loss:  0.3011270591053895\n",
      "Epoch:  610 , step:  610 , train loss:  0.2871409274778972 , test loss:  0.2997068120791617\n",
      "Epoch:  620 , step:  620 , train loss:  0.285721509605125 , test loss:  0.29831293507399725\n",
      "Epoch:  630 , step:  630 , train loss:  0.2843284886918597 , test loss:  0.2969445263112134\n",
      "Epoch:  640 , step:  640 , train loss:  0.282960946656175 , test loss:  0.29560072848482394\n",
      "Epoch:  650 , step:  650 , train loss:  0.28161801099656686 , test loss:  0.294280726000782\n",
      "Epoch:  660 , step:  660 , train loss:  0.2802988519728713 , test loss:  0.2929837424170196\n",
      "Epoch:  670 , step:  670 , train loss:  0.27900267999050243 , test loss:  0.29170903806782134\n",
      "Epoch:  680 , step:  680 , train loss:  0.2777287431714985 , test loss:  0.2904559078575461\n",
      "Epoch:  690 , step:  690 , train loss:  0.27647632509734277 , test loss:  0.2892236792100609\n",
      "Epoch:  700 , step:  700 , train loss:  0.2752447427098573 , test loss:  0.2880117101614603\n",
      "Epoch:  710 , step:  710 , train loss:  0.2740333443576743 , test loss:  0.2868193875847388\n",
      "Epoch:  720 , step:  720 , train loss:  0.27284150797687867 , test loss:  0.2856461255360728\n",
      "Epoch:  730 , step:  730 , train loss:  0.2716686393954054 , test loss:  0.28449136371327205\n",
      "Epoch:  740 , step:  740 , train loss:  0.27051417075166506 , test loss:  0.2833545660177608\n",
      "Epoch:  750 , step:  750 , train loss:  0.26937755901868576 , test loss:  0.28223521921219624\n",
      "Epoch:  760 , step:  760 , train loss:  0.2682582846257926 , test loss:  0.2811328316664911\n",
      "Epoch:  770 , step:  770 , train loss:  0.2671558501705162 , test loss:  0.2800469321856184\n",
      "Epoch:  780 , step:  780 , train loss:  0.26606977921402597 , test loss:  0.27897706891312407\n",
      "Epoch:  790 , step:  790 , train loss:  0.2649996151539387 , test loss:  0.27792280830477595\n",
      "Epoch:  800 , step:  800 , train loss:  0.2639449201688541 , test loss:  0.27688373416723144\n",
      "Epoch:  810 , step:  810 , train loss:  0.262905274229427 , test loss:  0.27585944675702345\n",
      "Epoch:  820 , step:  820 , train loss:  0.2618802741712022 , test loss:  0.2748495619355395\n",
      "Epoch:  830 , step:  830 , train loss:  0.260869532824821 , test loss:  0.27385371037601697\n",
      "Epoch:  840 , step:  840 , train loss:  0.25987267819955207 , test loss:  0.27287153681888904\n",
      "Epoch:  850 , step:  850 , train loss:  0.2588893527164197 , test loss:  0.27190269937210637\n",
      "Epoch:  860 , step:  860 , train loss:  0.25791921248749183 , test loss:  0.27094686885332125\n",
      "Epoch:  870 , step:  870 , train loss:  0.25696192663815437 , test loss:  0.2700037281710592\n",
      "Epoch:  880 , step:  880 , train loss:  0.25601717666944285 , test loss:  0.26907297174222805\n",
      "Epoch:  890 , step:  890 , train loss:  0.25508465585772383 , test loss:  0.2681543049435102\n",
      "Epoch:  900 , step:  900 , train loss:  0.2541640686892234 , test loss:  0.2672474435943742\n",
      "Epoch:  910 , step:  910 , train loss:  0.2532551303270863 , test loss:  0.26635211346960586\n",
      "Epoch:  920 , step:  920 , train loss:  0.25235756610882176 , test loss:  0.2654680498394201\n",
      "Epoch:  930 , step:  930 , train loss:  0.25147111107215026 , test loss:  0.26459499703535216\n",
      "Epoch:  940 , step:  940 , train loss:  0.2505955095074085 , test loss:  0.26373270804026344\n",
      "Epoch:  950 , step:  950 , train loss:  0.24973051453480585 , test loss:  0.2628809441009128\n",
      "Epoch:  960 , step:  960 , train loss:  0.24887588770494729 , test loss:  0.26203947436166103\n",
      "Epoch:  970 , step:  970 , train loss:  0.24803139862114973 , test loss:  0.26120807551797215\n",
      "Epoch:  980 , step:  980 , train loss:  0.24719682458218473 , test loss:  0.2603865314884763\n",
      "Epoch:  990 , step:  990 , train loss:  0.2463719502441759 , test loss:  0.25957463310443907\n",
      "Epoch:  1000 , step:  1000 , train loss:  0.245556567300468 , test loss:  0.25877217781556927\n",
      "Epoch:  1010 , step:  1010 , train loss:  0.24475047417836585 , test loss:  0.2579789694111652\n",
      "Epoch:  1020 , step:  1020 , train loss:  0.24395347575171863 , test loss:  0.25719481775567193\n",
      "Epoch:  1030 , step:  1030 , train loss:  0.24316538306839322 , test loss:  0.2564195385377835\n",
      "Epoch:  1040 , step:  1040 , train loss:  0.24238601309174562 , test loss:  0.2556529530322832\n",
      "Epoch:  1050 , step:  1050 , train loss:  0.24161518845525987 , test loss:  0.2548948878738692\n",
      "Epoch:  1060 , step:  1060 , train loss:  0.240852737229578 , test loss:  0.2541451748422618\n",
      "Epoch:  1070 , step:  1070 , train loss:  0.2400984927011978 , test loss:  0.25340365065793685\n",
      "Epoch:  1080 , step:  1080 , train loss:  0.23935229316215967 , test loss:  0.2526701567878724\n",
      "Epoch:  1090 , step:  1090 , train loss:  0.23861398171009227 , test loss:  0.25194453926073335\n",
      "Epoch:  1100 , step:  1100 , train loss:  0.2378834060580234 , test loss:  0.25122664849096066\n",
      "Epoch:  1110 , step:  1110 , train loss:  0.23716041835340335 , test loss:  0.25051633911126053\n",
      "Epoch:  1120 , step:  1120 , train loss:  0.23644487500582223 , test loss:  0.24981346981302527\n",
      "Epoch:  1130 , step:  1130 , train loss:  0.23573663652293564 , test loss:  0.2491179031942456\n",
      "Epoch:  1140 , step:  1140 , train loss:  0.23503556735414402 , test loss:  0.24842950561450064\n",
      "Epoch:  1150 , step:  1150 , train loss:  0.23434153574159888 , test loss:  0.24774814705664092\n",
      "Epoch:  1160 , step:  1160 , train loss:  0.23365441357813607 , test loss:  0.24707370099479894\n",
      "Epoch:  1170 , step:  1170 , train loss:  0.23297407627176048 , test loss:  0.2464060442683891\n",
      "Epoch:  1180 , step:  1180 , train loss:  0.23230040261632984 , test loss:  0.2457450569617755\n",
      "Epoch:  1190 , step:  1190 , train loss:  0.2316332746681067 , test loss:  0.24509062228930817\n",
      "Epoch:  1200 , step:  1200 , train loss:  0.2309725776278671 , test loss:  0.24444262648544482\n",
      "Epoch:  1210 , step:  1210 , train loss:  0.2303181997282741 , test loss:  0.24380095869969326\n",
      "Epoch:  1220 , step:  1220 , train loss:  0.22967003212624038 , test loss:  0.24316551089612384\n",
      "Epoch:  1230 , step:  1230 , train loss:  0.22902796880002194 , test loss:  0.24253617775721803\n",
      "Epoch:  1240 , step:  1240 , train loss:  0.2283919064507987 , test loss:  0.24191285659183057\n",
      "Epoch:  1250 , step:  1250 , train loss:  0.22776174440851277 , test loss:  0.24129544724705837\n",
      "Epoch:  1260 , step:  1260 , train loss:  0.22713738454174853 , test loss:  0.24068385202381784\n",
      "Epoch:  1270 , step:  1270 , train loss:  0.22651873117145044 , test loss:  0.2400779755959476\n",
      "Epoch:  1280 , step:  1280 , train loss:  0.22590569098828683 , test loss:  0.2394777249326602\n",
      "Epoch:  1290 , step:  1290 , train loss:  0.2252981729734785 , test loss:  0.23888300922417915\n",
      "Epoch:  1300 , step:  1300 , train loss:  0.2246960883229211 , test loss:  0.23829373981040544\n",
      "Epoch:  1310 , step:  1310 , train loss:  0.2240993503744397 , test loss:  0.23770983011246694\n",
      "Epoch:  1320 , step:  1320 , train loss:  0.22350787453802418 , test loss:  0.23713119556701165\n",
      "Epoch:  1330 , step:  1330 , train loss:  0.2229215782288998 , test loss:  0.2365577535631141\n",
      "Epoch:  1340 , step:  1340 , train loss:  0.22234038080329876 , test loss:  0.2359894233816707\n",
      "Epoch:  1350 , step:  1350 , train loss:  0.22176420349680262 , test loss:  0.23542612613716754\n",
      "Epoch:  1360 , step:  1360 , train loss:  0.2211929693651349 , test loss:  0.23486778472170866\n",
      "Epoch:  1370 , step:  1370 , train loss:  0.22062660322728833 , test loss:  0.23431432375120143\n",
      "Epoch:  1380 , step:  1380 , train loss:  0.22006503161087776 , test loss:  0.2337656695135975\n",
      "Epoch:  1390 , step:  1390 , train loss:  0.2195081826996159 , test loss:  0.23322174991909836\n",
      "Epoch:  1400 , step:  1400 , train loss:  0.21895598628281396 , test loss:  0.232682494452233\n",
      "Epoch:  1410 , step:  1410 , train loss:  0.2184083737068149 , test loss:  0.232147834125726\n",
      "Epoch:  1420 , step:  1420 , train loss:  0.21786527782827078 , test loss:  0.23161770143607419\n",
      "Epoch:  1430 , step:  1430 , train loss:  0.21732663296918242 , test loss:  0.23109203032075656\n",
      "Epoch:  1440 , step:  1440 , train loss:  0.21679237487362096 , test loss:  0.23057075611700542\n",
      "Epoch:  1450 , step:  1450 , train loss:  0.21626244066605751 , test loss:  0.23005381552207022\n",
      "Epoch:  1460 , step:  1460 , train loss:  0.2157367688112295 , test loss:  0.22954114655490912\n",
      "Epoch:  1470 , step:  1470 , train loss:  0.215215299075476 , test loss:  0.22903268851924644\n",
      "Epoch:  1480 , step:  1480 , train loss:  0.21469797248947864 , test loss:  0.22852838196793765\n",
      "Epoch:  1490 , step:  1490 , train loss:  0.21418473131234633 , test loss:  0.2280281686685863\n",
      "Epoch:  1500 , step:  1500 , train loss:  0.21367551899698695 , test loss:  0.22753199157035908\n",
      "Epoch:  1510 , step:  1510 , train loss:  0.2131702801567101 , test loss:  0.22703979477194994\n",
      "Epoch:  1520 , step:  1520 , train loss:  0.21266896053300938 , test loss:  0.22655152349064397\n",
      "Epoch:  1530 , step:  1530 , train loss:  0.2121715069644738 , test loss:  0.22606712403243645\n",
      "Epoch:  1540 , step:  1540 , train loss:  0.21167786735678149 , test loss:  0.22558654376316306\n",
      "Epoch:  1550 , step:  1550 , train loss:  0.21118799065373037 , test loss:  0.22510973108060078\n",
      "Epoch:  1560 , step:  1560 , train loss:  0.21070182680926314 , test loss:  0.22463663538749806\n",
      "Epoch:  1570 , step:  1570 , train loss:  0.21021932676044536 , test loss:  0.22416720706549964\n",
      "Epoch:  1580 , step:  1580 , train loss:  0.20974044240135795 , test loss:  0.22370139744992737\n",
      "Epoch:  1590 , step:  1590 , train loss:  0.20926512655786708 , test loss:  0.2232391588053852\n",
      "Epoch:  1600 , step:  1600 , train loss:  0.20879333296323585 , test loss:  0.22278044430215407\n",
      "Epoch:  1610 , step:  1610 , train loss:  0.2083250162345443 , test loss:  0.2223252079933472\n",
      "Epoch:  1620 , step:  1620 , train loss:  0.20786013184988528 , test loss:  0.22187340479279466\n",
      "Epoch:  1630 , step:  1630 , train loss:  0.20739863612630577 , test loss:  0.2214249904536299\n",
      "Epoch:  1640 , step:  1640 , train loss:  0.20694048619846436 , test loss:  0.22097992154755125\n",
      "Epoch:  1650 , step:  1650 , train loss:  0.20648563999797684 , test loss:  0.22053815544473204\n",
      "Epoch:  1660 , step:  1660 , train loss:  0.2060340562334227 , test loss:  0.22009965029435533\n",
      "Epoch:  1670 , step:  1670 , train loss:  0.2055856943709885 , test loss:  0.2196643650057487\n",
      "Epoch:  1680 , step:  1680 , train loss:  0.20514051461572197 , test loss:  0.21923225923009823\n",
      "Epoch:  1690 , step:  1690 , train loss:  0.2046984778933747 , test loss:  0.21880329334271836\n",
      "Epoch:  1700 , step:  1700 , train loss:  0.2042595458328108 , test loss:  0.2183774284258583\n",
      "Epoch:  1710 , step:  1710 , train loss:  0.20382368074896068 , test loss:  0.21795462625202475\n",
      "Epoch:  1720 , step:  1720 , train loss:  0.20339084562629842 , test loss:  0.2175348492678019\n",
      "Epoch:  1730 , step:  1730 , train loss:  0.2029610041028248 , test loss:  0.21711806057815147\n",
      "Epoch:  1740 , step:  1740 , train loss:  0.20253412045453628 , test loss:  0.21670422393117456\n",
      "Epoch:  1750 , step:  1750 , train loss:  0.20211015958036244 , test loss:  0.2162933037033191\n",
      "Epoch:  1760 , step:  1760 , train loss:  0.20168908698755503 , test loss:  0.21588526488501755\n",
      "Epoch:  1770 , step:  1770 , train loss:  0.2012708687775115 , test loss:  0.21548007306673878\n",
      "Epoch:  1780 , step:  1780 , train loss:  0.20085547163201872 , test loss:  0.21507769442544034\n",
      "Epoch:  1790 , step:  1790 , train loss:  0.20044286279990017 , test loss:  0.21467809571140636\n",
      "Epoch:  1800 , step:  1800 , train loss:  0.20003301008405378 , test loss:  0.21428124423545872\n",
      "Epoch:  1810 , step:  1810 , train loss:  0.19962588182886548 , test loss:  0.21388710785652765\n",
      "Epoch:  1820 , step:  1820 , train loss:  0.1992214469079861 , test loss:  0.21349565496956976\n",
      "Epoch:  1830 , step:  1830 , train loss:  0.19881967471245818 , test loss:  0.21310685449382247\n",
      "Epoch:  1840 , step:  1840 , train loss:  0.19842053513918115 , test loss:  0.21272067586138163\n",
      "Epoch:  1850 , step:  1850 , train loss:  0.19802399857970288 , test loss:  0.21233708900609355\n",
      "Epoch:  1860 , step:  1860 , train loss:  0.1976300359093265 , test loss:  0.21195606435274975\n",
      "Epoch:  1870 , step:  1870 , train loss:  0.19723861847652163 , test loss:  0.21157757280657385\n",
      "Epoch:  1880 , step:  1880 , train loss:  0.1968497180926297 , test loss:  0.211201585742993\n",
      "Epoch:  1890 , step:  1890 , train loss:  0.1964633070218538 , test loss:  0.2108280749976821\n",
      "Epoch:  1900 , step:  1900 , train loss:  0.19607935797152315 , test loss:  0.2104570128568734\n",
      "Epoch:  1910 , step:  1910 , train loss:  0.19569784408262275 , test loss:  0.21008837204792288\n",
      "Epoch:  1920 , step:  1920 , train loss:  0.19531873892058055 , test loss:  0.20972212573012392\n",
      "Epoch:  1930 , step:  1930 , train loss:  0.1949420164663021 , test loss:  0.20935824748576215\n",
      "Epoch:  1940 , step:  1940 , train loss:  0.19456765110744617 , test loss:  0.20899671131140188\n",
      "Epoch:  1950 , step:  1950 , train loss:  0.1941956176299321 , test loss:  0.2086374916093987\n",
      "Epoch:  1960 , step:  1960 , train loss:  0.19382589120967236 , test loss:  0.20828056317962929\n",
      "Epoch:  1970 , step:  1970 , train loss:  0.19345844740452273 , test loss:  0.2079259012114329\n",
      "Epoch:  1980 , step:  1980 , train loss:  0.19309326214644276 , test loss:  0.2075734812757579\n",
      "Epoch:  1990 , step:  1990 , train loss:  0.19273031173386074 , test loss:  0.20722327931750578\n",
      "Epoch:  2000 , step:  2000 , train loss:  0.19236957282423572 , test loss:  0.2068752716480684\n",
      "Epoch:  2010 , step:  2010 , train loss:  0.19201102242681095 , test loss:  0.20652943493805123\n",
      "Epoch:  2020 , step:  2020 , train loss:  0.19165463789555268 , test loss:  0.20618574621017696\n",
      "Epoch:  2030 , step:  2030 , train loss:  0.19130039692226838 , test loss:  0.20584418283236516\n",
      "Epoch:  2040 , step:  2040 , train loss:  0.19094827752989915 , test loss:  0.2055047225109813\n",
      "Epoch:  2050 , step:  2050 , train loss:  0.19059825806598044 , test loss:  0.2051673432842513\n",
      "Epoch:  2060 , step:  2060 , train loss:  0.19025031719626656 , test loss:  0.2048320235158353\n",
      "Epoch:  2070 , step:  2070 , train loss:  0.18990443389851355 , test loss:  0.2044987418885576\n",
      "Epoch:  2080 , step:  2080 , train loss:  0.18956058745641563 , test loss:  0.20416747739828706\n",
      "Epoch:  2090 , step:  2090 , train loss:  0.1892187574536909 , test loss:  0.2038382093479632\n",
      "Epoch:  2100 , step:  2100 , train loss:  0.1888789237683119 , test loss:  0.20351091734176577\n",
      "Epoch:  2110 , step:  2110 , train loss:  0.18854106656687572 , test loss:  0.20318558127942077\n",
      "Epoch:  2120 , step:  2120 , train loss:  0.18820516629911113 , test loss:  0.20286218135064166\n",
      "Epoch:  2130 , step:  2130 , train loss:  0.18787120369251736 , test loss:  0.2025406980297002\n",
      "Epoch:  2140 , step:  2140 , train loss:  0.18753915974713128 , test loss:  0.202221112070124\n",
      "Epoch:  2150 , step:  2150 , train loss:  0.1872090157304191 , test loss:  0.2019034044995169\n",
      "Epoch:  2160 , step:  2160 , train loss:  0.18688075317228917 , test loss:  0.2015875566144986\n",
      "Epoch:  2170 , step:  2170 , train loss:  0.18655435386022207 , test loss:  0.20127354997576113\n",
      "Epoch:  2180 , step:  2180 , train loss:  0.18622979983451465 , test loss:  0.20096136640323736\n",
      "Epoch:  2190 , step:  2190 , train loss:  0.1859070733836355 , test loss:  0.20065098797137965\n",
      "Epoch:  2200 , step:  2200 , train loss:  0.18558615703968714 , test loss:  0.2003423970045454\n",
      "Epoch:  2210 , step:  2210 , train loss:  0.18526703357397387 , test loss:  0.20003557607248645\n",
      "Epoch:  2220 , step:  2220 , train loss:  0.1849496859926706 , test loss:  0.1997305079859388\n",
      "Epoch:  2230 , step:  2230 , train loss:  0.18463409753259102 , test loss:  0.19942717579231145\n",
      "Epoch:  2240 , step:  2240 , train loss:  0.1843202516570516 , test loss:  0.1991255627714707\n",
      "Epoch:  2250 , step:  2250 , train loss:  0.1840081320518291 , test loss:  0.1988256524316166\n",
      "Epoch:  2260 , step:  2260 , train loss:  0.18369772262120912 , test loss:  0.19852742850525154\n",
      "Epoch:  2270 , step:  2270 , train loss:  0.18338900748412304 , test loss:  0.19823087494523522\n",
      "Epoch:  2280 , step:  2280 , train loss:  0.18308197097037082 , test loss:  0.19793597592092718\n",
      "Epoch:  2290 , step:  2290 , train loss:  0.1827765976169276 , test loss:  0.1976427158144114\n",
      "Epoch:  2300 , step:  2300 , train loss:  0.18247287216433158 , test loss:  0.1973510792168033\n",
      "Epoch:  2310 , step:  2310 , train loss:  0.18217077955315142 , test loss:  0.1970610509246355\n",
      "Epoch:  2320 , step:  2320 , train loss:  0.1818703049205305 , test loss:  0.19677261593632128\n",
      "Epoch:  2330 , step:  2330 , train loss:  0.1815714335968063 , test loss:  0.19648575944869234\n",
      "Epoch:  2340 , step:  2340 , train loss:  0.181274151102203 , test loss:  0.19620046685361103\n",
      "Epoch:  2350 , step:  2350 , train loss:  0.18097844314359496 , test loss:  0.19591672373465308\n",
      "Epoch:  2360 , step:  2360 , train loss:  0.18068429561134017 , test loss:  0.19563451586386035\n",
      "Epoch:  2370 , step:  2370 , train loss:  0.1803916945761802 , test loss:  0.19535382919856148\n",
      "Epoch:  2380 , step:  2380 , train loss:  0.18010062628620688 , test loss:  0.19507464987825843\n",
      "Epoch:  2390 , step:  2390 , train loss:  0.17981107716389236 , test loss:  0.1947969642215776\n",
      "Epoch:  2400 , step:  2400 , train loss:  0.17952303380318205 , test loss:  0.19452075872328403\n",
      "Epoch:  2410 , step:  2410 , train loss:  0.17923648296664796 , test loss:  0.19424602005135655\n",
      "Epoch:  2420 , step:  2420 , train loss:  0.1789514115827018 , test loss:  0.19397273504412307\n",
      "Epoch:  2430 , step:  2430 , train loss:  0.17866780674286536 , test loss:  0.19370089070745403\n",
      "Epoch:  2440 , step:  2440 , train loss:  0.17838565569909762 , test loss:  0.19343047421201262\n",
      "Epoch:  2450 , step:  2450 , train loss:  0.17810494586117656 , test loss:  0.19316147289056096\n",
      "Epoch:  2460 , step:  2460 , train loss:  0.17782566479413456 , test loss:  0.19289387423531984\n",
      "Epoch:  2470 , step:  2470 , train loss:  0.17754780021574598 , test loss:  0.19262766589538202\n",
      "Epoch:  2480 , step:  2480 , train loss:  0.17727133999406572 , test loss:  0.19236283567417647\n",
      "Epoch:  2490 , step:  2490 , train loss:  0.1769962721450176 , test loss:  0.19209937152698325\n",
      "Epoch:  2500 , step:  2500 , train loss:  0.17672258483003067 , test loss:  0.19183726155849773\n",
      "Epoch:  2510 , step:  2510 , train loss:  0.17645026635372352 , test loss:  0.19157649402044247\n",
      "Epoch:  2520 , step:  2520 , train loss:  0.1761793051616342 , test loss:  0.19131705730922624\n",
      "Epoch:  2530 , step:  2530 , train loss:  0.17590968983799532 , test loss:  0.19105893996364842\n",
      "Epoch:  2540 , step:  2540 , train loss:  0.175641409103553 , test loss:  0.1908021306626486\n",
      "Epoch:  2550 , step:  2550 , train loss:  0.17537445181342895 , test loss:  0.19054661822309948\n",
      "Epoch:  2560 , step:  2560 , train loss:  0.17510880695502404 , test loss:  0.19029239159764225\n",
      "Epoch:  2570 , step:  2570 , train loss:  0.17484446364596307 , test loss:  0.1900394398725642\n",
      "Epoch:  2580 , step:  2580 , train loss:  0.17458141113207917 , test loss:  0.18978775226571631\n",
      "Epoch:  2590 , step:  2590 , train loss:  0.17431963878543752 , test loss:  0.18953731812447125\n",
      "Epoch:  2600 , step:  2600 , train loss:  0.17405913610239657 , test loss:  0.18928812692371952\n",
      "Epoch:  2610 , step:  2610 , train loss:  0.17379989270170704 , test loss:  0.18904016826390443\n",
      "Epoch:  2620 , step:  2620 , train loss:  0.1735418983226467 , test loss:  0.18879343186909325\n",
      "Epoch:  2630 , step:  2630 , train loss:  0.17328514282319102 , test loss:  0.18854790758508533\n",
      "Epoch:  2640 , step:  2640 , train loss:  0.1730296161782181 , test loss:  0.1883035853775554\n",
      "Epoch:  2650 , step:  2650 , train loss:  0.17277530847774789 , test loss:  0.18806045533023136\n",
      "Epoch:  2660 , step:  2660 , train loss:  0.17252220992521403 , test loss:  0.1878185076431065\n",
      "Epoch:  2670 , step:  2670 , train loss:  0.17227031083576824 , test loss:  0.18757773263068403\n",
      "Epoch:  2680 , step:  2680 , train loss:  0.17201960163461633 , test loss:  0.1873381207202548\n",
      "Epoch:  2690 , step:  2690 , train loss:  0.17177007285538518 , test loss:  0.18709966245020648\n",
      "Epoch:  2700 , step:  2700 , train loss:  0.1715217151385195 , test loss:  0.18686234846836328\n",
      "Epoch:  2710 , step:  2710 , train loss:  0.17127451922970877 , test loss:  0.18662616953035677\n",
      "Epoch:  2720 , step:  2720 , train loss:  0.17102847597834236 , test loss:  0.18639111649802553\n",
      "Epoch:  2730 , step:  2730 , train loss:  0.17078357633599334 , test loss:  0.1861571803378446\n",
      "Epoch:  2740 , step:  2740 , train loss:  0.17053981135492965 , test loss:  0.18592435211938269\n",
      "Epoch:  2750 , step:  2750 , train loss:  0.1702971721866524 , test loss:  0.1856926230137876\n",
      "Epoch:  2760 , step:  2760 , train loss:  0.17005565008046056 , test loss:  0.18546198429229863\n",
      "Epoch:  2770 , step:  2770 , train loss:  0.1698152363820413 , test loss:  0.18523242732478543\n",
      "Epoch:  2780 , step:  2780 , train loss:  0.16957592253208575 , test loss:  0.18500394357831337\n",
      "Epoch:  2790 , step:  2790 , train loss:  0.16933770006492962 , test loss:  0.18477652461573382\n",
      "Epoch:  2800 , step:  2800 , train loss:  0.16910056060721765 , test loss:  0.18455016209429986\n",
      "Epoch:  2810 , step:  2810 , train loss:  0.16886449587659194 , test loss:  0.18432484776430624\n",
      "Epoch:  2820 , step:  2820 , train loss:  0.16862949768040353 , test loss:  0.18410057346775288\n",
      "Epoch:  2830 , step:  2830 , train loss:  0.1683955579144465 , test loss:  0.18387733113703258\n",
      "Epoch:  2840 , step:  2840 , train loss:  0.16816266856171413 , test loss:  0.18365511279364094\n",
      "Epoch:  2850 , step:  2850 , train loss:  0.16793082169117723 , test loss:  0.18343391054690905\n",
      "Epoch:  2860 , step:  2860 , train loss:  0.16770000945658325 , test loss:  0.18321371659275817\n",
      "Epoch:  2870 , step:  2870 , train loss:  0.16747022409527637 , test loss:  0.1829945232124757\n",
      "Epoch:  2880 , step:  2880 , train loss:  0.16724145792703823 , test loss:  0.18277632277151265\n",
      "Epoch:  2890 , step:  2890 , train loss:  0.1670137033529481 , test loss:  0.1825591077183016\n",
      "Epoch:  2900 , step:  2900 , train loss:  0.16678695285426312 , test loss:  0.18234287058309434\n",
      "Epoch:  2910 , step:  2910 , train loss:  0.16656119899131733 , test loss:  0.18212760397682065\n",
      "Epoch:  2920 , step:  2920 , train loss:  0.1663364344024396 , test loss:  0.18191330058996494\n",
      "Epoch:  2930 , step:  2930 , train loss:  0.16611265180288987 , test loss:  0.1816999531914633\n",
      "Epoch:  2940 , step:  2940 , train loss:  0.16588984398381337 , test loss:  0.18148755462761815\n",
      "Epoch:  2950 , step:  2950 , train loss:  0.16566800381121255 , test loss:  0.18127609782103185\n",
      "Epoch:  2960 , step:  2960 , train loss:  0.16544712422493615 , test loss:  0.18106557576955823\n",
      "Epoch:  2970 , step:  2970 , train loss:  0.16522719823768528 , test loss:  0.18085598154527074\n",
      "Epoch:  2980 , step:  2980 , train loss:  0.16500821893403606 , test loss:  0.18064730829344947\n",
      "Epoch:  2990 , step:  2990 , train loss:  0.1647901794694784 , test loss:  0.18043954923158337\n",
      "Epoch:  3000 , step:  3000 , train loss:  0.16457307306947094 , test loss:  0.1802326976483899\n",
      "Epoch:  3010 , step:  3010 , train loss:  0.16435689302851123 , test loss:  0.18002674690285103\n",
      "Epoch:  3020 , step:  3020 , train loss:  0.16414163270922152 , test loss:  0.1798216904232647\n",
      "Epoch:  3030 , step:  3030 , train loss:  0.16392728554144959 , test loss:  0.17961752170631162\n",
      "Epoch:  3040 , step:  3040 , train loss:  0.16371384502138378 , test loss:  0.17941423431613854\n",
      "Epoch:  3050 , step:  3050 , train loss:  0.163501304710683 , test loss:  0.17921182188345494\n",
      "Epoch:  3060 , step:  3060 , train loss:  0.1632896582356206 , test loss:  0.17901027810464581\n",
      "Epoch:  3070 , step:  3070 , train loss:  0.16307889928624206 , test loss:  0.17880959674089783\n",
      "Epoch:  3080 , step:  3080 , train loss:  0.16286902161553643 , test loss:  0.17860977161734037\n",
      "Epoch:  3090 , step:  3090 , train loss:  0.16266001903862085 , test loss:  0.17841079662219989\n",
      "Epoch:  3100 , step:  3100 , train loss:  0.16245188543193817 , test loss:  0.1782126657059685\n",
      "Epoch:  3110 , step:  3110 , train loss:  0.16224461473246793 , test loss:  0.17801537288058497\n",
      "Epoch:  3120 , step:  3120 , train loss:  0.16203820093694882 , test loss:  0.17781891221863003\n",
      "Epoch:  3130 , step:  3130 , train loss:  0.1618326381011147 , test loss:  0.17762327785253343\n",
      "Epoch:  3140 , step:  3140 , train loss:  0.1616279203389416 , test loss:  0.1774284639737942\n",
      "Epoch:  3150 , step:  3150 , train loss:  0.16142404182190745 , test loss:  0.17723446483221317\n",
      "Epoch:  3160 , step:  3160 , train loss:  0.1612209967782628 , test loss:  0.1770412747351376\n",
      "Epoch:  3170 , step:  3170 , train loss:  0.16101877949231355 , test loss:  0.17684888804671756\n",
      "Epoch:  3180 , step:  3180 , train loss:  0.1608173843037143 , test loss:  0.17665729918717452\n",
      "Epoch:  3190 , step:  3190 , train loss:  0.1606168056067732 , test loss:  0.1764665026320806\n",
      "Epoch:  3200 , step:  3200 , train loss:  0.16041703784976707 , test loss:  0.17627649291164993\n",
      "Epoch:  3210 , step:  3210 , train loss:  0.16021807553426778 , test loss:  0.17608726461004043\n",
      "Epoch:  3220 , step:  3220 , train loss:  0.16001991321447842 , test loss:  0.17589881236466684\n",
      "Epoch:  3230 , step:  3230 , train loss:  0.15982254549657993 , test loss:  0.175711130865524\n",
      "Epoch:  3240 , step:  3240 , train loss:  0.15962596703808765 , test loss:  0.17552421485452127\n",
      "Epoch:  3250 , step:  3250 , train loss:  0.15943017254721792 , test loss:  0.17533805912482628\n",
      "Epoch:  3260 , step:  3260 , train loss:  0.15923515678226405 , test loss:  0.17515265852021977\n",
      "Epoch:  3270 , step:  3270 , train loss:  0.15904091455098196 , test loss:  0.17496800793445935\n",
      "Epoch:  3280 , step:  3280 , train loss:  0.15884744070998502 , test loss:  0.1747841023106538\n",
      "Epoch:  3290 , step:  3290 , train loss:  0.15865473016414802 , test loss:  0.17460093664064638\n",
      "Epoch:  3300 , step:  3300 , train loss:  0.1584627778660202 , test loss:  0.1744185059644078\n",
      "Epoch:  3310 , step:  3310 , train loss:  0.158271578815247 , test loss:  0.1742368053694383\n",
      "Epoch:  3320 , step:  3320 , train loss:  0.15808112805800054 , test loss:  0.17405582999017874\n",
      "Epoch:  3330 , step:  3330 , train loss:  0.1578914206864185 , test loss:  0.17387557500743053\n",
      "Epoch:  3340 , step:  3340 , train loss:  0.15770245183805132 , test loss:  0.1736960356477845\n",
      "Epoch:  3350 , step:  3350 , train loss:  0.15751421669531784 , test loss:  0.17351720718305821\n",
      "Epoch:  3360 , step:  3360 , train loss:  0.15732671048496866 , test loss:  0.1733390849297415\n",
      "Epoch:  3370 , step:  3370 , train loss:  0.15713992847755767 , test loss:  0.17316166424845048\n",
      "Epoch:  3380 , step:  3380 , train loss:  0.15695386598692118 , test loss:  0.17298494054338995\n",
      "Epoch:  3390 , step:  3390 , train loss:  0.1567685183696647 , test loss:  0.17280890926182305\n",
      "Epoch:  3400 , step:  3400 , train loss:  0.15658388102465748 , test loss:  0.17263356589354953\n",
      "Epoch:  3410 , step:  3410 , train loss:  0.1563999493925338 , test loss:  0.1724589059703912\n",
      "Epoch:  3420 , step:  3420 , train loss:  0.15621671895520237 , test loss:  0.1722849250656853\n",
      "Epoch:  3430 , step:  3430 , train loss:  0.15603418523536197 , test loss:  0.17211161879378487\n",
      "Epoch:  3440 , step:  3440 , train loss:  0.15585234379602486 , test loss:  0.1719389828095672\n",
      "Epoch:  3450 , step:  3450 , train loss:  0.15567119024004633 , test loss:  0.1717670128079485\n",
      "Epoch:  3460 , step:  3460 , train loss:  0.15549072020966156 , test loss:  0.17159570452340644\n",
      "Epoch:  3470 , step:  3470 , train loss:  0.15531092938602892 , test loss:  0.17142505372950909\n",
      "Epoch:  3480 , step:  3480 , train loss:  0.15513181348877964 , test loss:  0.17125505623845103\n",
      "Epoch:  3490 , step:  3490 , train loss:  0.1549533682755743 , test loss:  0.17108570790059616\n",
      "Epoch:  3500 , step:  3500 , train loss:  0.1547755895416651 , test loss:  0.1709170046040266\n",
      "Epoch:  3510 , step:  3510 , train loss:  0.1545984731194646 , test loss:  0.17074894227409898\n",
      "Epoch:  3520 , step:  3520 , train loss:  0.15442201487812088 , test loss:  0.17058151687300613\n",
      "Epoch:  3530 , step:  3530 , train loss:  0.1542462107230979 , test loss:  0.1704147243993459\n",
      "Epoch:  3540 , step:  3540 , train loss:  0.1540710565957625 , test loss:  0.17024856088769527\n",
      "Epoch:  3550 , step:  3550 , train loss:  0.15389654847297685 , test loss:  0.17008302240819131\n",
      "Epoch:  3560 , step:  3560 , train loss:  0.1537226823666964 , test loss:  0.16991810506611774\n",
      "Epoch:  3570 , step:  3570 , train loss:  0.153549454323574 , test loss:  0.16975380500149728\n",
      "Epoch:  3580 , step:  3580 , train loss:  0.153376860424569 , test loss:  0.16959011838868976\n",
      "Epoch:  3590 , step:  3590 , train loss:  0.1532048967845619 , test loss:  0.16942704143599613\n",
      "Epoch:  3600 , step:  3600 , train loss:  0.15303355955197454 , test loss:  0.16926457038526802\n",
      "Epoch:  3610 , step:  3610 , train loss:  0.1528628449083951 , test loss:  0.1691027015115222\n",
      "Epoch:  3620 , step:  3620 , train loss:  0.15269274906820907 , test loss:  0.1689414311225614\n",
      "Epoch:  3630 , step:  3630 , train loss:  0.15252326827823381 , test loss:  0.16878075555859928\n",
      "Epoch:  3640 , step:  3640 , train loss:  0.15235439881736004 , test loss:  0.16862067119189217\n",
      "Epoch:  3650 , step:  3650 , train loss:  0.15218613699619632 , test loss:  0.16846117442637387\n",
      "Epoch:  3660 , step:  3660 , train loss:  0.15201847915671995 , test loss:  0.16830226169729748\n",
      "Epoch:  3670 , step:  3670 , train loss:  0.15185142167193139 , test loss:  0.16814392947088103\n",
      "Epoch:  3680 , step:  3680 , train loss:  0.15168496094551417 , test loss:  0.16798617424395826\n",
      "Epoch:  3690 , step:  3690 , train loss:  0.15151909341149886 , test loss:  0.16782899254363423\n",
      "Epoch:  3700 , step:  3700 , train loss:  0.15135381553393207 , test loss:  0.1676723809269458\n",
      "Epoch:  3710 , step:  3710 , train loss:  0.1511891238065493 , test loss:  0.16751633598052654\n",
      "Epoch:  3720 , step:  3720 , train loss:  0.15102501475245267 , test loss:  0.1673608543202762\n",
      "Epoch:  3730 , step:  3730 , train loss:  0.15086148492379284 , test loss:  0.1672059325910348\n",
      "Epoch:  3740 , step:  3740 , train loss:  0.15069853090145477 , test loss:  0.16705156746626154\n",
      "Epoch:  3750 , step:  3750 , train loss:  0.15053614929474837 , test loss:  0.1668977556477167\n",
      "Epoch:  3760 , step:  3760 , train loss:  0.1503743367411025 , test loss:  0.16674449386514983\n",
      "Epoch:  3770 , step:  3770 , train loss:  0.1502130899057638 , test loss:  0.16659177887599033\n",
      "Epoch:  3780 , step:  3780 , train loss:  0.15005240548149842 , test loss:  0.1664396074650435\n",
      "Epoch:  3790 , step:  3790 , train loss:  0.14989228018829906 , test loss:  0.16628797644418966\n",
      "Epoch:  3800 , step:  3800 , train loss:  0.14973271077309455 , test loss:  0.16613688265208812\n",
      "Epoch:  3810 , step:  3810 , train loss:  0.14957369400946402 , test loss:  0.16598632295388452\n",
      "Epoch:  3820 , step:  3820 , train loss:  0.14941522669735452 , test loss:  0.1658362942409226\n",
      "Epoch:  3830 , step:  3830 , train loss:  0.14925730566280238 , test loss:  0.16568679343045908\n",
      "Epoch:  3840 , step:  3840 , train loss:  0.1490999277576582 , test loss:  0.1655378174653832\n",
      "Epoch:  3850 , step:  3850 , train loss:  0.1489430898593153 , test loss:  0.16538936331393916\n",
      "Epoch:  3860 , step:  3860 , train loss:  0.14878678887044175 , test loss:  0.16524142796945288\n",
      "Epoch:  3870 , step:  3870 , train loss:  0.148631021718716 , test loss:  0.1650940084500618\n",
      "Epoch:  3880 , step:  3880 , train loss:  0.14847578535656572 , test loss:  0.16494710179844868\n",
      "Epoch:  3890 , step:  3890 , train loss:  0.14832107676090972 , test loss:  0.16480070508157865\n",
      "Epoch:  3900 , step:  3900 , train loss:  0.14816689293290408 , test loss:  0.16465481539043972\n",
      "Epoch:  3910 , step:  3910 , train loss:  0.14801323089769045 , test loss:  0.16450942983978675\n",
      "Epoch:  3920 , step:  3920 , train loss:  0.1478600877041482 , test loss:  0.16436454556788876\n",
      "Epoch:  3930 , step:  3930 , train loss:  0.14770746042464958 , test loss:  0.1642201597362795\n",
      "Epoch:  3940 , step:  3940 , train loss:  0.14755534615481797 , test loss:  0.16407626952951113\n",
      "Epoch:  3950 , step:  3950 , train loss:  0.147403742013289 , test loss:  0.16393287215491104\n",
      "Epoch:  3960 , step:  3960 , train loss:  0.14725264514147518 , test loss:  0.16378996484234246\n",
      "Epoch:  3970 , step:  3970 , train loss:  0.14710205270333276 , test loss:  0.1636475448439671\n",
      "Epoch:  3980 , step:  3980 , train loss:  0.1469519618851323 , test loss:  0.16350560943401168\n",
      "Epoch:  3990 , step:  3990 , train loss:  0.14680236989523143 , test loss:  0.16336415590853726\n",
      "Epoch:  4000 , step:  4000 , train loss:  0.146653273963851 , test loss:  0.16322318158521096\n",
      "Epoch:  4010 , step:  4010 , train loss:  0.1465046713428536 , test loss:  0.1630826838030817\n",
      "Epoch:  4020 , step:  4020 , train loss:  0.14635655930552505 , test loss:  0.16294265992235743\n",
      "Epoch:  4030 , step:  4030 , train loss:  0.1462089351463586 , test loss:  0.1628031073241865\n",
      "Epoch:  4040 , step:  4040 , train loss:  0.14606179618084172 , test loss:  0.16266402341044098\n",
      "Epoch:  4050 , step:  4050 , train loss:  0.14591513974524548 , test loss:  0.16252540560350276\n",
      "Epoch:  4060 , step:  4060 , train loss:  0.14576896319641686 , test loss:  0.16238725134605275\n",
      "Epoch:  4070 , step:  4070 , train loss:  0.1456232639115729 , test loss:  0.16224955810086258\n",
      "Epoch:  4080 , step:  4080 , train loss:  0.14547803928809827 , test loss:  0.1621123233505886\n",
      "Epoch:  4090 , step:  4090 , train loss:  0.1453332867433447 , test loss:  0.1619755445975688\n",
      "Epoch:  4100 , step:  4100 , train loss:  0.14518900371443288 , test loss:  0.16183921936362247\n",
      "Epoch:  4110 , step:  4110 , train loss:  0.1450451876580572 , test loss:  0.16170334518985152\n",
      "Epoch:  4120 , step:  4120 , train loss:  0.14490183605029225 , test loss:  0.16156791963644526\n",
      "Epoch:  4130 , step:  4130 , train loss:  0.14475894638640238 , test loss:  0.16143294028248695\n",
      "Epoch:  4140 , step:  4140 , train loss:  0.14461651618065274 , test loss:  0.1612984047257631\n",
      "Epoch:  4150 , step:  4150 , train loss:  0.14447454296612353 , test loss:  0.16116431058257488\n",
      "Epoch:  4160 , step:  4160 , train loss:  0.14433302429452535 , test loss:  0.16103065548755202\n",
      "Epoch:  4170 , step:  4170 , train loss:  0.1441919577360181 , test loss:  0.16089743709346896\n",
      "Epoch:  4180 , step:  4180 , train loss:  0.14405134087903085 , test loss:  0.16076465307106308\n",
      "Epoch:  4190 , step:  4190 , train loss:  0.14391117133008463 , test loss:  0.16063230110885562\n",
      "Epoch:  4200 , step:  4200 , train loss:  0.14377144671361688 , test loss:  0.1605003789129743\n",
      "Epoch:  4210 , step:  4210 , train loss:  0.14363216467180837 , test loss:  0.1603688842069783\n",
      "Epoch:  4220 , step:  4220 , train loss:  0.1434933228644118 , test loss:  0.1602378147316854\n",
      "Epoch:  4230 , step:  4230 , train loss:  0.14335491896858285 , test loss:  0.16010716824500124\n",
      "Epoch:  4240 , step:  4240 , train loss:  0.14321695067871262 , test loss:  0.1599769425217504\n",
      "Epoch:  4250 , step:  4250 , train loss:  0.1430794157062628 , test loss:  0.1598471353535102\n",
      "Epoch:  4260 , step:  4260 , train loss:  0.1429423117796022 , test loss:  0.15971774454844523\n",
      "Epoch:  4270 , step:  4270 , train loss:  0.14280563664384535 , test loss:  0.15958876793114563\n",
      "Epoch:  4280 , step:  4280 , train loss:  0.14266938806069324 , test loss:  0.15946020334246555\n",
      "Epoch:  4290 , step:  4290 , train loss:  0.14253356380827537 , test loss:  0.15933204863936473\n",
      "Epoch:  4300 , step:  4300 , train loss:  0.14239816168099437 , test loss:  0.1592043016947517\n",
      "Epoch:  4310 , step:  4310 , train loss:  0.1422631794893718 , test loss:  0.1590769603973285\n",
      "Epoch:  4320 , step:  4320 , train loss:  0.14212861505989607 , test loss:  0.15895002265143746\n",
      "Epoch:  4330 , step:  4330 , train loss:  0.14199446623487197 , test loss:  0.15882348637691038\n",
      "Epoch:  4340 , step:  4340 , train loss:  0.141860730872272 , test loss:  0.15869734950891815\n",
      "Epoch:  4350 , step:  4350 , train loss:  0.1417274068455896 , test loss:  0.15857160999782355\n",
      "Epoch:  4360 , step:  4360 , train loss:  0.1415944920436937 , test loss:  0.15844626580903512\n",
      "Epoch:  4370 , step:  4370 , train loss:  0.14146198437068513 , test loss:  0.15832131492286278\n",
      "Epoch:  4380 , step:  4380 , train loss:  0.14132988174575478 , test loss:  0.15819675533437505\n",
      "Epoch:  4390 , step:  4390 , train loss:  0.14119818210304313 , test loss:  0.15807258505325844\n",
      "Epoch:  4400 , step:  4400 , train loss:  0.14106688339150183 , test loss:  0.1579488021036778\n",
      "Epoch:  4410 , step:  4410 , train loss:  0.14093598357475606 , test loss:  0.1578254045241388\n",
      "Epoch:  4420 , step:  4420 , train loss:  0.1408054806309694 , test loss:  0.15770239036735217\n",
      "Epoch:  4430 , step:  4430 , train loss:  0.1406753725527096 , test loss:  0.15757975770009855\n",
      "Epoch:  4440 , step:  4440 , train loss:  0.140545657346816 , test loss:  0.15745750460309627\n",
      "Epoch:  4450 , step:  4450 , train loss:  0.14041633303426856 , test loss:  0.15733562917086952\n",
      "Epoch:  4460 , step:  4460 , train loss:  0.14028739765005832 , test loss:  0.15721412951161864\n",
      "Epoch:  4470 , step:  4470 , train loss:  0.14015884924305927 , test loss:  0.157093003747092\n",
      "Epoch:  4480 , step:  4480 , train loss:  0.14003068587590162 , test loss:  0.15697225001245865\n",
      "Epoch:  4490 , step:  4490 , train loss:  0.13990290562484664 , test loss:  0.1568518664561834\n",
      "Epoch:  4500 , step:  4500 , train loss:  0.13977550657966278 , test loss:  0.15673185123990235\n",
      "Epoch:  4510 , step:  4510 , train loss:  0.1396484868435032 , test loss:  0.1566122025383009\n",
      "Epoch:  4520 , step:  4520 , train loss:  0.13952184453278463 , test loss:  0.15649291853899197\n",
      "Epoch:  4530 , step:  4530 , train loss:  0.13939557777706768 , test loss:  0.1563739974423969\n",
      "Epoch:  4540 , step:  4540 , train loss:  0.13926968471893825 , test loss:  0.1562554374616264\n",
      "Epoch:  4550 , step:  4550 , train loss:  0.13914416351389064 , test loss:  0.15613723682236397\n",
      "Epoch:  4560 , step:  4560 , train loss:  0.1390190123302115 , test loss:  0.15601939376274987\n",
      "Epoch:  4570 , step:  4570 , train loss:  0.1388942293488654 , test loss:  0.15590190653326685\n",
      "Epoch:  4580 , step:  4580 , train loss:  0.13876981276338138 , test loss:  0.1557847733966269\n",
      "Epoch:  4590 , step:  4590 , train loss:  0.1386457607797411 , test loss:  0.1556679926276596\n",
      "Epoch:  4600 , step:  4600 , train loss:  0.13852207161626787 , test loss:  0.15555156251320124\n",
      "Epoch:  4610 , step:  4610 , train loss:  0.13839874350351694 , test loss:  0.15543548135198565\n",
      "Epoch:  4620 , step:  4620 , train loss:  0.13827577468416727 , test loss:  0.15531974745453594\n",
      "Epoch:  4630 , step:  4630 , train loss:  0.1381531634129142 , test loss:  0.1552043591430576\n",
      "Epoch:  4640 , step:  4640 , train loss:  0.13803090795636325 , test loss:  0.15508931475133295\n",
      "Epoch:  4650 , step:  4650 , train loss:  0.13790900659292538 , test loss:  0.1549746126246163\n",
      "Epoch:  4660 , step:  4660 , train loss:  0.13778745761271297 , test loss:  0.15486025111953075\n",
      "Epoch:  4670 , step:  4670 , train loss:  0.13766625931743728 , test loss:  0.15474622860396584\n",
      "Epoch:  4680 , step:  4680 , train loss:  0.13754541002030696 , test loss:  0.1546325434569765\n",
      "Epoch:  4690 , step:  4690 , train loss:  0.13742490804592714 , test loss:  0.1545191940686828\n",
      "Epoch:  4700 , step:  4700 , train loss:  0.13730475173020065 , test loss:  0.1544061788401716\n",
      "Epoch:  4710 , step:  4710 , train loss:  0.13718493942022908 , test loss:  0.15429349618339822\n",
      "Epoch:  4720 , step:  4720 , train loss:  0.13706546947421586 , test loss:  0.15418114452108972\n",
      "Epoch:  4730 , step:  4730 , train loss:  0.1369463402613698 , test loss:  0.15406912228664965\n",
      "Epoch:  4740 , step:  4740 , train loss:  0.1368275501618099 , test loss:  0.15395742792406297\n",
      "Epoch:  4750 , step:  4750 , train loss:  0.13670909756647126 , test loss:  0.15384605988780264\n",
      "Epoch:  4760 , step:  4760 , train loss:  0.1365909808770116 , test loss:  0.15373501664273706\n",
      "Epoch:  4770 , step:  4770 , train loss:  0.13647319850571923 , test loss:  0.15362429666403846\n",
      "Epoch:  4780 , step:  4780 , train loss:  0.1363557488754217 , test loss:  0.15351389843709215\n",
      "Epoch:  4790 , step:  4790 , train loss:  0.13623863041939538 , test loss:  0.15340382045740722\n",
      "Epoch:  4800 , step:  4800 , train loss:  0.1361218415812763 , test loss:  0.15329406123052786\n",
      "Epoch:  4810 , step:  4810 , train loss:  0.13600538081497163 , test loss:  0.15318461927194527\n",
      "Epoch:  4820 , step:  4820 , train loss:  0.13588924658457205 , test loss:  0.15307549310701152\n",
      "Epoch:  4830 , step:  4830 , train loss:  0.13577343736426553 , test loss:  0.15296668127085325\n",
      "Epoch:  4840 , step:  4840 , train loss:  0.13565795163825126 , test loss:  0.15285818230828724\n",
      "Epoch:  4850 , step:  4850 , train loss:  0.13554278790065494 , test loss:  0.15274999477373596\n",
      "Epoch:  4860 , step:  4860 , train loss:  0.13542794465544503 , test loss:  0.1526421172311448\n",
      "Epoch:  4870 , step:  4870 , train loss:  0.13531342041634978 , test loss:  0.1525345482538999\n",
      "Epoch:  4880 , step:  4880 , train loss:  0.1351992137067746 , test loss:  0.1524272864247466\n",
      "Epoch:  4890 , step:  4890 , train loss:  0.13508532305972124 , test loss:  0.15232033033570908\n",
      "Epoch:  4900 , step:  4900 , train loss:  0.1349717470177069 , test loss:  0.152213678588011\n",
      "Epoch:  4910 , step:  4910 , train loss:  0.13485848413268492 , test loss:  0.15210732979199637\n",
      "Epoch:  4920 , step:  4920 , train loss:  0.13474553296596561 , test loss:  0.15200128256705173\n",
      "Epoch:  4930 , step:  4930 , train loss:  0.13463289208813844 , test loss:  0.1518955355415292\n",
      "Epoch:  4940 , step:  4940 , train loss:  0.1345205600789947 , test loss:  0.15179008735267002\n",
      "Epoch:  4950 , step:  4950 , train loss:  0.13440853552745097 , test loss:  0.15168493664652913\n",
      "Epoch:  4960 , step:  4960 , train loss:  0.13429681703147353 , test loss:  0.15158008207790044\n",
      "Epoch:  4970 , step:  4970 , train loss:  0.1341854031980036 , test loss:  0.15147552231024278\n",
      "Epoch:  4980 , step:  4980 , train loss:  0.1340742926428829 , test loss:  0.15137125601560694\n",
      "Epoch:  4990 , step:  4990 , train loss:  0.13396348399078042 , test loss:  0.15126728187456323\n",
      "Epoch:  5000 , step:  5000 , train loss:  0.1338529758751199 , test loss:  0.15116359857612982\n",
      "Epoch:  5010 , step:  5010 , train loss:  0.1337427669380075 , test loss:  0.1510602048177017\n",
      "Epoch:  5020 , step:  5020 , train loss:  0.13363285583016107 , test loss:  0.15095709930498058\n",
      "Epoch:  5030 , step:  5030 , train loss:  0.13352324121083933 , test loss:  0.15085428075190588\n",
      "Epoch:  5040 , step:  5040 , train loss:  0.13341392174777242 , test loss:  0.15075174788058526\n",
      "Epoch:  5050 , step:  5050 , train loss:  0.1333048961170924 , test loss:  0.1506494994212271\n",
      "Epoch:  5060 , step:  5060 , train loss:  0.13319616300326537 , test loss:  0.15054753411207314\n",
      "Epoch:  5070 , step:  5070 , train loss:  0.1330877210990232 , test loss:  0.1504458506993316\n",
      "Epoch:  5080 , step:  5080 , train loss:  0.1329795691052971 , test loss:  0.15034444793711163\n",
      "Epoch:  5090 , step:  5090 , train loss:  0.13287170573115079 , test loss:  0.15024332458735762\n",
      "Epoch:  5100 , step:  5100 , train loss:  0.13276412969371512 , test loss:  0.150142479419785\n",
      "Epoch:  5110 , step:  5110 , train loss:  0.13265683971812273 , test loss:  0.1500419112118159\n",
      "Epoch:  5120 , step:  5120 , train loss:  0.13254983453744384 , test loss:  0.14994161874851608\n",
      "Epoch:  5130 , step:  5130 , train loss:  0.13244311289262256 , test loss:  0.1498416008225324\n",
      "Epoch:  5140 , step:  5140 , train loss:  0.1323366735324135 , test loss:  0.14974185623403044\n",
      "Epoch:  5150 , step:  5150 , train loss:  0.1322305152133193 , test loss:  0.14964238379063324\n",
      "Epoch:  5160 , step:  5160 , train loss:  0.1321246366995289 , test loss:  0.1495431823073609\n",
      "Epoch:  5170 , step:  5170 , train loss:  0.13201903676285612 , test loss:  0.1494442506065698\n",
      "Epoch:  5180 , step:  5180 , train loss:  0.1319137141826789 , test loss:  0.1493455875178934\n",
      "Epoch:  5190 , step:  5190 , train loss:  0.1318086677458793 , test loss:  0.14924719187818325\n",
      "Epoch:  5200 , step:  5200 , train loss:  0.13170389624678386 , test loss:  0.14914906253145063\n",
      "Epoch:  5210 , step:  5210 , train loss:  0.1315993984871049 , test loss:  0.14905119832880853\n",
      "Epoch:  5220 , step:  5220 , train loss:  0.13149517327588167 , test loss:  0.14895359812841497\n",
      "Epoch:  5230 , step:  5230 , train loss:  0.13139121942942306 , test loss:  0.1488562607954156\n",
      "Epoch:  5240 , step:  5240 , train loss:  0.13128753577125007 , test loss:  0.14875918520188816\n",
      "Epoch:  5250 , step:  5250 , train loss:  0.13118412113203912 , test loss:  0.14866237022678702\n",
      "Epoch:  5260 , step:  5260 , train loss:  0.131080974349566 , test loss:  0.14856581475588768\n",
      "Epoch:  5270 , step:  5270 , train loss:  0.13097809426865023 , test loss:  0.14846951768173294\n",
      "Epoch:  5280 , step:  5280 , train loss:  0.1308754797410999 , test loss:  0.14837347790357847\n",
      "Epoch:  5290 , step:  5290 , train loss:  0.13077312962565718 , test loss:  0.1482776943273399\n",
      "Epoch:  5300 , step:  5300 , train loss:  0.13067104278794436 , test loss:  0.1481821658655397\n",
      "Epoch:  5310 , step:  5310 , train loss:  0.13056921810041025 , test loss:  0.14808689143725498\n",
      "Epoch:  5320 , step:  5320 , train loss:  0.1304676544422773 , test loss:  0.14799186996806546\n",
      "Epoch:  5330 , step:  5330 , train loss:  0.13036635069948874 , test loss:  0.1478971003900027\n",
      "Epoch:  5340 , step:  5340 , train loss:  0.13026530576465714 , test loss:  0.14780258164149856\n",
      "Epoch:  5350 , step:  5350 , train loss:  0.1301645185370125 , test loss:  0.14770831266733545\n",
      "Epoch:  5360 , step:  5360 , train loss:  0.13006398792235144 , test loss:  0.14761429241859633\n",
      "Epoch:  5370 , step:  5370 , train loss:  0.12996371283298638 , test loss:  0.1475205198526154\n",
      "Epoch:  5380 , step:  5380 , train loss:  0.129863692187696 , test loss:  0.14742699393292918\n",
      "Epoch:  5390 , step:  5390 , train loss:  0.12976392491167502 , test loss:  0.14733371362922812\n",
      "Epoch:  5400 , step:  5400 , train loss:  0.12966440993648562 , test loss:  0.14724067791730874\n",
      "Epoch:  5410 , step:  5410 , train loss:  0.12956514620000845 , test loss:  0.14714788577902624\n",
      "Epoch:  5420 , step:  5420 , train loss:  0.12946613264639456 , test loss:  0.14705533620224734\n",
      "Epoch:  5430 , step:  5430 , train loss:  0.1293673682260177 , test loss:  0.14696302818080376\n",
      "Epoch:  5440 , step:  5440 , train loss:  0.1292688518954268 , test loss:  0.14687096071444627\n",
      "Epoch:  5450 , step:  5450 , train loss:  0.1291705826172994 , test loss:  0.14677913280879887\n",
      "Epoch:  5460 , step:  5460 , train loss:  0.12907255936039494 , test loss:  0.1466875434753135\n",
      "Epoch:  5470 , step:  5470 , train loss:  0.12897478109950894 , test loss:  0.14659619173122554\n",
      "Epoch:  5480 , step:  5480 , train loss:  0.12887724681542737 , test loss:  0.14650507659950912\n",
      "Epoch:  5490 , step:  5490 , train loss:  0.1287799554948815 , test loss:  0.14641419710883327\n",
      "Epoch:  5500 , step:  5500 , train loss:  0.12868290613050318 , test loss:  0.1463235522935186\n",
      "Epoch:  5510 , step:  5510 , train loss:  0.12858609772078047 , test loss:  0.14623314119349376\n",
      "Epoch:  5520 , step:  5520 , train loss:  0.1284895292700139 , test loss:  0.14614296285425316\n",
      "Epoch:  5530 , step:  5530 , train loss:  0.1283931997882726 , test loss:  0.1460530163268144\n",
      "Epoch:  5540 , step:  5540 , train loss:  0.12829710829135163 , test loss:  0.1459633006676763\n",
      "Epoch:  5550 , step:  5550 , train loss:  0.12820125380072894 , test loss:  0.14587381493877763\n",
      "Epoch:  5560 , step:  5560 , train loss:  0.12810563534352323 , test loss:  0.14578455820745592\n",
      "Epoch:  5570 , step:  5570 , train loss:  0.12801025195245194 , test loss:  0.1456955295464065\n",
      "Epoch:  5580 , step:  5580 , train loss:  0.12791510266578962 , test loss:  0.1456067280336423\n",
      "Epoch:  5590 , step:  5590 , train loss:  0.12782018652732688 , test loss:  0.14551815275245394\n",
      "Epoch:  5600 , step:  5600 , train loss:  0.12772550258632953 , test loss:  0.14542980279136988\n",
      "Epoch:  5610 , step:  5610 , train loss:  0.12763104989749824 , test loss:  0.14534167724411748\n",
      "Epoch:  5620 , step:  5620 , train loss:  0.12753682752092818 , test loss:  0.14525377520958382\n",
      "Epoch:  5630 , step:  5630 , train loss:  0.1274428345220698 , test loss:  0.14516609579177725\n",
      "Epoch:  5640 , step:  5640 , train loss:  0.12734906997168893 , test loss:  0.14507863809978955\n",
      "Epoch:  5650 , step:  5650 , train loss:  0.12725553294582823 , test loss:  0.14499140124775753\n",
      "Epoch:  5660 , step:  5660 , train loss:  0.12716222252576834 , test loss:  0.14490438435482617\n",
      "Epoch:  5670 , step:  5670 , train loss:  0.12706913779798962 , test loss:  0.14481758654511087\n",
      "Epoch:  5680 , step:  5680 , train loss:  0.12697627785413418 , test loss:  0.14473100694766142\n",
      "Epoch:  5690 , step:  5690 , train loss:  0.12688364179096834 , test loss:  0.14464464469642482\n",
      "Epoch:  5700 , step:  5700 , train loss:  0.1267912287103452 , test loss:  0.1445584989302096\n",
      "Epoch:  5710 , step:  5710 , train loss:  0.12669903771916793 , test loss:  0.14447256879265\n",
      "Epoch:  5720 , step:  5720 , train loss:  0.1266070679293529 , test loss:  0.14438685343217045\n",
      "Epoch:  5730 , step:  5730 , train loss:  0.12651531845779362 , test loss:  0.1443013520019503\n",
      "Epoch:  5740 , step:  5740 , train loss:  0.12642378842632462 , test loss:  0.14421606365988934\n",
      "Epoch:  5750 , step:  5750 , train loss:  0.12633247696168579 , test loss:  0.14413098756857295\n",
      "Epoch:  5760 , step:  5760 , train loss:  0.12624138319548714 , test loss:  0.14404612289523822\n",
      "Epoch:  5770 , step:  5770 , train loss:  0.1261505062641738 , test loss:  0.1439614688117399\n",
      "Epoch:  5780 , step:  5780 , train loss:  0.12605984530899111 , test loss:  0.14387702449451678\n",
      "Epoch:  5790 , step:  5790 , train loss:  0.12596939947595048 , test loss:  0.1437927891245587\n",
      "Epoch:  5800 , step:  5800 , train loss:  0.12587916791579506 , test loss:  0.1437087618873732\n",
      "Epoch:  5810 , step:  5810 , train loss:  0.1257891497839661 , test loss:  0.14362494197295317\n",
      "Epoch:  5820 , step:  5820 , train loss:  0.12569934424056922 , test loss:  0.1435413285757443\n",
      "Epoch:  5830 , step:  5830 , train loss:  0.12560975045034156 , test loss:  0.14345792089461315\n",
      "Epoch:  5840 , step:  5840 , train loss:  0.1255203675826184 , test loss:  0.14337471813281502\n",
      "Epoch:  5850 , step:  5850 , train loss:  0.12543119481130088 , test loss:  0.14329171949796296\n",
      "Epoch:  5860 , step:  5860 , train loss:  0.1253422313148235 , test loss:  0.14320892420199588\n",
      "Epoch:  5870 , step:  5870 , train loss:  0.125253476276122 , test loss:  0.1431263314611482\n",
      "Epoch:  5880 , step:  5880 , train loss:  0.12516492888260158 , test loss:  0.1430439404959187\n",
      "Epoch:  5890 , step:  5890 , train loss:  0.12507658832610558 , test loss:  0.14296175053104057\n",
      "Epoch:  5900 , step:  5900 , train loss:  0.12498845380288388 , test loss:  0.14287976079545076\n",
      "Epoch:  5910 , step:  5910 , train loss:  0.12490052451356223 , test loss:  0.14279797052226045\n",
      "Epoch:  5920 , step:  5920 , train loss:  0.12481279966311132 , test loss:  0.14271637894872527\n",
      "Epoch:  5930 , step:  5930 , train loss:  0.12472527846081649 , test loss:  0.14263498531621613\n",
      "Epoch:  5940 , step:  5940 , train loss:  0.12463796012024746 , test loss:  0.1425537888701899\n",
      "Epoch:  5950 , step:  5950 , train loss:  0.12455084385922832 , test loss:  0.14247278886016063\n",
      "Epoch:  5960 , step:  5960 , train loss:  0.1244639288998081 , test loss:  0.14239198453967108\n",
      "Epoch:  5970 , step:  5970 , train loss:  0.12437721446823109 , test loss:  0.14231137516626433\n",
      "Epoch:  5980 , step:  5980 , train loss:  0.12429069979490785 , test loss:  0.14223096000145555\n",
      "Epoch:  5990 , step:  5990 , train loss:  0.1242043841143862 , test loss:  0.14215073831070432\n",
      "Epoch:  6000 , step:  6000 , train loss:  0.12411826666532255 , test loss:  0.14207070936338703\n",
      "Epoch:  6010 , step:  6010 , train loss:  0.12403234669045354 , test loss:  0.14199087243276923\n",
      "Epoch:  6020 , step:  6020 , train loss:  0.12394662343656776 , test loss:  0.14191122679597912\n",
      "Epoch:  6030 , step:  6030 , train loss:  0.1238610961544779 , test loss:  0.14183177173397987\n",
      "Epoch:  6040 , step:  6040 , train loss:  0.12377576409899288 , test loss:  0.14175250653154348\n",
      "Epoch:  6050 , step:  6050 , train loss:  0.1236906265288906 , test loss:  0.14167343047722433\n",
      "Epoch:  6060 , step:  6060 , train loss:  0.12360568270689046 , test loss:  0.1415945428633328\n",
      "Epoch:  6070 , step:  6070 , train loss:  0.1235209318996265 , test loss:  0.14151584298590925\n",
      "Epoch:  6080 , step:  6080 , train loss:  0.12343637337762053 , test loss:  0.14143733014469848\n",
      "Epoch:  6090 , step:  6090 , train loss:  0.12335200641525552 , test loss:  0.14135900364312406\n",
      "Epoch:  6100 , step:  6100 , train loss:  0.12326783029074949 , test loss:  0.14128086278826307\n",
      "Epoch:  6110 , step:  6110 , train loss:  0.12318384428612907 , test loss:  0.14120290689082093\n",
      "Epoch:  6120 , step:  6120 , train loss:  0.12310004768720374 , test loss:  0.14112513526510656\n",
      "Epoch:  6130 , step:  6130 , train loss:  0.12301643978354022 , test loss:  0.14104754722900772\n",
      "Epoch:  6140 , step:  6140 , train loss:  0.12293301986843684 , test loss:  0.14097014210396652\n",
      "Epoch:  6150 , step:  6150 , train loss:  0.12284978723889836 , test loss:  0.14089291921495514\n",
      "Epoch:  6160 , step:  6160 , train loss:  0.122766741195611 , test loss:  0.14081587789045194\n",
      "Epoch:  6170 , step:  6170 , train loss:  0.1226838810429173 , test loss:  0.14073901746241727\n",
      "Epoch:  6180 , step:  6180 , train loss:  0.12260120608879184 , test loss:  0.1406623372662701\n",
      "Epoch:  6190 , step:  6190 , train loss:  0.12251871564481656 , test loss:  0.14058583664086463\n",
      "Epoch:  6200 , step:  6200 , train loss:  0.12243640902615663 , test loss:  0.14050951492846678\n",
      "Epoch:  6210 , step:  6210 , train loss:  0.12235428555153631 , test loss:  0.14043337147473128\n",
      "Epoch:  6220 , step:  6220 , train loss:  0.12227234454321526 , test loss:  0.14035740562867893\n",
      "Epoch:  6230 , step:  6230 , train loss:  0.12219058532696477 , test loss:  0.14028161674267353\n",
      "Epoch:  6240 , step:  6240 , train loss:  0.12210900723204435 , test loss:  0.14020600417239995\n",
      "Epoch:  6250 , step:  6250 , train loss:  0.12202760959117846 , test loss:  0.14013056727684142\n",
      "Epoch:  6260 , step:  6260 , train loss:  0.12194639174053354 , test loss:  0.1400553054182576\n",
      "Epoch:  6270 , step:  6270 , train loss:  0.12186535301969492 , test loss:  0.13998021796216248\n",
      "Epoch:  6280 , step:  6280 , train loss:  0.12178449277164435 , test loss:  0.13990530427730288\n",
      "Epoch:  6290 , step:  6290 , train loss:  0.12170381034273738 , test loss:  0.1398305637356369\n",
      "Epoch:  6300 , step:  6300 , train loss:  0.12162330508268095 , test loss:  0.1397559957123122\n",
      "Epoch:  6310 , step:  6310 , train loss:  0.12154297634451147 , test loss:  0.13968159958564508\n",
      "Epoch:  6320 , step:  6320 , train loss:  0.12146282348457267 , test loss:  0.13960737473709953\n",
      "Epoch:  6330 , step:  6330 , train loss:  0.12138284586249382 , test loss:  0.13953332055126594\n",
      "Epoch:  6340 , step:  6340 , train loss:  0.12130304284116823 , test loss:  0.13945943641584096\n",
      "Epoch:  6350 , step:  6350 , train loss:  0.12122341378673175 , test loss:  0.13938572172160668\n",
      "Epoch:  6360 , step:  6360 , train loss:  0.12114395806854138 , test loss:  0.13931217586241013\n",
      "Epoch:  6370 , step:  6370 , train loss:  0.12106467505915443 , test loss:  0.1392387982351435\n",
      "Epoch:  6380 , step:  6380 , train loss:  0.12098556413430742 , test loss:  0.13916558823972386\n",
      "Epoch:  6390 , step:  6390 , train loss:  0.1209066246728953 , test loss:  0.13909254527907342\n",
      "Epoch:  6400 , step:  6400 , train loss:  0.12082785605695097 , test loss:  0.1390196687590996\n",
      "Epoch:  6410 , step:  6410 , train loss:  0.12074925767162477 , test loss:  0.13894695808867594\n",
      "Epoch:  6420 , step:  6420 , train loss:  0.12067082890516431 , test loss:  0.13887441267962217\n",
      "Epoch:  6430 , step:  6430 , train loss:  0.12059256914889421 , test loss:  0.1388020319466857\n",
      "Epoch:  6440 , step:  6440 , train loss:  0.12051447779719633 , test loss:  0.13872981530752174\n",
      "Epoch:  6450 , step:  6450 , train loss:  0.12043655424748978 , test loss:  0.13865776218267511\n",
      "Epoch:  6460 , step:  6460 , train loss:  0.12035879790021159 , test loss:  0.13858587199556102\n",
      "Epoch:  6470 , step:  6470 , train loss:  0.1202812081587969 , test loss:  0.1385141441724468\n",
      "Epoch:  6480 , step:  6480 , train loss:  0.12020378442965993 , test loss:  0.1384425781424331\n",
      "Epoch:  6490 , step:  6490 , train loss:  0.12012652612217462 , test loss:  0.13837117333743593\n",
      "Epoch:  6500 , step:  6500 , train loss:  0.12004943264865563 , test loss:  0.1382999291921683\n",
      "Epoch:  6510 , step:  6510 , train loss:  0.11997250342433965 , test loss:  0.13822884514412212\n",
      "Epoch:  6520 , step:  6520 , train loss:  0.11989573786736651 , test loss:  0.1381579206335508\n",
      "Epoch:  6530 , step:  6530 , train loss:  0.1198191353987606 , test loss:  0.13808715510345088\n",
      "Epoch:  6540 , step:  6540 , train loss:  0.11974269544241263 , test loss:  0.13801654799954485\n",
      "Epoch:  6550 , step:  6550 , train loss:  0.11966641742506105 , test loss:  0.13794609877026368\n",
      "Epoch:  6560 , step:  6560 , train loss:  0.11959030077627424 , test loss:  0.13787580686672943\n",
      "Epoch:  6570 , step:  6570 , train loss:  0.11951434492843233 , test loss:  0.13780567174273814\n",
      "Epoch:  6580 , step:  6580 , train loss:  0.11943854931670934 , test loss:  0.1377356928547427\n",
      "Epoch:  6590 , step:  6590 , train loss:  0.11936291337905547 , test loss:  0.13766586966183592\n",
      "Epoch:  6600 , step:  6600 , train loss:  0.11928743655617965 , test loss:  0.13759620162573422\n",
      "Epoch:  6610 , step:  6610 , train loss:  0.11921211829153193 , test loss:  0.13752668821076028\n",
      "Epoch:  6620 , step:  6620 , train loss:  0.11913695803128621 , test loss:  0.13745732888382706\n",
      "Epoch:  6630 , step:  6630 , train loss:  0.11906195522432315 , test loss:  0.13738812311442128\n",
      "Epoch:  6640 , step:  6640 , train loss:  0.11898710932221307 , test loss:  0.1373190703745872\n",
      "Epoch:  6650 , step:  6650 , train loss:  0.11891241977919904 , test loss:  0.13725017013891044\n",
      "Epoch:  6660 , step:  6660 , train loss:  0.11883788605218007 , test loss:  0.13718142188450197\n",
      "Epoch:  6670 , step:  6670 , train loss:  0.11876350760069455 , test loss:  0.1371128250909825\n",
      "Epoch:  6680 , step:  6680 , train loss:  0.1186892838869036 , test loss:  0.1370443792404663\n",
      "Epoch:  6690 , step:  6690 , train loss:  0.11861521437557482 , test loss:  0.136976083817546\n",
      "Epoch:  6700 , step:  6700 , train loss:  0.11854129853406592 , test loss:  0.13690793830927692\n",
      "Epoch:  6710 , step:  6710 , train loss:  0.11846753583230857 , test loss:  0.13683994220516163\n",
      "Epoch:  6720 , step:  6720 , train loss:  0.11839392574279246 , test loss:  0.1367720949971349\n",
      "Epoch:  6730 , step:  6730 , train loss:  0.1183204677405493 , test loss:  0.1367043961795482\n",
      "Epoch:  6740 , step:  6740 , train loss:  0.11824716130313712 , test loss:  0.13663684524915518\n",
      "Epoch:  6750 , step:  6750 , train loss:  0.11817400591062455 , test loss:  0.1365694417050963\n",
      "Epoch:  6760 , step:  6760 , train loss:  0.11810100104557537 , test loss:  0.13650218504888423\n",
      "Epoch:  6770 , step:  6770 , train loss:  0.11802814619303295 , test loss:  0.13643507478438938\n",
      "Epoch:  6780 , step:  6780 , train loss:  0.11795544084050508 , test loss:  0.136368110417825\n",
      "Epoch:  6790 , step:  6790 , train loss:  0.11788288447794869 , test loss:  0.13630129145773281\n",
      "Epoch:  6800 , step:  6800 , train loss:  0.11781047659775477 , test loss:  0.13623461741496876\n",
      "Epoch:  6810 , step:  6810 , train loss:  0.11773821669473358 , test loss:  0.13616808780268883\n",
      "Epoch:  6820 , step:  6820 , train loss:  0.11766610426609952 , test loss:  0.13610170213633468\n",
      "Epoch:  6830 , step:  6830 , train loss:  0.1175941388114566 , test loss:  0.13603545993361996\n",
      "Epoch:  6840 , step:  6840 , train loss:  0.11752231983278373 , test loss:  0.13596936071451615\n",
      "Epoch:  6850 , step:  6850 , train loss:  0.11745064683442027 , test loss:  0.1359034040012388\n",
      "Epoch:  6860 , step:  6860 , train loss:  0.11737911932305155 , test loss:  0.13583758931823386\n",
      "Epoch:  6870 , step:  6870 , train loss:  0.11730773680769457 , test loss:  0.1357719161921642\n",
      "Epoch:  6880 , step:  6880 , train loss:  0.11723649879968388 , test loss:  0.13570638415189593\n",
      "Epoch:  6890 , step:  6890 , train loss:  0.11716540481265747 , test loss:  0.13564099272848512\n",
      "Epoch:  6900 , step:  6900 , train loss:  0.11709445436254261 , test loss:  0.13557574145516454\n",
      "Epoch:  6910 , step:  6910 , train loss:  0.11702364696754235 , test loss:  0.13551062986733026\n",
      "Epoch:  6920 , step:  6920 , train loss:  0.11695298214812135 , test loss:  0.13544565750252896\n",
      "Epoch:  6930 , step:  6930 , train loss:  0.11688245942699242 , test loss:  0.13538082390044456\n",
      "Epoch:  6940 , step:  6940 , train loss:  0.11681207832910291 , test loss:  0.13531612860288544\n",
      "Epoch:  6950 , step:  6950 , train loss:  0.11674183838162126 , test loss:  0.13525157115377187\n",
      "Epoch:  6960 , step:  6960 , train loss:  0.11667173911392358 , test loss:  0.13518715109912283\n",
      "Epoch:  6970 , step:  6970 , train loss:  0.11660178005758036 , test loss:  0.13512286798704387\n",
      "Epoch:  6980 , step:  6980 , train loss:  0.11653196074634335 , test loss:  0.13505872136771455\n",
      "Epoch:  6990 , step:  6990 , train loss:  0.11646228071613247 , test loss:  0.1349947107933756\n",
      "Epoch:  7000 , step:  7000 , train loss:  0.11639273950502285 , test loss:  0.13493083581831714\n",
      "Epoch:  7010 , step:  7010 , train loss:  0.11632333665323182 , test loss:  0.13486709599886595\n",
      "Epoch:  7020 , step:  7020 , train loss:  0.11625407170310628 , test loss:  0.13480349089337393\n",
      "Epoch:  7030 , step:  7030 , train loss:  0.1161849441991099 , test loss:  0.13474002006220526\n",
      "Epoch:  7040 , step:  7040 , train loss:  0.11611595368781052 , test loss:  0.13467668306772526\n",
      "Epoch:  7050 , step:  7050 , train loss:  0.11604709971786771 , test loss:  0.1346134794742878\n",
      "Epoch:  7060 , step:  7060 , train loss:  0.11597838184002023 , test loss:  0.13455040884822408\n",
      "Epoch:  7070 , step:  7070 , train loss:  0.1159097996070738 , test loss:  0.13448747075783057\n",
      "Epoch:  7080 , step:  7080 , train loss:  0.11584135257388875 , test loss:  0.13442466477335746\n",
      "Epoch:  7090 , step:  7090 , train loss:  0.11577304029736803 , test loss:  0.13436199046699726\n",
      "Epoch:  7100 , step:  7100 , train loss:  0.11570486233644493 , test loss:  0.13429944741287317\n",
      "Epoch:  7110 , step:  7110 , train loss:  0.11563681825207128 , test loss:  0.13423703518702804\n",
      "Epoch:  7120 , step:  7120 , train loss:  0.11556890760720545 , test loss:  0.13417475336741255\n",
      "Epoch:  7130 , step:  7130 , train loss:  0.11550112996680068 , test loss:  0.13411260153387464\n",
      "Epoch:  7140 , step:  7140 , train loss:  0.11543348489779309 , test loss:  0.13405057926814784\n",
      "Epoch:  7150 , step:  7150 , train loss:  0.11536597196909035 , test loss:  0.13398868615384063\n",
      "Epoch:  7160 , step:  7160 , train loss:  0.1152985907515599 , test loss:  0.13392692177642543\n",
      "Epoch:  7170 , step:  7170 , train loss:  0.11523134081801763 , test loss:  0.13386528572322737\n",
      "Epoch:  7180 , step:  7180 , train loss:  0.1151642217432164 , test loss:  0.1338037775834139\n",
      "Epoch:  7190 , step:  7190 , train loss:  0.11509723310383475 , test loss:  0.133742396947984\n",
      "Epoch:  7200 , step:  7200 , train loss:  0.11503037447846572 , test loss:  0.1336811434097575\n",
      "Epoch:  7210 , step:  7210 , train loss:  0.11496364544760561 , test loss:  0.1336200165633643\n",
      "Epoch:  7220 , step:  7220 , train loss:  0.114897045593643 , test loss:  0.13355901600523423\n",
      "Epoch:  7230 , step:  7230 , train loss:  0.11483057450084777 , test loss:  0.13349814133358642\n",
      "Epoch:  7240 , step:  7240 , train loss:  0.11476423175536006 , test loss:  0.13343739214841907\n",
      "Epoch:  7250 , step:  7250 , train loss:  0.11469801694517964 , test loss:  0.13337676805149914\n",
      "Epoch:  7260 , step:  7260 , train loss:  0.11463192966015497 , test loss:  0.13331626864635213\n",
      "Epoch:  7270 , step:  7270 , train loss:  0.11456596949197262 , test loss:  0.133255893538252\n",
      "Epoch:  7280 , step:  7280 , train loss:  0.11450013603414665 , test loss:  0.13319564233421127\n",
      "Epoch:  7290 , step:  7290 , train loss:  0.11443442888200803 , test loss:  0.13313551464297063\n",
      "Epoch:  7300 , step:  7300 , train loss:  0.11436884763269427 , test loss:  0.13307551007498958\n",
      "Epoch:  7310 , step:  7310 , train loss:  0.114303391885139 , test loss:  0.13301562824243615\n",
      "Epoch:  7320 , step:  7320 , train loss:  0.11423806124006164 , test loss:  0.13295586875917748\n",
      "Epoch:  7330 , step:  7330 , train loss:  0.11417285529995719 , test loss:  0.13289623124076985\n",
      "Epoch:  7340 , step:  7340 , train loss:  0.11410777366908606 , test loss:  0.13283671530444924\n",
      "Epoch:  7350 , step:  7350 , train loss:  0.11404281595346398 , test loss:  0.1327773205691216\n",
      "Epoch:  7360 , step:  7360 , train loss:  0.11397798176085198 , test loss:  0.1327180466553536\n",
      "Epoch:  7370 , step:  7370 , train loss:  0.11391327070074643 , test loss:  0.1326588931853632\n",
      "Epoch:  7380 , step:  7380 , train loss:  0.11384868238436921 , test loss:  0.1325998597830099\n",
      "Epoch:  7390 , step:  7390 , train loss:  0.11378421642465772 , test loss:  0.1325409460737858\n",
      "Epoch:  7400 , step:  7400 , train loss:  0.11371987243625543 , test loss:  0.13248215168480648\n",
      "Epoch:  7410 , step:  7410 , train loss:  0.11365565003550192 , test loss:  0.13242347624480172\n",
      "Epoch:  7420 , step:  7420 , train loss:  0.11359154884042344 , test loss:  0.13236491938410586\n",
      "Epoch:  7430 , step:  7430 , train loss:  0.11352756847072333 , test loss:  0.13230648073464993\n",
      "Epoch:  7440 , step:  7440 , train loss:  0.11346370854777246 , test loss:  0.1322481599299516\n",
      "Epoch:  7450 , step:  7450 , train loss:  0.11339996869459999 , test loss:  0.13218995660510677\n",
      "Epoch:  7460 , step:  7460 , train loss:  0.11333634853588385 , test loss:  0.13213187039678082\n",
      "Epoch:  7470 , step:  7470 , train loss:  0.11327284769794153 , test loss:  0.1320739009431994\n",
      "Epoch:  7480 , step:  7480 , train loss:  0.1132094658087209 , test loss:  0.13201604788414023\n",
      "Epoch:  7490 , step:  7490 , train loss:  0.11314620249779102 , test loss:  0.13195831086092397\n",
      "Epoch:  7500 , step:  7500 , train loss:  0.11308305739633295 , test loss:  0.1319006895164059\n",
      "Epoch:  7510 , step:  7510 , train loss:  0.11302003013713098 , test loss:  0.1318431834949675\n",
      "Epoch:  7520 , step:  7520 , train loss:  0.11295712035456337 , test loss:  0.13178579244250746\n",
      "Epoch:  7530 , step:  7530 , train loss:  0.11289432768459362 , test loss:  0.13172851600643393\n",
      "Epoch:  7540 , step:  7540 , train loss:  0.11283165176476158 , test loss:  0.13167135383565537\n",
      "Epoch:  7550 , step:  7550 , train loss:  0.11276909223417472 , test loss:  0.13161430558057316\n",
      "Epoch:  7560 , step:  7560 , train loss:  0.11270664873349923 , test loss:  0.13155737089307246\n",
      "Epoch:  7570 , step:  7570 , train loss:  0.11264432090495156 , test loss:  0.13150054942651465\n",
      "Epoch:  7580 , step:  7580 , train loss:  0.11258210839228978 , test loss:  0.13144384083572905\n",
      "Epoch:  7590 , step:  7590 , train loss:  0.11252001084080487 , test loss:  0.13138724477700436\n",
      "Epoch:  7600 , step:  7600 , train loss:  0.11245802789731241 , test loss:  0.13133076090808157\n",
      "Epoch:  7610 , step:  7610 , train loss:  0.11239615921014405 , test loss:  0.13127438888814497\n",
      "Epoch:  7620 , step:  7620 , train loss:  0.11233440442913928 , test loss:  0.13121812837781502\n",
      "Epoch:  7630 , step:  7630 , train loss:  0.1122727632056368 , test loss:  0.1311619790391401\n",
      "Epoch:  7640 , step:  7640 , train loss:  0.11221123519246663 , test loss:  0.1311059405355886\n",
      "Epoch:  7650 , step:  7650 , train loss:  0.11214982004394174 , test loss:  0.13105001253204163\n",
      "Epoch:  7660 , step:  7660 , train loss:  0.11208851741584973 , test loss:  0.13099419469478488\n",
      "Epoch:  7670 , step:  7670 , train loss:  0.11202732696544504 , test loss:  0.13093848669150107\n",
      "Epoch:  7680 , step:  7680 , train loss:  0.11196624835144073 , test loss:  0.13088288819126243\n",
      "Epoch:  7690 , step:  7690 , train loss:  0.11190528123400052 , test loss:  0.1308273988645233\n",
      "Epoch:  7700 , step:  7700 , train loss:  0.11184442527473087 , test loss:  0.1307720183831121\n",
      "Epoch:  7710 , step:  7710 , train loss:  0.1117836801366731 , test loss:  0.1307167464202245\n",
      "Epoch:  7720 , step:  7720 , train loss:  0.11172304548429553 , test loss:  0.13066158265041564\n",
      "Epoch:  7730 , step:  7730 , train loss:  0.11166252098348574 , test loss:  0.13060652674959305\n",
      "Epoch:  7740 , step:  7740 , train loss:  0.11160210630154287 , test loss:  0.13055157839500897\n",
      "Epoch:  7750 , step:  7750 , train loss:  0.11154180110716985 , test loss:  0.13049673726525343\n",
      "Epoch:  7760 , step:  7760 , train loss:  0.11148160507046592 , test loss:  0.13044200304024697\n",
      "Epoch:  7770 , step:  7770 , train loss:  0.11142151786291894 , test loss:  0.1303873754012335\n",
      "Epoch:  7780 , step:  7780 , train loss:  0.1113615391573979 , test loss:  0.130332854030773\n",
      "Epoch:  7790 , step:  7790 , train loss:  0.11130166862814547 , test loss:  0.1302784386127348\n",
      "Epoch:  7800 , step:  7800 , train loss:  0.11124190595077058 , test loss:  0.13022412883229034\n",
      "Epoch:  7810 , step:  7810 , train loss:  0.11118225080224103 , test loss:  0.13016992437590622\n",
      "Epoch:  7820 , step:  7820 , train loss:  0.1111227028608762 , test loss:  0.13011582493133728\n",
      "Epoch:  7830 , step:  7830 , train loss:  0.11106326180633969 , test loss:  0.13006183018761983\n",
      "Epoch:  7840 , step:  7840 , train loss:  0.11100392731963223 , test loss:  0.13000793983506473\n",
      "Epoch:  7850 , step:  7850 , train loss:  0.11094469908308438 , test loss:  0.1299541535652508\n",
      "Epoch:  7860 , step:  7860 , train loss:  0.11088557678034945 , test loss:  0.12990047107101751\n",
      "Epoch:  7870 , step:  7870 , train loss:  0.11082656009639638 , test loss:  0.12984689204645922\n",
      "Epoch:  7880 , step:  7880 , train loss:  0.11076764871750278 , test loss:  0.12979341618691762\n",
      "Epoch:  7890 , step:  7890 , train loss:  0.11070884233124782 , test loss:  0.12974004318897564\n",
      "Epoch:  7900 , step:  7900 , train loss:  0.11065014062650542 , test loss:  0.12968677275045074\n",
      "Epoch:  7910 , step:  7910 , train loss:  0.11059154329343725 , test loss:  0.12963360457038853\n",
      "Epoch:  7920 , step:  7920 , train loss:  0.11053305002348585 , test loss:  0.1295805383490561\n",
      "Epoch:  7930 , step:  7930 , train loss:  0.11047466050936795 , test loss:  0.12952757378793558\n",
      "Epoch:  7940 , step:  7940 , train loss:  0.11041637444506755 , test loss:  0.12947471058971793\n",
      "Epoch:  7950 , step:  7950 , train loss:  0.11035819152582928 , test loss:  0.12942194845829652\n",
      "Epoch:  7960 , step:  7960 , train loss:  0.11030011144815176 , test loss:  0.1293692870987608\n",
      "Epoch:  7970 , step:  7970 , train loss:  0.11024213390978084 , test loss:  0.12931672621738982\n",
      "Epoch:  7980 , step:  7980 , train loss:  0.11018425860970313 , test loss:  0.1292642655216465\n",
      "Epoch:  7990 , step:  7990 , train loss:  0.11012648524813937 , test loss:  0.12921190472017105\n",
      "Epoch:  8000 , step:  8000 , train loss:  0.11006881352653794 , test loss:  0.12915964352277493\n",
      "Epoch:  8010 , step:  8010 , train loss:  0.11001124314756842 , test loss:  0.12910748164043465\n",
      "Epoch:  8020 , step:  8020 , train loss:  0.10995377381511513 , test loss:  0.12905541878528598\n",
      "Epoch:  8030 , step:  8030 , train loss:  0.10989640523427079 , test loss:  0.12900345467061766\n",
      "Epoch:  8040 , step:  8040 , train loss:  0.10983913711133014 , test loss:  0.12895158901086545\n",
      "Epoch:  8050 , step:  8050 , train loss:  0.10978196915378362 , test loss:  0.12889982152160623\n",
      "Epoch:  8060 , step:  8060 , train loss:  0.10972490107031116 , test loss:  0.12884815191955215\n",
      "Epoch:  8070 , step:  8070 , train loss:  0.10966793257077596 , test loss:  0.12879657992254476\n",
      "Epoch:  8080 , step:  8080 , train loss:  0.10961106336621826 , test loss:  0.12874510524954882\n",
      "Epoch:  8090 , step:  8090 , train loss:  0.10955429316884921 , test loss:  0.12869372762064707\n",
      "Epoch:  8100 , step:  8100 , train loss:  0.10949762169204477 , test loss:  0.12864244675703404\n",
      "Epoch:  8110 , step:  8110 , train loss:  0.10944104865033977 , test loss:  0.12859126238101057\n",
      "Epoch:  8120 , step:  8120 , train loss:  0.10938457375942164 , test loss:  0.1285401742159779\n",
      "Epoch:  8130 , step:  8130 , train loss:  0.10932819673612462 , test loss:  0.1284891819864322\n",
      "Epoch:  8140 , step:  8140 , train loss:  0.10927191729842373 , test loss:  0.12843828541795882\n",
      "Epoch:  8150 , step:  8150 , train loss:  0.10921573516542886 , test loss:  0.12838748423722696\n",
      "Epoch:  8160 , step:  8160 , train loss:  0.109159650057379 , test loss:  0.1283367781719838\n",
      "Epoch:  8170 , step:  8170 , train loss:  0.10910366169563622 , test loss:  0.1282861669510492\n",
      "Epoch:  8180 , step:  8180 , train loss:  0.10904776980268005 , test loss:  0.12823565030431008\n",
      "Epoch:  8190 , step:  8190 , train loss:  0.10899197410210157 , test loss:  0.12818522796271517\n",
      "Epoch:  8200 , step:  8200 , train loss:  0.10893627431859776 , test loss:  0.12813489965826944\n",
      "Epoch:  8210 , step:  8210 , train loss:  0.10888067017796586 , test loss:  0.12808466512402888\n",
      "Epoch:  8220 , step:  8220 , train loss:  0.10882516140709761 , test loss:  0.1280345240940949\n",
      "Epoch:  8230 , step:  8230 , train loss:  0.1087697477339737 , test loss:  0.12798447630360932\n",
      "Epoch:  8240 , step:  8240 , train loss:  0.10871442888765812 , test loss:  0.1279345214887489\n",
      "Epoch:  8250 , step:  8250 , train loss:  0.10865920459829273 , test loss:  0.12788465938672025\n",
      "Epoch:  8260 , step:  8260 , train loss:  0.10860407459709163 , test loss:  0.12783488973575458\n",
      "Epoch:  8270 , step:  8270 , train loss:  0.10854903861633576 , test loss:  0.12778521227510242\n",
      "Epoch:  8280 , step:  8280 , train loss:  0.10849409638936736 , test loss:  0.1277356267450285\n",
      "Epoch:  8290 , step:  8290 , train loss:  0.1084392476505847 , test loss:  0.12768613288680686\n",
      "Epoch:  8300 , step:  8300 , train loss:  0.10838449213543651 , test loss:  0.1276367304427155\n",
      "Epoch:  8310 , step:  8310 , train loss:  0.10832982958041693 , test loss:  0.1275874191560316\n",
      "Epoch:  8320 , step:  8320 , train loss:  0.10827525972305983 , test loss:  0.12753819877102623\n",
      "Epoch:  8330 , step:  8330 , train loss:  0.10822078230193385 , test loss:  0.1274890690329594\n",
      "Epoch:  8340 , step:  8340 , train loss:  0.10816639705663701 , test loss:  0.12744002968807555\n",
      "Epoch:  8350 , step:  8350 , train loss:  0.10811210372779145 , test loss:  0.1273910804835979\n",
      "Epoch:  8360 , step:  8360 , train loss:  0.10805790205703837 , test loss:  0.12734222116772423\n",
      "Epoch:  8370 , step:  8370 , train loss:  0.10800379178703289 , test loss:  0.1272934514896216\n",
      "Epoch:  8380 , step:  8380 , train loss:  0.10794977266143878 , test loss:  0.1272447711994217\n",
      "Epoch:  8390 , step:  8390 , train loss:  0.10789584442492348 , test loss:  0.12719618004821595\n",
      "Epoch:  8400 , step:  8400 , train loss:  0.10784200682315312 , test loss:  0.12714767778805097\n",
      "Epoch:  8410 , step:  8410 , train loss:  0.10778825960278733 , test loss:  0.12709926417192347\n",
      "Epoch:  8420 , step:  8420 , train loss:  0.10773460251147439 , test loss:  0.12705093895377587\n",
      "Epoch:  8430 , step:  8430 , train loss:  0.1076810352978462 , test loss:  0.12700270188849136\n",
      "Epoch:  8440 , step:  8440 , train loss:  0.10762755771151339 , test loss:  0.1269545527318894\n",
      "Epoch:  8450 , step:  8450 , train loss:  0.10757416950306034 , test loss:  0.12690649124072115\n",
      "Epoch:  8460 , step:  8460 , train loss:  0.10752087042404047 , test loss:  0.12685851717266464\n",
      "Epoch:  8470 , step:  8470 , train loss:  0.10746766022697121 , test loss:  0.12681063028632059\n",
      "Epoch:  8480 , step:  8480 , train loss:  0.10741453866532932 , test loss:  0.12676283034120722\n",
      "Epoch:  8490 , step:  8490 , train loss:  0.10736150549354605 , test loss:  0.12671511709775649\n",
      "Epoch:  8500 , step:  8500 , train loss:  0.10730856046700248 , test loss:  0.1266674903173091\n",
      "Epoch:  8510 , step:  8510 , train loss:  0.10725570334202462 , test loss:  0.1266199497621103\n",
      "Epoch:  8520 , step:  8520 , train loss:  0.10720293387587894 , test loss:  0.12657249519530528\n",
      "Epoch:  8530 , step:  8530 , train loss:  0.10715025182676746 , test loss:  0.12652512638093477\n",
      "Epoch:  8540 , step:  8540 , train loss:  0.10709765695382334 , test loss:  0.12647784308393098\n",
      "Epoch:  8550 , step:  8550 , train loss:  0.1070451490171062 , test loss:  0.12643064507011276\n",
      "Epoch:  8560 , step:  8560 , train loss:  0.10699272777759739 , test loss:  0.1263835321061816\n",
      "Epoch:  8570 , step:  8570 , train loss:  0.10694039299719563 , test loss:  0.12633650395971727\n",
      "Epoch:  8580 , step:  8580 , train loss:  0.10688814443871247 , test loss:  0.12628956039917355\n",
      "Epoch:  8590 , step:  8590 , train loss:  0.10683598186586765 , test loss:  0.12624270119387382\n",
      "Epoch:  8600 , step:  8600 , train loss:  0.10678390504328475 , test loss:  0.1261959261140071\n",
      "Epoch:  8610 , step:  8610 , train loss:  0.10673191373648673 , test loss:  0.12614923493062383\n",
      "Epoch:  8620 , step:  8620 , train loss:  0.10668000771189146 , test loss:  0.12610262741563127\n",
      "Epoch:  8630 , step:  8630 , train loss:  0.10662818673680736 , test loss:  0.12605610334179007\n",
      "Epoch:  8640 , step:  8640 , train loss:  0.1065764505794291 , test loss:  0.12600966248270953\n",
      "Epoch:  8650 , step:  8650 , train loss:  0.1065247990088332 , test loss:  0.12596330461284375\n",
      "Epoch:  8660 , step:  8660 , train loss:  0.1064732317949736 , test loss:  0.12591702950748765\n",
      "Epoch:  8670 , step:  8670 , train loss:  0.10642174870867765 , test loss:  0.12587083694277268\n",
      "Epoch:  8680 , step:  8680 , train loss:  0.10637034952164157 , test loss:  0.12582472669566303\n",
      "Epoch:  8690 , step:  8690 , train loss:  0.10631903400642641 , test loss:  0.12577869854395157\n",
      "Epoch:  8700 , step:  8700 , train loss:  0.10626780193645373 , test loss:  0.12573275226625572\n",
      "Epoch:  8710 , step:  8710 , train loss:  0.10621665308600149 , test loss:  0.1256868876420137\n",
      "Epoch:  8720 , step:  8720 , train loss:  0.1061655872301998 , test loss:  0.12564110445148036\n",
      "Epoch:  8730 , step:  8730 , train loss:  0.10611460414502691 , test loss:  0.1255954024757238\n",
      "Epoch:  8740 , step:  8740 , train loss:  0.10606370360730492 , test loss:  0.12554978149662072\n",
      "Epoch:  8750 , step:  8750 , train loss:  0.10601288539469596 , test loss:  0.12550424129685323\n",
      "Epoch:  8760 , step:  8760 , train loss:  0.10596214928569778 , test loss:  0.12545878165990448\n",
      "Epoch:  8770 , step:  8770 , train loss:  0.10591149505964 , test loss:  0.12541340237005533\n",
      "Epoch:  8780 , step:  8780 , train loss:  0.10586092249668001 , test loss:  0.1253681032123804\n",
      "Epoch:  8790 , step:  8790 , train loss:  0.10581043137779894 , test loss:  0.12532288397274388\n",
      "Epoch:  8800 , step:  8800 , train loss:  0.10576002148479771 , test loss:  0.12527774443779655\n",
      "Epoch:  8810 , step:  8810 , train loss:  0.1057096926002931 , test loss:  0.12523268439497143\n",
      "Epoch:  8820 , step:  8820 , train loss:  0.10565944450771385 , test loss:  0.12518770363248027\n",
      "Epoch:  8830 , step:  8830 , train loss:  0.10560927699129666 , test loss:  0.12514280193930996\n",
      "Epoch:  8840 , step:  8840 , train loss:  0.10555918983608249 , test loss:  0.12509797910521875\n",
      "Epoch:  8850 , step:  8850 , train loss:  0.10550918282791252 , test loss:  0.12505323492073286\n",
      "Epoch:  8860 , step:  8860 , train loss:  0.1054592557534245 , test loss:  0.12500856917714234\n",
      "Epoch:  8870 , step:  8870 , train loss:  0.10540940840004878 , test loss:  0.12496398166649803\n",
      "Epoch:  8880 , step:  8880 , train loss:  0.10535964055600464 , test loss:  0.12491947218160775\n",
      "Epoch:  8890 , step:  8890 , train loss:  0.10530995201029646 , test loss:  0.12487504051603251\n",
      "Epoch:  8900 , step:  8900 , train loss:  0.10526034255271002 , test loss:  0.12483068646408353\n",
      "Epoch:  8910 , step:  8910 , train loss:  0.10521081197380881 , test loss:  0.12478640982081822\n",
      "Epoch:  8920 , step:  8920 , train loss:  0.10516136006493021 , test loss:  0.12474221038203694\n",
      "Epoch:  8930 , step:  8930 , train loss:  0.10511198661818194 , test loss:  0.12469808794427942\n",
      "Epoch:  8940 , step:  8940 , train loss:  0.10506269142643836 , test loss:  0.12465404230482141\n",
      "Epoch:  8950 , step:  8950 , train loss:  0.10501347428333685 , test loss:  0.1246100732616711\n",
      "Epoch:  8960 , step:  8960 , train loss:  0.10496433498327416 , test loss:  0.12456618061356593\n",
      "Epoch:  8970 , step:  8970 , train loss:  0.10491527332140287 , test loss:  0.12452236415996894\n",
      "Epoch:  8980 , step:  8980 , train loss:  0.10486628909362777 , test loss:  0.12447862370106562\n",
      "Epoch:  8990 , step:  8990 , train loss:  0.10481738209660237 , test loss:  0.12443495903776029\n",
      "Epoch:  9000 , step:  9000 , train loss:  0.10476855212772533 , test loss:  0.12439136997167315\n",
      "Epoch:  9010 , step:  9010 , train loss:  0.10471979898513693 , test loss:  0.12434785630513653\n",
      "Epoch:  9020 , step:  9020 , train loss:  0.10467112246771562 , test loss:  0.12430441784119196\n",
      "Epoch:  9030 , step:  9030 , train loss:  0.10462252237507458 , test loss:  0.1242610543835867\n",
      "Epoch:  9040 , step:  9040 , train loss:  0.10457399850755814 , test loss:  0.1242177657367704\n",
      "Epoch:  9050 , step:  9050 , train loss:  0.10452555066623857 , test loss:  0.1241745517058922\n",
      "Epoch:  9060 , step:  9060 , train loss:  0.10447717865291248 , test loss:  0.12413141209679707\n",
      "Epoch:  9070 , step:  9070 , train loss:  0.10442888227009747 , test loss:  0.12408834671602284\n",
      "Epoch:  9080 , step:  9080 , train loss:  0.10438066132102884 , test loss:  0.1240453553707972\n",
      "Epoch:  9090 , step:  9090 , train loss:  0.10433251560965617 , test loss:  0.12400243786903413\n",
      "Epoch:  9100 , step:  9100 , train loss:  0.10428444494063996 , test loss:  0.123959594019331\n",
      "Epoch:  9110 , step:  9110 , train loss:  0.10423644911934846 , test loss:  0.12391682363096539\n",
      "Epoch:  9120 , step:  9120 , train loss:  0.10418852795185411 , test loss:  0.12387412651389192\n",
      "Epoch:  9130 , step:  9130 , train loss:  0.10414068124493067 , test loss:  0.12383150247873925\n",
      "Epoch:  9140 , step:  9140 , train loss:  0.10409290880604949 , test loss:  0.12378895133680697\n",
      "Epoch:  9150 , step:  9150 , train loss:  0.10404521044337664 , test loss:  0.1237464729000625\n",
      "Epoch:  9160 , step:  9160 , train loss:  0.10399758596576957 , test loss:  0.12370406698113819\n",
      "Epoch:  9170 , step:  9170 , train loss:  0.10395003518277382 , test loss:  0.12366173339332792\n",
      "Epoch:  9180 , step:  9180 , train loss:  0.10390255790461997 , test loss:  0.12361947195058448\n",
      "Epoch:  9190 , step:  9190 , train loss:  0.10385515394222039 , test loss:  0.12357728246751645\n",
      "Epoch:  9200 , step:  9200 , train loss:  0.10380782310716614 , test loss:  0.12353516475938524\n",
      "Epoch:  9210 , step:  9210 , train loss:  0.10376056521172386 , test loss:  0.12349311864210197\n",
      "Epoch:  9220 , step:  9220 , train loss:  0.10371338006883253 , test loss:  0.12345114393222482\n",
      "Epoch:  9230 , step:  9230 , train loss:  0.10366626749210053 , test loss:  0.1234092404469558\n",
      "Epoch:  9240 , step:  9240 , train loss:  0.10361922729580253 , test loss:  0.12336740800413815\n",
      "Epoch:  9250 , step:  9250 , train loss:  0.10357225929487629 , test loss:  0.12332564642225306\n",
      "Epoch:  9260 , step:  9260 , train loss:  0.10352536330491985 , test loss:  0.12328395552041714\n",
      "Epoch:  9270 , step:  9270 , train loss:  0.1034785391421883 , test loss:  0.12324233511837941\n",
      "Epoch:  9280 , step:  9280 , train loss:  0.1034317866235909 , test loss:  0.12320078503651849\n",
      "Epoch:  9290 , step:  9290 , train loss:  0.10338510556668806 , test loss:  0.12315930509583974\n",
      "Epoch:  9300 , step:  9300 , train loss:  0.1033384957896883 , test loss:  0.12311789511797244\n",
      "Epoch:  9310 , step:  9310 , train loss:  0.10329195711144533 , test loss:  0.12307655492516695\n",
      "Epoch:  9320 , step:  9320 , train loss:  0.1032454893514552 , test loss:  0.12303528434029212\n",
      "Epoch:  9330 , step:  9330 , train loss:  0.10319909232985319 , test loss:  0.12299408318683225\n",
      "Epoch:  9340 , step:  9340 , train loss:  0.10315276586741108 , test loss:  0.12295295128888452\n",
      "Epoch:  9350 , step:  9350 , train loss:  0.1031065097855342 , test loss:  0.12291188847115633\n",
      "Epoch:  9360 , step:  9360 , train loss:  0.10306032390625847 , test loss:  0.12287089455896223\n",
      "Epoch:  9370 , step:  9370 , train loss:  0.10301420805224759 , test loss:  0.12282996937822159\n",
      "Epoch:  9380 , step:  9380 , train loss:  0.10296816204679035 , test loss:  0.12278911275545566\n",
      "Epoch:  9390 , step:  9390 , train loss:  0.10292218571379748 , test loss:  0.12274832451778507\n",
      "Epoch:  9400 , step:  9400 , train loss:  0.10287627887779915 , test loss:  0.12270760449292709\n",
      "Epoch:  9410 , step:  9410 , train loss:  0.10283044136394198 , test loss:  0.1226669525091928\n",
      "Epoch:  9420 , step:  9420 , train loss:  0.10278467299798635 , test loss:  0.1226263683954849\n",
      "Epoch:  9430 , step:  9430 , train loss:  0.10273897360630356 , test loss:  0.12258585198129461\n",
      "Epoch:  9440 , step:  9440 , train loss:  0.10269334301587317 , test loss:  0.12254540309669933\n",
      "Epoch:  9450 , step:  9450 , train loss:  0.10264778105428017 , test loss:  0.12250502157236\n",
      "Epoch:  9460 , step:  9460 , train loss:  0.10260228754971233 , test loss:  0.12246470723951862\n",
      "Epoch:  9470 , step:  9470 , train loss:  0.10255686233095745 , test loss:  0.12242445992999532\n",
      "Epoch:  9480 , step:  9480 , train loss:  0.10251150522740073 , test loss:  0.12238427947618638\n",
      "Epoch:  9490 , step:  9490 , train loss:  0.10246621606902194 , test loss:  0.12234416571106127\n",
      "Epoch:  9500 , step:  9500 , train loss:  0.10242099468639296 , test loss:  0.1223041184681602\n",
      "Epoch:  9510 , step:  9510 , train loss:  0.10237584091067504 , test loss:  0.12226413758159164\n",
      "Epoch:  9520 , step:  9520 , train loss:  0.10233075457361611 , test loss:  0.12222422288602995\n",
      "Epoch:  9530 , step:  9530 , train loss:  0.10228573550754821 , test loss:  0.12218437421671283\n",
      "Epoch:  9540 , step:  9540 , train loss:  0.10224078354538496 , test loss:  0.12214459140943863\n",
      "Epoch:  9550 , step:  9550 , train loss:  0.10219589852061886 , test loss:  0.1221048743005643\n",
      "Epoch:  9560 , step:  9560 , train loss:  0.10215108026731874 , test loss:  0.12206522272700258\n",
      "Epoch:  9570 , step:  9570 , train loss:  0.10210632862012721 , test loss:  0.1220256365262198\n",
      "Epoch:  9580 , step:  9580 , train loss:  0.10206164341425815 , test loss:  0.12198611553623341\n",
      "Epoch:  9590 , step:  9590 , train loss:  0.10201702448549413 , test loss:  0.1219466595956095\n",
      "Epoch:  9600 , step:  9600 , train loss:  0.10197247167018389 , test loss:  0.12190726854346054\n",
      "Epoch:  9610 , step:  9610 , train loss:  0.10192798480523979 , test loss:  0.12186794221944298\n",
      "Epoch:  9620 , step:  9620 , train loss:  0.1018835637281355 , test loss:  0.12182868046375471\n",
      "Epoch:  9630 , step:  9630 , train loss:  0.10183920827690328 , test loss:  0.12178948311713289\n",
      "Epoch:  9640 , step:  9640 , train loss:  0.1017949182901316 , test loss:  0.12175035002085172\n",
      "Epoch:  9650 , step:  9650 , train loss:  0.10175069360696284 , test loss:  0.12171128101671982\n",
      "Epoch:  9660 , step:  9660 , train loss:  0.10170653406709061 , test loss:  0.12167227594707804\n",
      "Epoch:  9670 , step:  9670 , train loss:  0.10166243951075749 , test loss:  0.12163333465479725\n",
      "Epoch:  9680 , step:  9680 , train loss:  0.1016184097787525 , test loss:  0.12159445698327602\n",
      "Epoch:  9690 , step:  9690 , train loss:  0.10157444471240878 , test loss:  0.12155564277643827\n",
      "Epoch:  9700 , step:  9700 , train loss:  0.10153054415360124 , test loss:  0.12151689187873094\n",
      "Epoch:  9710 , step:  9710 , train loss:  0.10148670794474404 , test loss:  0.12147820413512202\n",
      "Epoch:  9720 , step:  9720 , train loss:  0.10144293592878839 , test loss:  0.12143957939109803\n",
      "Epoch:  9730 , step:  9730 , train loss:  0.10139922794922007 , test loss:  0.12140101749266194\n",
      "Epoch:  9740 , step:  9740 , train loss:  0.10135558385005713 , test loss:  0.1213625182863309\n",
      "Epoch:  9750 , step:  9750 , train loss:  0.10131200347584766 , test loss:  0.12132408161913405\n",
      "Epoch:  9760 , step:  9760 , train loss:  0.10126848667166737 , test loss:  0.12128570733861029\n",
      "Epoch:  9770 , step:  9770 , train loss:  0.10122503328311722 , test loss:  0.12124739529280612\n",
      "Epoch:  9780 , step:  9780 , train loss:  0.10118164315632137 , test loss:  0.12120914533027355\n",
      "Epoch:  9790 , step:  9790 , train loss:  0.10113831613792473 , test loss:  0.12117095730006791\n",
      "Epoch:  9800 , step:  9800 , train loss:  0.1010950520750907 , test loss:  0.12113283105174548\n",
      "Epoch:  9810 , step:  9810 , train loss:  0.10105185081549899 , test loss:  0.1210947664353617\n",
      "Epoch:  9820 , step:  9820 , train loss:  0.10100871220734328 , test loss:  0.12105676330146893\n",
      "Epoch:  9830 , step:  9830 , train loss:  0.10096563609932917 , test loss:  0.12101882150111412\n",
      "Epoch:  9840 , step:  9840 , train loss:  0.10092262234067172 , test loss:  0.12098094088583705\n",
      "Epoch:  9850 , step:  9850 , train loss:  0.10087967078109346 , test loss:  0.12094312130766793\n",
      "Epoch:  9860 , step:  9860 , train loss:  0.10083678127082206 , test loss:  0.12090536261912557\n",
      "Epoch:  9870 , step:  9870 , train loss:  0.10079395366058831 , test loss:  0.12086766467321522\n",
      "Epoch:  9880 , step:  9880 , train loss:  0.10075118780162369 , test loss:  0.12083002732342632\n",
      "Epoch:  9890 , step:  9890 , train loss:  0.10070848354565849 , test loss:  0.1207924504237309\n",
      "Epoch:  9900 , step:  9900 , train loss:  0.1006658407449195 , test loss:  0.12075493382858099\n",
      "Epoch:  9910 , step:  9910 , train loss:  0.1006232592521279 , test loss:  0.12071747739290711\n",
      "Epoch:  9920 , step:  9920 , train loss:  0.10058073892049714 , test loss:  0.12068008097211572\n",
      "Epoch:  9930 , step:  9930 , train loss:  0.10053827960373089 , test loss:  0.1206427444220879\n",
      "Epoch:  9940 , step:  9940 , train loss:  0.1004958811560209 , test loss:  0.12060546759917669\n",
      "Epoch:  9950 , step:  9950 , train loss:  0.10045354343204478 , test loss:  0.12056825036020542\n",
      "Epoch:  9960 , step:  9960 , train loss:  0.10041126628696417 , test loss:  0.12053109256246584\n",
      "Epoch:  9970 , step:  9970 , train loss:  0.10036904957642245 , test loss:  0.12049399406371572\n",
      "Epoch:  9980 , step:  9980 , train loss:  0.10032689315654283 , test loss:  0.12045695472217759\n",
      "Epoch:  9990 , step:  9990 , train loss:  0.10028479688392622 , test loss:  0.12041997439653614\n",
      "Epoch:  10000 , step:  10000 , train loss:  0.1002427606156492 , test loss:  0.12038305294593642\n",
      "Epoch:  10010 , step:  10010 , train loss:  0.100200784209262 , test loss:  0.12034619022998234\n",
      "Epoch:  10020 , step:  10020 , train loss:  0.10015886752278656 , test loss:  0.12030938610873429\n",
      "Epoch:  10030 , step:  10030 , train loss:  0.10011701041471441 , test loss:  0.12027264044270747\n",
      "Epoch:  10040 , step:  10040 , train loss:  0.10007521274400476 , test loss:  0.12023595309286982\n",
      "Epoch:  10050 , step:  10050 , train loss:  0.1000334743700824 , test loss:  0.12019932392064034\n",
      "Epoch:  10060 , step:  10060 , train loss:  0.09999179515283593 , test loss:  0.12016275278788696\n",
      "Epoch:  10070 , step:  10070 , train loss:  0.09995017495261563 , test loss:  0.12012623955692509\n",
      "Epoch:  10080 , step:  10080 , train loss:  0.09990861363023146 , test loss:  0.12008978409051525\n",
      "Epoch:  10090 , step:  10090 , train loss:  0.09986711104695133 , test loss:  0.12005338625186171\n",
      "Epoch:  10100 , step:  10100 , train loss:  0.09982566706449901 , test loss:  0.1200170459046102\n",
      "Epoch:  10110 , step:  10110 , train loss:  0.09978428154505224 , test loss:  0.11998076291284643\n",
      "Epoch:  10120 , step:  10120 , train loss:  0.09974295435124081 , test loss:  0.11994453714109403\n",
      "Epoch:  10130 , step:  10130 , train loss:  0.09970168534614461 , test loss:  0.119908368454313\n",
      "Epoch:  10140 , step:  10140 , train loss:  0.0996604743932919 , test loss:  0.11987225671789777\n",
      "Epoch:  10150 , step:  10150 , train loss:  0.09961932135665726 , test loss:  0.1198362017976752\n",
      "Epoch:  10160 , step:  10160 , train loss:  0.0995782261006598 , test loss:  0.11980020355990323\n",
      "Epoch:  10170 , step:  10170 , train loss:  0.0995371884901612 , test loss:  0.11976426187126872\n",
      "Epoch:  10180 , step:  10180 , train loss:  0.099496208390464 , test loss:  0.119728376598886\n",
      "Epoch:  10190 , step:  10190 , train loss:  0.09945528566730966 , test loss:  0.11969254761029478\n",
      "Epoch:  10200 , step:  10200 , train loss:  0.09941442018687673 , test loss:  0.11965677477345875\n",
      "Epoch:  10210 , step:  10210 , train loss:  0.09937361181577899 , test loss:  0.11962105795676356\n",
      "Epoch:  10220 , step:  10220 , train loss:  0.09933286042106379 , test loss:  0.11958539702901529\n",
      "Epoch:  10230 , step:  10230 , train loss:  0.09929216587021002 , test loss:  0.11954979185943854\n",
      "Epoch:  10240 , step:  10240 , train loss:  0.09925152803112651 , test loss:  0.119514242317675\n",
      "Epoch:  10250 , step:  10250 , train loss:  0.09921094677215007 , test loss:  0.11947874827378153\n",
      "Epoch:  10260 , step:  10260 , train loss:  0.09917042196204386 , test loss:  0.11944330959822834\n",
      "Epoch:  10270 , step:  10270 , train loss:  0.09912995346999547 , test loss:  0.1194079261618977\n",
      "Epoch:  10280 , step:  10280 , train loss:  0.09908954116561532 , test loss:  0.11937259783608184\n",
      "Epoch:  10290 , step:  10290 , train loss:  0.0990491849189348 , test loss:  0.11933732449248173\n",
      "Epoch:  10300 , step:  10300 , train loss:  0.09900888460040447 , test loss:  0.11930210600320497\n",
      "Epoch:  10310 , step:  10310 , train loss:  0.09896864008089254 , test loss:  0.11926694224076438\n",
      "Epoch:  10320 , step:  10320 , train loss:  0.09892845123168291 , test loss:  0.11923183307807642\n",
      "Epoch:  10330 , step:  10330 , train loss:  0.09888831792447353 , test loss:  0.11919677838845924\n",
      "Epoch:  10340 , step:  10340 , train loss:  0.09884824003137477 , test loss:  0.11916177804563152\n",
      "Epoch:  10350 , step:  10350 , train loss:  0.09880821742490764 , test loss:  0.11912683192371057\n",
      "Epoch:  10360 , step:  10360 , train loss:  0.0987682499780021 , test loss:  0.11909193989721044\n",
      "Epoch:  10370 , step:  10370 , train loss:  0.09872833756399539 , test loss:  0.11905710184104086\n",
      "Epoch:  10380 , step:  10380 , train loss:  0.09868848005663036 , test loss:  0.11902231763050544\n",
      "Epoch:  10390 , step:  10390 , train loss:  0.09864867733005378 , test loss:  0.11898758714130006\n",
      "Epoch:  10400 , step:  10400 , train loss:  0.09860892925881475 , test loss:  0.1189529102495111\n",
      "Epoch:  10410 , step:  10410 , train loss:  0.09856923571786291 , test loss:  0.11891828683161426\n",
      "Epoch:  10420 , step:  10420 , train loss:  0.09852959658254698 , test loss:  0.11888371676447261\n",
      "Epoch:  10430 , step:  10430 , train loss:  0.09849001172861295 , test loss:  0.11884919992533528\n",
      "Epoch:  10440 , step:  10440 , train loss:  0.09845048103220262 , test loss:  0.11881473619183586\n",
      "Epoch:  10450 , step:  10450 , train loss:  0.09841100436985181 , test loss:  0.11878032544199095\n",
      "Epoch:  10460 , step:  10460 , train loss:  0.0983715816184889 , test loss:  0.11874596755419815\n",
      "Epoch:  10470 , step:  10470 , train loss:  0.09833221265543315 , test loss:  0.11871166240723535\n",
      "Epoch:  10480 , step:  10480 , train loss:  0.09829289735839317 , test loss:  0.11867740988025849\n",
      "Epoch:  10490 , step:  10490 , train loss:  0.09825363560546517 , test loss:  0.1186432098528004\n",
      "Epoch:  10500 , step:  10500 , train loss:  0.09821442727513158 , test loss:  0.11860906220476918\n",
      "Epoch:  10510 , step:  10510 , train loss:  0.09817527224625945 , test loss:  0.11857496681644686\n",
      "Epoch:  10520 , step:  10520 , train loss:  0.09813617039809876 , test loss:  0.11854092356848762\n",
      "Epoch:  10530 , step:  10530 , train loss:  0.09809712161028095 , test loss:  0.11850693234191667\n",
      "Epoch:  10540 , step:  10540 , train loss:  0.0980581257628174 , test loss:  0.11847299301812846\n",
      "Epoch:  10550 , step:  10550 , train loss:  0.09801918273609789 , test loss:  0.11843910547888536\n",
      "Epoch:  10560 , step:  10560 , train loss:  0.09798029241088896 , test loss:  0.1184052696063163\n",
      "Epoch:  10570 , step:  10570 , train loss:  0.0979414546683325 , test loss:  0.11837148528291505\n",
      "Epoch:  10580 , step:  10580 , train loss:  0.09790266938994423 , test loss:  0.1183377523915389\n",
      "Epoch:  10590 , step:  10590 , train loss:  0.0978639364576121 , test loss:  0.11830407081540753\n",
      "Epoch:  10600 , step:  10600 , train loss:  0.09782525575359494 , test loss:  0.11827044043810096\n",
      "Epoch:  10610 , step:  10610 , train loss:  0.09778662716052072 , test loss:  0.11823686114355865\n",
      "Epoch:  10620 , step:  10620 , train loss:  0.09774805056138543 , test loss:  0.11820333281607792\n",
      "Epoch:  10630 , step:  10630 , train loss:  0.0977095258395512 , test loss:  0.11816985534031231\n",
      "Epoch:  10640 , step:  10640 , train loss:  0.09767105287874511 , test loss:  0.11813642860127066\n",
      "Epoch:  10650 , step:  10650 , train loss:  0.09763263156305763 , test loss:  0.11810305248431525\n",
      "Epoch:  10660 , step:  10660 , train loss:  0.09759426177694115 , test loss:  0.11806972687516068\n",
      "Epoch:  10670 , step:  10670 , train loss:  0.09755594340520854 , test loss:  0.11803645165987228\n",
      "Epoch:  10680 , step:  10680 , train loss:  0.09751767633303174 , test loss:  0.11800322672486496\n",
      "Epoch:  10690 , step:  10690 , train loss:  0.09747946044594027 , test loss:  0.11797005195690174\n",
      "Epoch:  10700 , step:  10700 , train loss:  0.09744129562981982 , test loss:  0.11793692724309235\n",
      "Epoch:  10710 , step:  10710 , train loss:  0.09740318177091087 , test loss:  0.11790385247089179\n",
      "Epoch:  10720 , step:  10720 , train loss:  0.09736511875580721 , test loss:  0.11787082752809944\n",
      "Epoch:  10730 , step:  10730 , train loss:  0.09732710647145457 , test loss:  0.11783785230285689\n",
      "Epoch:  10740 , step:  10740 , train loss:  0.09728914480514923 , test loss:  0.11780492668364743\n",
      "Epoch:  10750 , step:  10750 , train loss:  0.09725123364453654 , test loss:  0.11777205055929427\n",
      "Epoch:  10760 , step:  10760 , train loss:  0.0972133728776097 , test loss:  0.1177392238189593\n",
      "Epoch:  10770 , step:  10770 , train loss:  0.09717556239270819 , test loss:  0.11770644635214177\n",
      "Epoch:  10780 , step:  10780 , train loss:  0.09713780207851652 , test loss:  0.11767371804867703\n",
      "Epoch:  10790 , step:  10790 , train loss:  0.09710009182406279 , test loss:  0.11764103879873525\n",
      "Epoch:  10800 , step:  10800 , train loss:  0.09706243151871746 , test loss:  0.11760840849281978\n",
      "Epoch:  10810 , step:  10810 , train loss:  0.09702482105219176 , test loss:  0.11757582702176661\n",
      "Epoch:  10820 , step:  10820 , train loss:  0.09698726031453657 , test loss:  0.11754329427674222\n",
      "Epoch:  10830 , step:  10830 , train loss:  0.096949749196141 , test loss:  0.11751081014924278\n",
      "Epoch:  10840 , step:  10840 , train loss:  0.09691228758773099 , test loss:  0.11747837453109289\n",
      "Epoch:  10850 , step:  10850 , train loss:  0.09687487538036803 , test loss:  0.11744598731444396\n",
      "Epoch:  10860 , step:  10860 , train loss:  0.09683751246544794 , test loss:  0.11741364839177348\n",
      "Epoch:  10870 , step:  10870 , train loss:  0.09680019873469933 , test loss:  0.11738135765588335\n",
      "Epoch:  10880 , step:  10880 , train loss:  0.09676293408018254 , test loss:  0.11734911499989872\n",
      "Epoch:  10890 , step:  10890 , train loss:  0.09672571839428805 , test loss:  0.11731692031726683\n",
      "Epoch:  10900 , step:  10900 , train loss:  0.09668855156973556 , test loss:  0.1172847735017556\n",
      "Epoch:  10910 , step:  10910 , train loss:  0.09665143349957235 , test loss:  0.11725267444745273\n",
      "Epoch:  10920 , step:  10920 , train loss:  0.09661436407717218 , test loss:  0.11722062304876406\n",
      "Epoch:  10930 , step:  10930 , train loss:  0.09657734319623391 , test loss:  0.11718861920041257\n",
      "Epoch:  10940 , step:  10940 , train loss:  0.09654037075078035 , test loss:  0.1171566627974373\n",
      "Epoch:  10950 , step:  10950 , train loss:  0.09650344663515688 , test loss:  0.11712475373519181\n",
      "Epoch:  10960 , step:  10960 , train loss:  0.09646657074403027 , test loss:  0.11709289190934315\n",
      "Epoch:  10970 , step:  10970 , train loss:  0.09642974297238739 , test loss:  0.11706107721587079\n",
      "Epoch:  10980 , step:  10980 , train loss:  0.09639296321553394 , test loss:  0.11702930955106515\n",
      "Epoch:  10990 , step:  10990 , train loss:  0.09635623136909321 , test loss:  0.11699758881152658\n",
      "Epoch:  11000 , step:  11000 , train loss:  0.09631954732900497 , test loss:  0.11696591489416427\n",
      "Epoch:  11010 , step:  11010 , train loss:  0.09628291099152404 , test loss:  0.11693428769619486\n",
      "Epoch:  11020 , step:  11020 , train loss:  0.09624632225321919 , test loss:  0.11690270711514121\n",
      "Epoch:  11030 , step:  11030 , train loss:  0.09620978101097186 , test loss:  0.11687117304883168\n",
      "Epoch:  11040 , step:  11040 , train loss:  0.09617328716197504 , test loss:  0.11683968539539855\n",
      "Epoch:  11050 , step:  11050 , train loss:  0.09613684060373195 , test loss:  0.11680824405327697\n",
      "Epoch:  11060 , step:  11060 , train loss:  0.09610044123405496 , test loss:  0.11677684892120382\n",
      "Epoch:  11070 , step:  11070 , train loss:  0.09606408895106427 , test loss:  0.11674549989821661\n",
      "Epoch:  11080 , step:  11080 , train loss:  0.09602778365318672 , test loss:  0.1167141968836524\n",
      "Epoch:  11090 , step:  11090 , train loss:  0.09599152523915482 , test loss:  0.11668293977714639\n",
      "Epoch:  11100 , step:  11100 , train loss:  0.09595531360800523 , test loss:  0.11665172847863108\n",
      "Epoch:  11110 , step:  11110 , train loss:  0.09591914865907791 , test loss:  0.11662056288833503\n",
      "Epoch:  11120 , step:  11120 , train loss:  0.09588303029201474 , test loss:  0.11658944290678186\n",
      "Epoch:  11130 , step:  11130 , train loss:  0.09584695840675847 , test loss:  0.11655836843478894\n",
      "Epoch:  11140 , step:  11140 , train loss:  0.09581093290355146 , test loss:  0.11652733937346632\n",
      "Epoch:  11150 , step:  11150 , train loss:  0.09577495368293465 , test loss:  0.11649635562421581\n",
      "Epoch:  11160 , step:  11160 , train loss:  0.09573902064574631 , test loss:  0.11646541708872968\n",
      "Epoch:  11170 , step:  11170 , train loss:  0.09570313369312097 , test loss:  0.11643452366898964\n",
      "Epoch:  11180 , step:  11180 , train loss:  0.09566729272648827 , test loss:  0.1164036752672658\n",
      "Epoch:  11190 , step:  11190 , train loss:  0.09563149764757178 , test loss:  0.11637287178611565\n",
      "Epoch:  11200 , step:  11200 , train loss:  0.09559574835838798 , test loss:  0.11634211312838258\n",
      "Epoch:  11210 , step:  11210 , train loss:  0.09556004476124497 , test loss:  0.11631139919719535\n",
      "Epoch:  11220 , step:  11220 , train loss:  0.09552438675874153 , test loss:  0.11628072989596677\n",
      "Epoch:  11230 , step:  11230 , train loss:  0.09548877425376597 , test loss:  0.11625010512839268\n",
      "Epoch:  11240 , step:  11240 , train loss:  0.0954532071494949 , test loss:  0.1162195247984506\n",
      "Epoch:  11250 , step:  11250 , train loss:  0.09541768534939235 , test loss:  0.11618898881039921\n",
      "Epoch:  11260 , step:  11260 , train loss:  0.09538220875720842 , test loss:  0.11615849706877696\n",
      "Epoch:  11270 , step:  11270 , train loss:  0.09534677727697843 , test loss:  0.11612804947840115\n",
      "Epoch:  11280 , step:  11280 , train loss:  0.09531139081302172 , test loss:  0.1160976459443667\n",
      "Epoch:  11290 , step:  11290 , train loss:  0.0952760492699405 , test loss:  0.11606728637204518\n",
      "Epoch:  11300 , step:  11300 , train loss:  0.09524075255261899 , test loss:  0.11603697066708421\n",
      "Epoch:  11310 , step:  11310 , train loss:  0.09520550056622212 , test loss:  0.11600669873540576\n",
      "Epoch:  11320 , step:  11320 , train loss:  0.09517029321619463 , test loss:  0.11597647048320563\n",
      "Epoch:  11330 , step:  11330 , train loss:  0.09513513040825991 , test loss:  0.11594628581695218\n",
      "Epoch:  11340 , step:  11340 , train loss:  0.09510001204841903 , test loss:  0.11591614464338533\n",
      "Epoch:  11350 , step:  11350 , train loss:  0.09506493804294959 , test loss:  0.1158860468695158\n",
      "Epoch:  11360 , step:  11360 , train loss:  0.09502990829840476 , test loss:  0.11585599240262388\n",
      "Epoch:  11370 , step:  11370 , train loss:  0.09499492272161227 , test loss:  0.1158259811502583\n",
      "Epoch:  11380 , step:  11380 , train loss:  0.09495998121967321 , test loss:  0.11579601302023573\n",
      "Epoch:  11390 , step:  11390 , train loss:  0.09492508369996122 , test loss:  0.11576608792063911\n",
      "Epoch:  11400 , step:  11400 , train loss:  0.0948902300701213 , test loss:  0.11573620575981743\n",
      "Epoch:  11410 , step:  11410 , train loss:  0.09485542023806885 , test loss:  0.11570636644638413\n",
      "Epoch:  11420 , step:  11420 , train loss:  0.09482065411198866 , test loss:  0.11567656988921639\n",
      "Epoch:  11430 , step:  11430 , train loss:  0.09478593160033388 , test loss:  0.1156468159974541\n",
      "Epoch:  11440 , step:  11440 , train loss:  0.09475125261182503 , test loss:  0.11561710468049913\n",
      "Epoch:  11450 , step:  11450 , train loss:  0.09471661705544901 , test loss:  0.11558743584801388\n",
      "Epoch:  11460 , step:  11460 , train loss:  0.09468202484045803 , test loss:  0.11555780940992082\n",
      "Epoch:  11470 , step:  11470 , train loss:  0.09464747587636876 , test loss:  0.11552822527640118\n",
      "Epoch:  11480 , step:  11480 , train loss:  0.0946129700729612 , test loss:  0.1154986833578942\n",
      "Epoch:  11490 , step:  11490 , train loss:  0.09457850734027767 , test loss:  0.11546918356509617\n",
      "Epoch:  11500 , step:  11500 , train loss:  0.09454408758862212 , test loss:  0.11543972580895938\n",
      "Epoch:  11510 , step:  11510 , train loss:  0.09450971072855875 , test loss:  0.11541031000069135\n",
      "Epoch:  11520 , step:  11520 , train loss:  0.09447537667091135 , test loss:  0.1153809360517537\n",
      "Epoch:  11530 , step:  11530 , train loss:  0.09444108532676217 , test loss:  0.11535160387386134\n",
      "Epoch:  11540 , step:  11540 , train loss:  0.09440683660745103 , test loss:  0.11532231337898168\n",
      "Epoch:  11550 , step:  11550 , train loss:  0.09437263042457435 , test loss:  0.11529306447933332\n",
      "Epoch:  11560 , step:  11560 , train loss:  0.09433846668998416 , test loss:  0.1152638570873856\n",
      "Epoch:  11570 , step:  11570 , train loss:  0.09430434531578727 , test loss:  0.11523469111585742\n",
      "Epoch:  11580 , step:  11580 , train loss:  0.09427026621434412 , test loss:  0.11520556647771626\n",
      "Epoch:  11590 , step:  11590 , train loss:  0.09423622929826805 , test loss:  0.11517648308617756\n",
      "Epoch:  11600 , step:  11600 , train loss:  0.09420223448042424 , test loss:  0.11514744085470355\n",
      "Epoch:  11610 , step:  11610 , train loss:  0.09416828167392881 , test loss:  0.11511843969700246\n",
      "Epoch:  11620 , step:  11620 , train loss:  0.09413437079214791 , test loss:  0.11508947952702782\n",
      "Epoch:  11630 , step:  11630 , train loss:  0.09410050174869677 , test loss:  0.1150605602589772\n",
      "Epoch:  11640 , step:  11640 , train loss:  0.09406667445743883 , test loss:  0.11503168180729154\n",
      "Epoch:  11650 , step:  11650 , train loss:  0.09403288883248474 , test loss:  0.11500284408665429\n",
      "Epoch:  11660 , step:  11660 , train loss:  0.09399914478819156 , test loss:  0.11497404701199052\n",
      "Epoch:  11670 , step:  11670 , train loss:  0.09396544223916174 , test loss:  0.11494529049846587\n",
      "Epoch:  11680 , step:  11680 , train loss:  0.09393178110024229 , test loss:  0.11491657446148613\n",
      "Epoch:  11690 , step:  11690 , train loss:  0.09389816128652388 , test loss:  0.11488789881669581\n",
      "Epoch:  11700 , step:  11700 , train loss:  0.09386458271333992 , test loss:  0.11485926347997756\n",
      "Epoch:  11710 , step:  11710 , train loss:  0.09383104529626567 , test loss:  0.11483066836745165\n",
      "Epoch:  11720 , step:  11720 , train loss:  0.09379754895111737 , test loss:  0.1148021133954742\n",
      "Epoch:  11730 , step:  11730 , train loss:  0.0937640935939514 , test loss:  0.11477359848063741\n",
      "Epoch:  11740 , step:  11740 , train loss:  0.09373067914106326 , test loss:  0.11474512353976798\n",
      "Epoch:  11750 , step:  11750 , train loss:  0.09369730550898683 , test loss:  0.11471668848992662\n",
      "Epoch:  11760 , step:  11760 , train loss:  0.09366397261449354 , test loss:  0.11468829324840683\n",
      "Epoch:  11770 , step:  11770 , train loss:  0.09363068037459127 , test loss:  0.11465993773273463\n",
      "Epoch:  11780 , step:  11780 , train loss:  0.09359742870652375 , test loss:  0.11463162186066729\n",
      "Epoch:  11790 , step:  11790 , train loss:  0.09356421752776958 , test loss:  0.11460334555019262\n",
      "Epoch:  11800 , step:  11800 , train loss:  0.09353104675604139 , test loss:  0.11457510871952835\n",
      "Epoch:  11810 , step:  11810 , train loss:  0.0934979163092849 , test loss:  0.11454691128712086\n",
      "Epoch:  11820 , step:  11820 , train loss:  0.09346482610567827 , test loss:  0.1145187531716448\n",
      "Epoch:  11830 , step:  11830 , train loss:  0.09343177606363107 , test loss:  0.1144906342920021\n",
      "Epoch:  11840 , step:  11840 , train loss:  0.09339876610178356 , test loss:  0.11446255456732111\n",
      "Epoch:  11850 , step:  11850 , train loss:  0.09336579613900577 , test loss:  0.11443451391695605\n",
      "Epoch:  11860 , step:  11860 , train loss:  0.09333286609439677 , test loss:  0.1144065122604857\n",
      "Epoch:  11870 , step:  11870 , train loss:  0.09329997588728368 , test loss:  0.11437854951771319\n",
      "Epoch:  11880 , step:  11880 , train loss:  0.093267125437221 , test loss:  0.11435062560866482\n",
      "Epoch:  11890 , step:  11890 , train loss:  0.09323431466398978 , test loss:  0.1143227404535894\n",
      "Epoch:  11900 , step:  11900 , train loss:  0.09320154348759664 , test loss:  0.1142948939729575\n",
      "Epoch:  11910 , step:  11910 , train loss:  0.09316881182827315 , test loss:  0.11426708608746051\n",
      "Epoch:  11920 , step:  11920 , train loss:  0.09313611960647492 , test loss:  0.11423931671800999\n",
      "Epoch:  11930 , step:  11930 , train loss:  0.09310346674288077 , test loss:  0.11421158578573702\n",
      "Epoch:  11940 , step:  11940 , train loss:  0.09307085315839203 , test loss:  0.11418389321199109\n",
      "Epoch:  11950 , step:  11950 , train loss:  0.09303827877413165 , test loss:  0.11415623891833948\n",
      "Epoch:  11960 , step:  11960 , train loss:  0.09300574351144344 , test loss:  0.11412862282656674\n",
      "Epoch:  11970 , step:  11970 , train loss:  0.09297324729189123 , test loss:  0.11410104485867349\n",
      "Epoch:  11980 , step:  11980 , train loss:  0.09294079003725818 , test loss:  0.11407350493687597\n",
      "Epoch:  11990 , step:  11990 , train loss:  0.09290837166954588 , test loss:  0.11404600298360526\n",
      "Epoch:  12000 , step:  12000 , train loss:  0.09287599211097365 , test loss:  0.1140185389215063\n",
      "Epoch:  12010 , step:  12010 , train loss:  0.09284365128397769 , test loss:  0.11399111267343744\n",
      "Epoch:  12020 , step:  12020 , train loss:  0.0928113491112104 , test loss:  0.11396372416246946\n",
      "Epoch:  12030 , step:  12030 , train loss:  0.09277908551553952 , test loss:  0.11393637331188493\n",
      "Epoch:  12040 , step:  12040 , train loss:  0.09274686042004741 , test loss:  0.11390906004517741\n",
      "Epoch:  12050 , step:  12050 , train loss:  0.09271467374803026 , test loss:  0.11388178428605092\n",
      "Epoch:  12060 , step:  12060 , train loss:  0.09268252542299732 , test loss:  0.11385454595841879\n",
      "Epoch:  12070 , step:  12070 , train loss:  0.09265041536867016 , test loss:  0.11382734498640341\n",
      "Epoch:  12080 , step:  12080 , train loss:  0.09261834350898193 , test loss:  0.11380018129433514\n",
      "Epoch:  12090 , step:  12090 , train loss:  0.09258630976807661 , test loss:  0.11377305480675166\n",
      "Epoch:  12100 , step:  12100 , train loss:  0.09255431407030816 , test loss:  0.11374596544839745\n",
      "Epoch:  12110 , step:  12110 , train loss:  0.09252235634023993 , test loss:  0.11371891314422282\n",
      "Epoch:  12120 , step:  12120 , train loss:  0.09249043650264382 , test loss:  0.11369189781938328\n",
      "Epoch:  12130 , step:  12130 , train loss:  0.09245855448249954 , test loss:  0.11366491939923891\n",
      "Epoch:  12140 , step:  12140 , train loss:  0.0924267102049939 , test loss:  0.11363797780935353\n",
      "Epoch:  12150 , step:  12150 , train loss:  0.09239490359552008 , test loss:  0.11361107297549403\n",
      "Epoch:  12160 , step:  12160 , train loss:  0.09236313457967692 , test loss:  0.1135842048236298\n",
      "Epoch:  12170 , step:  12170 , train loss:  0.0923314030832681 , test loss:  0.11355737327993172\n",
      "Epoch:  12180 , step:  12180 , train loss:  0.09229970903230154 , test loss:  0.11353057827077169\n",
      "Epoch:  12190 , step:  12190 , train loss:  0.0922680523529886 , test loss:  0.11350381972272196\n",
      "Epoch:  12200 , step:  12200 , train loss:  0.09223643297174337 , test loss:  0.11347709756255447\n",
      "Epoch:  12210 , step:  12210 , train loss:  0.09220485081518201 , test loss:  0.11345041171723984\n",
      "Epoch:  12220 , step:  12220 , train loss:  0.09217330581012197 , test loss:  0.11342376211394696\n",
      "Epoch:  12230 , step:  12230 , train loss:  0.09214179788358128 , test loss:  0.1133971486800423\n",
      "Epoch:  12240 , step:  12240 , train loss:  0.09211032696277793 , test loss:  0.11337057134308916\n",
      "Epoch:  12250 , step:  12250 , train loss:  0.09207889297512913 , test loss:  0.11334403003084706\n",
      "Epoch:  12260 , step:  12260 , train loss:  0.09204749584825046 , test loss:  0.11331752467127093\n",
      "Epoch:  12270 , step:  12270 , train loss:  0.09201613550995547 , test loss:  0.1132910551925106\n",
      "Epoch:  12280 , step:  12280 , train loss:  0.09198481188825475 , test loss:  0.11326462152290995\n",
      "Epoch:  12290 , step:  12290 , train loss:  0.0919535249113553 , test loss:  0.1132382235910065\n",
      "Epoch:  12300 , step:  12300 , train loss:  0.09192227450765991 , test loss:  0.1132118613255304\n",
      "Epoch:  12310 , step:  12310 , train loss:  0.09189106060576635 , test loss:  0.11318553465540415\n",
      "Epoch:  12320 , step:  12320 , train loss:  0.09185988313446686 , test loss:  0.11315924350974163\n",
      "Epoch:  12330 , step:  12330 , train loss:  0.09182874202274725 , test loss:  0.1131329878178476\n",
      "Epoch:  12340 , step:  12340 , train loss:  0.09179763719978647 , test loss:  0.11310676750921698\n",
      "Epoch:  12350 , step:  12350 , train loss:  0.09176656859495573 , test loss:  0.11308058251353434\n",
      "Epoch:  12360 , step:  12360 , train loss:  0.09173553613781797 , test loss:  0.11305443276067312\n",
      "Epoch:  12370 , step:  12370 , train loss:  0.09170453975812709 , test loss:  0.11302831818069488\n",
      "Epoch:  12380 , step:  12380 , train loss:  0.09167357938582736 , test loss:  0.1130022387038489\n",
      "Epoch:  12390 , step:  12390 , train loss:  0.09164265495105274 , test loss:  0.11297619426057141\n",
      "Epoch:  12400 , step:  12400 , train loss:  0.09161176638412621 , test loss:  0.11295018478148502\n",
      "Epoch:  12410 , step:  12410 , train loss:  0.0915809136155591 , test loss:  0.112924210197398\n",
      "Epoch:  12420 , step:  12420 , train loss:  0.0915500965760505 , test loss:  0.11289827043930366\n",
      "Epoch:  12430 , step:  12430 , train loss:  0.09151931519648651 , test loss:  0.11287236543837968\n",
      "Epoch:  12440 , step:  12440 , train loss:  0.09148856940793967 , test loss:  0.11284649512598778\n",
      "Epoch:  12450 , step:  12450 , train loss:  0.0914578591416683 , test loss:  0.11282065943367275\n",
      "Epoch:  12460 , step:  12460 , train loss:  0.09142718432911588 , test loss:  0.11279485829316184\n",
      "Epoch:  12470 , step:  12470 , train loss:  0.0913965449019103 , test loss:  0.11276909163636435\n",
      "Epoch:  12480 , step:  12480 , train loss:  0.09136594079186343 , test loss:  0.11274335939537103\n",
      "Epoch:  12490 , step:  12490 , train loss:  0.09133537193097022 , test loss:  0.11271766150245305\n",
      "Epoch:  12500 , step:  12500 , train loss:  0.09130483825140834 , test loss:  0.11269199789006208\n",
      "Epoch:  12510 , step:  12510 , train loss:  0.0912743396855373 , test loss:  0.11266636849082898\n",
      "Epoch:  12520 , step:  12520 , train loss:  0.09124387616589806 , test loss:  0.11264077323756358\n",
      "Epoch:  12530 , step:  12530 , train loss:  0.09121344762521225 , test loss:  0.1126152120632541\n",
      "Epoch:  12540 , step:  12540 , train loss:  0.09118305399638156 , test loss:  0.11258968490106636\n",
      "Epoch:  12550 , step:  12550 , train loss:  0.09115269521248719 , test loss:  0.11256419168434331\n",
      "Epoch:  12560 , step:  12560 , train loss:  0.09112237120678916 , test loss:  0.1125387323466044\n",
      "Epoch:  12570 , step:  12570 , train loss:  0.09109208191272583 , test loss:  0.11251330682154485\n",
      "Epoch:  12580 , step:  12580 , train loss:  0.09106182726391306 , test loss:  0.11248791504303554\n",
      "Epoch:  12590 , step:  12590 , train loss:  0.09103160719414387 , test loss:  0.11246255694512172\n",
      "Epoch:  12600 , step:  12600 , train loss:  0.09100142163738761 , test loss:  0.11243723246202293\n",
      "Epoch:  12610 , step:  12610 , train loss:  0.09097127052778949 , test loss:  0.1124119415281323\n",
      "Epoch:  12620 , step:  12620 , train loss:  0.09094115379966991 , test loss:  0.11238668407801577\n",
      "Epoch:  12630 , step:  12630 , train loss:  0.09091107138752395 , test loss:  0.11236146004641187\n",
      "Epoch:  12640 , step:  12640 , train loss:  0.0908810232260207 , test loss:  0.11233626936823082\n",
      "Epoch:  12650 , step:  12650 , train loss:  0.09085100925000258 , test loss:  0.11231111197855415\n",
      "Epoch:  12660 , step:  12660 , train loss:  0.090821029394485 , test loss:  0.1122859878126341\n",
      "Epoch:  12670 , step:  12670 , train loss:  0.09079108359465558 , test loss:  0.11226089680589281\n",
      "Epoch:  12680 , step:  12680 , train loss:  0.09076117178587358 , test loss:  0.1122358388939222\n",
      "Epoch:  12690 , step:  12690 , train loss:  0.09073129390366934 , test loss:  0.11221081401248313\n",
      "Epoch:  12700 , step:  12700 , train loss:  0.09070144988374375 , test loss:  0.11218582209750466\n",
      "Epoch:  12710 , step:  12710 , train loss:  0.09067163966196762 , test loss:  0.11216086308508394\n",
      "Epoch:  12720 , step:  12720 , train loss:  0.09064186317438111 , test loss:  0.1121359369114851\n",
      "Epoch:  12730 , step:  12730 , train loss:  0.09061212035719313 , test loss:  0.1121110435131395\n",
      "Epoch:  12740 , step:  12740 , train loss:  0.09058241114678083 , test loss:  0.11208618282664419\n",
      "Epoch:  12750 , step:  12750 , train loss:  0.090552735479689 , test loss:  0.11206135478876214\n",
      "Epoch:  12760 , step:  12760 , train loss:  0.0905230932926295 , test loss:  0.11203655933642118\n",
      "Epoch:  12770 , step:  12770 , train loss:  0.0904934845224807 , test loss:  0.11201179640671392\n",
      "Epoch:  12780 , step:  12780 , train loss:  0.09046390910628693 , test loss:  0.11198706593689679\n",
      "Epoch:  12790 , step:  12790 , train loss:  0.09043436698125787 , test loss:  0.1119623678643897\n",
      "Epoch:  12800 , step:  12800 , train loss:  0.09040485808476809 , test loss:  0.11193770212677542\n",
      "Epoch:  12810 , step:  12810 , train loss:  0.0903753823543564 , test loss:  0.11191306866179929\n",
      "Epoch:  12820 , step:  12820 , train loss:  0.09034593972772531 , test loss:  0.11188846740736821\n",
      "Epoch:  12830 , step:  12830 , train loss:  0.09031653014274056 , test loss:  0.11186389830155054\n",
      "Epoch:  12840 , step:  12840 , train loss:  0.09028715353743051 , test loss:  0.11183936128257535\n",
      "Epoch:  12850 , step:  12850 , train loss:  0.09025780984998559 , test loss:  0.111814856288832\n",
      "Epoch:  12860 , step:  12860 , train loss:  0.09022849901875774 , test loss:  0.11179038325886942\n",
      "Epoch:  12870 , step:  12870 , train loss:  0.09019922098225992 , test loss:  0.11176594213139586\n",
      "Epoch:  12880 , step:  12880 , train loss:  0.0901699756791656 , test loss:  0.11174153284527832\n",
      "Epoch:  12890 , step:  12890 , train loss:  0.09014076304830805 , test loss:  0.11171715533954163\n",
      "Epoch:  12900 , step:  12900 , train loss:  0.09011158302868004 , test loss:  0.1116928095533685\n",
      "Epoch:  12910 , step:  12910 , train loss:  0.09008243555943313 , test loss:  0.11166849542609868\n",
      "Epoch:  12920 , step:  12920 , train loss:  0.09005332057987726 , test loss:  0.11164421289722845\n",
      "Epoch:  12930 , step:  12930 , train loss:  0.09002423802948013 , test loss:  0.11161996190641028\n",
      "Epoch:  12940 , step:  12940 , train loss:  0.08999518784786666 , test loss:  0.11159574239345202\n",
      "Epoch:  12950 , step:  12950 , train loss:  0.0899661699748186 , test loss:  0.11157155429831676\n",
      "Epoch:  12960 , step:  12960 , train loss:  0.0899371843502739 , test loss:  0.1115473975611221\n",
      "Epoch:  12970 , step:  12970 , train loss:  0.08990823091432619 , test loss:  0.11152327212213958\n",
      "Epoch:  12980 , step:  12980 , train loss:  0.08987930960722433 , test loss:  0.11149917792179448\n",
      "Epoch:  12990 , step:  12990 , train loss:  0.08985042036937181 , test loss:  0.11147511490066485\n",
      "Epoch:  13000 , step:  13000 , train loss:  0.08982156314132632 , test loss:  0.11145108299948142\n",
      "Epoch:  13010 , step:  13010 , train loss:  0.0897927378637991 , test loss:  0.11142708215912715\n",
      "Epoch:  13020 , step:  13020 , train loss:  0.08976394447765469 , test loss:  0.1114031123206362\n",
      "Epoch:  13030 , step:  13030 , train loss:  0.08973518292391013 , test loss:  0.11137917342519413\n",
      "Epoch:  13040 , step:  13040 , train loss:  0.08970645314373463 , test loss:  0.11135526541413675\n",
      "Epoch:  13050 , step:  13050 , train loss:  0.08967775507844904 , test loss:  0.11133138822895015\n",
      "Epoch:  13060 , step:  13060 , train loss:  0.0896490886695253 , test loss:  0.11130754181127007\n",
      "Epoch:  13070 , step:  13070 , train loss:  0.089620453858586 , test loss:  0.11128372610288116\n",
      "Epoch:  13080 , step:  13080 , train loss:  0.08959185058740386 , test loss:  0.11125994104571678\n",
      "Epoch:  13090 , step:  13090 , train loss:  0.08956327879790119 , test loss:  0.1112361865818585\n",
      "Epoch:  13100 , step:  13100 , train loss:  0.0895347384321495 , test loss:  0.11121246265353549\n",
      "Epoch:  13110 , step:  13110 , train loss:  0.08950622943236893 , test loss:  0.11118876920312408\n",
      "Epoch:  13120 , step:  13120 , train loss:  0.08947775174092767 , test loss:  0.11116510617314732\n",
      "Epoch:  13130 , step:  13130 , train loss:  0.0894493053003418 , test loss:  0.11114147350627464\n",
      "Epoch:  13140 , step:  13140 , train loss:  0.0894208900532744 , test loss:  0.11111787114532103\n",
      "Epoch:  13150 , step:  13150 , train loss:  0.08939250594253535 , test loss:  0.1110942990332468\n",
      "Epoch:  13160 , step:  13160 , train loss:  0.08936415291108067 , test loss:  0.1110707571131573\n",
      "Epoch:  13170 , step:  13170 , train loss:  0.08933583090201225 , test loss:  0.11104724532830196\n",
      "Epoch:  13180 , step:  13180 , train loss:  0.08930753985857715 , test loss:  0.1110237636220744\n",
      "Epoch:  13190 , step:  13190 , train loss:  0.08927927972416722 , test loss:  0.11100031193801134\n",
      "Epoch:  13200 , step:  13200 , train loss:  0.08925105044231867 , test loss:  0.11097689021979279\n",
      "Epoch:  13210 , step:  13210 , train loss:  0.0892228519567115 , test loss:  0.11095349841124111\n",
      "Epoch:  13220 , step:  13220 , train loss:  0.08919468421116919 , test loss:  0.11093013645632067\n",
      "Epoch:  13230 , step:  13230 , train loss:  0.08916654714965805 , test loss:  0.11090680429913766\n",
      "Epoch:  13240 , step:  13240 , train loss:  0.08913844071628682 , test loss:  0.11088350188393907\n",
      "Epoch:  13250 , step:  13250 , train loss:  0.08911036485530627 , test loss:  0.11086022915511305\n",
      "Epoch:  13260 , step:  13260 , train loss:  0.08908231951110863 , test loss:  0.11083698605718757\n",
      "Epoch:  13270 , step:  13270 , train loss:  0.08905430462822723 , test loss:  0.11081377253483052\n",
      "Epoch:  13280 , step:  13280 , train loss:  0.08902632015133602 , test loss:  0.11079058853284937\n",
      "Epoch:  13290 , step:  13290 , train loss:  0.08899836602524899 , test loss:  0.11076743399619021\n",
      "Epoch:  13300 , step:  13300 , train loss:  0.08897044219491992 , test loss:  0.11074430886993775\n",
      "Epoch:  13310 , step:  13310 , train loss:  0.08894254860544178 , test loss:  0.11072121309931462\n",
      "Epoch:  13320 , step:  13320 , train loss:  0.08891468520204628 , test loss:  0.1106981466296812\n",
      "Epoch:  13330 , step:  13330 , train loss:  0.08888685193010355 , test loss:  0.11067510940653481\n",
      "Epoch:  13340 , step:  13340 , train loss:  0.08885904873512154 , test loss:  0.11065210137550964\n",
      "Epoch:  13350 , step:  13350 , train loss:  0.08883127556274564 , test loss:  0.11062912248237598\n",
      "Epoch:  13360 , step:  13360 , train loss:  0.08880353235875828 , test loss:  0.11060617267304024\n",
      "Epoch:  13370 , step:  13370 , train loss:  0.08877581906907837 , test loss:  0.11058325189354395\n",
      "Epoch:  13380 , step:  13380 , train loss:  0.08874813563976099 , test loss:  0.11056036009006381\n",
      "Epoch:  13390 , step:  13390 , train loss:  0.08872048201699681 , test loss:  0.11053749720891101\n",
      "Epoch:  13400 , step:  13400 , train loss:  0.08869285814711185 , test loss:  0.11051466319653085\n",
      "Epoch:  13410 , step:  13410 , train loss:  0.08866526397656685 , test loss:  0.11049185799950237\n",
      "Epoch:  13420 , step:  13420 , train loss:  0.08863769945195693 , test loss:  0.11046908156453789\n",
      "Epoch:  13430 , step:  13430 , train loss:  0.08861016452001114 , test loss:  0.11044633383848251\n",
      "Epoch:  13440 , step:  13440 , train loss:  0.088582659127592 , test loss:  0.11042361476831383\n",
      "Epoch:  13450 , step:  13450 , train loss:  0.08855518322169519 , test loss:  0.11040092430114157\n",
      "Epoch:  13460 , step:  13460 , train loss:  0.0885277367494489 , test loss:  0.11037826238420682\n",
      "Epoch:  13470 , step:  13470 , train loss:  0.0885003196581137 , test loss:  0.11035562896488205\n",
      "Epoch:  13480 , step:  13480 , train loss:  0.08847293189508185 , test loss:  0.11033302399067052\n",
      "Epoch:  13490 , step:  13490 , train loss:  0.08844557340787697 , test loss:  0.11031044740920569\n",
      "Epoch:  13500 , step:  13500 , train loss:  0.0884182441441537 , test loss:  0.11028789916825107\n",
      "Epoch:  13510 , step:  13510 , train loss:  0.08839094405169717 , test loss:  0.11026537921569983\n",
      "Epoch:  13520 , step:  13520 , train loss:  0.08836367307842267 , test loss:  0.11024288749957406\n",
      "Epoch:  13530 , step:  13530 , train loss:  0.08833643117237513 , test loss:  0.11022042396802466\n",
      "Epoch:  13540 , step:  13540 , train loss:  0.08830921828172883 , test loss:  0.11019798856933102\n",
      "Epoch:  13550 , step:  13550 , train loss:  0.08828203435478685 , test loss:  0.1101755812519001\n",
      "Epoch:  13560 , step:  13560 , train loss:  0.08825487933998082 , test loss:  0.11015320196426687\n",
      "Epoch:  13570 , step:  13570 , train loss:  0.08822775318587035 , test loss:  0.11013085065509302\n",
      "Epoch:  13580 , step:  13580 , train loss:  0.08820065584114277 , test loss:  0.11010852727316707\n",
      "Epoch:  13590 , step:  13590 , train loss:  0.08817358725461256 , test loss:  0.11008623176740408\n",
      "Epoch:  13600 , step:  13600 , train loss:  0.08814654737522114 , test loss:  0.11006396408684488\n",
      "Epoch:  13610 , step:  13610 , train loss:  0.0881195361520363 , test loss:  0.11004172418065569\n",
      "Epoch:  13620 , step:  13620 , train loss:  0.08809255353425188 , test loss:  0.11001951199812814\n",
      "Epoch:  13630 , step:  13630 , train loss:  0.08806559947118733 , test loss:  0.10999732748867862\n",
      "Epoch:  13640 , step:  13640 , train loss:  0.08803867391228737 , test loss:  0.10997517060184764\n",
      "Epoch:  13650 , step:  13650 , train loss:  0.08801177680712154 , test loss:  0.1099530412873\n",
      "Epoch:  13660 , step:  13660 , train loss:  0.08798490810538384 , test loss:  0.10993093949482399\n",
      "Epoch:  13670 , step:  13670 , train loss:  0.0879580677568923 , test loss:  0.1099088651743309\n",
      "Epoch:  13680 , step:  13680 , train loss:  0.08793125571158862 , test loss:  0.10988681827585524\n",
      "Epoch:  13690 , step:  13690 , train loss:  0.08790447191953775 , test loss:  0.10986479874955372\n",
      "Epoch:  13700 , step:  13700 , train loss:  0.08787771633092756 , test loss:  0.10984280654570516\n",
      "Epoch:  13710 , step:  13710 , train loss:  0.08785098889606836 , test loss:  0.10982084161471022\n",
      "Epoch:  13720 , step:  13720 , train loss:  0.08782428956539258 , test loss:  0.1097989039070905\n",
      "Epoch:  13730 , step:  13730 , train loss:  0.08779761828945437 , test loss:  0.10977699337348909\n",
      "Epoch:  13740 , step:  13740 , train loss:  0.08777097501892928 , test loss:  0.10975510996466921\n",
      "Epoch:  13750 , step:  13750 , train loss:  0.08774435970461372 , test loss:  0.10973325363151443\n",
      "Epoch:  13760 , step:  13760 , train loss:  0.08771777229742468 , test loss:  0.10971142432502813\n",
      "Epoch:  13770 , step:  13770 , train loss:  0.08769121274839944 , test loss:  0.10968962199633317\n",
      "Epoch:  13780 , step:  13780 , train loss:  0.08766468100869505 , test loss:  0.10966784659667152\n",
      "Epoch:  13790 , step:  13790 , train loss:  0.08763817702958793 , test loss:  0.10964609807740369\n",
      "Epoch:  13800 , step:  13800 , train loss:  0.08761170076247367 , test loss:  0.10962437639000881\n",
      "Epoch:  13810 , step:  13810 , train loss:  0.08758525215886652 , test loss:  0.10960268148608376\n",
      "Epoch:  13820 , step:  13820 , train loss:  0.08755883117039907 , test loss:  0.10958101331734321\n",
      "Epoch:  13830 , step:  13830 , train loss:  0.08753243774882181 , test loss:  0.10955937183561894\n",
      "Epoch:  13840 , step:  13840 , train loss:  0.08750607184600293 , test loss:  0.1095377569928598\n",
      "Epoch:  13850 , step:  13850 , train loss:  0.08747973341392766 , test loss:  0.109516168741131\n",
      "Epoch:  13860 , step:  13860 , train loss:  0.08745342240469824 , test loss:  0.10949460703261406\n",
      "Epoch:  13870 , step:  13870 , train loss:  0.08742713877053336 , test loss:  0.10947307181960642\n",
      "Epoch:  13880 , step:  13880 , train loss:  0.08740088246376784 , test loss:  0.10945156305452065\n",
      "Epoch:  13890 , step:  13890 , train loss:  0.08737465343685218 , test loss:  0.10943008068988484\n",
      "Epoch:  13900 , step:  13900 , train loss:  0.08734845164235239 , test loss:  0.10940862467834159\n",
      "Epoch:  13910 , step:  13910 , train loss:  0.08732227703294947 , test loss:  0.10938719497264796\n",
      "Epoch:  13920 , step:  13920 , train loss:  0.08729612956143913 , test loss:  0.10936579152567515\n",
      "Epoch:  13930 , step:  13930 , train loss:  0.08727000918073138 , test loss:  0.109344414290408\n",
      "Epoch:  13940 , step:  13940 , train loss:  0.08724391584385027 , test loss:  0.1093230632199448\n",
      "Epoch:  13950 , step:  13950 , train loss:  0.08721784950393337 , test loss:  0.10930173826749683\n",
      "Epoch:  13960 , step:  13960 , train loss:  0.08719181011423163 , test loss:  0.10928043938638793\n",
      "Epoch:  13970 , step:  13970 , train loss:  0.08716579762810883 , test loss:  0.10925916653005446\n",
      "Epoch:  13980 , step:  13980 , train loss:  0.08713981199904142 , test loss:  0.10923791965204468\n",
      "Epoch:  13990 , step:  13990 , train loss:  0.08711385318061787 , test loss:  0.10921669870601851\n",
      "Epoch:  14000 , step:  14000 , train loss:  0.08708792112653883 , test loss:  0.10919550364574711\n",
      "Epoch:  14010 , step:  14010 , train loss:  0.08706201579061623 , test loss:  0.1091743344251128\n",
      "Epoch:  14020 , step:  14020 , train loss:  0.08703613712677326 , test loss:  0.10915319099810838\n",
      "Epoch:  14030 , step:  14030 , train loss:  0.08701028508904396 , test loss:  0.10913207331883705\n",
      "Epoch:  14040 , step:  14040 , train loss:  0.08698445963157289 , test loss:  0.10911098134151184\n",
      "Epoch:  14050 , step:  14050 , train loss:  0.08695866070861476 , test loss:  0.10908991502045552\n",
      "Epoch:  14060 , step:  14060 , train loss:  0.086932888274534 , test loss:  0.10906887431010018\n",
      "Epoch:  14070 , step:  14070 , train loss:  0.08690714228380472 , test loss:  0.10904785916498688\n",
      "Epoch:  14080 , step:  14080 , train loss:  0.08688142269101001 , test loss:  0.10902686953976522\n",
      "Epoch:  14090 , step:  14090 , train loss:  0.08685572945084183 , test loss:  0.10900590538919316\n",
      "Epoch:  14100 , step:  14100 , train loss:  0.0868300625181006 , test loss:  0.10898496666813673\n",
      "Epoch:  14110 , step:  14110 , train loss:  0.08680442184769492 , test loss:  0.10896405333156951\n",
      "Epoch:  14120 , step:  14120 , train loss:  0.08677880739464117 , test loss:  0.10894316533457242\n",
      "Epoch:  14130 , step:  14130 , train loss:  0.08675321911406321 , test loss:  0.10892230263233348\n",
      "Epoch:  14140 , step:  14140 , train loss:  0.08672765696119206 , test loss:  0.10890146518014726\n",
      "Epoch:  14150 , step:  14150 , train loss:  0.08670212089136557 , test loss:  0.10888065293341491\n",
      "Epoch:  14160 , step:  14160 , train loss:  0.0866766108600281 , test loss:  0.10885986584764334\n",
      "Epoch:  14170 , step:  14170 , train loss:  0.08665112682273016 , test loss:  0.10883910387844556\n",
      "Epoch:  14180 , step:  14180 , train loss:  0.08662566873512809 , test loss:  0.1088183669815396\n",
      "Epoch:  14190 , step:  14190 , train loss:  0.08660023655298381 , test loss:  0.10879765511274872\n",
      "Epoch:  14200 , step:  14200 , train loss:  0.08657483023216442 , test loss:  0.10877696822800113\n",
      "Epoch:  14210 , step:  14210 , train loss:  0.08654944972864184 , test loss:  0.10875630628332929\n",
      "Epoch:  14220 , step:  14220 , train loss:  0.08652409499849267 , test loss:  0.10873566923486984\n",
      "Epoch:  14230 , step:  14230 , train loss:  0.08649876599789767 , test loss:  0.10871505703886343\n",
      "Epoch:  14240 , step:  14240 , train loss:  0.08647346268314154 , test loss:  0.10869446965165402\n",
      "Epoch:  14250 , step:  14250 , train loss:  0.08644818501061263 , test loss:  0.1086739070296888\n",
      "Epoch:  14260 , step:  14260 , train loss:  0.08642293293680255 , test loss:  0.10865336912951815\n",
      "Epoch:  14270 , step:  14270 , train loss:  0.08639770641830591 , test loss:  0.10863285590779473\n",
      "Epoch:  14280 , step:  14280 , train loss:  0.08637250541181998 , test loss:  0.10861236732127366\n",
      "Epoch:  14290 , step:  14290 , train loss:  0.08634732987414442 , test loss:  0.1085919033268119\n",
      "Epoch:  14300 , step:  14300 , train loss:  0.08632217976218086 , test loss:  0.10857146388136825\n",
      "Epoch:  14310 , step:  14310 , train loss:  0.08629705503293282 , test loss:  0.10855104894200286\n",
      "Epoch:  14320 , step:  14320 , train loss:  0.0862719556435051 , test loss:  0.10853065846587696\n",
      "Epoch:  14330 , step:  14330 , train loss:  0.08624688155110369 , test loss:  0.10851029241025242\n",
      "Epoch:  14340 , step:  14340 , train loss:  0.08622183271303543 , test loss:  0.10848995073249189\n",
      "Epoch:  14350 , step:  14350 , train loss:  0.08619680908670763 , test loss:  0.10846963339005791\n",
      "Epoch:  14360 , step:  14360 , train loss:  0.08617181062962785 , test loss:  0.10844934034051298\n",
      "Epoch:  14370 , step:  14370 , train loss:  0.08614683729940352 , test loss:  0.10842907154151926\n",
      "Epoch:  14380 , step:  14380 , train loss:  0.08612188905374174 , test loss:  0.10840882695083813\n",
      "Epoch:  14390 , step:  14390 , train loss:  0.08609696585044883 , test loss:  0.10838860652633005\n",
      "Epoch:  14400 , step:  14400 , train loss:  0.08607206764743022 , test loss:  0.10836841022595409\n",
      "Epoch:  14410 , step:  14410 , train loss:  0.08604719440269001 , test loss:  0.10834823800776776\n",
      "Epoch:  14420 , step:  14420 , train loss:  0.08602234607433071 , test loss:  0.10832808982992669\n",
      "Epoch:  14430 , step:  14430 , train loss:  0.08599752262055294 , test loss:  0.10830796565068443\n",
      "Epoch:  14440 , step:  14440 , train loss:  0.08597272399965519 , test loss:  0.10828786542839186\n",
      "Epoch:  14450 , step:  14450 , train loss:  0.08594795017003345 , test loss:  0.10826778912149729\n",
      "Epoch:  14460 , step:  14460 , train loss:  0.08592320109018099 , test loss:  0.10824773668854597\n",
      "Epoch:  14470 , step:  14470 , train loss:  0.08589847671868797 , test loss:  0.10822770808817979\n",
      "Epoch:  14480 , step:  14480 , train loss:  0.08587377701424125 , test loss:  0.10820770327913704\n",
      "Epoch:  14490 , step:  14490 , train loss:  0.08584910193562408 , test loss:  0.10818772222025212\n",
      "Epoch:  14500 , step:  14500 , train loss:  0.08582445144171573 , test loss:  0.10816776487045524\n",
      "Epoch:  14510 , step:  14510 , train loss:  0.0857998254914913 , test loss:  0.10814783118877228\n",
      "Epoch:  14520 , step:  14520 , train loss:  0.08577522404402148 , test loss:  0.10812792113432414\n",
      "Epoch:  14530 , step:  14530 , train loss:  0.085750647058472 , test loss:  0.10810803466632693\n",
      "Epoch:  14540 , step:  14540 , train loss:  0.0857260944941037 , test loss:  0.10808817174409123\n",
      "Epoch:  14550 , step:  14550 , train loss:  0.08570156631027202 , test loss:  0.10806833232702243\n",
      "Epoch:  14560 , step:  14560 , train loss:  0.08567706246642676 , test loss:  0.10804851637461976\n",
      "Epoch:  14570 , step:  14570 , train loss:  0.08565258292211182 , test loss:  0.10802872384647635\n",
      "Epoch:  14580 , step:  14580 , train loss:  0.08562812763696495 , test loss:  0.10800895470227889\n",
      "Epoch:  14590 , step:  14590 , train loss:  0.0856036965707174 , test loss:  0.10798920890180765\n",
      "Epoch:  14600 , step:  14600 , train loss:  0.08557928968319366 , test loss:  0.10796948640493577\n",
      "Epoch:  14610 , step:  14610 , train loss:  0.08555490693431128 , test loss:  0.10794978717162904\n",
      "Epoch:  14620 , step:  14620 , train loss:  0.08553054828408047 , test loss:  0.10793011116194597\n",
      "Epoch:  14630 , step:  14630 , train loss:  0.08550621369260383 , test loss:  0.10791045833603728\n",
      "Epoch:  14640 , step:  14640 , train loss:  0.08548190312007618 , test loss:  0.1078908286541456\n",
      "Epoch:  14650 , step:  14650 , train loss:  0.08545761652678424 , test loss:  0.1078712220766054\n",
      "Epoch:  14660 , step:  14660 , train loss:  0.08543335387310627 , test loss:  0.10785163856384221\n",
      "Epoch:  14670 , step:  14670 , train loss:  0.08540911511951191 , test loss:  0.10783207807637317\n",
      "Epoch:  14680 , step:  14680 , train loss:  0.08538490022656191 , test loss:  0.10781254057480599\n",
      "Epoch:  14690 , step:  14690 , train loss:  0.08536070915490776 , test loss:  0.10779302601983924\n",
      "Epoch:  14700 , step:  14700 , train loss:  0.08533654186529152 , test loss:  0.10777353437226182\n",
      "Epoch:  14710 , step:  14710 , train loss:  0.08531239831854555 , test loss:  0.10775406559295261\n",
      "Epoch:  14720 , step:  14720 , train loss:  0.08528827847559209 , test loss:  0.1077346196428805\n",
      "Epoch:  14730 , step:  14730 , train loss:  0.0852641822974433 , test loss:  0.10771519648310385\n",
      "Epoch:  14740 , step:  14740 , train loss:  0.08524010974520063 , test loss:  0.1076957960747704\n",
      "Epoch:  14750 , step:  14750 , train loss:  0.08521606078005495 , test loss:  0.10767641837911703\n",
      "Epoch:  14760 , step:  14760 , train loss:  0.08519203536328586 , test loss:  0.10765706335746926\n",
      "Epoch:  14770 , step:  14770 , train loss:  0.08516803345626175 , test loss:  0.10763773097124132\n",
      "Epoch:  14780 , step:  14780 , train loss:  0.0851440550204395 , test loss:  0.10761842118193576\n",
      "Epoch:  14790 , step:  14790 , train loss:  0.08512010001736404 , test loss:  0.10759913395114294\n",
      "Epoch:  14800 , step:  14800 , train loss:  0.0850961684086683 , test loss:  0.10757986924054122\n",
      "Epoch:  14810 , step:  14810 , train loss:  0.08507226015607279 , test loss:  0.10756062701189652\n",
      "Epoch:  14820 , step:  14820 , train loss:  0.08504837522138545 , test loss:  0.10754140722706187\n",
      "Epoch:  14830 , step:  14830 , train loss:  0.0850245135665014 , test loss:  0.10752220984797735\n",
      "Epoch:  14840 , step:  14840 , train loss:  0.08500067515340255 , test loss:  0.1075030348366701\n",
      "Epoch:  14850 , step:  14850 , train loss:  0.08497685994415753 , test loss:  0.1074838821552534\n",
      "Epoch:  14860 , step:  14860 , train loss:  0.08495306790092133 , test loss:  0.10746475176592706\n",
      "Epoch:  14870 , step:  14870 , train loss:  0.08492929898593497 , test loss:  0.10744564363097686\n",
      "Epoch:  14880 , step:  14880 , train loss:  0.08490555316152551 , test loss:  0.10742655771277429\n",
      "Epoch:  14890 , step:  14890 , train loss:  0.08488183039010552 , test loss:  0.1074074939737766\n",
      "Epoch:  14900 , step:  14900 , train loss:  0.08485813063417295 , test loss:  0.10738845237652608\n",
      "Epoch:  14910 , step:  14910 , train loss:  0.08483445385631096 , test loss:  0.10736943288365018\n",
      "Epoch:  14920 , step:  14920 , train loss:  0.08481080001918746 , test loss:  0.10735043545786105\n",
      "Epoch:  14930 , step:  14930 , train loss:  0.0847871690855551 , test loss:  0.10733146006195564\n",
      "Epoch:  14940 , step:  14940 , train loss:  0.08476356101825087 , test loss:  0.10731250665881498\n",
      "Epoch:  14950 , step:  14950 , train loss:  0.08473997578019594 , test loss:  0.10729357521140435\n",
      "Epoch:  14960 , step:  14960 , train loss:  0.08471641333439532 , test loss:  0.10727466568277275\n",
      "Epoch:  14970 , step:  14970 , train loss:  0.08469287364393771 , test loss:  0.1072557780360528\n",
      "Epoch:  14980 , step:  14980 , train loss:  0.0846693566719952 , test loss:  0.10723691223446044\n",
      "Epoch:  14990 , step:  14990 , train loss:  0.08464586238182306 , test loss:  0.10721806824129483\n",
      "Epoch:  15000 , step:  15000 , train loss:  0.0846223907367595 , test loss:  0.10719924601993802\n",
      "Epoch:  15010 , step:  15010 , train loss:  0.0845989417002255 , test loss:  0.10718044553385461\n",
      "Epoch:  15020 , step:  15020 , train loss:  0.08457551523572426 , test loss:  0.10716166674659162\n",
      "Epoch:  15030 , step:  15030 , train loss:  0.08455211130684145 , test loss:  0.10714290962177835\n",
      "Epoch:  15040 , step:  15040 , train loss:  0.08452872987724455 , test loss:  0.1071241741231259\n",
      "Epoch:  15050 , step:  15050 , train loss:  0.08450537091068284 , test loss:  0.10710546021442714\n",
      "Epoch:  15060 , step:  15060 , train loss:  0.08448203437098711 , test loss:  0.1070867678595565\n",
      "Epoch:  15070 , step:  15070 , train loss:  0.08445872022206942 , test loss:  0.10706809702246949\n",
      "Epoch:  15080 , step:  15080 , train loss:  0.08443542842792277 , test loss:  0.10704944766720273\n",
      "Epoch:  15090 , step:  15090 , train loss:  0.08441215895262115 , test loss:  0.10703081975787385\n",
      "Epoch:  15100 , step:  15100 , train loss:  0.08438891176031893 , test loss:  0.10701221325868046\n",
      "Epoch:  15110 , step:  15110 , train loss:  0.0843656868152509 , test loss:  0.10699362813390106\n",
      "Epoch:  15120 , step:  15120 , train loss:  0.08434248408173203 , test loss:  0.10697506434789394\n",
      "Epoch:  15130 , step:  15130 , train loss:  0.08431930352415698 , test loss:  0.10695652186509742\n",
      "Epoch:  15140 , step:  15140 , train loss:  0.08429614510700026 , test loss:  0.10693800065002931\n",
      "Epoch:  15150 , step:  15150 , train loss:  0.08427300879481568 , test loss:  0.10691950066728703\n",
      "Epoch:  15160 , step:  15160 , train loss:  0.08424989455223622 , test loss:  0.10690102188154704\n",
      "Epoch:  15170 , step:  15170 , train loss:  0.08422680234397396 , test loss:  0.10688256425756479\n",
      "Epoch:  15180 , step:  15180 , train loss:  0.08420373213481956 , test loss:  0.10686412776017463\n",
      "Epoch:  15190 , step:  15190 , train loss:  0.08418068388964234 , test loss:  0.10684571235428922\n",
      "Epoch:  15200 , step:  15200 , train loss:  0.08415765757338978 , test loss:  0.10682731800489972\n",
      "Epoch:  15210 , step:  15210 , train loss:  0.08413465315108748 , test loss:  0.10680894467707518\n",
      "Epoch:  15220 , step:  15220 , train loss:  0.08411167058783893 , test loss:  0.10679059233596273\n",
      "Epoch:  15230 , step:  15230 , train loss:  0.08408870984882519 , test loss:  0.106772260946787\n",
      "Epoch:  15240 , step:  15240 , train loss:  0.08406577089930471 , test loss:  0.10675395047485008\n",
      "Epoch:  15250 , step:  15250 , train loss:  0.08404285370461317 , test loss:  0.10673566088553131\n",
      "Epoch:  15260 , step:  15260 , train loss:  0.08401995823016317 , test loss:  0.1067173921442869\n",
      "Epoch:  15270 , step:  15270 , train loss:  0.08399708444144406 , test loss:  0.10669914421665008\n",
      "Epoch:  15280 , step:  15280 , train loss:  0.0839742323040217 , test loss:  0.10668091706823037\n",
      "Epoch:  15290 , step:  15290 , train loss:  0.08395140178353827 , test loss:  0.1066627106647138\n",
      "Epoch:  15300 , step:  15300 , train loss:  0.08392859284571207 , test loss:  0.10664452497186248\n",
      "Epoch:  15310 , step:  15310 , train loss:  0.08390580545633723 , test loss:  0.10662635995551449\n",
      "Epoch:  15320 , step:  15320 , train loss:  0.08388303958128353 , test loss:  0.10660821558158352\n",
      "Epoch:  15330 , step:  15330 , train loss:  0.08386029518649626 , test loss:  0.10659009181605887\n",
      "Epoch:  15340 , step:  15340 , train loss:  0.08383757223799586 , test loss:  0.10657198862500512\n",
      "Epoch:  15350 , step:  15350 , train loss:  0.08381487070187778 , test loss:  0.10655390597456163\n",
      "Epoch:  15360 , step:  15360 , train loss:  0.08379219054431239 , test loss:  0.10653584383094311\n",
      "Epoch:  15370 , step:  15370 , train loss:  0.08376953173154457 , test loss:  0.10651780216043867\n",
      "Epoch:  15380 , step:  15380 , train loss:  0.08374689422989352 , test loss:  0.10649978092941177\n",
      "Epoch:  15390 , step:  15390 , train loss:  0.08372427800575277 , test loss:  0.10648178010430023\n",
      "Epoch:  15400 , step:  15400 , train loss:  0.08370168302558967 , test loss:  0.106463799651616\n",
      "Epoch:  15410 , step:  15410 , train loss:  0.0836791092559454 , test loss:  0.10644583953794463\n",
      "Epoch:  15420 , step:  15420 , train loss:  0.08365655666343468 , test loss:  0.10642789972994554\n",
      "Epoch:  15430 , step:  15430 , train loss:  0.08363402521474553 , test loss:  0.10640998019435129\n",
      "Epoch:  15440 , step:  15440 , train loss:  0.08361151487663916 , test loss:  0.10639208089796788\n",
      "Epoch:  15450 , step:  15450 , train loss:  0.08358902561594966 , test loss:  0.10637420180767423\n",
      "Epoch:  15460 , step:  15460 , train loss:  0.08356655739958385 , test loss:  0.10635634289042212\n",
      "Epoch:  15470 , step:  15470 , train loss:  0.08354411019452113 , test loss:  0.10633850411323573\n",
      "Epoch:  15480 , step:  15480 , train loss:  0.08352168396781305 , test loss:  0.106320685443212\n",
      "Epoch:  15490 , step:  15490 , train loss:  0.08349927868658352 , test loss:  0.10630288684751982\n",
      "Epoch:  15500 , step:  15500 , train loss:  0.08347689431802814 , test loss:  0.10628510829340006\n",
      "Epoch:  15510 , step:  15510 , train loss:  0.08345453082941431 , test loss:  0.10626734974816553\n",
      "Epoch:  15520 , step:  15520 , train loss:  0.08343218818808097 , test loss:  0.10624961117920051\n",
      "Epoch:  15530 , step:  15530 , train loss:  0.08340986636143827 , test loss:  0.10623189255396089\n",
      "Epoch:  15540 , step:  15540 , train loss:  0.08338756531696755 , test loss:  0.1062141938399735\n",
      "Epoch:  15550 , step:  15550 , train loss:  0.08336528502222103 , test loss:  0.10619651500483651\n",
      "Epoch:  15560 , step:  15560 , train loss:  0.08334302544482165 , test loss:  0.10617885601621854\n",
      "Epoch:  15570 , step:  15570 , train loss:  0.0833207865524628 , test loss:  0.1061612168418591\n",
      "Epoch:  15580 , step:  15580 , train loss:  0.08329856831290831 , test loss:  0.1061435974495681\n",
      "Epoch:  15590 , step:  15590 , train loss:  0.08327637069399203 , test loss:  0.10612599780722544\n",
      "Epoch:  15600 , step:  15600 , train loss:  0.08325419366361776 , test loss:  0.10610841788278137\n",
      "Epoch:  15610 , step:  15610 , train loss:  0.08323203718975905 , test loss:  0.1060908576442558\n",
      "Epoch:  15620 , step:  15620 , train loss:  0.08320990124045893 , test loss:  0.10607331705973842\n",
      "Epoch:  15630 , step:  15630 , train loss:  0.08318778578382988 , test loss:  0.10605579609738817\n",
      "Epoch:  15640 , step:  15640 , train loss:  0.08316569078805337 , test loss:  0.10603829472543354\n",
      "Epoch:  15650 , step:  15650 , train loss:  0.08314361622138002 , test loss:  0.10602081291217198\n",
      "Epoch:  15660 , step:  15660 , train loss:  0.08312156205212905 , test loss:  0.10600335062596974\n",
      "Epoch:  15670 , step:  15670 , train loss:  0.08309952824868831 , test loss:  0.1059859078352619\n",
      "Epoch:  15680 , step:  15680 , train loss:  0.08307751477951412 , test loss:  0.10596848450855201\n",
      "Epoch:  15690 , step:  15690 , train loss:  0.08305552161313084 , test loss:  0.10595108061441189\n",
      "Epoch:  15700 , step:  15700 , train loss:  0.08303354871813097 , test loss:  0.10593369612148162\n",
      "Epoch:  15710 , step:  15710 , train loss:  0.08301159606317475 , test loss:  0.10591633099846913\n",
      "Epoch:  15720 , step:  15720 , train loss:  0.08298966361699008 , test loss:  0.10589898521415023\n",
      "Epoch:  15730 , step:  15730 , train loss:  0.08296775134837228 , test loss:  0.10588165873736816\n",
      "Epoch:  15740 , step:  15740 , train loss:  0.08294585922618397 , test loss:  0.10586435153703368\n",
      "Epoch:  15750 , step:  15750 , train loss:  0.08292398721935483 , test loss:  0.10584706358212473\n",
      "Epoch:  15760 , step:  15760 , train loss:  0.08290213529688137 , test loss:  0.10582979484168625\n",
      "Epoch:  15770 , step:  15770 , train loss:  0.08288030342782689 , test loss:  0.10581254528483008\n",
      "Epoch:  15780 , step:  15780 , train loss:  0.08285849158132111 , test loss:  0.10579531488073471\n",
      "Epoch:  15790 , step:  15790 , train loss:  0.08283669972656023 , test loss:  0.10577810359864505\n",
      "Epoch:  15800 , step:  15800 , train loss:  0.08281492783280642 , test loss:  0.10576091140787247\n",
      "Epoch:  15810 , step:  15810 , train loss:  0.08279317586938793 , test loss:  0.10574373827779428\n",
      "Epoch:  15820 , step:  15820 , train loss:  0.0827714438056988 , test loss:  0.10572658417785386\n",
      "Epoch:  15830 , step:  15830 , train loss:  0.08274973161119865 , test loss:  0.10570944907756034\n",
      "Epoch:  15840 , step:  15840 , train loss:  0.08272803925541254 , test loss:  0.10569233294648833\n",
      "Epoch:  15850 , step:  15850 , train loss:  0.08270636670793075 , test loss:  0.105675235754278\n",
      "Epoch:  15860 , step:  15860 , train loss:  0.08268471393840866 , test loss:  0.10565815747063462\n",
      "Epoch:  15870 , step:  15870 , train loss:  0.08266308091656653 , test loss:  0.10564109806532868\n",
      "Epoch:  15880 , step:  15880 , train loss:  0.08264146761218938 , test loss:  0.10562405750819531\n",
      "Epoch:  15890 , step:  15890 , train loss:  0.08261987399512663 , test loss:  0.10560703576913452\n",
      "Epoch:  15900 , step:  15900 , train loss:  0.08259830003529221 , test loss:  0.10559003281811088\n",
      "Epoch:  15910 , step:  15910 , train loss:  0.08257674570266418 , test loss:  0.10557304862515314\n",
      "Epoch:  15920 , step:  15920 , train loss:  0.08255521096728458 , test loss:  0.10555608316035431\n",
      "Epoch:  15930 , step:  15930 , train loss:  0.08253369579925927 , test loss:  0.10553913639387157\n",
      "Epoch:  15940 , step:  15940 , train loss:  0.08251220016875786 , test loss:  0.10552220829592557\n",
      "Epoch:  15950 , step:  15950 , train loss:  0.08249072404601336 , test loss:  0.10550529883680097\n",
      "Epoch:  15960 , step:  15960 , train loss:  0.08246926740132209 , test loss:  0.10548840798684574\n",
      "Epoch:  15970 , step:  15970 , train loss:  0.08244783020504357 , test loss:  0.10547153571647122\n",
      "Epoch:  15980 , step:  15980 , train loss:  0.08242641242760025 , test loss:  0.10545468199615188\n",
      "Epoch:  15990 , step:  15990 , train loss:  0.08240501403947735 , test loss:  0.10543784679642516\n",
      "Epoch:  16000 , step:  16000 , train loss:  0.08238363501122278 , test loss:  0.10542103008789133\n",
      "Epoch:  16010 , step:  16010 , train loss:  0.08236227531344686 , test loss:  0.10540423184121331\n",
      "Epoch:  16020 , step:  16020 , train loss:  0.08234093491682218 , test loss:  0.10538745202711643\n",
      "Epoch:  16030 , step:  16030 , train loss:  0.0823196137920835 , test loss:  0.10537069061638832\n",
      "Epoch:  16040 , step:  16040 , train loss:  0.08229831191002748 , test loss:  0.10535394757987893\n",
      "Epoch:  16050 , step:  16050 , train loss:  0.08227702924151256 , test loss:  0.10533722288849992\n",
      "Epoch:  16060 , step:  16060 , train loss:  0.08225576575745883 , test loss:  0.10532051651322481\n",
      "Epoch:  16070 , step:  16070 , train loss:  0.0822345214288478 , test loss:  0.10530382842508904\n",
      "Epoch:  16080 , step:  16080 , train loss:  0.08221329622672222 , test loss:  0.10528715859518908\n",
      "Epoch:  16090 , step:  16090 , train loss:  0.08219209012218608 , test loss:  0.10527050699468307\n",
      "Epoch:  16100 , step:  16100 , train loss:  0.08217090308640415 , test loss:  0.10525387359479002\n",
      "Epoch:  16110 , step:  16110 , train loss:  0.08214973509060208 , test loss:  0.10523725836679013\n",
      "Epoch:  16120 , step:  16120 , train loss:  0.08212858610606616 , test loss:  0.10522066128202436\n",
      "Epoch:  16130 , step:  16130 , train loss:  0.08210745610414305 , test loss:  0.10520408231189429\n",
      "Epoch:  16140 , step:  16140 , train loss:  0.08208634505623978 , test loss:  0.10518752142786192\n",
      "Epoch:  16150 , step:  16150 , train loss:  0.08206525293382347 , test loss:  0.10517097860144975\n",
      "Epoch:  16160 , step:  16160 , train loss:  0.08204417970842122 , test loss:  0.1051544538042404\n",
      "Epoch:  16170 , step:  16170 , train loss:  0.0820231253516199 , test loss:  0.10513794700787636\n",
      "Epoch:  16180 , step:  16180 , train loss:  0.0820020898350661 , test loss:  0.10512145818406012\n",
      "Epoch:  16190 , step:  16190 , train loss:  0.08198107313046582 , test loss:  0.10510498730455381\n",
      "Epoch:  16200 , step:  16200 , train loss:  0.08196007520958443 , test loss:  0.1050885343411792\n",
      "Epoch:  16210 , step:  16210 , train loss:  0.08193909604424644 , test loss:  0.10507209926581723\n",
      "Epoch:  16220 , step:  16220 , train loss:  0.08191813560633542 , test loss:  0.10505568205040816\n",
      "Epoch:  16230 , step:  16230 , train loss:  0.08189719386779369 , test loss:  0.10503928266695131\n",
      "Epoch:  16240 , step:  16240 , train loss:  0.0818762708006223 , test loss:  0.105022901087505\n",
      "Epoch:  16250 , step:  16250 , train loss:  0.08185536637688091 , test loss:  0.10500653728418617\n",
      "Epoch:  16260 , step:  16260 , train loss:  0.08183448056868746 , test loss:  0.10499019122917028\n",
      "Epoch:  16270 , step:  16270 , train loss:  0.08181361334821816 , test loss:  0.10497386289469143\n",
      "Epoch:  16280 , step:  16280 , train loss:  0.08179276468770726 , test loss:  0.1049575522530418\n",
      "Epoch:  16290 , step:  16290 , train loss:  0.0817719345594469 , test loss:  0.10494125927657191\n",
      "Epoch:  16300 , step:  16300 , train loss:  0.08175112293578703 , test loss:  0.10492498393769009\n",
      "Epoch:  16310 , step:  16310 , train loss:  0.08173032978913518 , test loss:  0.10490872620886246\n",
      "Epoch:  16320 , step:  16320 , train loss:  0.08170955509195627 , test loss:  0.10489248606261302\n",
      "Epoch:  16330 , step:  16330 , train loss:  0.08168879881677263 , test loss:  0.10487626347152303\n",
      "Epoch:  16340 , step:  16340 , train loss:  0.08166806093616359 , test loss:  0.1048600584082313\n",
      "Epoch:  16350 , step:  16350 , train loss:  0.08164734142276558 , test loss:  0.10484387084543383\n",
      "Epoch:  16360 , step:  16360 , train loss:  0.08162664024927181 , test loss:  0.10482770075588349\n",
      "Epoch:  16370 , step:  16370 , train loss:  0.0816059573884322 , test loss:  0.1048115481123903\n",
      "Epoch:  16380 , step:  16380 , train loss:  0.08158529281305317 , test loss:  0.10479541288782089\n",
      "Epoch:  16390 , step:  16390 , train loss:  0.08156464649599754 , test loss:  0.10477929505509861\n",
      "Epoch:  16400 , step:  16400 , train loss:  0.08154401841018441 , test loss:  0.10476319458720317\n",
      "Epoch:  16410 , step:  16410 , train loss:  0.08152340852858891 , test loss:  0.10474711145717068\n",
      "Epoch:  16420 , step:  16420 , train loss:  0.08150281682424212 , test loss:  0.10473104563809336\n",
      "Epoch:  16430 , step:  16430 , train loss:  0.08148224327023089 , test loss:  0.10471499710311939\n",
      "Epoch:  16440 , step:  16440 , train loss:  0.08146168783969775 , test loss:  0.10469896582545302\n",
      "Epoch:  16450 , step:  16450 , train loss:  0.08144115050584066 , test loss:  0.10468295177835402\n",
      "Epoch:  16460 , step:  16460 , train loss:  0.08142063124191305 , test loss:  0.10466695493513786\n",
      "Epoch:  16470 , step:  16470 , train loss:  0.08140013002122336 , test loss:  0.10465097526917526\n",
      "Epoch:  16480 , step:  16480 , train loss:  0.08137964681713525 , test loss:  0.1046350127538926\n",
      "Epoch:  16490 , step:  16490 , train loss:  0.08135918160306722 , test loss:  0.10461906736277102\n",
      "Epoch:  16500 , step:  16500 , train loss:  0.08133873435249253 , test loss:  0.1046031390693469\n",
      "Epoch:  16510 , step:  16510 , train loss:  0.0813183050389391 , test loss:  0.10458722784721133\n",
      "Epoch:  16520 , step:  16520 , train loss:  0.08129789363598922 , test loss:  0.10457133367001012\n",
      "Epoch:  16530 , step:  16530 , train loss:  0.08127750011727963 , test loss:  0.10455545651144382\n",
      "Epoch:  16540 , step:  16540 , train loss:  0.08125712445650118 , test loss:  0.10453959634526717\n",
      "Epoch:  16550 , step:  16550 , train loss:  0.08123676662739883 , test loss:  0.10452375314528942\n",
      "Epoch:  16560 , step:  16560 , train loss:  0.08121642660377136 , test loss:  0.10450792688537375\n",
      "Epoch:  16570 , step:  16570 , train loss:  0.08119610435947139 , test loss:  0.10449211753943749\n",
      "Epoch:  16580 , step:  16580 , train loss:  0.08117579986840509 , test loss:  0.10447632508145176\n",
      "Epoch:  16590 , step:  16590 , train loss:  0.08115551310453217 , test loss:  0.10446054948544148\n",
      "Epoch:  16600 , step:  16600 , train loss:  0.08113524404186562 , test loss:  0.10444479072548503\n",
      "Epoch:  16610 , step:  16610 , train loss:  0.0811149926544717 , test loss:  0.10442904877571436\n",
      "Epoch:  16620 , step:  16620 , train loss:  0.08109475891646963 , test loss:  0.10441332361031465\n",
      "Epoch:  16630 , step:  16630 , train loss:  0.08107454280203163 , test loss:  0.10439761520352427\n",
      "Epoch:  16640 , step:  16640 , train loss:  0.08105434428538272 , test loss:  0.10438192352963456\n",
      "Epoch:  16650 , step:  16650 , train loss:  0.08103416334080042 , test loss:  0.10436624856298983\n",
      "Epoch:  16660 , step:  16660 , train loss:  0.0810139999426149 , test loss:  0.10435059027798707\n",
      "Epoch:  16670 , step:  16670 , train loss:  0.08099385406520865 , test loss:  0.10433494864907598\n",
      "Epoch:  16680 , step:  16680 , train loss:  0.08097372568301642 , test loss:  0.1043193236507587\n",
      "Epoch:  16690 , step:  16690 , train loss:  0.08095361477052496 , test loss:  0.10430371525758962\n",
      "Epoch:  16700 , step:  16700 , train loss:  0.08093352130227306 , test loss:  0.10428812344417539\n",
      "Epoch:  16710 , step:  16710 , train loss:  0.08091344525285134 , test loss:  0.10427254818517492\n",
      "Epoch:  16720 , step:  16720 , train loss:  0.08089338659690205 , test loss:  0.10425698945529877\n",
      "Epoch:  16730 , step:  16730 , train loss:  0.08087334530911904 , test loss:  0.10424144722930939\n",
      "Epoch:  16740 , step:  16740 , train loss:  0.08085332136424754 , test loss:  0.10422592148202105\n",
      "Epoch:  16750 , step:  16750 , train loss:  0.0808333147370841 , test loss:  0.10421041218829936\n",
      "Epoch:  16760 , step:  16760 , train loss:  0.08081332540247639 , test loss:  0.10419491932306152\n",
      "Epoch:  16770 , step:  16770 , train loss:  0.08079335333532316 , test loss:  0.10417944286127578\n",
      "Epoch:  16780 , step:  16780 , train loss:  0.08077339851057393 , test loss:  0.10416398277796184\n",
      "Epoch:  16790 , step:  16790 , train loss:  0.08075346090322909 , test loss:  0.10414853904819009\n",
      "Epoch:  16800 , step:  16800 , train loss:  0.08073354048833958 , test loss:  0.10413311164708196\n",
      "Epoch:  16810 , step:  16810 , train loss:  0.08071363724100686 , test loss:  0.10411770054980958\n",
      "Epoch:  16820 , step:  16820 , train loss:  0.08069375113638273 , test loss:  0.1041023057315957\n",
      "Epoch:  16830 , step:  16830 , train loss:  0.08067388214966925 , test loss:  0.10408692716771363\n",
      "Epoch:  16840 , step:  16840 , train loss:  0.08065403025611854 , test loss:  0.10407156483348698\n",
      "Epoch:  16850 , step:  16850 , train loss:  0.08063419543103272 , test loss:  0.10405621870428952\n",
      "Epoch:  16860 , step:  16860 , train loss:  0.08061437764976373 , test loss:  0.10404088875554515\n",
      "Epoch:  16870 , step:  16870 , train loss:  0.08059457688771324 , test loss:  0.10402557496272787\n",
      "Epoch:  16880 , step:  16880 , train loss:  0.08057479312033246 , test loss:  0.1040102773013614\n",
      "Epoch:  16890 , step:  16890 , train loss:  0.08055502632312213 , test loss:  0.10399499574701919\n",
      "Epoch:  16900 , step:  16900 , train loss:  0.08053527647163225 , test loss:  0.10397973027532426\n",
      "Epoch:  16910 , step:  16910 , train loss:  0.080515543541462 , test loss:  0.10396448086194912\n",
      "Epoch:  16920 , step:  16920 , train loss:  0.08049582750825975 , test loss:  0.1039492474826157\n",
      "Epoch:  16930 , step:  16930 , train loss:  0.08047612834772266 , test loss:  0.10393403011309485\n",
      "Epoch:  16940 , step:  16940 , train loss:  0.08045644603559683 , test loss:  0.10391882872920684\n",
      "Epoch:  16950 , step:  16950 , train loss:  0.080436780547677 , test loss:  0.10390364330682074\n",
      "Epoch:  16960 , step:  16960 , train loss:  0.08041713185980652 , test loss:  0.10388847382185443\n",
      "Epoch:  16970 , step:  16970 , train loss:  0.08039749994787714 , test loss:  0.10387332025027457\n",
      "Epoch:  16980 , step:  16980 , train loss:  0.08037788478782891 , test loss:  0.1038581825680964\n",
      "Epoch:  16990 , step:  16990 , train loss:  0.08035828635565015 , test loss:  0.10384306075138348\n",
      "Epoch:  17000 , step:  17000 , train loss:  0.08033870462737719 , test loss:  0.10382795477624794\n",
      "Epoch:  17010 , step:  17010 , train loss:  0.08031913957909435 , test loss:  0.10381286461884985\n",
      "Epoch:  17020 , step:  17020 , train loss:  0.08029959118693375 , test loss:  0.10379779025539768\n",
      "Epoch:  17030 , step:  17030 , train loss:  0.0802800594270752 , test loss:  0.10378273166214756\n",
      "Epoch:  17040 , step:  17040 , train loss:  0.08026054427574615 , test loss:  0.10376768881540374\n",
      "Epoch:  17050 , step:  17050 , train loss:  0.08024104570922143 , test loss:  0.10375266169151803\n",
      "Epoch:  17060 , step:  17060 , train loss:  0.0802215637038233 , test loss:  0.10373765026688979\n",
      "Epoch:  17070 , step:  17070 , train loss:  0.08020209823592112 , test loss:  0.10372265451796607\n",
      "Epoch:  17080 , step:  17080 , train loss:  0.08018264928193146 , test loss:  0.10370767442124121\n",
      "Epoch:  17090 , step:  17090 , train loss:  0.08016321681831777 , test loss:  0.10369270995325659\n",
      "Epoch:  17100 , step:  17100 , train loss:  0.08014380082159048 , test loss:  0.10367776109060095\n",
      "Epoch:  17110 , step:  17110 , train loss:  0.08012440126830658 , test loss:  0.10366282780991004\n",
      "Epoch:  17120 , step:  17120 , train loss:  0.08010501813506986 , test loss:  0.10364791008786627\n",
      "Epoch:  17130 , step:  17130 , train loss:  0.08008565139853045 , test loss:  0.1036330079011991\n",
      "Epoch:  17140 , step:  17140 , train loss:  0.08006630103538499 , test loss:  0.10361812122668447\n",
      "Epoch:  17150 , step:  17150 , train loss:  0.08004696702237633 , test loss:  0.10360325004114476\n",
      "Epoch:  17160 , step:  17160 , train loss:  0.08002764933629344 , test loss:  0.10358839432144909\n",
      "Epoch:  17170 , step:  17170 , train loss:  0.08000834795397133 , test loss:  0.10357355404451252\n",
      "Epoch:  17180 , step:  17180 , train loss:  0.07998906285229092 , test loss:  0.10355872918729647\n",
      "Epoch:  17190 , step:  17190 , train loss:  0.07996979400817898 , test loss:  0.10354391972680846\n",
      "Epoch:  17200 , step:  17200 , train loss:  0.07995054139860784 , test loss:  0.10352912564010193\n",
      "Epoch:  17210 , step:  17210 , train loss:  0.07993130500059553 , test loss:  0.10351434690427586\n",
      "Epoch:  17220 , step:  17220 , train loss:  0.07991208479120543 , test loss:  0.1034995834964755\n",
      "Epoch:  17230 , step:  17230 , train loss:  0.07989288074754625 , test loss:  0.10348483539389132\n",
      "Epoch:  17240 , step:  17240 , train loss:  0.079873692846772 , test loss:  0.10347010257375933\n",
      "Epoch:  17250 , step:  17250 , train loss:  0.07985452106608165 , test loss:  0.10345538501336093\n",
      "Epoch:  17260 , step:  17260 , train loss:  0.07983536538271935 , test loss:  0.10344068269002288\n",
      "Epoch:  17270 , step:  17270 , train loss:  0.07981622577397393 , test loss:  0.10342599558111687\n",
      "Epoch:  17280 , step:  17280 , train loss:  0.0797971022171791 , test loss:  0.10341132366405975\n",
      "Epoch:  17290 , step:  17290 , train loss:  0.07977799468971318 , test loss:  0.10339666691631348\n",
      "Epoch:  17300 , step:  17300 , train loss:  0.07975890316899902 , test loss:  0.10338202531538444\n",
      "Epoch:  17310 , step:  17310 , train loss:  0.07973982763250394 , test loss:  0.10336739883882402\n",
      "Epoch:  17320 , step:  17320 , train loss:  0.07972076805773948 , test loss:  0.10335278746422807\n",
      "Epoch:  17330 , step:  17330 , train loss:  0.07970172442226148 , test loss:  0.10333819116923691\n",
      "Epoch:  17340 , step:  17340 , train loss:  0.07968269670366974 , test loss:  0.10332360993153514\n",
      "Epoch:  17350 , step:  17350 , train loss:  0.07966368487960822 , test loss:  0.10330904372885182\n",
      "Epoch:  17360 , step:  17360 , train loss:  0.07964468892776456 , test loss:  0.10329449253895995\n",
      "Epoch:  17370 , step:  17370 , train loss:  0.07962570882587028 , test loss:  0.10327995633967661\n",
      "Epoch:  17380 , step:  17380 , train loss:  0.07960674455170047 , test loss:  0.10326543510886291\n",
      "Epoch:  17390 , step:  17390 , train loss:  0.07958779608307379 , test loss:  0.1032509288244236\n",
      "Epoch:  17400 , step:  17400 , train loss:  0.07956886339785234 , test loss:  0.10323643746430732\n",
      "Epoch:  17410 , step:  17410 , train loss:  0.07954994647394151 , test loss:  0.10322196100650609\n",
      "Epoch:  17420 , step:  17420 , train loss:  0.07953104528928993 , test loss:  0.10320749942905558\n",
      "Epoch:  17430 , step:  17430 , train loss:  0.07951215982188929 , test loss:  0.10319305271003472\n",
      "Epoch:  17440 , step:  17440 , train loss:  0.0794932900497743 , test loss:  0.10317862082756586\n",
      "Epoch:  17450 , step:  17450 , train loss:  0.07947443595102256 , test loss:  0.10316420375981433\n",
      "Epoch:  17460 , step:  17460 , train loss:  0.07945559750375443 , test loss:  0.10314980148498851\n",
      "Epoch:  17470 , step:  17470 , train loss:  0.07943677468613293 , test loss:  0.10313541398134006\n",
      "Epoch:  17480 , step:  17480 , train loss:  0.07941796747636372 , test loss:  0.10312104122716291\n",
      "Epoch:  17490 , step:  17490 , train loss:  0.0793991758526948 , test loss:  0.10310668320079432\n",
      "Epoch:  17500 , step:  17500 , train loss:  0.07938039979341661 , test loss:  0.10309233988061374\n",
      "Epoch:  17510 , step:  17510 , train loss:  0.07936163927686184 , test loss:  0.1030780112450433\n",
      "Epoch:  17520 , step:  17520 , train loss:  0.07934289428140523 , test loss:  0.10306369727254759\n",
      "Epoch:  17530 , step:  17530 , train loss:  0.07932416478546367 , test loss:  0.10304939794163345\n",
      "Epoch:  17540 , step:  17540 , train loss:  0.0793054507674959 , test loss:  0.10303511323084988\n",
      "Epoch:  17550 , step:  17550 , train loss:  0.07928675220600252 , test loss:  0.10302084311878815\n",
      "Epoch:  17560 , step:  17560 , train loss:  0.07926806907952588 , test loss:  0.10300658758408138\n",
      "Epoch:  17570 , step:  17570 , train loss:  0.0792494013666499 , test loss:  0.10299234660540454\n",
      "Epoch:  17580 , step:  17580 , train loss:  0.07923074904600001 , test loss:  0.1029781201614745\n",
      "Epoch:  17590 , step:  17590 , train loss:  0.0792121120962431 , test loss:  0.10296390823104988\n",
      "Epoch:  17600 , step:  17600 , train loss:  0.07919349049608738 , test loss:  0.10294971079293076\n",
      "Epoch:  17610 , step:  17610 , train loss:  0.07917488422428215 , test loss:  0.10293552782595868\n",
      "Epoch:  17620 , step:  17620 , train loss:  0.07915629325961801 , test loss:  0.1029213593090167\n",
      "Epoch:  17630 , step:  17630 , train loss:  0.07913771758092637 , test loss:  0.10290720522102897\n",
      "Epoch:  17640 , step:  17640 , train loss:  0.07911915716707967 , test loss:  0.102893065540961\n",
      "Epoch:  17650 , step:  17650 , train loss:  0.07910061199699107 , test loss:  0.10287894024781934\n",
      "Epoch:  17660 , step:  17660 , train loss:  0.0790820820496145 , test loss:  0.10286482932065137\n",
      "Epoch:  17670 , step:  17670 , train loss:  0.07906356730394444 , test loss:  0.10285073273854543\n",
      "Epoch:  17680 , step:  17680 , train loss:  0.07904506773901589 , test loss:  0.10283665048063079\n",
      "Epoch:  17690 , step:  17690 , train loss:  0.07902658333390424 , test loss:  0.10282258252607705\n",
      "Epoch:  17700 , step:  17700 , train loss:  0.07900811406772515 , test loss:  0.10280852885409483\n",
      "Epoch:  17710 , step:  17710 , train loss:  0.07898965991963457 , test loss:  0.10279448944393482\n",
      "Epoch:  17720 , step:  17720 , train loss:  0.07897122086882843 , test loss:  0.1027804642748883\n",
      "Epoch:  17730 , step:  17730 , train loss:  0.07895279689454276 , test loss:  0.10276645332628666\n",
      "Epoch:  17740 , step:  17740 , train loss:  0.07893438797605348 , test loss:  0.10275245657750173\n",
      "Epoch:  17750 , step:  17750 , train loss:  0.07891599409267622 , test loss:  0.10273847400794521\n",
      "Epoch:  17760 , step:  17760 , train loss:  0.07889761522376643 , test loss:  0.10272450559706882\n",
      "Epoch:  17770 , step:  17770 , train loss:  0.07887925134871913 , test loss:  0.10271055132436417\n",
      "Epoch:  17780 , step:  17780 , train loss:  0.07886090244696882 , test loss:  0.10269661116936273\n",
      "Epoch:  17790 , step:  17790 , train loss:  0.07884256849798944 , test loss:  0.10268268511163552\n",
      "Epoch:  17800 , step:  17800 , train loss:  0.0788242494812943 , test loss:  0.10266877313079327\n",
      "Epoch:  17810 , step:  17810 , train loss:  0.07880594537643579 , test loss:  0.10265487520648621\n",
      "Epoch:  17820 , step:  17820 , train loss:  0.0787876561630056 , test loss:  0.10264099131840379\n",
      "Epoch:  17830 , step:  17830 , train loss:  0.07876938182063432 , test loss:  0.10262712144627498\n",
      "Epoch:  17840 , step:  17840 , train loss:  0.07875112232899154 , test loss:  0.10261326556986787\n",
      "Epoch:  17850 , step:  17850 , train loss:  0.07873287766778564 , test loss:  0.10259942366898964\n",
      "Epoch:  17860 , step:  17860 , train loss:  0.07871464781676381 , test loss:  0.10258559572348658\n",
      "Epoch:  17870 , step:  17870 , train loss:  0.0786964327557118 , test loss:  0.10257178171324378\n",
      "Epoch:  17880 , step:  17880 , train loss:  0.07867823246445396 , test loss:  0.1025579816181851\n",
      "Epoch:  17890 , step:  17890 , train loss:  0.07866004692285314 , test loss:  0.1025441954182734\n",
      "Epoch:  17900 , step:  17900 , train loss:  0.07864187611081051 , test loss:  0.10253042309350996\n",
      "Epoch:  17910 , step:  17910 , train loss:  0.07862372000826548 , test loss:  0.1025166646239347\n",
      "Epoch:  17920 , step:  17920 , train loss:  0.07860557859519572 , test loss:  0.10250291998962595\n",
      "Epoch:  17930 , step:  17930 , train loss:  0.07858745185161689 , test loss:  0.10248918917070038\n",
      "Epoch:  17940 , step:  17940 , train loss:  0.07856933975758275 , test loss:  0.10247547214731294\n",
      "Epoch:  17950 , step:  17950 , train loss:  0.07855124229318489 , test loss:  0.1024617688996567\n",
      "Epoch:  17960 , step:  17960 , train loss:  0.07853315943855269 , test loss:  0.1024480794079631\n",
      "Epoch:  17970 , step:  17970 , train loss:  0.07851509117385326 , test loss:  0.10243440365250107\n",
      "Epoch:  17980 , step:  17980 , train loss:  0.07849703747929145 , test loss:  0.1024207416135779\n",
      "Epoch:  17990 , step:  17990 , train loss:  0.07847899833510945 , test loss:  0.10240709327153846\n",
      "Epoch:  18000 , step:  18000 , train loss:  0.07846097372158704 , test loss:  0.10239345860676523\n",
      "Epoch:  18010 , step:  18010 , train loss:  0.07844296361904125 , test loss:  0.10237983759967859\n",
      "Epoch:  18020 , step:  18020 , train loss:  0.0784249680078265 , test loss:  0.10236623023073632\n",
      "Epoch:  18030 , step:  18030 , train loss:  0.07840698686833417 , test loss:  0.10235263648043358\n",
      "Epoch:  18040 , step:  18040 , train loss:  0.07838902018099297 , test loss:  0.10233905632930292\n",
      "Epoch:  18050 , step:  18050 , train loss:  0.07837106792626837 , test loss:  0.10232548975791415\n",
      "Epoch:  18060 , step:  18060 , train loss:  0.07835313008466292 , test loss:  0.1023119367468743\n",
      "Epoch:  18070 , step:  18070 , train loss:  0.07833520663671589 , test loss:  0.10229839727682746\n",
      "Epoch:  18080 , step:  18080 , train loss:  0.07831729756300325 , test loss:  0.10228487132845464\n",
      "Epoch:  18090 , step:  18090 , train loss:  0.0782994028441377 , test loss:  0.10227135888247388\n",
      "Epoch:  18100 , step:  18100 , train loss:  0.07828152246076833 , test loss:  0.10225785991963986\n",
      "Epoch:  18110 , step:  18110 , train loss:  0.0782636563935809 , test loss:  0.10224437442074426\n",
      "Epoch:  18120 , step:  18120 , train loss:  0.07824580462329735 , test loss:  0.10223090236661513\n",
      "Epoch:  18130 , step:  18130 , train loss:  0.07822796713067597 , test loss:  0.10221744373811707\n",
      "Epoch:  18140 , step:  18140 , train loss:  0.07821014389651124 , test loss:  0.10220399851615138\n",
      "Epoch:  18150 , step:  18150 , train loss:  0.07819233490163371 , test loss:  0.10219056668165555\n",
      "Epoch:  18160 , step:  18160 , train loss:  0.07817454012691011 , test loss:  0.10217714821560343\n",
      "Epoch:  18170 , step:  18170 , train loss:  0.07815675955324285 , test loss:  0.10216374309900503\n",
      "Epoch:  18180 , step:  18180 , train loss:  0.07813899316157039 , test loss:  0.1021503513129065\n",
      "Epoch:  18190 , step:  18190 , train loss:  0.07812124093286679 , test loss:  0.10213697283838996\n",
      "Epoch:  18200 , step:  18200 , train loss:  0.07810350284814196 , test loss:  0.10212360765657359\n",
      "Epoch:  18210 , step:  18210 , train loss:  0.07808577888844123 , test loss:  0.10211025574861125\n",
      "Epoch:  18220 , step:  18220 , train loss:  0.07806806903484557 , test loss:  0.10209691709569281\n",
      "Epoch:  18230 , step:  18230 , train loss:  0.07805037326847122 , test loss:  0.10208359167904359\n",
      "Epoch:  18240 , step:  18240 , train loss:  0.07803269157046991 , test loss:  0.10207027947992464\n",
      "Epoch:  18250 , step:  18250 , train loss:  0.07801502392202848 , test loss:  0.10205698047963249\n",
      "Epoch:  18260 , step:  18260 , train loss:  0.07799737030436905 , test loss:  0.10204369465949913\n",
      "Epoch:  18270 , step:  18270 , train loss:  0.07797973069874872 , test loss:  0.10203042200089178\n",
      "Epoch:  18280 , step:  18280 , train loss:  0.07796210508645966 , test loss:  0.10201716248521316\n",
      "Epoch:  18290 , step:  18290 , train loss:  0.07794449344882888 , test loss:  0.10200391609390085\n",
      "Epoch:  18300 , step:  18300 , train loss:  0.07792689576721827 , test loss:  0.10199068280842781\n",
      "Epoch:  18310 , step:  18310 , train loss:  0.07790931202302444 , test loss:  0.1019774626103019\n",
      "Epoch:  18320 , step:  18320 , train loss:  0.0778917421976787 , test loss:  0.1019642554810658\n",
      "Epoch:  18330 , step:  18330 , train loss:  0.07787418627264689 , test loss:  0.10195106140229716\n",
      "Epoch:  18340 , step:  18340 , train loss:  0.07785664422942937 , test loss:  0.10193788035560848\n",
      "Epoch:  18350 , step:  18350 , train loss:  0.07783911604956088 , test loss:  0.10192471232264666\n",
      "Epoch:  18360 , step:  18360 , train loss:  0.07782160171461056 , test loss:  0.10191155728509342\n",
      "Epoch:  18370 , step:  18370 , train loss:  0.07780410120618172 , test loss:  0.10189841522466497\n",
      "Epoch:  18380 , step:  18380 , train loss:  0.07778661450591191 , test loss:  0.10188528612311183\n",
      "Epoch:  18390 , step:  18390 , train loss:  0.07776914159547274 , test loss:  0.10187216996221907\n",
      "Epoch:  18400 , step:  18400 , train loss:  0.07775168245656977 , test loss:  0.10185906672380582\n",
      "Epoch:  18410 , step:  18410 , train loss:  0.07773423707094258 , test loss:  0.10184597638972556\n",
      "Epoch:  18420 , step:  18420 , train loss:  0.07771680542036455 , test loss:  0.10183289894186576\n",
      "Epoch:  18430 , step:  18430 , train loss:  0.0776993874866428 , test loss:  0.10181983436214799\n",
      "Epoch:  18440 , step:  18440 , train loss:  0.07768198325161817 , test loss:  0.10180678263252776\n",
      "Epoch:  18450 , step:  18450 , train loss:  0.07766459269716511 , test loss:  0.10179374373499439\n",
      "Epoch:  18460 , step:  18460 , train loss:  0.07764721580519154 , test loss:  0.10178071765157108\n",
      "Epoch:  18470 , step:  18470 , train loss:  0.07762985255763886 , test loss:  0.10176770436431459\n",
      "Epoch:  18480 , step:  18480 , train loss:  0.07761250293648189 , test loss:  0.1017547038553156\n",
      "Epoch:  18490 , step:  18490 , train loss:  0.07759516692372861 , test loss:  0.10174171610669784\n",
      "Epoch:  18500 , step:  18500 , train loss:  0.07757784450142033 , test loss:  0.10172874110061911\n",
      "Epoch:  18510 , step:  18510 , train loss:  0.07756053565163143 , test loss:  0.10171577881927008\n",
      "Epoch:  18520 , step:  18520 , train loss:  0.07754324035646937 , test loss:  0.10170282924487513\n",
      "Epoch:  18530 , step:  18530 , train loss:  0.0775259585980745 , test loss:  0.10168989235969157\n",
      "Epoch:  18540 , step:  18540 , train loss:  0.07750869035862021 , test loss:  0.10167696814601\n",
      "Epoch:  18550 , step:  18550 , train loss:  0.07749143562031258 , test loss:  0.10166405658615406\n",
      "Epoch:  18560 , step:  18560 , train loss:  0.07747419436539045 , test loss:  0.10165115766248053\n",
      "Epoch:  18570 , step:  18570 , train loss:  0.07745696657612541 , test loss:  0.10163827135737896\n",
      "Epoch:  18580 , step:  18580 , train loss:  0.07743975223482155 , test loss:  0.10162539765327168\n",
      "Epoch:  18590 , step:  18590 , train loss:  0.07742255132381551 , test loss:  0.10161253653261398\n",
      "Epoch:  18600 , step:  18600 , train loss:  0.07740536382547628 , test loss:  0.10159968797789372\n",
      "Epoch:  18610 , step:  18610 , train loss:  0.07738818972220536 , test loss:  0.10158685197163143\n",
      "Epoch:  18620 , step:  18620 , train loss:  0.07737102899643639 , test loss:  0.10157402849638007\n",
      "Epoch:  18630 , step:  18630 , train loss:  0.0773538816306353 , test loss:  0.1015612175347253\n",
      "Epoch:  18640 , step:  18640 , train loss:  0.07733674760730007 , test loss:  0.10154841906928486\n",
      "Epoch:  18650 , step:  18650 , train loss:  0.07731962690896081 , test loss:  0.10153563308270905\n",
      "Epoch:  18660 , step:  18660 , train loss:  0.07730251951817958 , test loss:  0.10152285955768024\n",
      "Epoch:  18670 , step:  18670 , train loss:  0.07728542541755033 , test loss:  0.10151009847691303\n",
      "Epoch:  18680 , step:  18680 , train loss:  0.07726834458969886 , test loss:  0.10149734982315414\n",
      "Epoch:  18690 , step:  18690 , train loss:  0.07725127701728271 , test loss:  0.10148461357918223\n",
      "Epoch:  18700 , step:  18700 , train loss:  0.07723422268299107 , test loss:  0.10147188972780788\n",
      "Epoch:  18710 , step:  18710 , train loss:  0.07721718156954482 , test loss:  0.10145917825187363\n",
      "Epoch:  18720 , step:  18720 , train loss:  0.07720015365969629 , test loss:  0.10144647913425366\n",
      "Epoch:  18730 , step:  18730 , train loss:  0.07718313893622926 , test loss:  0.10143379235785405\n",
      "Epoch:  18740 , step:  18740 , train loss:  0.07716613738195899 , test loss:  0.10142111790561237\n",
      "Epoch:  18750 , step:  18750 , train loss:  0.07714914897973195 , test loss:  0.1014084557604978\n",
      "Epoch:  18760 , step:  18760 , train loss:  0.07713217371242592 , test loss:  0.101395805905511\n",
      "Epoch:  18770 , step:  18770 , train loss:  0.07711521156294973 , test loss:  0.10138316832368408\n",
      "Epoch:  18780 , step:  18780 , train loss:  0.07709826251424352 , test loss:  0.1013705429980804\n",
      "Epoch:  18790 , step:  18790 , train loss:  0.07708132654927818 , test loss:  0.10135792991179472\n",
      "Epoch:  18800 , step:  18800 , train loss:  0.07706440365105575 , test loss:  0.10134532904795296\n",
      "Epoch:  18810 , step:  18810 , train loss:  0.07704749380260906 , test loss:  0.10133274038971217\n",
      "Epoch:  18820 , step:  18820 , train loss:  0.07703059698700175 , test loss:  0.10132016392026034\n",
      "Epoch:  18830 , step:  18830 , train loss:  0.07701371318732822 , test loss:  0.10130759962281659\n",
      "Epoch:  18840 , step:  18840 , train loss:  0.07699684238671348 , test loss:  0.10129504748063092\n",
      "Epoch:  18850 , step:  18850 , train loss:  0.07697998456831319 , test loss:  0.10128250747698403\n",
      "Epoch:  18860 , step:  18860 , train loss:  0.0769631397153135 , test loss:  0.1012699795951877\n",
      "Epoch:  18870 , step:  18870 , train loss:  0.07694630781093095 , test loss:  0.10125746381858407\n",
      "Epoch:  18880 , step:  18880 , train loss:  0.07692948883841255 , test loss:  0.101244960130546\n",
      "Epoch:  18890 , step:  18890 , train loss:  0.0769126827810356 , test loss:  0.10123246851447705\n",
      "Epoch:  18900 , step:  18900 , train loss:  0.07689588962210755 , test loss:  0.10121998895381117\n",
      "Epoch:  18910 , step:  18910 , train loss:  0.07687910934496613 , test loss:  0.10120752143201261\n",
      "Epoch:  18920 , step:  18920 , train loss:  0.07686234193297906 , test loss:  0.10119506593257623\n",
      "Epoch:  18930 , step:  18930 , train loss:  0.07684558736954418 , test loss:  0.10118262243902683\n",
      "Epoch:  18940 , step:  18940 , train loss:  0.07682884563808925 , test loss:  0.10117019093491975\n",
      "Epoch:  18950 , step:  18950 , train loss:  0.07681211672207189 , test loss:  0.10115777140384012\n",
      "Epoch:  18960 , step:  18960 , train loss:  0.07679540060497955 , test loss:  0.10114536382940344\n",
      "Epoch:  18970 , step:  18970 , train loss:  0.07677869727032946 , test loss:  0.10113296819525505\n",
      "Epoch:  18980 , step:  18980 , train loss:  0.0767620067016685 , test loss:  0.10112058448507022\n",
      "Epoch:  18990 , step:  18990 , train loss:  0.07674532888257318 , test loss:  0.10110821268255422\n",
      "Epoch:  19000 , step:  19000 , train loss:  0.07672866379664957 , test loss:  0.10109585277144181\n",
      "Epoch:  19010 , step:  19010 , train loss:  0.07671201142753313 , test loss:  0.10108350473549779\n",
      "Epoch:  19020 , step:  19020 , train loss:  0.07669537175888884 , test loss:  0.10107116855851657\n",
      "Epoch:  19030 , step:  19030 , train loss:  0.07667874477441097 , test loss:  0.10105884422432178\n",
      "Epoch:  19040 , step:  19040 , train loss:  0.07666213045782304 , test loss:  0.10104653171676699\n",
      "Epoch:  19050 , step:  19050 , train loss:  0.07664552879287781 , test loss:  0.10103423101973506\n",
      "Epoch:  19060 , step:  19060 , train loss:  0.07662893976335716 , test loss:  0.10102194211713823\n",
      "Epoch:  19070 , step:  19070 , train loss:  0.07661236335307206 , test loss:  0.10100966499291801\n",
      "Epoch:  19080 , step:  19080 , train loss:  0.07659579954586246 , test loss:  0.10099739963104531\n",
      "Epoch:  19090 , step:  19090 , train loss:  0.07657924832559727 , test loss:  0.10098514601552\n",
      "Epoch:  19100 , step:  19100 , train loss:  0.07656270967617425 , test loss:  0.1009729041303712\n",
      "Epoch:  19110 , step:  19110 , train loss:  0.07654618358151999 , test loss:  0.10096067395965715\n",
      "Epoch:  19120 , step:  19120 , train loss:  0.07652967002558977 , test loss:  0.10094845548746484\n",
      "Epoch:  19130 , step:  19130 , train loss:  0.07651316899236763 , test loss:  0.10093624869791036\n",
      "Epoch:  19140 , step:  19140 , train loss:  0.07649668046586613 , test loss:  0.10092405357513864\n",
      "Epoch:  19150 , step:  19150 , train loss:  0.07648020443012644 , test loss:  0.10091187010332321\n",
      "Epoch:  19160 , step:  19160 , train loss:  0.07646374086921813 , test loss:  0.10089969826666668\n",
      "Epoch:  19170 , step:  19170 , train loss:  0.07644728976723927 , test loss:  0.10088753804939982\n",
      "Epoch:  19180 , step:  19180 , train loss:  0.0764308511083162 , test loss:  0.10087538943578245\n",
      "Epoch:  19190 , step:  19190 , train loss:  0.07641442487660359 , test loss:  0.10086325241010256\n",
      "Epoch:  19200 , step:  19200 , train loss:  0.07639801105628434 , test loss:  0.10085112695667675\n",
      "Epoch:  19210 , step:  19210 , train loss:  0.07638160963156944 , test loss:  0.1008390130598501\n",
      "Epoch:  19220 , step:  19220 , train loss:  0.07636522058669801 , test loss:  0.10082691070399591\n",
      "Epoch:  19230 , step:  19230 , train loss:  0.0763488439059372 , test loss:  0.10081481987351566\n",
      "Epoch:  19240 , step:  19240 , train loss:  0.07633247957358212 , test loss:  0.10080274055283922\n",
      "Epoch:  19250 , step:  19250 , train loss:  0.07631612757395573 , test loss:  0.10079067272642434\n",
      "Epoch:  19260 , step:  19260 , train loss:  0.07629978789140893 , test loss:  0.1007786163787571\n",
      "Epoch:  19270 , step:  19270 , train loss:  0.07628346051032024 , test loss:  0.1007665714943514\n",
      "Epoch:  19280 , step:  19280 , train loss:  0.07626714541509602 , test loss:  0.10075453805774919\n",
      "Epoch:  19290 , step:  19290 , train loss:  0.07625084259017026 , test loss:  0.10074251605352029\n",
      "Epoch:  19300 , step:  19300 , train loss:  0.07623455202000438 , test loss:  0.10073050546626221\n",
      "Epoch:  19310 , step:  19310 , train loss:  0.07621827368908755 , test loss:  0.10071850628060033\n",
      "Epoch:  19320 , step:  19320 , train loss:  0.07620200758193625 , test loss:  0.10070651848118789\n",
      "Epoch:  19330 , step:  19330 , train loss:  0.07618575368309434 , test loss:  0.1006945420527053\n",
      "Epoch:  19340 , step:  19340 , train loss:  0.07616951197713309 , test loss:  0.10068257697986095\n",
      "Epoch:  19350 , step:  19350 , train loss:  0.07615328244865104 , test loss:  0.1006706232473905\n",
      "Epoch:  19360 , step:  19360 , train loss:  0.07613706508227382 , test loss:  0.1006586808400572\n",
      "Epoch:  19370 , step:  19370 , train loss:  0.07612085986265442 , test loss:  0.1006467497426515\n",
      "Epoch:  19380 , step:  19380 , train loss:  0.07610466677447265 , test loss:  0.1006348299399914\n",
      "Epoch:  19390 , step:  19390 , train loss:  0.07608848580243559 , test loss:  0.10062292141692188\n",
      "Epoch:  19400 , step:  19400 , train loss:  0.07607231693127713 , test loss:  0.10061102415831542\n",
      "Epoch:  19410 , step:  19410 , train loss:  0.07605616014575807 , test loss:  0.10059913814907127\n",
      "Epoch:  19420 , step:  19420 , train loss:  0.07604001543066613 , test loss:  0.10058726337411616\n",
      "Epoch:  19430 , step:  19430 , train loss:  0.07602388277081579 , test loss:  0.10057539981840345\n",
      "Epoch:  19440 , step:  19440 , train loss:  0.07600776215104817 , test loss:  0.1005635474669137\n",
      "Epoch:  19450 , step:  19450 , train loss:  0.07599165355623119 , test loss:  0.1005517063046543\n",
      "Epoch:  19460 , step:  19460 , train loss:  0.0759755569712592 , test loss:  0.10053987631665934\n",
      "Epoch:  19470 , step:  19470 , train loss:  0.07595947238105324 , test loss:  0.10052805748798985\n",
      "Epoch:  19480 , step:  19480 , train loss:  0.07594339977056075 , test loss:  0.10051624980373347\n",
      "Epoch:  19490 , step:  19490 , train loss:  0.07592733912475562 , test loss:  0.10050445324900453\n",
      "Epoch:  19500 , step:  19500 , train loss:  0.07591129042863809 , test loss:  0.10049266780894386\n",
      "Epoch:  19510 , step:  19510 , train loss:  0.07589525366723468 , test loss:  0.10048089346871897\n",
      "Epoch:  19520 , step:  19520 , train loss:  0.07587922882559821 , test loss:  0.1004691302135237\n",
      "Epoch:  19530 , step:  19530 , train loss:  0.07586321588880762 , test loss:  0.10045737802857842\n",
      "Epoch:  19540 , step:  19540 , train loss:  0.07584721484196802 , test loss:  0.10044563689912968\n",
      "Epoch:  19550 , step:  19550 , train loss:  0.07583122567021057 , test loss:  0.10043390681045049\n",
      "Epoch:  19560 , step:  19560 , train loss:  0.07581524835869245 , test loss:  0.10042218774783994\n",
      "Epoch:  19570 , step:  19570 , train loss:  0.07579928289259678 , test loss:  0.10041047969662345\n",
      "Epoch:  19580 , step:  19580 , train loss:  0.07578332925713255 , test loss:  0.1003987826421524\n",
      "Epoch:  19590 , step:  19590 , train loss:  0.07576738743753461 , test loss:  0.10038709656980438\n",
      "Epoch:  19600 , step:  19600 , train loss:  0.0757514574190636 , test loss:  0.10037542146498285\n",
      "Epoch:  19610 , step:  19610 , train loss:  0.07573553918700586 , test loss:  0.10036375731311731\n",
      "Epoch:  19620 , step:  19620 , train loss:  0.07571963272667337 , test loss:  0.10035210409966298\n",
      "Epoch:  19630 , step:  19630 , train loss:  0.07570373802340374 , test loss:  0.1003404618101011\n",
      "Epoch:  19640 , step:  19640 , train loss:  0.07568785506256012 , test loss:  0.10032883042993862\n",
      "Epoch:  19650 , step:  19650 , train loss:  0.07567198382953114 , test loss:  0.10031720994470802\n",
      "Epoch:  19660 , step:  19660 , train loss:  0.07565612430973091 , test loss:  0.10030560033996767\n",
      "Epoch:  19670 , step:  19670 , train loss:  0.07564027648859882 , test loss:  0.10029400160130145\n",
      "Epoch:  19680 , step:  19680 , train loss:  0.07562444035159968 , test loss:  0.10028241371431883\n",
      "Epoch:  19690 , step:  19690 , train loss:  0.07560861588422349 , test loss:  0.10027083666465453\n",
      "Epoch:  19700 , step:  19700 , train loss:  0.07559280307198549 , test loss:  0.10025927043796896\n",
      "Epoch:  19710 , step:  19710 , train loss:  0.07557700190042607 , test loss:  0.10024771501994782\n",
      "Epoch:  19720 , step:  19720 , train loss:  0.07556121235511071 , test loss:  0.10023617039630207\n",
      "Epoch:  19730 , step:  19730 , train loss:  0.07554543442162995 , test loss:  0.10022463655276795\n",
      "Epoch:  19740 , step:  19740 , train loss:  0.0755296680855992 , test loss:  0.10021311347510688\n",
      "Epoch:  19750 , step:  19750 , train loss:  0.07551391333265899 , test loss:  0.1002016011491055\n",
      "Epoch:  19760 , step:  19760 , train loss:  0.07549817014847456 , test loss:  0.10019009956057534\n",
      "Epoch:  19770 , step:  19770 , train loss:  0.07548243851873605 , test loss:  0.10017860869535326\n",
      "Epoch:  19780 , step:  19780 , train loss:  0.07546671842915828 , test loss:  0.10016712853930085\n",
      "Epoch:  19790 , step:  19790 , train loss:  0.07545100986548094 , test loss:  0.10015565907830472\n",
      "Epoch:  19800 , step:  19800 , train loss:  0.07543531281346817 , test loss:  0.10014420029827638\n",
      "Epoch:  19810 , step:  19810 , train loss:  0.07541962725890881 , test loss:  0.100132752185152\n",
      "Epoch:  19820 , step:  19820 , train loss:  0.07540395318761631 , test loss:  0.10012131472489268\n",
      "Epoch:  19830 , step:  19830 , train loss:  0.07538829058542842 , test loss:  0.10010988790348413\n",
      "Epoch:  19840 , step:  19840 , train loss:  0.07537263943820754 , test loss:  0.10009847170693664\n",
      "Epoch:  19850 , step:  19850 , train loss:  0.07535699973184029 , test loss:  0.1000870661212853\n",
      "Epoch:  19860 , step:  19860 , train loss:  0.0753413714522377 , test loss:  0.10007567113258946\n",
      "Epoch:  19870 , step:  19870 , train loss:  0.07532575458533504 , test loss:  0.10006428672693321\n",
      "Epoch:  19880 , step:  19880 , train loss:  0.07531014911709177 , test loss:  0.10005291289042495\n",
      "Epoch:  19890 , step:  19890 , train loss:  0.07529455503349158 , test loss:  0.10004154960919746\n",
      "Epoch:  19900 , step:  19900 , train loss:  0.07527897232054227 , test loss:  0.10003019686940773\n",
      "Epoch:  19910 , step:  19910 , train loss:  0.07526340096427561 , test loss:  0.10001885465723728\n",
      "Epoch:  19920 , step:  19920 , train loss:  0.07524784095074752 , test loss:  0.10000752295889172\n",
      "Epoch:  19930 , step:  19930 , train loss:  0.07523229226603771 , test loss:  0.09999620176060056\n",
      "Epoch:  19940 , step:  19940 , train loss:  0.07521675489624993 , test loss:  0.09998489104861791\n",
      "Epoch:  19950 , step:  19950 , train loss:  0.07520122882751169 , test loss:  0.09997359080922158\n",
      "Epoch:  19960 , step:  19960 , train loss:  0.07518571404597432 , test loss:  0.09996230102871342\n",
      "Epoch:  19970 , step:  19970 , train loss:  0.0751702105378129 , test loss:  0.09995102169341936\n",
      "Epoch:  19980 , step:  19980 , train loss:  0.07515471828922624 , test loss:  0.09993975278968908\n",
      "Epoch:  19990 , step:  19990 , train loss:  0.0751392372864367 , test loss:  0.09992849430389623\n",
      "Epoch:  20000 , step:  20000 , train loss:  0.0751237675156903 , test loss:  0.09991724622243815\n",
      "Epoch:  20010 , step:  20010 , train loss:  0.07510830896325657 , test loss:  0.09990600853173612\n",
      "Epoch:  20020 , step:  20020 , train loss:  0.07509286161542851 , test loss:  0.09989478121823483\n",
      "Epoch:  20030 , step:  20030 , train loss:  0.07507742545852258 , test loss:  0.09988356426840274\n",
      "Epoch:  20040 , step:  20040 , train loss:  0.07506200047887861 , test loss:  0.09987235766873179\n",
      "Epoch:  20050 , step:  20050 , train loss:  0.07504658666285977 , test loss:  0.09986116140573771\n",
      "Epoch:  20060 , step:  20060 , train loss:  0.07503118399685242 , test loss:  0.09984997546595943\n",
      "Epoch:  20070 , step:  20070 , train loss:  0.07501579246726632 , test loss:  0.09983879983595947\n",
      "Epoch:  20080 , step:  20080 , train loss:  0.07500041206053425 , test loss:  0.09982763450232376\n",
      "Epoch:  20090 , step:  20090 , train loss:  0.07498504276311223 , test loss:  0.0998164794516613\n",
      "Epoch:  20100 , step:  20100 , train loss:  0.07496968456147923 , test loss:  0.0998053346706047\n",
      "Epoch:  20110 , step:  20110 , train loss:  0.07495433744213738 , test loss:  0.09979420014580952\n",
      "Epoch:  20120 , step:  20120 , train loss:  0.0749390013916117 , test loss:  0.09978307586395473\n",
      "Epoch:  20130 , step:  20130 , train loss:  0.07492367639645015 , test loss:  0.09977196181174225\n",
      "Epoch:  20140 , step:  20140 , train loss:  0.0749083624432236 , test loss:  0.09976085797589708\n",
      "Epoch:  20150 , step:  20150 , train loss:  0.07489305951852568 , test loss:  0.09974976434316744\n",
      "Epoch:  20160 , step:  20160 , train loss:  0.07487776760897283 , test loss:  0.09973868090032417\n",
      "Epoch:  20170 , step:  20170 , train loss:  0.07486248670120427 , test loss:  0.0997276076341613\n",
      "Epoch:  20180 , step:  20180 , train loss:  0.07484721678188183 , test loss:  0.09971654453149581\n",
      "Epoch:  20190 , step:  20190 , train loss:  0.07483195783768992 , test loss:  0.09970549157916724\n",
      "Epoch:  20200 , step:  20200 , train loss:  0.07481670985533567 , test loss:  0.09969444876403813\n",
      "Epoch:  20210 , step:  20210 , train loss:  0.0748014728215486 , test loss:  0.09968341607299351\n",
      "Epoch:  20220 , step:  20220 , train loss:  0.07478624672308082 , test loss:  0.09967239349294121\n",
      "Epoch:  20230 , step:  20230 , train loss:  0.07477103154670679 , test loss:  0.09966138101081184\n",
      "Epoch:  20240 , step:  20240 , train loss:  0.07475582727922342 , test loss:  0.09965037861355838\n",
      "Epoch:  20250 , step:  20250 , train loss:  0.07474063390744982 , test loss:  0.09963938628815626\n",
      "Epoch:  20260 , step:  20260 , train loss:  0.07472545141822755 , test loss:  0.09962840402160372\n",
      "Epoch:  20270 , step:  20270 , train loss:  0.07471027979842036 , test loss:  0.09961743180092106\n",
      "Epoch:  20280 , step:  20280 , train loss:  0.07469511903491415 , test loss:  0.09960646961315131\n",
      "Epoch:  20290 , step:  20290 , train loss:  0.07467996911461691 , test loss:  0.0995955174453595\n",
      "Epoch:  20300 , step:  20300 , train loss:  0.0746648300244589 , test loss:  0.0995845752846332\n",
      "Epoch:  20310 , step:  20310 , train loss:  0.07464970175139223 , test loss:  0.09957364311808199\n",
      "Epoch:  20320 , step:  20320 , train loss:  0.07463458428239114 , test loss:  0.09956272093283797\n",
      "Epoch:  20330 , step:  20330 , train loss:  0.07461947760445177 , test loss:  0.09955180871605501\n",
      "Epoch:  20340 , step:  20340 , train loss:  0.07460438170459215 , test loss:  0.09954090645490928\n",
      "Epoch:  20350 , step:  20350 , train loss:  0.07458929656985215 , test loss:  0.09953001413659902\n",
      "Epoch:  20360 , step:  20360 , train loss:  0.07457422218729357 , test loss:  0.0995191317483443\n",
      "Epoch:  20370 , step:  20370 , train loss:  0.0745591585439998 , test loss:  0.09950825927738725\n",
      "Epoch:  20380 , step:  20380 , train loss:  0.07454410562707608 , test loss:  0.09949739671099198\n",
      "Epoch:  20390 , step:  20390 , train loss:  0.07452906342364925 , test loss:  0.09948654403644425\n",
      "Epoch:  20400 , step:  20400 , train loss:  0.07451403192086772 , test loss:  0.0994757012410518\n",
      "Epoch:  20410 , step:  20410 , train loss:  0.07449901110590162 , test loss:  0.09946486831214404\n",
      "Epoch:  20420 , step:  20420 , train loss:  0.07448400096594249 , test loss:  0.0994540452370722\n",
      "Epoch:  20430 , step:  20430 , train loss:  0.07446900148820339 , test loss:  0.09944323200320901\n",
      "Epoch:  20440 , step:  20440 , train loss:  0.07445401265991879 , test loss:  0.09943242859794899\n",
      "Epoch:  20450 , step:  20450 , train loss:  0.07443903446834461 , test loss:  0.09942163500870808\n",
      "Epoch:  20460 , step:  20460 , train loss:  0.07442406690075806 , test loss:  0.0994108512229239\n",
      "Epoch:  20470 , step:  20470 , train loss:  0.07440910994445765 , test loss:  0.09940007722805544\n",
      "Epoch:  20480 , step:  20480 , train loss:  0.07439416358676315 , test loss:  0.09938931301158321\n",
      "Epoch:  20490 , step:  20490 , train loss:  0.07437922781501559 , test loss:  0.0993785585610089\n",
      "Epoch:  20500 , step:  20500 , train loss:  0.07436430261657705 , test loss:  0.0993678138638559\n",
      "Epoch:  20510 , step:  20510 , train loss:  0.07434938797883082 , test loss:  0.09935707890766865\n",
      "Epoch:  20520 , step:  20520 , train loss:  0.07433448388918121 , test loss:  0.09934635368001295\n",
      "Epoch:  20530 , step:  20530 , train loss:  0.07431959033505357 , test loss:  0.09933563816847556\n",
      "Epoch:  20540 , step:  20540 , train loss:  0.07430470730389427 , test loss:  0.09932493236066475\n",
      "Epoch:  20550 , step:  20550 , train loss:  0.07428983478317054 , test loss:  0.09931423624420968\n",
      "Epoch:  20560 , step:  20560 , train loss:  0.07427497276037054 , test loss:  0.09930354980676065\n",
      "Epoch:  20570 , step:  20570 , train loss:  0.07426012122300328 , test loss:  0.09929287303598905\n",
      "Epoch:  20580 , step:  20580 , train loss:  0.07424528015859858 , test loss:  0.09928220591958713\n",
      "Epoch:  20590 , step:  20590 , train loss:  0.074230449554707 , test loss:  0.09927154844526821\n",
      "Epoch:  20600 , step:  20600 , train loss:  0.07421562939889978 , test loss:  0.09926090060076627\n",
      "Epoch:  20610 , step:  20610 , train loss:  0.0742008196787689 , test loss:  0.0992502623738364\n",
      "Epoch:  20620 , step:  20620 , train loss:  0.07418602038192691 , test loss:  0.0992396337522543\n",
      "Epoch:  20630 , step:  20630 , train loss:  0.07417123149600698 , test loss:  0.09922901472381665\n",
      "Epoch:  20640 , step:  20640 , train loss:  0.07415645300866275 , test loss:  0.09921840527634043\n",
      "Epoch:  20650 , step:  20650 , train loss:  0.07414168490756845 , test loss:  0.09920780539766383\n",
      "Epoch:  20660 , step:  20660 , train loss:  0.07412692718041868 , test loss:  0.09919721507564533\n",
      "Epoch:  20670 , step:  20670 , train loss:  0.07411217981492847 , test loss:  0.09918663429816392\n",
      "Epoch:  20680 , step:  20680 , train loss:  0.07409744279883324 , test loss:  0.09917606305311949\n",
      "Epoch:  20690 , step:  20690 , train loss:  0.07408271611988869 , test loss:  0.09916550132843199\n",
      "Epoch:  20700 , step:  20700 , train loss:  0.07406799976587079 , test loss:  0.09915494911204217\n",
      "Epoch:  20710 , step:  20710 , train loss:  0.0740532937245758 , test loss:  0.09914440639191083\n",
      "Epoch:  20720 , step:  20720 , train loss:  0.07403859798382015 , test loss:  0.09913387315601963\n",
      "Epoch:  20730 , step:  20730 , train loss:  0.07402391253144036 , test loss:  0.09912334939237007\n",
      "Epoch:  20740 , step:  20740 , train loss:  0.07400923735529309 , test loss:  0.09911283508898416\n",
      "Epoch:  20750 , step:  20750 , train loss:  0.07399457244325511 , test loss:  0.09910233023390422\n",
      "Epoch:  20760 , step:  20760 , train loss:  0.07397991778322316 , test loss:  0.09909183481519242\n",
      "Epoch:  20770 , step:  20770 , train loss:  0.07396527336311394 , test loss:  0.09908134882093153\n",
      "Epoch:  20780 , step:  20780 , train loss:  0.07395063917086414 , test loss:  0.09907087223922405\n",
      "Epoch:  20790 , step:  20790 , train loss:  0.07393601519443029 , test loss:  0.09906040505819283\n",
      "Epoch:  20800 , step:  20800 , train loss:  0.07392140142178881 , test loss:  0.09904994726598047\n",
      "Epoch:  20810 , step:  20810 , train loss:  0.07390679784093593 , test loss:  0.09903949885074975\n",
      "Epoch:  20820 , step:  20820 , train loss:  0.07389220443988757 , test loss:  0.0990290598006833\n",
      "Epoch:  20830 , step:  20830 , train loss:  0.07387762120667953 , test loss:  0.09901863010398358\n",
      "Epoch:  20840 , step:  20840 , train loss:  0.07386304812936709 , test loss:  0.0990082097488731\n",
      "Epoch:  20850 , step:  20850 , train loss:  0.07384848519602534 , test loss:  0.09899779872359413\n",
      "Epoch:  20860 , step:  20860 , train loss:  0.07383393239474893 , test loss:  0.09898739701640842\n",
      "Epoch:  20870 , step:  20870 , train loss:  0.07381938971365203 , test loss:  0.0989770046155978\n",
      "Epoch:  20880 , step:  20880 , train loss:  0.07380485714086835 , test loss:  0.0989666215094637\n",
      "Epoch:  20890 , step:  20890 , train loss:  0.07379033466455105 , test loss:  0.09895624768632705\n",
      "Epoch:  20900 , step:  20900 , train loss:  0.07377582227287281 , test loss:  0.09894588313452843\n",
      "Epoch:  20910 , step:  20910 , train loss:  0.0737613199540256 , test loss:  0.09893552784242812\n",
      "Epoch:  20920 , step:  20920 , train loss:  0.07374682769622083 , test loss:  0.09892518179840583\n",
      "Epoch:  20930 , step:  20930 , train loss:  0.07373234548768916 , test loss:  0.09891484499086067\n",
      "Epoch:  20940 , step:  20940 , train loss:  0.07371787331668053 , test loss:  0.09890451740821134\n",
      "Epoch:  20950 , step:  20950 , train loss:  0.0737034111714642 , test loss:  0.0988941990388958\n",
      "Epoch:  20960 , step:  20960 , train loss:  0.07368895904032852 , test loss:  0.09888388987137142\n",
      "Epoch:  20970 , step:  20970 , train loss:  0.07367451691158101 , test loss:  0.09887358989411488\n",
      "Epoch:  20980 , step:  20980 , train loss:  0.07366008477354834 , test loss:  0.09886329909562212\n",
      "Epoch:  20990 , step:  20990 , train loss:  0.07364566261457622 , test loss:  0.09885301746440846\n",
      "Epoch:  21000 , step:  21000 , train loss:  0.07363125042302944 , test loss:  0.09884274498900807\n",
      "Epoch:  21010 , step:  21010 , train loss:  0.0736168481872917 , test loss:  0.09883248165797463\n",
      "Epoch:  21020 , step:  21020 , train loss:  0.07360245589576571 , test loss:  0.09882222745988081\n",
      "Epoch:  21030 , step:  21030 , train loss:  0.0735880735368731 , test loss:  0.09881198238331823\n",
      "Epoch:  21040 , step:  21040 , train loss:  0.07357370109905435 , test loss:  0.09880174641689773\n",
      "Epoch:  21050 , step:  21050 , train loss:  0.0735593385707688 , test loss:  0.09879151954924904\n",
      "Epoch:  21060 , step:  21060 , train loss:  0.07354498594049451 , test loss:  0.09878130176902082\n",
      "Epoch:  21070 , step:  21070 , train loss:  0.07353064319672838 , test loss:  0.0987710930648807\n",
      "Epoch:  21080 , step:  21080 , train loss:  0.07351631032798599 , test loss:  0.09876089342551522\n",
      "Epoch:  21090 , step:  21090 , train loss:  0.07350198732280161 , test loss:  0.09875070283962963\n",
      "Epoch:  21100 , step:  21100 , train loss:  0.07348767416972816 , test loss:  0.09874052129594814\n",
      "Epoch:  21110 , step:  21110 , train loss:  0.07347337085733714 , test loss:  0.0987303487832135\n",
      "Epoch:  21120 , step:  21120 , train loss:  0.07345907737421856 , test loss:  0.09872018529018742\n",
      "Epoch:  21130 , step:  21130 , train loss:  0.07344479370898105 , test loss:  0.09871003080565006\n",
      "Epoch:  21140 , step:  21140 , train loss:  0.07343051985025166 , test loss:  0.09869988531840038\n",
      "Epoch:  21150 , step:  21150 , train loss:  0.07341625578667592 , test loss:  0.09868974881725598\n",
      "Epoch:  21160 , step:  21160 , train loss:  0.07340200150691772 , test loss:  0.09867962129105272\n",
      "Epoch:  21170 , step:  21170 , train loss:  0.07338775699965938 , test loss:  0.09866950272864525\n",
      "Epoch:  21180 , step:  21180 , train loss:  0.07337352225360148 , test loss:  0.09865939311890683\n",
      "Epoch:  21190 , step:  21190 , train loss:  0.07335929725746299 , test loss:  0.09864929245072876\n",
      "Epoch:  21200 , step:  21200 , train loss:  0.073345081999981 , test loss:  0.09863920071302107\n",
      "Epoch:  21210 , step:  21210 , train loss:  0.07333087646991095 , test loss:  0.09862911789471199\n",
      "Epoch:  21220 , step:  21220 , train loss:  0.07331668065602635 , test loss:  0.09861904398474827\n",
      "Epoch:  21230 , step:  21230 , train loss:  0.07330249454711897 , test loss:  0.09860897897209474\n",
      "Epoch:  21240 , step:  21240 , train loss:  0.07328831813199856 , test loss:  0.0985989228457345\n",
      "Epoch:  21250 , step:  21250 , train loss:  0.07327415139949298 , test loss:  0.09858887559466899\n",
      "Epoch:  21260 , step:  21260 , train loss:  0.07325999433844817 , test loss:  0.09857883720791784\n",
      "Epoch:  21270 , step:  21270 , train loss:  0.073245846937728 , test loss:  0.09856880767451863\n",
      "Epoch:  21280 , step:  21280 , train loss:  0.0732317091862143 , test loss:  0.0985587869835274\n",
      "Epoch:  21290 , step:  21290 , train loss:  0.07321758107280686 , test loss:  0.09854877512401773\n",
      "Epoch:  21300 , step:  21300 , train loss:  0.07320346258642331 , test loss:  0.09853877208508166\n",
      "Epoch:  21310 , step:  21310 , train loss:  0.07318935371599913 , test loss:  0.09852877785582917\n",
      "Epoch:  21320 , step:  21320 , train loss:  0.07317525445048763 , test loss:  0.09851879242538805\n",
      "Epoch:  21330 , step:  21330 , train loss:  0.07316116477885984 , test loss:  0.09850881578290398\n",
      "Epoch:  21340 , step:  21340 , train loss:  0.07314708469010457 , test loss:  0.09849884791754077\n",
      "Epoch:  21350 , step:  21350 , train loss:  0.0731330141732283 , test loss:  0.09848888881847961\n",
      "Epoch:  21360 , step:  21360 , train loss:  0.0731189532172552 , test loss:  0.09847893847492006\n",
      "Epoch:  21370 , step:  21370 , train loss:  0.073104901811227 , test loss:  0.09846899687607917\n",
      "Epoch:  21380 , step:  21380 , train loss:  0.0730908599442031 , test loss:  0.09845906401119166\n",
      "Epoch:  21390 , step:  21390 , train loss:  0.07307682760526041 , test loss:  0.09844913986950994\n",
      "Epoch:  21400 , step:  21400 , train loss:  0.07306280478349333 , test loss:  0.09843922444030431\n",
      "Epoch:  21410 , step:  21410 , train loss:  0.07304879146801377 , test loss:  0.09842931771286247\n",
      "Epoch:  21420 , step:  21420 , train loss:  0.0730347876479511 , test loss:  0.09841941967648976\n",
      "Epoch:  21430 , step:  21430 , train loss:  0.07302079331245206 , test loss:  0.09840953032050918\n",
      "Epoch:  21440 , step:  21440 , train loss:  0.07300680845068079 , test loss:  0.09839964963426104\n",
      "Epoch:  21450 , step:  21450 , train loss:  0.07299283305181872 , test loss:  0.09838977760710332\n",
      "Epoch:  21460 , step:  21460 , train loss:  0.07297886710506464 , test loss:  0.09837991422841127\n",
      "Epoch:  21470 , step:  21470 , train loss:  0.07296491059963456 , test loss:  0.0983700594875777\n",
      "Epoch:  21480 , step:  21480 , train loss:  0.07295096352476176 , test loss:  0.09836021337401282\n",
      "Epoch:  21490 , step:  21490 , train loss:  0.07293702586969666 , test loss:  0.09835037587714382\n",
      "Epoch:  21500 , step:  21500 , train loss:  0.0729230976237069 , test loss:  0.09834054698641566\n",
      "Epoch:  21510 , step:  21510 , train loss:  0.0729091787760772 , test loss:  0.09833072669129032\n",
      "Epoch:  21520 , step:  21520 , train loss:  0.07289526931610936 , test loss:  0.09832091498124708\n",
      "Epoch:  21530 , step:  21530 , train loss:  0.07288136923312231 , test loss:  0.09831111184578224\n",
      "Epoch:  21540 , step:  21540 , train loss:  0.0728674785164519 , test loss:  0.0983013172744096\n",
      "Epoch:  21550 , step:  21550 , train loss:  0.07285359715545103 , test loss:  0.09829153125665968\n",
      "Epoch:  21560 , step:  21560 , train loss:  0.07283972513948953 , test loss:  0.09828175378208036\n",
      "Epoch:  21570 , step:  21570 , train loss:  0.07282586245795411 , test loss:  0.09827198484023643\n",
      "Epoch:  21580 , step:  21580 , train loss:  0.07281200910024845 , test loss:  0.09826222442070992\n",
      "Epoch:  21590 , step:  21590 , train loss:  0.072798165055793 , test loss:  0.09825247251309953\n",
      "Epoch:  21600 , step:  21600 , train loss:  0.07278433031402502 , test loss:  0.09824272910702116\n",
      "Epoch:  21610 , step:  21610 , train loss:  0.0727705048643986 , test loss:  0.09823299419210739\n",
      "Epoch:  21620 , step:  21620 , train loss:  0.0727566886963845 , test loss:  0.09822326775800799\n",
      "Epoch:  21630 , step:  21630 , train loss:  0.07274288179947028 , test loss:  0.09821354979438936\n",
      "Epoch:  21640 , step:  21640 , train loss:  0.07272908416316012 , test loss:  0.09820384029093465\n",
      "Epoch:  21650 , step:  21650 , train loss:  0.07271529577697483 , test loss:  0.0981941392373439\n",
      "Epoch:  21660 , step:  21660 , train loss:  0.07270151663045187 , test loss:  0.0981844466233339\n",
      "Epoch:  21670 , step:  21670 , train loss:  0.07268774671314525 , test loss:  0.0981747624386382\n",
      "Epoch:  21680 , step:  21680 , train loss:  0.07267398601462546 , test loss:  0.0981650866730068\n",
      "Epoch:  21690 , step:  21690 , train loss:  0.07266023452447964 , test loss:  0.09815541931620668\n",
      "Epoch:  21700 , step:  21700 , train loss:  0.0726464922323113 , test loss:  0.09814576035802097\n",
      "Epoch:  21710 , step:  21710 , train loss:  0.07263275912774038 , test loss:  0.09813610978824978\n",
      "Epoch:  21720 , step:  21720 , train loss:  0.07261903520040326 , test loss:  0.09812646759670964\n",
      "Epoch:  21730 , step:  21730 , train loss:  0.07260532043995273 , test loss:  0.09811683377323345\n",
      "Epoch:  21740 , step:  21740 , train loss:  0.07259161483605783 , test loss:  0.09810720830767074\n",
      "Epoch:  21750 , step:  21750 , train loss:  0.07257791837840398 , test loss:  0.09809759118988738\n",
      "Epoch:  21760 , step:  21760 , train loss:  0.07256423105669285 , test loss:  0.09808798240976573\n",
      "Epoch:  21770 , step:  21770 , train loss:  0.07255055286064237 , test loss:  0.09807838195720446\n",
      "Epoch:  21780 , step:  21780 , train loss:  0.07253688377998661 , test loss:  0.0980687898221185\n",
      "Epoch:  21790 , step:  21790 , train loss:  0.07252322380447593 , test loss:  0.0980592059944393\n",
      "Epoch:  21800 , step:  21800 , train loss:  0.0725095729238767 , test loss:  0.09804963046411438\n",
      "Epoch:  21810 , step:  21810 , train loss:  0.0724959311279715 , test loss:  0.0980400632211076\n",
      "Epoch:  21820 , step:  21820 , train loss:  0.07248229840655895 , test loss:  0.09803050425539893\n",
      "Epoch:  21830 , step:  21830 , train loss:  0.07246867474945376 , test loss:  0.09802095355698467\n",
      "Epoch:  21840 , step:  21840 , train loss:  0.07245506014648655 , test loss:  0.09801141111587705\n",
      "Epoch:  21850 , step:  21850 , train loss:  0.07244145458750398 , test loss:  0.09800187692210467\n",
      "Epoch:  21860 , step:  21860 , train loss:  0.07242785806236872 , test loss:  0.09799235096571202\n",
      "Epoch:  21870 , step:  21870 , train loss:  0.0724142705609593 , test loss:  0.09798283323675953\n",
      "Epoch:  21880 , step:  21880 , train loss:  0.07240069207317006 , test loss:  0.09797332372532393\n",
      "Epoch:  21890 , step:  21890 , train loss:  0.07238712258891132 , test loss:  0.09796382242149765\n",
      "Epoch:  21900 , step:  21900 , train loss:  0.07237356209810913 , test loss:  0.09795432931538925\n",
      "Epoch:  21910 , step:  21910 , train loss:  0.0723600105907054 , test loss:  0.09794484439712314\n",
      "Epoch:  21920 , step:  21920 , train loss:  0.07234646805665776 , test loss:  0.09793536765683958\n",
      "Epoch:  21930 , step:  21930 , train loss:  0.07233293448593954 , test loss:  0.09792589908469479\n",
      "Epoch:  21940 , step:  21940 , train loss:  0.07231940986853981 , test loss:  0.09791643867086058\n",
      "Epoch:  21950 , step:  21950 , train loss:  0.07230589419446325 , test loss:  0.09790698640552487\n",
      "Epoch:  21960 , step:  21960 , train loss:  0.07229238745373022 , test loss:  0.09789754227889105\n",
      "Epoch:  21970 , step:  21970 , train loss:  0.07227888963637667 , test loss:  0.0978881062811783\n",
      "Epoch:  21980 , step:  21980 , train loss:  0.0722654007324541 , test loss:  0.09787867840262149\n",
      "Epoch:  21990 , step:  21990 , train loss:  0.07225192073202953 , test loss:  0.09786925863347132\n",
      "Epoch:  22000 , step:  22000 , train loss:  0.07223844962518558 , test loss:  0.0978598469639938\n",
      "Epoch:  22010 , step:  22010 , train loss:  0.07222498740202021 , test loss:  0.0978504433844709\n",
      "Epoch:  22020 , step:  22020 , train loss:  0.07221153405264692 , test loss:  0.09784104788519976\n",
      "Epoch:  22030 , step:  22030 , train loss:  0.07219808956719459 , test loss:  0.09783166045649334\n",
      "Epoch:  22040 , step:  22040 , train loss:  0.07218465393580747 , test loss:  0.09782228108868005\n",
      "Epoch:  22050 , step:  22050 , train loss:  0.07217122714864523 , test loss:  0.09781290977210381\n",
      "Epoch:  22060 , step:  22060 , train loss:  0.07215780919588272 , test loss:  0.0978035464971237\n",
      "Epoch:  22070 , step:  22070 , train loss:  0.07214440006771022 , test loss:  0.09779419125411448\n",
      "Epoch:  22080 , step:  22080 , train loss:  0.07213099975433324 , test loss:  0.09778484403346621\n",
      "Epoch:  22090 , step:  22090 , train loss:  0.07211760824597244 , test loss:  0.09777550482558443\n",
      "Epoch:  22100 , step:  22100 , train loss:  0.07210422553286377 , test loss:  0.09776617362088968\n",
      "Epoch:  22110 , step:  22110 , train loss:  0.07209085160525833 , test loss:  0.09775685040981802\n",
      "Epoch:  22120 , step:  22120 , train loss:  0.07207748645342232 , test loss:  0.0977475351828208\n",
      "Epoch:  22130 , step:  22130 , train loss:  0.07206413006763704 , test loss:  0.09773822793036446\n",
      "Epoch:  22140 , step:  22140 , train loss:  0.07205078243819896 , test loss:  0.09772892864293059\n",
      "Epoch:  22150 , step:  22150 , train loss:  0.0720374435554195 , test loss:  0.09771963731101625\n",
      "Epoch:  22160 , step:  22160 , train loss:  0.07202411340962514 , test loss:  0.0977103539251332\n",
      "Epoch:  22170 , step:  22170 , train loss:  0.07201079199115737 , test loss:  0.09770107847580864\n",
      "Epoch:  22180 , step:  22180 , train loss:  0.07199747929037253 , test loss:  0.09769181095358463\n",
      "Epoch:  22190 , step:  22190 , train loss:  0.07198417529764205 , test loss:  0.09768255134901845\n",
      "Epoch:  22200 , step:  22200 , train loss:  0.07197088000335221 , test loss:  0.09767329965268226\n",
      "Epoch:  22210 , step:  22210 , train loss:  0.071957593397904 , test loss:  0.0976640558551632\n",
      "Epoch:  22220 , step:  22220 , train loss:  0.07194431547171351 , test loss:  0.09765481994706349\n",
      "Epoch:  22230 , step:  22230 , train loss:  0.07193104621521143 , test loss:  0.09764559191900014\n",
      "Epoch:  22240 , step:  22240 , train loss:  0.07191778561884339 , test loss:  0.09763637176160517\n",
      "Epoch:  22250 , step:  22250 , train loss:  0.07190453367306963 , test loss:  0.09762715946552518\n",
      "Epoch:  22260 , step:  22260 , train loss:  0.07189129036836517 , test loss:  0.09761795502142206\n",
      "Epoch:  22270 , step:  22270 , train loss:  0.0718780556952198 , test loss:  0.09760875841997224\n",
      "Epoch:  22280 , step:  22280 , train loss:  0.07186482964413785 , test loss:  0.09759956965186688\n",
      "Epoch:  22290 , step:  22290 , train loss:  0.07185161220563835 , test loss:  0.09759038870781189\n",
      "Epoch:  22300 , step:  22300 , train loss:  0.07183840337025493 , test loss:  0.097581215578528\n",
      "Epoch:  22310 , step:  22310 , train loss:  0.07182520312853581 , test loss:  0.09757205025475063\n",
      "Epoch:  22320 , step:  22320 , train loss:  0.07181201147104374 , test loss:  0.09756289272722973\n",
      "Epoch:  22330 , step:  22330 , train loss:  0.07179882838835602 , test loss:  0.09755374298673003\n",
      "Epoch:  22340 , step:  22340 , train loss:  0.07178565387106439 , test loss:  0.09754460102403077\n",
      "Epoch:  22350 , step:  22350 , train loss:  0.07177248790977508 , test loss:  0.0975354668299258\n",
      "Epoch:  22360 , step:  22360 , train loss:  0.07175933049510877 , test loss:  0.09752634039522337\n",
      "Epoch:  22370 , step:  22370 , train loss:  0.07174618161770054 , test loss:  0.0975172217107465\n",
      "Epoch:  22380 , step:  22380 , train loss:  0.07173304126819982 , test loss:  0.09750811076733243\n",
      "Epoch:  22390 , step:  22390 , train loss:  0.07171990943727044 , test loss:  0.09749900755583311\n",
      "Epoch:  22400 , step:  22400 , train loss:  0.07170678611559052 , test loss:  0.0974899120671147\n",
      "Epoch:  22410 , step:  22410 , train loss:  0.07169367129385243 , test loss:  0.0974808242920579\n",
      "Epoch:  22420 , step:  22420 , train loss:  0.07168056496276289 , test loss:  0.09747174422155765\n",
      "Epoch:  22430 , step:  22430 , train loss:  0.07166746711304284 , test loss:  0.09746267184652342\n",
      "Epoch:  22440 , step:  22440 , train loss:  0.07165437773542736 , test loss:  0.0974536071578788\n",
      "Epoch:  22450 , step:  22450 , train loss:  0.07164129682066579 , test loss:  0.09744455014656171\n",
      "Epoch:  22460 , step:  22460 , train loss:  0.07162822435952161 , test loss:  0.09743550080352437\n",
      "Epoch:  22470 , step:  22470 , train loss:  0.07161516034277234 , test loss:  0.0974264591197332\n",
      "Epoch:  22480 , step:  22480 , train loss:  0.07160210476120973 , test loss:  0.09741742508616895\n",
      "Epoch:  22490 , step:  22490 , train loss:  0.07158905760563952 , test loss:  0.09740839869382632\n",
      "Epoch:  22500 , step:  22500 , train loss:  0.07157601886688149 , test loss:  0.0973993799337143\n",
      "Epoch:  22510 , step:  22510 , train loss:  0.0715629885357695 , test loss:  0.09739036879685602\n",
      "Epoch:  22520 , step:  22520 , train loss:  0.07154996660315133 , test loss:  0.09738136527428844\n",
      "Epoch:  22530 , step:  22530 , train loss:  0.07153695305988866 , test loss:  0.09737236935706288\n",
      "Epoch:  22540 , step:  22540 , train loss:  0.07152394789685737 , test loss:  0.09736338103624474\n",
      "Epoch:  22550 , step:  22550 , train loss:  0.07151095110494687 , test loss:  0.09735440030291317\n",
      "Epoch:  22560 , step:  22560 , train loss:  0.07149796267506077 , test loss:  0.09734542714816131\n",
      "Epoch:  22570 , step:  22570 , train loss:  0.07148498259811638 , test loss:  0.09733646156309644\n",
      "Epoch:  22580 , step:  22580 , train loss:  0.07147201086504479 , test loss:  0.09732750353883976\n",
      "Epoch:  22590 , step:  22590 , train loss:  0.07145904746679103 , test loss:  0.09731855306652605\n",
      "Epoch:  22600 , step:  22600 , train loss:  0.07144609239431379 , test loss:  0.09730961013730435\n",
      "Epoch:  22610 , step:  22610 , train loss:  0.07143314563858556 , test loss:  0.09730067474233735\n",
      "Epoch:  22620 , step:  22620 , train loss:  0.07142020719059253 , test loss:  0.0972917468728015\n",
      "Epoch:  22630 , step:  22630 , train loss:  0.07140727704133455 , test loss:  0.09728282651988726\n",
      "Epoch:  22640 , step:  22640 , train loss:  0.07139435518182516 , test loss:  0.09727391367479855\n",
      "Epoch:  22650 , step:  22650 , train loss:  0.07138144160309157 , test loss:  0.09726500832875326\n",
      "Epoch:  22660 , step:  22660 , train loss:  0.07136853629617455 , test loss:  0.09725611047298287\n",
      "Epoch:  22670 , step:  22670 , train loss:  0.0713556392521285 , test loss:  0.09724722009873257\n",
      "Epoch:  22680 , step:  22680 , train loss:  0.07134275046202129 , test loss:  0.0972383371972614\n",
      "Epoch:  22690 , step:  22690 , train loss:  0.07132986991693444 , test loss:  0.09722946175984147\n",
      "Epoch:  22700 , step:  22700 , train loss:  0.0713169976079629 , test loss:  0.09722059377775914\n",
      "Epoch:  22710 , step:  22710 , train loss:  0.07130413352621512 , test loss:  0.097211733242314\n",
      "Epoch:  22720 , step:  22720 , train loss:  0.07129127766281297 , test loss:  0.09720288014481916\n",
      "Epoch:  22730 , step:  22730 , train loss:  0.07127843000889181 , test loss:  0.09719403447660141\n",
      "Epoch:  22740 , step:  22740 , train loss:  0.07126559055560036 , test loss:  0.09718519622900082\n",
      "Epoch:  22750 , step:  22750 , train loss:  0.07125275929410073 , test loss:  0.09717636539337125\n",
      "Epoch:  22760 , step:  22760 , train loss:  0.07123993621556833 , test loss:  0.09716754196107975\n",
      "Epoch:  22770 , step:  22770 , train loss:  0.07122712131119197 , test loss:  0.09715872592350674\n",
      "Epoch:  22780 , step:  22780 , train loss:  0.0712143145721737 , test loss:  0.0971499172720463\n",
      "Epoch:  22790 , step:  22790 , train loss:  0.07120151598972888 , test loss:  0.0971411159981056\n",
      "Epoch:  22800 , step:  22800 , train loss:  0.07118872555508605 , test loss:  0.09713232209310525\n",
      "Epoch:  22810 , step:  22810 , train loss:  0.07117594325948706 , test loss:  0.09712353554847913\n",
      "Epoch:  22820 , step:  22820 , train loss:  0.07116316909418692 , test loss:  0.09711475635567463\n",
      "Epoch:  22830 , step:  22830 , train loss:  0.07115040305045377 , test loss:  0.09710598450615213\n",
      "Epoch:  22840 , step:  22840 , train loss:  0.07113764511956894 , test loss:  0.09709721999138533\n",
      "Epoch:  22850 , step:  22850 , train loss:  0.07112489529282683 , test loss:  0.09708846280286106\n",
      "Epoch:  22860 , step:  22860 , train loss:  0.071112153561535 , test loss:  0.09707971293207945\n",
      "Epoch:  22870 , step:  22870 , train loss:  0.07109941991701402 , test loss:  0.09707097037055376\n",
      "Epoch:  22880 , step:  22880 , train loss:  0.07108669435059754 , test loss:  0.09706223510981044\n",
      "Epoch:  22890 , step:  22890 , train loss:  0.07107397685363222 , test loss:  0.09705350714138881\n",
      "Epoch:  22900 , step:  22900 , train loss:  0.07106126741747769 , test loss:  0.09704478645684159\n",
      "Epoch:  22910 , step:  22910 , train loss:  0.07104856603350654 , test loss:  0.0970360730477343\n",
      "Epoch:  22920 , step:  22920 , train loss:  0.07103587269310435 , test loss:  0.09702736690564566\n",
      "Epoch:  22930 , step:  22930 , train loss:  0.07102318738766958 , test loss:  0.09701866802216726\n",
      "Epoch:  22940 , step:  22940 , train loss:  0.07101051010861359 , test loss:  0.09700997638890373\n",
      "Epoch:  22950 , step:  22950 , train loss:  0.07099784084736066 , test loss:  0.09700129199747261\n",
      "Epoch:  22960 , step:  22960 , train loss:  0.07098517959534781 , test loss:  0.09699261483950447\n",
      "Epoch:  22970 , step:  22970 , train loss:  0.07097252634402496 , test loss:  0.09698394490664278\n",
      "Epoch:  22980 , step:  22980 , train loss:  0.07095988108485474 , test loss:  0.09697528219054377\n",
      "Epoch:  22990 , step:  22990 , train loss:  0.07094724380931268 , test loss:  0.09696662668287644\n",
      "Epoch:  23000 , step:  23000 , train loss:  0.07093461450888694 , test loss:  0.09695797837532298\n",
      "Epoch:  23010 , step:  23010 , train loss:  0.07092199317507845 , test loss:  0.09694933725957798\n",
      "Epoch:  23020 , step:  23020 , train loss:  0.07090937979940082 , test loss:  0.09694070332734918\n",
      "Epoch:  23030 , step:  23030 , train loss:  0.07089677437338035 , test loss:  0.09693207657035675\n",
      "Epoch:  23040 , step:  23040 , train loss:  0.07088417688855596 , test loss:  0.09692345698033383\n",
      "Epoch:  23050 , step:  23050 , train loss:  0.07087158733647925 , test loss:  0.09691484454902607\n",
      "Epoch:  23060 , step:  23060 , train loss:  0.07085900570871435 , test loss:  0.0969062392681919\n",
      "Epoch:  23070 , step:  23070 , train loss:  0.07084643199683795 , test loss:  0.0968976411296025\n",
      "Epoch:  23080 , step:  23080 , train loss:  0.0708338661924394 , test loss:  0.09688905012504151\n",
      "Epoch:  23090 , step:  23090 , train loss:  0.07082130828712044 , test loss:  0.0968804662463053\n",
      "Epoch:  23100 , step:  23100 , train loss:  0.07080875827249544 , test loss:  0.0968718894852027\n",
      "Epoch:  23110 , step:  23110 , train loss:  0.0707962161401912 , test loss:  0.09686331983355524\n",
      "Epoch:  23120 , step:  23120 , train loss:  0.07078368188184694 , test loss:  0.09685475728319681\n",
      "Epoch:  23130 , step:  23130 , train loss:  0.07077115548911433 , test loss:  0.09684620182597409\n",
      "Epoch:  23140 , step:  23140 , train loss:  0.0707586369536575 , test loss:  0.09683765345374598\n",
      "Epoch:  23150 , step:  23150 , train loss:  0.07074612626715286 , test loss:  0.09682911215838394\n",
      "Epoch:  23160 , step:  23160 , train loss:  0.07073362342128929 , test loss:  0.09682057793177196\n",
      "Epoch:  23170 , step:  23170 , train loss:  0.07072112840776795 , test loss:  0.09681205076580629\n",
      "Epoch:  23180 , step:  23180 , train loss:  0.07070864121830231 , test loss:  0.09680353065239553\n",
      "Epoch:  23190 , step:  23190 , train loss:  0.07069616184461813 , test loss:  0.09679501758346092\n",
      "Epoch:  23200 , step:  23200 , train loss:  0.07068369027845348 , test loss:  0.09678651155093576\n",
      "Epoch:  23210 , step:  23210 , train loss:  0.07067122651155862 , test loss:  0.09677801254676589\n",
      "Epoch:  23220 , step:  23220 , train loss:  0.07065877053569608 , test loss:  0.09676952056290926\n",
      "Epoch:  23230 , step:  23230 , train loss:  0.07064632234264051 , test loss:  0.09676103559133624\n",
      "Epoch:  23240 , step:  23240 , train loss:  0.07063388192417883 , test loss:  0.0967525576240293\n",
      "Epoch:  23250 , step:  23250 , train loss:  0.07062144927211003 , test loss:  0.09674408665298338\n",
      "Epoch:  23260 , step:  23260 , train loss:  0.07060902437824532 , test loss:  0.09673562267020543\n",
      "Epoch:  23270 , step:  23270 , train loss:  0.07059660723440789 , test loss:  0.09672716566771457\n",
      "Epoch:  23280 , step:  23280 , train loss:  0.0705841978324331 , test loss:  0.09671871563754203\n",
      "Epoch:  23290 , step:  23290 , train loss:  0.07057179616416835 , test loss:  0.09671027257173148\n",
      "Epoch:  23300 , step:  23300 , train loss:  0.07055940222147306 , test loss:  0.09670183646233832\n",
      "Epoch:  23310 , step:  23310 , train loss:  0.0705470159962187 , test loss:  0.09669340730143025\n",
      "Epoch:  23320 , step:  23320 , train loss:  0.07053463748028871 , test loss:  0.09668498508108704\n",
      "Epoch:  23330 , step:  23330 , train loss:  0.07052226666557847 , test loss:  0.09667656979340039\n",
      "Epoch:  23340 , step:  23340 , train loss:  0.0705099035439953 , test loss:  0.09666816143047412\n",
      "Epoch:  23350 , step:  23350 , train loss:  0.07049754810745856 , test loss:  0.096659759984424\n",
      "Epoch:  23360 , step:  23360 , train loss:  0.07048520034789937 , test loss:  0.09665136544737776\n",
      "Epoch:  23370 , step:  23370 , train loss:  0.07047286025726078 , test loss:  0.09664297781147506\n",
      "Epoch:  23380 , step:  23380 , train loss:  0.07046052782749768 , test loss:  0.09663459706886765\n",
      "Epoch:  23390 , step:  23390 , train loss:  0.07044820305057681 , test loss:  0.09662622321171893\n",
      "Epoch:  23400 , step:  23400 , train loss:  0.07043588591847674 , test loss:  0.09661785623220429\n",
      "Epoch:  23410 , step:  23410 , train loss:  0.0704235764231878 , test loss:  0.09660949612251114\n",
      "Epoch:  23420 , step:  23420 , train loss:  0.07041127455671209 , test loss:  0.0966011428748384\n",
      "Epoch:  23430 , step:  23430 , train loss:  0.07039898031106344 , test loss:  0.09659279648139713\n",
      "Epoch:  23440 , step:  23440 , train loss:  0.07038669367826744 , test loss:  0.09658445693440987\n",
      "Epoch:  23450 , step:  23450 , train loss:  0.07037441465036136 , test loss:  0.09657612422611137\n",
      "Epoch:  23460 , step:  23460 , train loss:  0.07036214321939412 , test loss:  0.09656779834874764\n",
      "Epoch:  23470 , step:  23470 , train loss:  0.07034987937742632 , test loss:  0.09655947929457669\n",
      "Epoch:  23480 , step:  23480 , train loss:  0.07033762311653018 , test loss:  0.09655116705586816\n",
      "Epoch:  23490 , step:  23490 , train loss:  0.07032537442878961 , test loss:  0.09654286162490336\n",
      "Epoch:  23500 , step:  23500 , train loss:  0.0703131333063 , test loss:  0.09653456299397532\n",
      "Epoch:  23510 , step:  23510 , train loss:  0.07030089974116832 , test loss:  0.09652627115538874\n",
      "Epoch:  23520 , step:  23520 , train loss:  0.07028867372551319 , test loss:  0.09651798610145969\n",
      "Epoch:  23530 , step:  23530 , train loss:  0.07027645525146467 , test loss:  0.09650970782451607\n",
      "Epoch:  23540 , step:  23540 , train loss:  0.07026424431116429 , test loss:  0.09650143631689732\n",
      "Epoch:  23550 , step:  23550 , train loss:  0.07025204089676515 , test loss:  0.09649317157095431\n",
      "Epoch:  23560 , step:  23560 , train loss:  0.07023984500043176 , test loss:  0.09648491357904951\n",
      "Epoch:  23570 , step:  23570 , train loss:  0.07022765661434009 , test loss:  0.096476662333557\n",
      "Epoch:  23580 , step:  23580 , train loss:  0.0702154757306775 , test loss:  0.09646841782686219\n",
      "Epoch:  23590 , step:  23590 , train loss:  0.07020330234164278 , test loss:  0.09646018005136196\n",
      "Epoch:  23600 , step:  23600 , train loss:  0.07019113643944608 , test loss:  0.09645194899946473\n",
      "Epoch:  23610 , step:  23610 , train loss:  0.07017897801630886 , test loss:  0.09644372466359027\n",
      "Epoch:  23620 , step:  23620 , train loss:  0.07016682706446402 , test loss:  0.09643550703616967\n",
      "Epoch:  23630 , step:  23630 , train loss:  0.07015468357615563 , test loss:  0.09642729610964564\n",
      "Epoch:  23640 , step:  23640 , train loss:  0.07014254754363916 , test loss:  0.09641909187647196\n",
      "Epoch:  23650 , step:  23650 , train loss:  0.07013041895918133 , test loss:  0.09641089432911397\n",
      "Epoch:  23660 , step:  23660 , train loss:  0.07011829781506006 , test loss:  0.09640270346004812\n",
      "Epoch:  23670 , step:  23670 , train loss:  0.07010618410356455 , test loss:  0.09639451926176218\n",
      "Epoch:  23680 , step:  23680 , train loss:  0.07009407781699516 , test loss:  0.09638634172675535\n",
      "Epoch:  23690 , step:  23690 , train loss:  0.07008197894766346 , test loss:  0.09637817084753802\n",
      "Epoch:  23700 , step:  23700 , train loss:  0.07006988748789222 , test loss:  0.09637000661663175\n",
      "Epoch:  23710 , step:  23710 , train loss:  0.07005780343001526 , test loss:  0.09636184902656925\n",
      "Epoch:  23720 , step:  23720 , train loss:  0.07004572676637759 , test loss:  0.09635369806989465\n",
      "Epoch:  23730 , step:  23730 , train loss:  0.07003365748933534 , test loss:  0.09634555373916293\n",
      "Epoch:  23740 , step:  23740 , train loss:  0.07002159559125569 , test loss:  0.09633741602694038\n",
      "Epoch:  23750 , step:  23750 , train loss:  0.07000954106451684 , test loss:  0.09632928492580442\n",
      "Epoch:  23760 , step:  23760 , train loss:  0.06999749390150811 , test loss:  0.09632116042834343\n",
      "Epoch:  23770 , step:  23770 , train loss:  0.06998545409462978 , test loss:  0.09631304252715718\n",
      "Epoch:  23780 , step:  23780 , train loss:  0.06997342163629315 , test loss:  0.09630493121485616\n",
      "Epoch:  23790 , step:  23790 , train loss:  0.06996139651892053 , test loss:  0.09629682648406208\n",
      "Epoch:  23800 , step:  23800 , train loss:  0.0699493787349451 , test loss:  0.0962887283274076\n",
      "Epoch:  23810 , step:  23810 , train loss:  0.06993736827681109 , test loss:  0.09628063673753652\n",
      "Epoch:  23820 , step:  23820 , train loss:  0.06992536513697357 , test loss:  0.0962725517071034\n",
      "Epoch:  23830 , step:  23830 , train loss:  0.06991336930789852 , test loss:  0.09626447322877377\n",
      "Epoch:  23840 , step:  23840 , train loss:  0.06990138078206283 , test loss:  0.0962564012952242\n",
      "Epoch:  23850 , step:  23850 , train loss:  0.06988939955195421 , test loss:  0.09624833589914229\n",
      "Epoch:  23860 , step:  23860 , train loss:  0.06987742561007121 , test loss:  0.09624027703322631\n",
      "Epoch:  23870 , step:  23870 , train loss:  0.06986545894892326 , test loss:  0.09623222469018551\n",
      "Epoch:  23880 , step:  23880 , train loss:  0.06985349956103046 , test loss:  0.09622417886274\n",
      "Epoch:  23890 , step:  23890 , train loss:  0.06984154743892382 , test loss:  0.09621613954362064\n",
      "Epoch:  23900 , step:  23900 , train loss:  0.06982960257514503 , test loss:  0.09620810672556918\n",
      "Epoch:  23910 , step:  23910 , train loss:  0.06981766496224655 , test loss:  0.09620008040133815\n",
      "Epoch:  23920 , step:  23920 , train loss:  0.06980573459279153 , test loss:  0.09619206056369087\n",
      "Epoch:  23930 , step:  23930 , train loss:  0.06979381145935384 , test loss:  0.09618404720540133\n",
      "Epoch:  23940 , step:  23940 , train loss:  0.06978189555451803 , test loss:  0.0961760403192543\n",
      "Epoch:  23950 , step:  23950 , train loss:  0.06976998687087926 , test loss:  0.09616803989804537\n",
      "Epoch:  23960 , step:  23960 , train loss:  0.0697580854010434 , test loss:  0.09616004593458063\n",
      "Epoch:  23970 , step:  23970 , train loss:  0.06974619113762691 , test loss:  0.09615205842167704\n",
      "Epoch:  23980 , step:  23980 , train loss:  0.06973430407325681 , test loss:  0.09614407735216197\n",
      "Epoch:  23990 , step:  23990 , train loss:  0.06972242420057079 , test loss:  0.09613610271887368\n",
      "Epoch:  24000 , step:  24000 , train loss:  0.06971055151221701 , test loss:  0.09612813451466093\n",
      "Epoch:  24010 , step:  24010 , train loss:  0.06969868600085419 , test loss:  0.096120172732383\n",
      "Epoch:  24020 , step:  24020 , train loss:  0.06968682765915162 , test loss:  0.0961122173649099\n",
      "Epoch:  24030 , step:  24030 , train loss:  0.06967497647978906 , test loss:  0.09610426840512207\n",
      "Epoch:  24040 , step:  24040 , train loss:  0.0696631324554567 , test loss:  0.09609632584591056\n",
      "Epoch:  24050 , step:  24050 , train loss:  0.06965129557885535 , test loss:  0.09608838968017686\n",
      "Epoch:  24060 , step:  24060 , train loss:  0.06963946584269609 , test loss:  0.096080459900833\n",
      "Epoch:  24070 , step:  24070 , train loss:  0.06962764323970047 , test loss:  0.09607253650080153\n",
      "Epoch:  24080 , step:  24080 , train loss:  0.06961582776260057 , test loss:  0.0960646194730153\n",
      "Epoch:  24090 , step:  24090 , train loss:  0.06960401940413867 , test loss:  0.09605670881041792\n",
      "Epoch:  24100 , step:  24100 , train loss:  0.06959221815706754 , test loss:  0.09604880450596298\n",
      "Epoch:  24110 , step:  24110 , train loss:  0.06958042401415028 , test loss:  0.09604090655261481\n",
      "Epoch:  24120 , step:  24120 , train loss:  0.06956863696816032 , test loss:  0.09603301494334797\n",
      "Epoch:  24130 , step:  24130 , train loss:  0.06955685701188133 , test loss:  0.09602512967114753\n",
      "Epoch:  24140 , step:  24140 , train loss:  0.06954508413810735 , test loss:  0.09601725072900862\n",
      "Epoch:  24150 , step:  24150 , train loss:  0.06953331833964273 , test loss:  0.09600937810993704\n",
      "Epoch:  24160 , step:  24160 , train loss:  0.06952155960930198 , test loss:  0.0960015118069485\n",
      "Epoch:  24170 , step:  24170 , train loss:  0.06950980793990987 , test loss:  0.09599365181306942\n",
      "Epoch:  24180 , step:  24180 , train loss:  0.06949806332430143 , test loss:  0.09598579812133616\n",
      "Epoch:  24190 , step:  24190 , train loss:  0.06948632575532185 , test loss:  0.09597795072479544\n",
      "Epoch:  24200 , step:  24200 , train loss:  0.06947459522582651 , test loss:  0.09597010961650426\n",
      "Epoch:  24210 , step:  24210 , train loss:  0.06946287172868094 , test loss:  0.09596227478952991\n",
      "Epoch:  24220 , step:  24220 , train loss:  0.06945115525676086 , test loss:  0.09595444623694942\n",
      "Epoch:  24230 , step:  24230 , train loss:  0.06943944580295204 , test loss:  0.09594662395185054\n",
      "Epoch:  24240 , step:  24240 , train loss:  0.06942774336015042 , test loss:  0.09593880792733074\n",
      "Epoch:  24250 , step:  24250 , train loss:  0.06941604792126202 , test loss:  0.09593099815649812\n",
      "Epoch:  24260 , step:  24260 , train loss:  0.06940435947920287 , test loss:  0.0959231946324705\n",
      "Epoch:  24270 , step:  24270 , train loss:  0.0693926780268991 , test loss:  0.09591539734837572\n",
      "Epoch:  24280 , step:  24280 , train loss:  0.06938100355728691 , test loss:  0.095907606297352\n",
      "Epoch:  24290 , step:  24290 , train loss:  0.06936933606331243 , test loss:  0.09589982147254748\n",
      "Epoch:  24300 , step:  24300 , train loss:  0.06935767553793183 , test loss:  0.0958920428671204\n",
      "Epoch:  24310 , step:  24310 , train loss:  0.06934602197411127 , test loss:  0.095884270474239\n",
      "Epoch:  24320 , step:  24320 , train loss:  0.06933437536482688 , test loss:  0.09587650428708143\n",
      "Epoch:  24330 , step:  24330 , train loss:  0.06932273570306462 , test loss:  0.09586874429883593\n",
      "Epoch:  24340 , step:  24340 , train loss:  0.06931110298182053 , test loss:  0.09586099050270058\n",
      "Epoch:  24350 , step:  24350 , train loss:  0.06929947719410044 , test loss:  0.09585324289188368\n",
      "Epoch:  24360 , step:  24360 , train loss:  0.06928785833292016 , test loss:  0.09584550145960315\n",
      "Epoch:  24370 , step:  24370 , train loss:  0.0692762463913053 , test loss:  0.09583776619908697\n",
      "Epoch:  24380 , step:  24380 , train loss:  0.06926464136229137 , test loss:  0.09583003710357303\n",
      "Epoch:  24390 , step:  24390 , train loss:  0.06925304323892362 , test loss:  0.09582231416630925\n",
      "Epoch:  24400 , step:  24400 , train loss:  0.06924145201425723 , test loss:  0.09581459738055288\n",
      "Epoch:  24410 , step:  24410 , train loss:  0.06922986768135715 , test loss:  0.09580688673957166\n",
      "Epoch:  24420 , step:  24420 , train loss:  0.06921829023329802 , test loss:  0.09579918223664267\n",
      "Epoch:  24430 , step:  24430 , train loss:  0.06920671966316443 , test loss:  0.09579148386505305\n",
      "Epoch:  24440 , step:  24440 , train loss:  0.06919515596405049 , test loss:  0.0957837916180996\n",
      "Epoch:  24450 , step:  24450 , train loss:  0.06918359912906022 , test loss:  0.09577610548908903\n",
      "Epoch:  24460 , step:  24460 , train loss:  0.06917204915130724 , test loss:  0.09576842547133767\n",
      "Epoch:  24470 , step:  24470 , train loss:  0.06916050602391491 , test loss:  0.09576075155817158\n",
      "Epoch:  24480 , step:  24480 , train loss:  0.06914896974001629 , test loss:  0.09575308374292661\n",
      "Epoch:  24490 , step:  24490 , train loss:  0.06913744029275404 , test loss:  0.09574542201894834\n",
      "Epoch:  24500 , step:  24500 , train loss:  0.06912591767528048 , test loss:  0.0957377663795918\n",
      "Epoch:  24510 , step:  24510 , train loss:  0.06911440188075754 , test loss:  0.09573011681822197\n",
      "Epoch:  24520 , step:  24520 , train loss:  0.06910289290235681 , test loss:  0.09572247332821346\n",
      "Epoch:  24530 , step:  24530 , train loss:  0.06909139073325941 , test loss:  0.09571483590295024\n",
      "Epoch:  24540 , step:  24540 , train loss:  0.06907989536665607 , test loss:  0.09570720453582612\n",
      "Epoch:  24550 , step:  24550 , train loss:  0.06906840679574702 , test loss:  0.0956995792202444\n",
      "Epoch:  24560 , step:  24560 , train loss:  0.06905692501374208 , test loss:  0.09569195994961795\n",
      "Epoch:  24570 , step:  24570 , train loss:  0.0690454500138606 , test loss:  0.09568434671736936\n",
      "Epoch:  24580 , step:  24580 , train loss:  0.06903398178933136 , test loss:  0.09567673951693044\n",
      "Epoch:  24590 , step:  24590 , train loss:  0.06902252033339269 , test loss:  0.09566913834174297\n",
      "Epoch:  24600 , step:  24600 , train loss:  0.06901106563929234 , test loss:  0.0956615431852578\n",
      "Epoch:  24610 , step:  24610 , train loss:  0.06899961770028758 , test loss:  0.09565395404093549\n",
      "Epoch:  24620 , step:  24620 , train loss:  0.06898817650964505 , test loss:  0.09564637090224605\n",
      "Epoch:  24630 , step:  24630 , train loss:  0.06897674206064083 , test loss:  0.09563879376266896\n",
      "Epoch:  24640 , step:  24640 , train loss:  0.06896531434656039 , test loss:  0.09563122261569304\n",
      "Epoch:  24650 , step:  24650 , train loss:  0.06895389336069861 , test loss:  0.09562365745481671\n",
      "Epoch:  24660 , step:  24660 , train loss:  0.06894247909635973 , test loss:  0.09561609827354749\n",
      "Epoch:  24670 , step:  24670 , train loss:  0.06893107154685726 , test loss:  0.09560854506540245\n",
      "Epoch:  24680 , step:  24680 , train loss:  0.0689196707055142 , test loss:  0.09560099782390825\n",
      "Epoch:  24690 , step:  24690 , train loss:  0.06890827656566276 , test loss:  0.09559345654260061\n",
      "Epoch:  24700 , step:  24700 , train loss:  0.0688968891206444 , test loss:  0.09558592121502466\n",
      "Epoch:  24710 , step:  24710 , train loss:  0.06888550836381002 , test loss:  0.09557839183473484\n",
      "Epoch:  24720 , step:  24720 , train loss:  0.0688741342885196 , test loss:  0.09557086839529498\n",
      "Epoch:  24730 , step:  24730 , train loss:  0.06886276688814255 , test loss:  0.09556335089027797\n",
      "Epoch:  24740 , step:  24740 , train loss:  0.06885140615605741 , test loss:  0.09555583931326632\n",
      "Epoch:  24750 , step:  24750 , train loss:  0.06884005208565196 , test loss:  0.09554833365785144\n",
      "Epoch:  24760 , step:  24760 , train loss:  0.06882870467032312 , test loss:  0.09554083391763425\n",
      "Epoch:  24770 , step:  24770 , train loss:  0.06881736390347712 , test loss:  0.09553334008622472\n",
      "Epoch:  24780 , step:  24780 , train loss:  0.06880602977852929 , test loss:  0.09552585215724196\n",
      "Epoch:  24790 , step:  24790 , train loss:  0.06879470228890404 , test loss:  0.09551837012431454\n",
      "Epoch:  24800 , step:  24800 , train loss:  0.06878338142803506 , test loss:  0.09551089398107991\n",
      "Epoch:  24810 , step:  24810 , train loss:  0.06877206718936507 , test loss:  0.09550342372118474\n",
      "Epoch:  24820 , step:  24820 , train loss:  0.06876075956634586 , test loss:  0.09549595933828495\n",
      "Epoch:  24830 , step:  24830 , train loss:  0.06874945855243841 , test loss:  0.09548850082604551\n",
      "Epoch:  24840 , step:  24840 , train loss:  0.06873816414111267 , test loss:  0.09548104817814049\n",
      "Epoch:  24850 , step:  24850 , train loss:  0.0687268763258477 , test loss:  0.09547360138825303\n",
      "Epoch:  24860 , step:  24860 , train loss:  0.0687155951001316 , test loss:  0.09546616045007528\n",
      "Epoch:  24870 , step:  24870 , train loss:  0.06870432045746143 , test loss:  0.09545872535730865\n",
      "Epoch:  24880 , step:  24880 , train loss:  0.06869305239134335 , test loss:  0.09545129610366343\n",
      "Epoch:  24890 , step:  24890 , train loss:  0.06868179089529244 , test loss:  0.09544387268285878\n",
      "Epoch:  24900 , step:  24900 , train loss:  0.06867053596283276 , test loss:  0.09543645508862325\n",
      "Epoch:  24910 , step:  24910 , train loss:  0.06865928758749737 , test loss:  0.09542904331469404\n",
      "Epoch:  24920 , step:  24920 , train loss:  0.06864804576282822 , test loss:  0.09542163735481736\n",
      "Epoch:  24930 , step:  24930 , train loss:  0.06863681048237624 , test loss:  0.09541423720274868\n",
      "Epoch:  24940 , step:  24940 , train loss:  0.0686255817397012 , test loss:  0.09540684285225197\n",
      "Epoch:  24950 , step:  24950 , train loss:  0.06861435952837182 , test loss:  0.09539945429710045\n",
      "Epoch:  24960 , step:  24960 , train loss:  0.0686031438419657 , test loss:  0.09539207153107615\n",
      "Epoch:  24970 , step:  24970 , train loss:  0.06859193467406927 , test loss:  0.09538469454796993\n",
      "Epoch:  24980 , step:  24980 , train loss:  0.06858073201827783 , test loss:  0.09537732334158162\n",
      "Epoch:  24990 , step:  24990 , train loss:  0.0685695358681955 , test loss:  0.09536995790571978\n",
      "Epoch:  25000 , step:  25000 , train loss:  0.0685583462174352 , test loss:  0.09536259823420191\n",
      "Epoch:  25010 , step:  25010 , train loss:  0.06854716305961868 , test loss:  0.09535524432085438\n",
      "Epoch:  25020 , step:  25020 , train loss:  0.06853598638837652 , test loss:  0.09534789615951218\n",
      "Epoch:  25030 , step:  25030 , train loss:  0.06852481619734793 , test loss:  0.09534055374401942\n",
      "Epoch:  25040 , step:  25040 , train loss:  0.068513652480181 , test loss:  0.0953332170682287\n",
      "Epoch:  25050 , step:  25050 , train loss:  0.06850249523053252 , test loss:  0.0953258861260016\n",
      "Epoch:  25060 , step:  25060 , train loss:  0.068491344442068 , test loss:  0.09531856091120823\n",
      "Epoch:  25070 , step:  25070 , train loss:  0.06848020010846165 , test loss:  0.09531124141772745\n",
      "Epoch:  25080 , step:  25080 , train loss:  0.06846906222339637 , test loss:  0.09530392763944698\n",
      "Epoch:  25090 , step:  25090 , train loss:  0.06845793078056375 , test loss:  0.09529661957026325\n",
      "Epoch:  25100 , step:  25100 , train loss:  0.06844680577366405 , test loss:  0.09528931720408138\n",
      "Epoch:  25110 , step:  25110 , train loss:  0.06843568719640616 , test loss:  0.09528202053481487\n",
      "Epoch:  25120 , step:  25120 , train loss:  0.06842457504250758 , test loss:  0.09527472955638633\n",
      "Epoch:  25130 , step:  25130 , train loss:  0.06841346930569449 , test loss:  0.09526744426272667\n",
      "Epoch:  25140 , step:  25140 , train loss:  0.06840236997970162 , test loss:  0.09526016464777552\n",
      "Epoch:  25150 , step:  25150 , train loss:  0.06839127705827223 , test loss:  0.09525289070548124\n",
      "Epoch:  25160 , step:  25160 , train loss:  0.06838019053515833 , test loss:  0.09524562242980052\n",
      "Epoch:  25170 , step:  25170 , train loss:  0.06836911040412029 , test loss:  0.09523835981469893\n",
      "Epoch:  25180 , step:  25180 , train loss:  0.06835803665892712 , test loss:  0.09523110285415035\n",
      "Epoch:  25190 , step:  25190 , train loss:  0.06834696929335636 , test loss:  0.09522385154213744\n",
      "Epoch:  25200 , step:  25200 , train loss:  0.06833590830119403 , test loss:  0.09521660587265121\n",
      "Epoch:  25210 , step:  25210 , train loss:  0.06832485367623464 , test loss:  0.09520936583969129\n",
      "Epoch:  25220 , step:  25220 , train loss:  0.06831380541228119 , test loss:  0.09520213143726573\n",
      "Epoch:  25230 , step:  25230 , train loss:  0.06830276350314518 , test loss:  0.09519490265939116\n",
      "Epoch:  25240 , step:  25240 , train loss:  0.06829172794264651 , test loss:  0.09518767950009265\n",
      "Epoch:  25250 , step:  25250 , train loss:  0.06828069872461354 , test loss:  0.09518046195340363\n",
      "Epoch:  25260 , step:  25260 , train loss:  0.06826967584288304 , test loss:  0.09517325001336623\n",
      "Epoch:  25270 , step:  25270 , train loss:  0.06825865929130022 , test loss:  0.09516604367403059\n",
      "Epoch:  25280 , step:  25280 , train loss:  0.06824764906371862 , test loss:  0.09515884292945567\n",
      "Epoch:  25290 , step:  25290 , train loss:  0.0682366451540002 , test loss:  0.09515164777370855\n",
      "Epoch:  25300 , step:  25300 , train loss:  0.06822564755601532 , test loss:  0.095144458200865\n",
      "Epoch:  25310 , step:  25310 , train loss:  0.0682146562636426 , test loss:  0.09513727420500893\n",
      "Epoch:  25320 , step:  25320 , train loss:  0.06820367127076907 , test loss:  0.09513009578023253\n",
      "Epoch:  25330 , step:  25330 , train loss:  0.06819269257128997 , test loss:  0.09512292292063648\n",
      "Epoch:  25340 , step:  25340 , train loss:  0.06818172015910902 , test loss:  0.09511575562032983\n",
      "Epoch:  25350 , step:  25350 , train loss:  0.06817075402813806 , test loss:  0.09510859387342971\n",
      "Epoch:  25360 , step:  25360 , train loss:  0.06815979417229734 , test loss:  0.0951014376740619\n",
      "Epoch:  25370 , step:  25370 , train loss:  0.06814884058551524 , test loss:  0.09509428701636008\n",
      "Epoch:  25380 , step:  25380 , train loss:  0.06813789326172849 , test loss:  0.09508714189446642\n",
      "Epoch:  25390 , step:  25390 , train loss:  0.06812695219488199 , test loss:  0.09508000230253119\n",
      "Epoch:  25400 , step:  25400 , train loss:  0.0681160173789289 , test loss:  0.0950728682347132\n",
      "Epoch:  25410 , step:  25410 , train loss:  0.06810508880783056 , test loss:  0.09506573968517915\n",
      "Epoch:  25420 , step:  25420 , train loss:  0.0680941664755565 , test loss:  0.09505861664810406\n",
      "Epoch:  25430 , step:  25430 , train loss:  0.06808325037608445 , test loss:  0.09505149911767118\n",
      "Epoch:  25440 , step:  25440 , train loss:  0.06807234050340026 , test loss:  0.09504438708807192\n",
      "Epoch:  25450 , step:  25450 , train loss:  0.06806143685149793 , test loss:  0.09503728055350585\n",
      "Epoch:  25460 , step:  25460 , train loss:  0.06805053941437965 , test loss:  0.09503017950818075\n",
      "Epoch:  25470 , step:  25470 , train loss:  0.06803964818605565 , test loss:  0.09502308394631231\n",
      "Epoch:  25480 , step:  25480 , train loss:  0.06802876316054433 , test loss:  0.09501599386212463\n",
      "Epoch:  25490 , step:  25490 , train loss:  0.06801788433187213 , test loss:  0.09500890924984987\n",
      "Epoch:  25500 , step:  25500 , train loss:  0.0680070116940736 , test loss:  0.09500183010372806\n",
      "Epoch:  25510 , step:  25510 , train loss:  0.06799614524119134 , test loss:  0.0949947564180075\n",
      "Epoch:  25520 , step:  25520 , train loss:  0.06798528496727597 , test loss:  0.09498768818694452\n",
      "Epoch:  25530 , step:  25530 , train loss:  0.06797443086638622 , test loss:  0.09498062540480348\n",
      "Epoch:  25540 , step:  25540 , train loss:  0.06796358293258875 , test loss:  0.0949735680658568\n",
      "Epoch:  25550 , step:  25550 , train loss:  0.06795274115995828 , test loss:  0.09496651616438484\n",
      "Epoch:  25560 , step:  25560 , train loss:  0.0679419055425775 , test loss:  0.09495946969467607\n",
      "Epoch:  25570 , step:  25570 , train loss:  0.0679310760745371 , test loss:  0.09495242865102693\n",
      "Epoch:  25580 , step:  25580 , train loss:  0.06792025274993571 , test loss:  0.09494539302774184\n",
      "Epoch:  25590 , step:  25590 , train loss:  0.06790943556287996 , test loss:  0.09493836281913322\n",
      "Epoch:  25600 , step:  25600 , train loss:  0.06789862450748431 , test loss:  0.09493133801952128\n",
      "Epoch:  25610 , step:  25610 , train loss:  0.06788781957787127 , test loss:  0.09492431862323428\n",
      "Epoch:  25620 , step:  25620 , train loss:  0.06787702076817123 , test loss:  0.0949173046246086\n",
      "Epoch:  25630 , step:  25630 , train loss:  0.06786622807252238 , test loss:  0.09491029601798806\n",
      "Epoch:  25640 , step:  25640 , train loss:  0.0678554414850709 , test loss:  0.09490329279772478\n",
      "Epoch:  25650 , step:  25650 , train loss:  0.0678446609999708 , test loss:  0.09489629495817858\n",
      "Epoch:  25660 , step:  25660 , train loss:  0.06783388661138391 , test loss:  0.09488930249371735\n",
      "Epoch:  25670 , step:  25670 , train loss:  0.06782311831348004 , test loss:  0.0948823153987166\n",
      "Epoch:  25680 , step:  25680 , train loss:  0.06781235610043658 , test loss:  0.0948753336675597\n",
      "Epoch:  25690 , step:  25690 , train loss:  0.06780159996643899 , test loss:  0.09486835729463809\n",
      "Epoch:  25700 , step:  25700 , train loss:  0.06779084990568042 , test loss:  0.09486138627435067\n",
      "Epoch:  25710 , step:  25710 , train loss:  0.06778010591236173 , test loss:  0.09485442060110433\n",
      "Epoch:  25720 , step:  25720 , train loss:  0.06776936798069172 , test loss:  0.0948474602693139\n",
      "Epoch:  25730 , step:  25730 , train loss:  0.06775863610488685 , test loss:  0.09484050527340172\n",
      "Epoch:  25740 , step:  25740 , train loss:  0.0677479102791713 , test loss:  0.09483355560779796\n",
      "Epoch:  25750 , step:  25750 , train loss:  0.06773719049777707 , test loss:  0.09482661126694067\n",
      "Epoch:  25760 , step:  25760 , train loss:  0.06772647675494382 , test loss:  0.09481967224527536\n",
      "Epoch:  25770 , step:  25770 , train loss:  0.06771576904491897 , test loss:  0.09481273853725554\n",
      "Epoch:  25780 , step:  25780 , train loss:  0.06770506736195753 , test loss:  0.09480581013734223\n",
      "Epoch:  25790 , step:  25790 , train loss:  0.06769437170032237 , test loss:  0.09479888704000435\n",
      "Epoch:  25800 , step:  25800 , train loss:  0.06768368205428382 , test loss:  0.09479196923971835\n",
      "Epoch:  25810 , step:  25810 , train loss:  0.06767299841812002 , test loss:  0.0947850567309683\n",
      "Epoch:  25820 , step:  25820 , train loss:  0.06766232078611667 , test loss:  0.09477814950824603\n",
      "Epoch:  25830 , step:  25830 , train loss:  0.06765164915256719 , test loss:  0.09477124756605097\n",
      "Epoch:  25840 , step:  25840 , train loss:  0.06764098351177249 , test loss:  0.0947643508988901\n",
      "Epoch:  25850 , step:  25850 , train loss:  0.06763032385804116 , test loss:  0.09475745950127828\n",
      "Epoch:  25860 , step:  25860 , train loss:  0.06761967018568936 , test loss:  0.09475057336773764\n",
      "Epoch:  25870 , step:  25870 , train loss:  0.06760902248904087 , test loss:  0.09474369249279793\n",
      "Epoch:  25880 , step:  25880 , train loss:  0.06759838076242694 , test loss:  0.09473681687099675\n",
      "Epoch:  25890 , step:  25890 , train loss:  0.0675877450001865 , test loss:  0.09472994649687903\n",
      "Epoch:  25900 , step:  25900 , train loss:  0.06757711519666589 , test loss:  0.0947230813649973\n",
      "Epoch:  25910 , step:  25910 , train loss:  0.06756649134621905 , test loss:  0.09471622146991168\n",
      "Epoch:  25920 , step:  25920 , train loss:  0.06755587344320742 , test loss:  0.09470936680618965\n",
      "Epoch:  25930 , step:  25930 , train loss:  0.06754526148199991 , test loss:  0.09470251736840643\n",
      "Epoch:  25940 , step:  25940 , train loss:  0.06753465545697294 , test loss:  0.09469567315114451\n",
      "Epoch:  25950 , step:  25950 , train loss:  0.06752405536251042 , test loss:  0.09468883414899389\n",
      "Epoch:  25960 , step:  25960 , train loss:  0.06751346119300375 , test loss:  0.09468200035655237\n",
      "Epoch:  25970 , step:  25970 , train loss:  0.06750287294285164 , test loss:  0.09467517176842469\n",
      "Epoch:  25980 , step:  25980 , train loss:  0.06749229060646035 , test loss:  0.09466834837922349\n",
      "Epoch:  25990 , step:  25990 , train loss:  0.06748171417824358 , test loss:  0.09466153018356835\n",
      "Epoch:  26000 , step:  26000 , train loss:  0.06747114365262238 , test loss:  0.09465471717608685\n",
      "Epoch:  26010 , step:  26010 , train loss:  0.0674605790240252 , test loss:  0.0946479093514136\n",
      "Epoch:  26020 , step:  26020 , train loss:  0.06745002028688794 , test loss:  0.09464110670419053\n",
      "Epoch:  26030 , step:  26030 , train loss:  0.06743946743565378 , test loss:  0.09463430922906729\n",
      "Epoch:  26040 , step:  26040 , train loss:  0.06742892046477336 , test loss:  0.0946275169207006\n",
      "Epoch:  26050 , step:  26050 , train loss:  0.06741837936870457 , test loss:  0.09462072977375469\n",
      "Epoch:  26060 , step:  26060 , train loss:  0.0674078441419127 , test loss:  0.09461394778290104\n",
      "Epoch:  26070 , step:  26070 , train loss:  0.0673973147788703 , test loss:  0.09460717094281847\n",
      "Epoch:  26080 , step:  26080 , train loss:  0.06738679127405735 , test loss:  0.09460039924819326\n",
      "Epoch:  26090 , step:  26090 , train loss:  0.067376273621961 , test loss:  0.09459363269371872\n",
      "Epoch:  26100 , step:  26100 , train loss:  0.06736576181707575 , test loss:  0.0945868712740957\n",
      "Epoch:  26110 , step:  26110 , train loss:  0.06735525585390333 , test loss:  0.09458011498403233\n",
      "Epoch:  26120 , step:  26120 , train loss:  0.06734475572695278 , test loss:  0.09457336381824372\n",
      "Epoch:  26130 , step:  26130 , train loss:  0.06733426143074037 , test loss:  0.09456661777145256\n",
      "Epoch:  26140 , step:  26140 , train loss:  0.06732377295978964 , test loss:  0.09455987683838853\n",
      "Epoch:  26150 , step:  26150 , train loss:  0.06731329030863126 , test loss:  0.09455314101378873\n",
      "Epoch:  26160 , step:  26160 , train loss:  0.06730281347180317 , test loss:  0.09454641029239746\n",
      "Epoch:  26170 , step:  26170 , train loss:  0.06729234244385056 , test loss:  0.094539684668966\n",
      "Epoch:  26180 , step:  26180 , train loss:  0.06728187721932571 , test loss:  0.09453296413825305\n",
      "Epoch:  26190 , step:  26190 , train loss:  0.06727141779278814 , test loss:  0.09452624869502436\n",
      "Epoch:  26200 , step:  26200 , train loss:  0.06726096415880453 , test loss:  0.09451953833405297\n",
      "Epoch:  26210 , step:  26210 , train loss:  0.06725051631194863 , test loss:  0.09451283305011898\n",
      "Epoch:  26220 , step:  26220 , train loss:  0.06724007424680147 , test loss:  0.09450613283800983\n",
      "Epoch:  26230 , step:  26230 , train loss:  0.06722963795795112 , test loss:  0.09449943769251963\n",
      "Epoch:  26240 , step:  26240 , train loss:  0.06721920743999271 , test loss:  0.09449274760845013\n",
      "Epoch:  26250 , step:  26250 , train loss:  0.06720878268752861 , test loss:  0.09448606258060982\n",
      "Epoch:  26260 , step:  26260 , train loss:  0.06719836369516818 , test loss:  0.09447938260381442\n",
      "Epoch:  26270 , step:  26270 , train loss:  0.06718795045752789 , test loss:  0.09447270767288682\n",
      "Epoch:  26280 , step:  26280 , train loss:  0.06717754296923127 , test loss:  0.09446603778265678\n",
      "Epoch:  26290 , step:  26290 , train loss:  0.06716714122490892 , test loss:  0.09445937292796124\n",
      "Epoch:  26300 , step:  26300 , train loss:  0.06715674521919848 , test loss:  0.0944527131036442\n",
      "Epoch:  26310 , step:  26310 , train loss:  0.06714635494674458 , test loss:  0.09444605830455666\n",
      "Epoch:  26320 , step:  26320 , train loss:  0.06713597040219893 , test loss:  0.09443940852555663\n",
      "Epoch:  26330 , step:  26330 , train loss:  0.06712559158022018 , test loss:  0.09443276376150908\n",
      "Epoch:  26340 , step:  26340 , train loss:  0.06711521847547411 , test loss:  0.09442612400728609\n",
      "Epoch:  26350 , step:  26350 , train loss:  0.06710485108263327 , test loss:  0.09441948925776664\n",
      "Epoch:  26360 , step:  26360 , train loss:  0.06709448939637738 , test loss:  0.09441285950783676\n",
      "Epoch:  26370 , step:  26370 , train loss:  0.06708413341139306 , test loss:  0.09440623475238921\n",
      "Epoch:  26380 , step:  26380 , train loss:  0.06707378312237378 , test loss:  0.09439961498632407\n",
      "Epoch:  26390 , step:  26390 , train loss:  0.06706343852402008 , test loss:  0.09439300020454813\n",
      "Epoch:  26400 , step:  26400 , train loss:  0.06705309961103942 , test loss:  0.09438639040197519\n",
      "Epoch:  26410 , step:  26410 , train loss:  0.06704276637814605 , test loss:  0.09437978557352582\n",
      "Epoch:  26420 , step:  26420 , train loss:  0.06703243882006121 , test loss:  0.09437318571412755\n",
      "Epoch:  26430 , step:  26430 , train loss:  0.06702211693151308 , test loss:  0.0943665908187149\n",
      "Epoch:  26440 , step:  26440 , train loss:  0.06701180070723659 , test loss:  0.09436000088222929\n",
      "Epoch:  26450 , step:  26450 , train loss:  0.06700149014197367 , test loss:  0.09435341589961879\n",
      "Epoch:  26460 , step:  26460 , train loss:  0.066991185230473 , test loss:  0.09434683586583847\n",
      "Epoch:  26470 , step:  26470 , train loss:  0.06698088596749018 , test loss:  0.09434026077585025\n",
      "Epoch:  26480 , step:  26480 , train loss:  0.06697059234778761 , test loss:  0.09433369062462287\n",
      "Epoch:  26490 , step:  26490 , train loss:  0.06696030436613447 , test loss:  0.0943271254071318\n",
      "Epoch:  26500 , step:  26500 , train loss:  0.06695002201730688 , test loss:  0.09432056511835953\n",
      "Epoch:  26510 , step:  26510 , train loss:  0.06693974529608761 , test loss:  0.09431400975329515\n",
      "Epoch:  26520 , step:  26520 , train loss:  0.0669294741972663 , test loss:  0.09430745930693465\n",
      "Epoch:  26530 , step:  26530 , train loss:  0.06691920871563933 , test loss:  0.09430091377428074\n",
      "Epoch:  26540 , step:  26540 , train loss:  0.06690894884600991 , test loss:  0.09429437315034278\n",
      "Epoch:  26550 , step:  26550 , train loss:  0.0668986945831879 , test loss:  0.09428783743013704\n",
      "Epoch:  26560 , step:  26560 , train loss:  0.06688844592199002 , test loss:  0.09428130660868661\n",
      "Epoch:  26570 , step:  26570 , train loss:  0.06687820285723964 , test loss:  0.09427478068102103\n",
      "Epoch:  26580 , step:  26580 , train loss:  0.06686796538376683 , test loss:  0.09426825964217689\n",
      "Epoch:  26590 , step:  26590 , train loss:  0.06685773349640847 , test loss:  0.09426174348719707\n",
      "Epoch:  26600 , step:  26600 , train loss:  0.06684750719000805 , test loss:  0.09425523221113156\n",
      "Epoch:  26610 , step:  26610 , train loss:  0.06683728645941575 , test loss:  0.09424872580903687\n",
      "Epoch:  26620 , step:  26620 , train loss:  0.0668270712994885 , test loss:  0.09424222427597598\n",
      "Epoch:  26630 , step:  26630 , train loss:  0.06681686170508984 , test loss:  0.09423572760701898\n",
      "Epoch:  26640 , step:  26640 , train loss:  0.06680665767108993 , test loss:  0.09422923579724207\n",
      "Epoch:  26650 , step:  26650 , train loss:  0.06679645919236565 , test loss:  0.09422274884172864\n",
      "Epoch:  26660 , step:  26660 , train loss:  0.06678626626380044 , test loss:  0.09421626673556827\n",
      "Epoch:  26670 , step:  26670 , train loss:  0.06677607888028442 , test loss:  0.09420978947385722\n",
      "Epoch:  26680 , step:  26680 , train loss:  0.06676589703671425 , test loss:  0.09420331705169863\n",
      "Epoch:  26690 , step:  26690 , train loss:  0.06675572072799327 , test loss:  0.09419684946420201\n",
      "Epoch:  26700 , step:  26700 , train loss:  0.06674554994903135 , test loss:  0.09419038670648339\n",
      "Epoch:  26710 , step:  26710 , train loss:  0.06673538469474492 , test loss:  0.0941839287736655\n",
      "Epoch:  26720 , step:  26720 , train loss:  0.06672522496005708 , test loss:  0.09417747566087777\n",
      "Epoch:  26730 , step:  26730 , train loss:  0.06671507073989734 , test loss:  0.09417102736325596\n",
      "Epoch:  26740 , step:  26740 , train loss:  0.06670492202920188 , test loss:  0.09416458387594223\n",
      "Epoch:  26750 , step:  26750 , train loss:  0.06669477882291332 , test loss:  0.09415814519408566\n",
      "Epoch:  26760 , step:  26760 , train loss:  0.06668464111598087 , test loss:  0.09415171131284143\n",
      "Epoch:  26770 , step:  26770 , train loss:  0.06667450890336019 , test loss:  0.09414528222737173\n",
      "Epoch:  26780 , step:  26780 , train loss:  0.0666643821800135 , test loss:  0.09413885793284467\n",
      "Epoch:  26790 , step:  26790 , train loss:  0.06665426094090947 , test loss:  0.0941324384244354\n",
      "Epoch:  26800 , step:  26800 , train loss:  0.06664414518102325 , test loss:  0.09412602369732505\n",
      "Epoch:  26810 , step:  26810 , train loss:  0.06663403489533647 , test loss:  0.09411961374670144\n",
      "Epoch:  26820 , step:  26820 , train loss:  0.06662393007883724 , test loss:  0.09411320856775894\n",
      "Epoch:  26830 , step:  26830 , train loss:  0.06661383072652008 , test loss:  0.09410680815569823\n",
      "Epoch:  26840 , step:  26840 , train loss:  0.06660373683338595 , test loss:  0.09410041250572636\n",
      "Epoch:  26850 , step:  26850 , train loss:  0.06659364839444226 , test loss:  0.09409402161305686\n",
      "Epoch:  26860 , step:  26860 , train loss:  0.06658356540470281 , test loss:  0.09408763547290984\n",
      "Epoch:  26870 , step:  26870 , train loss:  0.06657348785918783 , test loss:  0.09408125408051139\n",
      "Epoch:  26880 , step:  26880 , train loss:  0.0665634157529239 , test loss:  0.09407487743109466\n",
      "Epoch:  26890 , step:  26890 , train loss:  0.06655334908094403 , test loss:  0.09406850551989829\n",
      "Epoch:  26900 , step:  26900 , train loss:  0.06654328783828757 , test loss:  0.09406213834216803\n",
      "Epoch:  26910 , step:  26910 , train loss:  0.06653323202000026 , test loss:  0.09405577589315563\n",
      "Epoch:  26920 , step:  26920 , train loss:  0.06652318162113423 , test loss:  0.09404941816811926\n",
      "Epoch:  26930 , step:  26930 , train loss:  0.06651313663674778 , test loss:  0.09404306516232358\n",
      "Epoch:  26940 , step:  26940 , train loss:  0.06650309706190576 , test loss:  0.0940367168710391\n",
      "Epoch:  26950 , step:  26950 , train loss:  0.06649306289167917 , test loss:  0.09403037328954308\n",
      "Epoch:  26960 , step:  26960 , train loss:  0.06648303412114547 , test loss:  0.0940240344131191\n",
      "Epoch:  26970 , step:  26970 , train loss:  0.06647301074538828 , test loss:  0.09401770023705681\n",
      "Epoch:  26980 , step:  26980 , train loss:  0.06646299275949759 , test loss:  0.09401137075665209\n",
      "Epoch:  26990 , step:  26990 , train loss:  0.06645298015856965 , test loss:  0.0940050459672074\n",
      "Epoch:  27000 , step:  27000 , train loss:  0.066442972937707 , test loss:  0.09399872586403117\n",
      "Epoch:  27010 , step:  27010 , train loss:  0.06643297109201834 , test loss:  0.09399241044243829\n",
      "Epoch:  27020 , step:  27020 , train loss:  0.0664229746166188 , test loss:  0.0939860996977497\n",
      "Epoch:  27030 , step:  27030 , train loss:  0.06641298350662958 , test loss:  0.09397979362529259\n",
      "Epoch:  27040 , step:  27040 , train loss:  0.06640299775717817 , test loss:  0.09397349222040047\n",
      "Epoch:  27050 , step:  27050 , train loss:  0.06639301736339831 , test loss:  0.0939671954784131\n",
      "Epoch:  27060 , step:  27060 , train loss:  0.0663830423204299 , test loss:  0.09396090339467632\n",
      "Epoch:  27070 , step:  27070 , train loss:  0.06637307262341904 , test loss:  0.09395461596454202\n",
      "Epoch:  27080 , step:  27080 , train loss:  0.06636310826751803 , test loss:  0.09394833318336868\n",
      "Epoch:  27090 , step:  27090 , train loss:  0.06635314924788538 , test loss:  0.09394205504652052\n",
      "Epoch:  27100 , step:  27100 , train loss:  0.06634319555968568 , test loss:  0.09393578154936823\n",
      "Epoch:  27110 , step:  27110 , train loss:  0.06633324719808978 , test loss:  0.09392951268728844\n",
      "Epoch:  27120 , step:  27120 , train loss:  0.06632330415827464 , test loss:  0.09392324845566392\n",
      "Epoch:  27130 , step:  27130 , train loss:  0.06631336643542328 , test loss:  0.09391698884988371\n",
      "Epoch:  27140 , step:  27140 , train loss:  0.06630343402472495 , test loss:  0.0939107338653429\n",
      "Epoch:  27150 , step:  27150 , train loss:  0.06629350692137498 , test loss:  0.09390448349744267\n",
      "Epoch:  27160 , step:  27160 , train loss:  0.06628358512057478 , test loss:  0.09389823774159015\n",
      "Epoch:  27170 , step:  27170 , train loss:  0.0662736686175319 , test loss:  0.09389199659319888\n",
      "Epoch:  27180 , step:  27180 , train loss:  0.06626375740745996 , test loss:  0.09388576004768817\n",
      "Epoch:  27190 , step:  27190 , train loss:  0.06625385148557864 , test loss:  0.09387952810048363\n",
      "Epoch:  27200 , step:  27200 , train loss:  0.0662439508471137 , test loss:  0.09387330074701673\n",
      "Epoch:  27210 , step:  27210 , train loss:  0.06623405548729695 , test loss:  0.09386707798272509\n",
      "Epoch:  27220 , step:  27220 , train loss:  0.06622416540136629 , test loss:  0.09386085980305221\n",
      "Epoch:  27230 , step:  27230 , train loss:  0.06621428058456558 , test loss:  0.09385464620344797\n",
      "Epoch:  27240 , step:  27240 , train loss:  0.0662044010321448 , test loss:  0.093848437179368\n",
      "Epoch:  27250 , step:  27250 , train loss:  0.06619452673935985 , test loss:  0.09384223272627369\n",
      "Epoch:  27260 , step:  27260 , train loss:  0.06618465770147272 , test loss:  0.09383603283963302\n",
      "Epoch:  27270 , step:  27270 , train loss:  0.06617479391375132 , test loss:  0.09382983751491951\n",
      "Epoch:  27280 , step:  27280 , train loss:  0.06616493537146963 , test loss:  0.09382364674761282\n",
      "Epoch:  27290 , step:  27290 , train loss:  0.06615508206990756 , test loss:  0.09381746053319852\n",
      "Epoch:  27300 , step:  27300 , train loss:  0.06614523400435099 , test loss:  0.09381127886716811\n",
      "Epoch:  27310 , step:  27310 , train loss:  0.06613539117009178 , test loss:  0.0938051017450192\n",
      "Epoch:  27320 , step:  27320 , train loss:  0.06612555356242773 , test loss:  0.09379892916225512\n",
      "Epoch:  27330 , step:  27330 , train loss:  0.06611572117666255 , test loss:  0.0937927611143852\n",
      "Epoch:  27340 , step:  27340 , train loss:  0.06610589400810594 , test loss:  0.09378659759692476\n",
      "Epoch:  27350 , step:  27350 , train loss:  0.0660960720520735 , test loss:  0.09378043860539499\n",
      "Epoch:  27360 , step:  27360 , train loss:  0.06608625530388668 , test loss:  0.09377428413532286\n",
      "Epoch:  27370 , step:  27370 , train loss:  0.06607644375887294 , test loss:  0.09376813418224132\n",
      "Epoch:  27380 , step:  27380 , train loss:  0.0660666374123655 , test loss:  0.09376198874168934\n",
      "Epoch:  27390 , step:  27390 , train loss:  0.0660568362597036 , test loss:  0.0937558478092115\n",
      "Epoch:  27400 , step:  27400 , train loss:  0.06604704029623229 , test loss:  0.09374971138035836\n",
      "Epoch:  27410 , step:  27410 , train loss:  0.06603724951730243 , test loss:  0.09374357945068647\n",
      "Epoch:  27420 , step:  27420 , train loss:  0.06602746391827083 , test loss:  0.09373745201575782\n",
      "Epoch:  27430 , step:  27430 , train loss:  0.06601768349450009 , test loss:  0.09373132907114066\n",
      "Epoch:  27440 , step:  27440 , train loss:  0.06600790824135863 , test loss:  0.09372521061240889\n",
      "Epoch:  27450 , step:  27450 , train loss:  0.06599813815422076 , test loss:  0.09371909663514198\n",
      "Epoch:  27460 , step:  27460 , train loss:  0.06598837322846654 , test loss:  0.0937129871349257\n",
      "Epoch:  27470 , step:  27470 , train loss:  0.06597861345948187 , test loss:  0.09370688210735112\n",
      "Epoch:  27480 , step:  27480 , train loss:  0.06596885884265846 , test loss:  0.09370078154801544\n",
      "Epoch:  27490 , step:  27490 , train loss:  0.06595910937339376 , test loss:  0.09369468545252149\n",
      "Epoch:  27500 , step:  27500 , train loss:  0.06594936504709106 , test loss:  0.09368859381647787\n",
      "Epoch:  27510 , step:  27510 , train loss:  0.06593962585915936 , test loss:  0.09368250663549887\n",
      "Epoch:  27520 , step:  27520 , train loss:  0.06592989180501344 , test loss:  0.09367642390520466\n",
      "Epoch:  27530 , step:  27530 , train loss:  0.06592016288007385 , test loss:  0.09367034562122097\n",
      "Epoch:  27540 , step:  27540 , train loss:  0.0659104390797669 , test loss:  0.09366427177917944\n",
      "Epoch:  27550 , step:  27550 , train loss:  0.0659007203995246 , test loss:  0.09365820237471731\n",
      "Epoch:  27560 , step:  27560 , train loss:  0.06589100683478462 , test loss:  0.09365213740347764\n",
      "Epoch:  27570 , step:  27570 , train loss:  0.06588129838099042 , test loss:  0.0936460768611089\n",
      "Epoch:  27580 , step:  27580 , train loss:  0.06587159503359125 , test loss:  0.09364002074326563\n",
      "Epoch:  27590 , step:  27590 , train loss:  0.06586189678804186 , test loss:  0.09363396904560772\n",
      "Epoch:  27600 , step:  27600 , train loss:  0.06585220363980285 , test loss:  0.09362792176380111\n",
      "Epoch:  27610 , step:  27610 , train loss:  0.06584251558434037 , test loss:  0.09362187889351685\n",
      "Epoch:  27620 , step:  27620 , train loss:  0.06583283261712634 , test loss:  0.0936158404304322\n",
      "Epoch:  27630 , step:  27630 , train loss:  0.06582315473363831 , test loss:  0.09360980637022961\n",
      "Epoch:  27640 , step:  27640 , train loss:  0.06581348192935944 , test loss:  0.09360377670859757\n",
      "Epoch:  27650 , step:  27650 , train loss:  0.06580381419977858 , test loss:  0.09359775144123002\n",
      "Epoch:  27660 , step:  27660 , train loss:  0.06579415154039021 , test loss:  0.09359173056382612\n",
      "Epoch:  27670 , step:  27670 , train loss:  0.06578449394669433 , test loss:  0.09358571407209126\n",
      "Epoch:  27680 , step:  27680 , train loss:  0.06577484141419676 , test loss:  0.0935797019617361\n",
      "Epoch:  27690 , step:  27690 , train loss:  0.06576519393840874 , test loss:  0.09357369422847699\n",
      "Epoch:  27700 , step:  27700 , train loss:  0.06575555151484716 , test loss:  0.09356769086803574\n",
      "Epoch:  27710 , step:  27710 , train loss:  0.06574591413903452 , test loss:  0.09356169187613987\n",
      "Epoch:  27720 , step:  27720 , train loss:  0.06573628180649893 , test loss:  0.0935556972485223\n",
      "Epoch:  27730 , step:  27730 , train loss:  0.06572665451277394 , test loss:  0.09354970698092169\n",
      "Epoch:  27740 , step:  27740 , train loss:  0.0657170322533988 , test loss:  0.09354372106908201\n",
      "Epoch:  27750 , step:  27750 , train loss:  0.06570741502391823 , test loss:  0.09353773950875305\n",
      "Epoch:  27760 , step:  27760 , train loss:  0.06569780281988255 , test loss:  0.09353176229568974\n",
      "Epoch:  27770 , step:  27770 , train loss:  0.06568819563684757 , test loss:  0.0935257894256529\n",
      "Epoch:  27780 , step:  27780 , train loss:  0.0656785934703746 , test loss:  0.09351982089440866\n",
      "Epoch:  27790 , step:  27790 , train loss:  0.06566899631603054 , test loss:  0.09351385669772866\n",
      "Epoch:  27800 , step:  27800 , train loss:  0.06565940416938776 , test loss:  0.09350789683138999\n",
      "Epoch:  27810 , step:  27810 , train loss:  0.0656498170260241 , test loss:  0.09350194129117549\n",
      "Epoch:  27820 , step:  27820 , train loss:  0.06564023488152293 , test loss:  0.09349599007287294\n",
      "Epoch:  27830 , step:  27830 , train loss:  0.0656306577314731 , test loss:  0.09349004317227612\n",
      "Epoch:  27840 , step:  27840 , train loss:  0.0656210855714689 , test loss:  0.09348410058518397\n",
      "Epoch:  27850 , step:  27850 , train loss:  0.06561151839711012 , test loss:  0.09347816230740102\n",
      "Epoch:  27860 , step:  27860 , train loss:  0.06560195620400197 , test loss:  0.09347222833473691\n",
      "Epoch:  27870 , step:  27870 , train loss:  0.06559239898775512 , test loss:  0.09346629866300715\n",
      "Epoch:  27880 , step:  27880 , train loss:  0.0655828467439857 , test loss:  0.09346037328803233\n",
      "Epoch:  27890 , step:  27890 , train loss:  0.06557329946831524 , test loss:  0.09345445220563875\n",
      "Epoch:  27900 , step:  27900 , train loss:  0.06556375715637071 , test loss:  0.09344853541165782\n",
      "Epoch:  27910 , step:  27910 , train loss:  0.06555421980378445 , test loss:  0.09344262290192644\n",
      "Epoch:  27920 , step:  27920 , train loss:  0.06554468740619426 , test loss:  0.09343671467228698\n",
      "Epoch:  27930 , step:  27930 , train loss:  0.0655351599592433 , test loss:  0.09343081071858704\n",
      "Epoch:  27940 , step:  27940 , train loss:  0.0655256374585801 , test loss:  0.09342491103667962\n",
      "Epoch:  27950 , step:  27950 , train loss:  0.06551611989985866 , test loss:  0.0934190156224232\n",
      "Epoch:  27960 , step:  27960 , train loss:  0.06550660727873821 , test loss:  0.09341312447168147\n",
      "Epoch:  27970 , step:  27970 , train loss:  0.06549709959088341 , test loss:  0.09340723758032345\n",
      "Epoch:  27980 , step:  27980 , train loss:  0.06548759683196431 , test loss:  0.09340135494422354\n",
      "Epoch:  27990 , step:  27990 , train loss:  0.06547809899765622 , test loss:  0.09339547655926161\n",
      "Epoch:  28000 , step:  28000 , train loss:  0.06546860608363987 , test loss:  0.09338960242132244\n",
      "Epoch:  28010 , step:  28010 , train loss:  0.06545911808560125 , test loss:  0.09338373252629653\n",
      "Epoch:  28020 , step:  28020 , train loss:  0.06544963499923168 , test loss:  0.09337786687007943\n",
      "Epoch:  28030 , step:  28030 , train loss:  0.06544015682022783 , test loss:  0.09337200544857216\n",
      "Epoch:  28040 , step:  28040 , train loss:  0.06543068354429162 , test loss:  0.0933661482576808\n",
      "Epoch:  28050 , step:  28050 , train loss:  0.0654212151671303 , test loss:  0.0933602952933168\n",
      "Epoch:  28060 , step:  28060 , train loss:  0.06541175168445634 , test loss:  0.09335444655139705\n",
      "Epoch:  28070 , step:  28070 , train loss:  0.06540229309198761 , test loss:  0.0933486020278432\n",
      "Epoch:  28080 , step:  28080 , train loss:  0.06539283938544714 , test loss:  0.09334276171858279\n",
      "Epoch:  28090 , step:  28090 , train loss:  0.06538339056056323 , test loss:  0.09333692561954805\n",
      "Epoch:  28100 , step:  28100 , train loss:  0.06537394661306947 , test loss:  0.09333109372667683\n",
      "Epoch:  28110 , step:  28110 , train loss:  0.0653645075387047 , test loss:  0.09332526603591194\n",
      "Epoch:  28120 , step:  28120 , train loss:  0.06535507333321289 , test loss:  0.09331944254320139\n",
      "Epoch:  28130 , step:  28130 , train loss:  0.06534564399234338 , test loss:  0.09331362324449854\n",
      "Epoch:  28140 , step:  28140 , train loss:  0.06533621951185063 , test loss:  0.09330780813576192\n",
      "Epoch:  28150 , step:  28150 , train loss:  0.06532679988749439 , test loss:  0.0933019972129553\n",
      "Epoch:  28160 , step:  28160 , train loss:  0.0653173851150395 , test loss:  0.09329619047204739\n",
      "Epoch:  28170 , step:  28170 , train loss:  0.06530797519025608 , test loss:  0.0932903879090123\n",
      "Epoch:  28180 , step:  28180 , train loss:  0.06529857010891943 , test loss:  0.09328458951982921\n",
      "Epoch:  28190 , step:  28190 , train loss:  0.06528916986680999 , test loss:  0.09327879530048243\n",
      "Epoch:  28200 , step:  28200 , train loss:  0.06527977445971336 , test loss:  0.0932730052469614\n",
      "Epoch:  28210 , step:  28210 , train loss:  0.06527038388342037 , test loss:  0.09326721935526083\n",
      "Epoch:  28220 , step:  28220 , train loss:  0.06526099813372693 , test loss:  0.09326143762138048\n",
      "Epoch:  28230 , step:  28230 , train loss:  0.06525161720643419 , test loss:  0.09325566004132513\n",
      "Epoch:  28240 , step:  28240 , train loss:  0.06524224109734827 , test loss:  0.09324988661110482\n",
      "Epoch:  28250 , step:  28250 , train loss:  0.06523286980228055 , test loss:  0.09324411732673456\n",
      "Epoch:  28260 , step:  28260 , train loss:  0.06522350331704756 , test loss:  0.09323835218423449\n",
      "Epoch:  28270 , step:  28270 , train loss:  0.06521414163747079 , test loss:  0.09323259117962997\n",
      "Epoch:  28280 , step:  28280 , train loss:  0.065204784759377 , test loss:  0.09322683430895133\n",
      "Epoch:  28290 , step:  28290 , train loss:  0.0651954326785979 , test loss:  0.09322108156823386\n",
      "Epoch:  28300 , step:  28300 , train loss:  0.06518608539097039 , test loss:  0.09321533295351819\n",
      "Epoch:  28310 , step:  28310 , train loss:  0.06517674289233642 , test loss:  0.09320958846084973\n",
      "Epoch:  28320 , step:  28320 , train loss:  0.06516740517854304 , test loss:  0.09320384808627914\n",
      "Epoch:  28330 , step:  28330 , train loss:  0.0651580722454423 , test loss:  0.09319811182586188\n",
      "Epoch:  28340 , step:  28340 , train loss:  0.06514874408889132 , test loss:  0.09319237967565855\n",
      "Epoch:  28350 , step:  28350 , train loss:  0.06513942070475233 , test loss:  0.09318665163173502\n",
      "Epoch:  28360 , step:  28360 , train loss:  0.06513010208889254 , test loss:  0.09318092769016181\n",
      "Epoch:  28370 , step:  28370 , train loss:  0.06512078823718422 , test loss:  0.09317520784701465\n",
      "Epoch:  28380 , step:  28380 , train loss:  0.06511147914550466 , test loss:  0.09316949209837425\n",
      "Epoch:  28390 , step:  28390 , train loss:  0.06510217480973612 , test loss:  0.09316378044032615\n",
      "Epoch:  28400 , step:  28400 , train loss:  0.06509287522576596 , test loss:  0.09315807286896104\n",
      "Epoch:  28410 , step:  28410 , train loss:  0.0650835803894865 , test loss:  0.09315236938037451\n",
      "Epoch:  28420 , step:  28420 , train loss:  0.065074290296795 , test loss:  0.09314666997066717\n",
      "Epoch:  28430 , step:  28430 , train loss:  0.0650650049435938 , test loss:  0.09314097463594459\n",
      "Epoch:  28440 , step:  28440 , train loss:  0.06505572432579013 , test loss:  0.09313528337231722\n",
      "Epoch:  28450 , step:  28450 , train loss:  0.06504644843929623 , test loss:  0.09312959617590048\n",
      "Epoch:  28460 , step:  28460 , train loss:  0.06503717728002935 , test loss:  0.09312391304281478\n",
      "Epoch:  28470 , step:  28470 , train loss:  0.0650279108439116 , test loss:  0.09311823396918541\n",
      "Epoch:  28480 , step:  28480 , train loss:  0.06501864912687011 , test loss:  0.09311255895114254\n",
      "Epoch:  28490 , step:  28490 , train loss:  0.06500939212483689 , test loss:  0.09310688798482135\n",
      "Epoch:  28500 , step:  28500 , train loss:  0.06500013983374894 , test loss:  0.09310122106636169\n",
      "Epoch:  28510 , step:  28510 , train loss:  0.06499089224954817 , test loss:  0.09309555819190862\n",
      "Epoch:  28520 , step:  28520 , train loss:  0.06498164936818135 , test loss:  0.093089899357612\n",
      "Epoch:  28530 , step:  28530 , train loss:  0.06497241118560025 , test loss:  0.0930842445596264\n",
      "Epoch:  28540 , step:  28540 , train loss:  0.06496317769776143 , test loss:  0.0930785937941115\n",
      "Epoch:  28550 , step:  28550 , train loss:  0.06495394890062646 , test loss:  0.09307294705723164\n",
      "Epoch:  28560 , step:  28560 , train loss:  0.06494472479016174 , test loss:  0.09306730434515612\n",
      "Epoch:  28570 , step:  28570 , train loss:  0.06493550536233855 , test loss:  0.0930616656540589\n",
      "Epoch:  28580 , step:  28580 , train loss:  0.06492629061313301 , test loss:  0.09305603098011919\n",
      "Epoch:  28590 , step:  28590 , train loss:  0.06491708053852609 , test loss:  0.0930504003195207\n",
      "Epoch:  28600 , step:  28600 , train loss:  0.06490787513450377 , test loss:  0.09304477366845212\n",
      "Epoch:  28610 , step:  28610 , train loss:  0.06489867439705671 , test loss:  0.09303915102310668\n",
      "Epoch:  28620 , step:  28620 , train loss:  0.06488947832218046 , test loss:  0.09303353237968283\n",
      "Epoch:  28630 , step:  28630 , train loss:  0.06488028690587541 , test loss:  0.0930279177343837\n",
      "Epoch:  28640 , step:  28640 , train loss:  0.06487110014414678 , test loss:  0.0930223070834169\n",
      "Epoch:  28650 , step:  28650 , train loss:  0.06486191803300456 , test loss:  0.09301670042299526\n",
      "Epoch:  28660 , step:  28660 , train loss:  0.06485274056846367 , test loss:  0.09301109774933615\n",
      "Epoch:  28670 , step:  28670 , train loss:  0.06484356774654372 , test loss:  0.09300549905866178\n",
      "Epoch:  28680 , step:  28680 , train loss:  0.06483439956326909 , test loss:  0.09299990434719906\n",
      "Epoch:  28690 , step:  28690 , train loss:  0.06482523601466911 , test loss:  0.09299431361117978\n",
      "Epoch:  28700 , step:  28700 , train loss:  0.06481607709677771 , test loss:  0.09298872684684026\n",
      "Epoch:  28710 , step:  28710 , train loss:  0.06480692280563369 , test loss:  0.0929831440504219\n",
      "Epoch:  28720 , step:  28720 , train loss:  0.06479777313728063 , test loss:  0.09297756521817056\n",
      "Epoch:  28730 , step:  28730 , train loss:  0.06478862808776677 , test loss:  0.09297199034633687\n",
      "Epoch:  28740 , step:  28740 , train loss:  0.06477948765314523 , test loss:  0.09296641943117637\n",
      "Epoch:  28750 , step:  28750 , train loss:  0.06477035182947379 , test loss:  0.09296085246894897\n",
      "Epoch:  28760 , step:  28760 , train loss:  0.06476122061281497 , test loss:  0.09295528945591962\n",
      "Epoch:  28770 , step:  28770 , train loss:  0.06475209399923607 , test loss:  0.09294973038835785\n",
      "Epoch:  28780 , step:  28780 , train loss:  0.06474297198480902 , test loss:  0.0929441752625378\n",
      "Epoch:  28790 , step:  28790 , train loss:  0.0647338545656106 , test loss:  0.09293862407473823\n",
      "Epoch:  28800 , step:  28800 , train loss:  0.06472474173772215 , test loss:  0.09293307682124294\n",
      "Epoch:  28810 , step:  28810 , train loss:  0.06471563349722978 , test loss:  0.09292753349834\n",
      "Epoch:  28820 , step:  28820 , train loss:  0.06470652984022436 , test loss:  0.0929219941023223\n",
      "Epoch:  28830 , step:  28830 , train loss:  0.06469743076280132 , test loss:  0.0929164586294875\n",
      "Epoch:  28840 , step:  28840 , train loss:  0.06468833626106084 , test loss:  0.0929109270761378\n",
      "Epoch:  28850 , step:  28850 , train loss:  0.06467924633110778 , test loss:  0.09290539943857985\n",
      "Epoch:  28860 , step:  28860 , train loss:  0.0646701609690516 , test loss:  0.0928998757131252\n",
      "Epoch:  28870 , step:  28870 , train loss:  0.06466108017100645 , test loss:  0.09289435589608999\n",
      "Epoch:  28880 , step:  28880 , train loss:  0.06465200393309119 , test loss:  0.09288883998379481\n",
      "Epoch:  28890 , step:  28890 , train loss:  0.06464293225142925 , test loss:  0.09288332797256509\n",
      "Epoch:  28900 , step:  28900 , train loss:  0.06463386512214869 , test loss:  0.09287781985873071\n",
      "Epoch:  28910 , step:  28910 , train loss:  0.06462480254138227 , test loss:  0.0928723156386263\n",
      "Epoch:  28920 , step:  28920 , train loss:  0.06461574450526726 , test loss:  0.09286681530859073\n",
      "Epoch:  28930 , step:  28930 , train loss:  0.06460669100994566 , test loss:  0.0928613188649678\n",
      "Epoch:  28940 , step:  28940 , train loss:  0.064597642051564 , test loss:  0.09285582630410581\n",
      "Epoch:  28950 , step:  28950 , train loss:  0.06458859762627346 , test loss:  0.0928503376223575\n",
      "Epoch:  28960 , step:  28960 , train loss:  0.06457955773022975 , test loss:  0.09284485281608025\n",
      "Epoch:  28970 , step:  28970 , train loss:  0.06457052235959325 , test loss:  0.09283937188163603\n",
      "Epoch:  28980 , step:  28980 , train loss:  0.06456149151052884 , test loss:  0.09283389481539142\n",
      "Epoch:  28990 , step:  28990 , train loss:  0.06455246517920599 , test loss:  0.09282842161371739\n",
      "Epoch:  29000 , step:  29000 , train loss:  0.06454344336179879 , test loss:  0.09282295227298938\n",
      "Epoch:  29010 , step:  29010 , train loss:  0.06453442605448584 , test loss:  0.09281748678958752\n",
      "Epoch:  29020 , step:  29020 , train loss:  0.06452541325345028 , test loss:  0.09281202515989648\n",
      "Epoch:  29030 , step:  29030 , train loss:  0.06451640495487977 , test loss:  0.09280656738030535\n",
      "Epoch:  29040 , step:  29040 , train loss:  0.06450740115496663 , test loss:  0.09280111344720766\n",
      "Epoch:  29050 , step:  29050 , train loss:  0.0644984018499076 , test loss:  0.09279566335700164\n",
      "Epoch:  29060 , step:  29060 , train loss:  0.06448940703590395 , test loss:  0.09279021710608974\n",
      "Epoch:  29070 , step:  29070 , train loss:  0.06448041670916149 , test loss:  0.09278477469087917\n",
      "Epoch:  29080 , step:  29080 , train loss:  0.06447143086589055 , test loss:  0.09277933610778133\n",
      "Epoch:  29090 , step:  29090 , train loss:  0.06446244950230592 , test loss:  0.09277390135321242\n",
      "Epoch:  29100 , step:  29100 , train loss:  0.06445347261462699 , test loss:  0.0927684704235928\n",
      "Epoch:  29110 , step:  29110 , train loss:  0.06444450019907748 , test loss:  0.09276304331534747\n",
      "Epoch:  29120 , step:  29120 , train loss:  0.0644355322518857 , test loss:  0.09275762002490573\n",
      "Epoch:  29130 , step:  29130 , train loss:  0.0644265687692844 , test loss:  0.09275220054870152\n",
      "Epoch:  29140 , step:  29140 , train loss:  0.06441760974751085 , test loss:  0.092746784883173\n",
      "Epoch:  29150 , step:  29150 , train loss:  0.0644086551828067 , test loss:  0.09274137302476286\n",
      "Epoch:  29160 , step:  29160 , train loss:  0.0643997050714181 , test loss:  0.09273596496991827\n",
      "Epoch:  29170 , step:  29170 , train loss:  0.06439075940959563 , test loss:  0.09273056071509073\n",
      "Epoch:  29180 , step:  29180 , train loss:  0.06438181819359434 , test loss:  0.09272516025673627\n",
      "Epoch:  29190 , step:  29190 , train loss:  0.0643728814196737 , test loss:  0.09271976359131495\n",
      "Epoch:  29200 , step:  29200 , train loss:  0.06436394908409758 , test loss:  0.09271437071529166\n",
      "Epoch:  29210 , step:  29210 , train loss:  0.0643550211831343 , test loss:  0.09270898162513554\n",
      "Epoch:  29220 , step:  29220 , train loss:  0.06434609771305662 , test loss:  0.09270359631732006\n",
      "Epoch:  29230 , step:  29230 , train loss:  0.06433717867014163 , test loss:  0.09269821478832284\n",
      "Epoch:  29240 , step:  29240 , train loss:  0.06432826405067091 , test loss:  0.09269283703462634\n",
      "Epoch:  29250 , step:  29250 , train loss:  0.06431935385093035 , test loss:  0.09268746305271706\n",
      "Epoch:  29260 , step:  29260 , train loss:  0.06431044806721027 , test loss:  0.09268209283908609\n",
      "Epoch:  29270 , step:  29270 , train loss:  0.0643015466958054 , test loss:  0.09267672639022845\n",
      "Epoch:  29280 , step:  29280 , train loss:  0.06429264973301477 , test loss:  0.09267136370264399\n",
      "Epoch:  29290 , step:  29290 , train loss:  0.06428375717514183 , test loss:  0.09266600477283649\n",
      "Epoch:  29300 , step:  29300 , train loss:  0.06427486901849441 , test loss:  0.09266064959731413\n",
      "Epoch:  29310 , step:  29310 , train loss:  0.06426598525938462 , test loss:  0.09265529817258968\n",
      "Epoch:  29320 , step:  29320 , train loss:  0.064257105894129 , test loss:  0.09264995049517998\n",
      "Epoch:  29330 , step:  29330 , train loss:  0.06424823091904837 , test loss:  0.09264460656160624\n",
      "Epoch:  29340 , step:  29340 , train loss:  0.06423936033046791 , test loss:  0.09263926636839402\n",
      "Epoch:  29350 , step:  29350 , train loss:  0.06423049412471711 , test loss:  0.09263392991207303\n",
      "Epoch:  29360 , step:  29360 , train loss:  0.06422163229812981 , test loss:  0.09262859718917732\n",
      "Epoch:  29370 , step:  29370 , train loss:  0.06421277484704416 , test loss:  0.09262326819624532\n",
      "Epoch:  29380 , step:  29380 , train loss:  0.06420392176780261 , test loss:  0.09261794292981972\n",
      "Epoch:  29390 , step:  29390 , train loss:  0.06419507305675186 , test loss:  0.09261262138644728\n",
      "Epoch:  29400 , step:  29400 , train loss:  0.06418622871024302 , test loss:  0.09260730356267931\n",
      "Epoch:  29410 , step:  29410 , train loss:  0.06417738872463136 , test loss:  0.09260198945507096\n",
      "Epoch:  29420 , step:  29420 , train loss:  0.06416855309627655 , test loss:  0.09259667906018203\n",
      "Epoch:  29430 , step:  29430 , train loss:  0.06415972182154245 , test loss:  0.09259137237457644\n",
      "Epoch:  29440 , step:  29440 , train loss:  0.06415089489679719 , test loss:  0.09258606939482228\n",
      "Epoch:  29450 , step:  29450 , train loss:  0.0641420723184133 , test loss:  0.09258077011749193\n",
      "Epoch:  29460 , step:  29460 , train loss:  0.06413325408276736 , test loss:  0.09257547453916187\n",
      "Epoch:  29470 , step:  29470 , train loss:  0.06412444018624032 , test loss:  0.09257018265641306\n",
      "Epoch:  29480 , step:  29480 , train loss:  0.06411563062521737 , test loss:  0.09256489446583036\n",
      "Epoch:  29490 , step:  29490 , train loss:  0.0641068253960879 , test loss:  0.0925596099640029\n",
      "Epoch:  29500 , step:  29500 , train loss:  0.06409802449524558 , test loss:  0.09255432914752414\n",
      "Epoch:  29510 , step:  29510 , train loss:  0.06408922791908826 , test loss:  0.0925490520129916\n",
      "Epoch:  29520 , step:  29520 , train loss:  0.064080435664018 , test loss:  0.09254377855700706\n",
      "Epoch:  29530 , step:  29530 , train loss:  0.06407164772644112 , test loss:  0.09253850877617649\n",
      "Epoch:  29540 , step:  29540 , train loss:  0.06406286410276814 , test loss:  0.09253324266710983\n",
      "Epoch:  29550 , step:  29550 , train loss:  0.06405408478941375 , test loss:  0.0925279802264215\n",
      "Epoch:  29560 , step:  29560 , train loss:  0.06404530978279677 , test loss:  0.09252272145072976\n",
      "Epoch:  29570 , step:  29570 , train loss:  0.0640365390793404 , test loss:  0.09251746633665738\n",
      "Epoch:  29580 , step:  29580 , train loss:  0.06402777267547179 , test loss:  0.09251221488083071\n",
      "Epoch:  29590 , step:  29590 , train loss:  0.06401901056762244 , test loss:  0.09250696707988096\n",
      "Epoch:  29600 , step:  29600 , train loss:  0.06401025275222795 , test loss:  0.09250172293044281\n",
      "Epoch:  29610 , step:  29610 , train loss:  0.06400149922572802 , test loss:  0.0924964824291554\n",
      "Epoch:  29620 , step:  29620 , train loss:  0.06399274998456662 , test loss:  0.09249124557266211\n",
      "Epoch:  29630 , step:  29630 , train loss:  0.0639840050251918 , test loss:  0.09248601235761011\n",
      "Epoch:  29640 , step:  29640 , train loss:  0.0639752643440558 , test loss:  0.09248078278065076\n",
      "Epoch:  29650 , step:  29650 , train loss:  0.06396652793761491 , test loss:  0.09247555683843978\n",
      "Epoch:  29660 , step:  29660 , train loss:  0.06395779580232965 , test loss:  0.09247033452763652\n",
      "Epoch:  29670 , step:  29670 , train loss:  0.06394906793466461 , test loss:  0.09246511584490481\n",
      "Epoch:  29680 , step:  29680 , train loss:  0.06394034433108849 , test loss:  0.09245990078691252\n",
      "Epoch:  29690 , step:  29690 , train loss:  0.06393162498807409 , test loss:  0.09245468935033126\n",
      "Epoch:  29700 , step:  29700 , train loss:  0.06392290990209842 , test loss:  0.09244948153183716\n",
      "Epoch:  29710 , step:  29710 , train loss:  0.06391419906964248 , test loss:  0.09244427732811004\n",
      "Epoch:  29720 , step:  29720 , train loss:  0.06390549248719139 , test loss:  0.09243907673583403\n",
      "Epoch:  29730 , step:  29730 , train loss:  0.06389679015123438 , test loss:  0.0924338797516971\n",
      "Epoch:  29740 , step:  29740 , train loss:  0.0638880920582647 , test loss:  0.09242868637239139\n",
      "Epoch:  29750 , step:  29750 , train loss:  0.06387939820477977 , test loss:  0.09242349659461313\n",
      "Epoch:  29760 , step:  29760 , train loss:  0.06387070858728103 , test loss:  0.09241831041506252\n",
      "Epoch:  29770 , step:  29770 , train loss:  0.06386202320227397 , test loss:  0.09241312783044354\n",
      "Epoch:  29780 , step:  29780 , train loss:  0.06385334204626812 , test loss:  0.09240794883746475\n",
      "Epoch:  29790 , step:  29790 , train loss:  0.0638446651157771 , test loss:  0.09240277343283819\n",
      "Epoch:  29800 , step:  29800 , train loss:  0.06383599240731862 , test loss:  0.09239760161328009\n",
      "Epoch:  29810 , step:  29810 , train loss:  0.06382732391741432 , test loss:  0.09239243337551073\n",
      "Epoch:  29820 , step:  29820 , train loss:  0.06381865964258995 , test loss:  0.09238726871625438\n",
      "Epoch:  29830 , step:  29830 , train loss:  0.06380999957937525 , test loss:  0.09238210763223913\n",
      "Epoch:  29840 , step:  29840 , train loss:  0.063801343724304 , test loss:  0.09237695012019723\n",
      "Epoch:  29850 , step:  29850 , train loss:  0.06379269207391398 , test loss:  0.09237179617686496\n",
      "Epoch:  29860 , step:  29860 , train loss:  0.063784044624747 , test loss:  0.09236664579898235\n",
      "Epoch:  29870 , step:  29870 , train loss:  0.06377540137334883 , test loss:  0.0923614989832936\n",
      "Epoch:  29880 , step:  29880 , train loss:  0.06376676231626931 , test loss:  0.0923563557265467\n",
      "Epoch:  29890 , step:  29890 , train loss:  0.0637581274500622 , test loss:  0.09235121602549362\n",
      "Epoch:  29900 , step:  29900 , train loss:  0.06374949677128526 , test loss:  0.09234607987689038\n",
      "Epoch:  29910 , step:  29910 , train loss:  0.06374087027650026 , test loss:  0.09234094727749696\n",
      "Epoch:  29920 , step:  29920 , train loss:  0.06373224796227292 , test loss:  0.09233581822407705\n",
      "Epoch:  29930 , step:  29930 , train loss:  0.06372362982517296 , test loss:  0.09233069271339839\n",
      "Epoch:  29940 , step:  29940 , train loss:  0.06371501586177399 , test loss:  0.09232557074223283\n",
      "Epoch:  29950 , step:  29950 , train loss:  0.06370640606865362 , test loss:  0.09232045230735582\n",
      "Epoch:  29960 , step:  29960 , train loss:  0.06369780044239343 , test loss:  0.09231533740554683\n",
      "Epoch:  29970 , step:  29970 , train loss:  0.0636891989795789 , test loss:  0.0923102260335894\n",
      "Epoch:  29980 , step:  29980 , train loss:  0.06368060167679948 , test loss:  0.09230511818827081\n",
      "Epoch:  29990 , step:  29990 , train loss:  0.06367200853064858 , test loss:  0.09230001386638201\n",
      "Epoch:  30000 , step:  30000 , train loss:  0.0636634195377234 , test loss:  0.09229491306471838\n",
      "Epoch:  30010 , step:  30010 , train loss:  0.06365483469462524 , test loss:  0.09228981578007878\n",
      "Epoch:  30020 , step:  30020 , train loss:  0.06364625399795922 , test loss:  0.09228472200926599\n",
      "Epoch:  30030 , step:  30030 , train loss:  0.06363767744433436 , test loss:  0.0922796317490868\n",
      "Epoch:  30040 , step:  30040 , train loss:  0.06362910503036362 , test loss:  0.09227454499635171\n",
      "Epoch:  30050 , step:  30050 , train loss:  0.06362053675266384 , test loss:  0.0922694617478751\n",
      "Epoch:  30060 , step:  30060 , train loss:  0.06361197260785573 , test loss:  0.09226438200047528\n",
      "Epoch:  30070 , step:  30070 , train loss:  0.06360341259256391 , test loss:  0.09225930575097431\n",
      "Epoch:  30080 , step:  30080 , train loss:  0.0635948567034169 , test loss:  0.09225423299619823\n",
      "Epoch:  30090 , step:  30090 , train loss:  0.06358630493704708 , test loss:  0.09224916373297677\n",
      "Epoch:  30100 , step:  30100 , train loss:  0.06357775729009067 , test loss:  0.09224409795814349\n",
      "Epoch:  30110 , step:  30110 , train loss:  0.06356921375918778 , test loss:  0.09223903566853596\n",
      "Epoch:  30120 , step:  30120 , train loss:  0.06356067434098238 , test loss:  0.09223397686099517\n",
      "Epoch:  30130 , step:  30130 , train loss:  0.06355213903212224 , test loss:  0.09222892153236655\n",
      "Epoch:  30140 , step:  30140 , train loss:  0.0635436078292591 , test loss:  0.09222386967949862\n",
      "Epoch:  30150 , step:  30150 , train loss:  0.06353508072904836 , test loss:  0.09221882129924414\n",
      "Epoch:  30160 , step:  30160 , train loss:  0.06352655772814943 , test loss:  0.09221377638845951\n",
      "Epoch:  30170 , step:  30170 , train loss:  0.06351803882322542 , test loss:  0.0922087349440052\n",
      "Epoch:  30180 , step:  30180 , train loss:  0.06350952401094334 , test loss:  0.09220369696274497\n",
      "Epoch:  30190 , step:  30190 , train loss:  0.06350101328797399 , test loss:  0.0921986624415467\n",
      "Epoch:  30200 , step:  30200 , train loss:  0.06349250665099197 , test loss:  0.09219363137728195\n",
      "Epoch:  30210 , step:  30210 , train loss:  0.06348400409667573 , test loss:  0.09218860376682608\n",
      "Epoch:  30220 , step:  30220 , train loss:  0.06347550562170745 , test loss:  0.09218357960705817\n",
      "Epoch:  30230 , step:  30230 , train loss:  0.06346701122277318 , test loss:  0.09217855889486097\n",
      "Epoch:  30240 , step:  30240 , train loss:  0.06345852089656273 , test loss:  0.09217354162712114\n",
      "Epoch:  30250 , step:  30250 , train loss:  0.06345003463976964 , test loss:  0.09216852780072897\n",
      "Epoch:  30260 , step:  30260 , train loss:  0.06344155244909132 , test loss:  0.0921635174125786\n",
      "Epoch:  30270 , step:  30270 , train loss:  0.0634330743212289 , test loss:  0.09215851045956772\n",
      "Epoch:  30280 , step:  30280 , train loss:  0.06342460025288726 , test loss:  0.09215350693859786\n",
      "Epoch:  30290 , step:  30290 , train loss:  0.06341613024077512 , test loss:  0.09214850684657423\n",
      "Epoch:  30300 , step:  30300 , train loss:  0.06340766428160488 , test loss:  0.092143510180406\n",
      "Epoch:  30310 , step:  30310 , train loss:  0.06339920237209269 , test loss:  0.0921385169370056\n",
      "Epoch:  30320 , step:  30320 , train loss:  0.06339074450895851 , test loss:  0.09213352711328938\n",
      "Epoch:  30330 , step:  30330 , train loss:  0.06338229068892601 , test loss:  0.09212854070617742\n",
      "Epoch:  30340 , step:  30340 , train loss:  0.06337384090872253 , test loss:  0.09212355771259344\n",
      "Epoch:  30350 , step:  30350 , train loss:  0.06336539516507926 , test loss:  0.0921185781294651\n",
      "Epoch:  30360 , step:  30360 , train loss:  0.063356953454731 , test loss:  0.09211360195372333\n",
      "Epoch:  30370 , step:  30370 , train loss:  0.06334851577441632 , test loss:  0.09210862918230285\n",
      "Epoch:  30380 , step:  30380 , train loss:  0.06334008212087751 , test loss:  0.09210365981214225\n",
      "Epoch:  30390 , step:  30390 , train loss:  0.06333165249086055 , test loss:  0.09209869384018356\n",
      "Epoch:  30400 , step:  30400 , train loss:  0.06332322688111512 , test loss:  0.0920937312633726\n",
      "Epoch:  30410 , step:  30410 , train loss:  0.06331480528839463 , test loss:  0.09208877207865884\n",
      "Epoch:  30420 , step:  30420 , train loss:  0.06330638770945612 , test loss:  0.09208381628299517\n",
      "Epoch:  30430 , step:  30430 , train loss:  0.06329797414106036 , test loss:  0.09207886387333858\n",
      "Epoch:  30440 , step:  30440 , train loss:  0.06328956457997179 , test loss:  0.0920739148466493\n",
      "Epoch:  30450 , step:  30450 , train loss:  0.06328115902295854 , test loss:  0.09206896919989122\n",
      "Epoch:  30460 , step:  30460 , train loss:  0.06327275746679237 , test loss:  0.09206402693003209\n",
      "Epoch:  30470 , step:  30470 , train loss:  0.0632643599082487 , test loss:  0.09205908803404307\n",
      "Epoch:  30480 , step:  30480 , train loss:  0.06325596634410671 , test loss:  0.09205415250889917\n",
      "Epoch:  30490 , step:  30490 , train loss:  0.0632475767711491 , test loss:  0.09204922035157856\n",
      "Epoch:  30500 , step:  30500 , train loss:  0.06323919118616228 , test loss:  0.09204429155906348\n",
      "Epoch:  30510 , step:  30510 , train loss:  0.06323080958593633 , test loss:  0.09203936612833963\n",
      "Epoch:  30520 , step:  30520 , train loss:  0.06322243196726489 , test loss:  0.092034444056396\n",
      "Epoch:  30530 , step:  30530 , train loss:  0.06321405832694531 , test loss:  0.09202952534022579\n",
      "Epoch:  30540 , step:  30540 , train loss:  0.06320568866177854 , test loss:  0.09202460997682518\n",
      "Epoch:  30550 , step:  30550 , train loss:  0.06319732296856913 , test loss:  0.09201969796319422\n",
      "Epoch:  30560 , step:  30560 , train loss:  0.06318896124412526 , test loss:  0.09201478929633652\n",
      "Epoch:  30570 , step:  30570 , train loss:  0.06318060348525874 , test loss:  0.0920098839732592\n",
      "Epoch:  30580 , step:  30580 , train loss:  0.06317224968878496 , test loss:  0.09200498199097293\n",
      "Epoch:  30590 , step:  30590 , train loss:  0.0631638998515229 , test loss:  0.09200008334649201\n",
      "Epoch:  30600 , step:  30600 , train loss:  0.06315555397029518 , test loss:  0.09199518803683415\n",
      "Epoch:  30610 , step:  30610 , train loss:  0.06314721204192794 , test loss:  0.09199029605902083\n",
      "Epoch:  30620 , step:  30620 , train loss:  0.06313887406325103 , test loss:  0.0919854074100768\n",
      "Epoch:  30630 , step:  30630 , train loss:  0.06313054003109773 , test loss:  0.09198052208703061\n",
      "Epoch:  30640 , step:  30640 , train loss:  0.06312220994230498 , test loss:  0.0919756400869142\n",
      "Epoch:  30650 , step:  30650 , train loss:  0.0631138837937133 , test loss:  0.09197076140676305\n",
      "Epoch:  30660 , step:  30660 , train loss:  0.06310556158216671 , test loss:  0.09196588604361605\n",
      "Epoch:  30670 , step:  30670 , train loss:  0.06309724330451283 , test loss:  0.09196101399451581\n",
      "Epoch:  30680 , step:  30680 , train loss:  0.06308892895760283 , test loss:  0.09195614525650835\n",
      "Epoch:  30690 , step:  30690 , train loss:  0.06308061853829143 , test loss:  0.09195127982664324\n",
      "Epoch:  30700 , step:  30700 , train loss:  0.06307231204343691 , test loss:  0.09194641770197323\n",
      "Epoch:  30710 , step:  30710 , train loss:  0.06306400946990104 , test loss:  0.09194155887955521\n",
      "Epoch:  30720 , step:  30720 , train loss:  0.06305571081454917 , test loss:  0.09193670335644898\n",
      "Epoch:  30730 , step:  30730 , train loss:  0.06304741607425014 , test loss:  0.09193185112971801\n",
      "Epoch:  30740 , step:  30740 , train loss:  0.06303912524587635 , test loss:  0.09192700219642931\n",
      "Epoch:  30750 , step:  30750 , train loss:  0.06303083832630366 , test loss:  0.09192215655365336\n",
      "Epoch:  30760 , step:  30760 , train loss:  0.06302255531241156 , test loss:  0.09191731419846405\n",
      "Epoch:  30770 , step:  30770 , train loss:  0.0630142762010829 , test loss:  0.09191247512793863\n",
      "Epoch:  30780 , step:  30780 , train loss:  0.06300600098920414 , test loss:  0.09190763933915799\n",
      "Epoch:  30790 , step:  30790 , train loss:  0.06299772967366518 , test loss:  0.09190280682920647\n",
      "Epoch:  30800 , step:  30800 , train loss:  0.06298946225135944 , test loss:  0.09189797759517171\n",
      "Epoch:  30810 , step:  30810 , train loss:  0.06298119871918383 , test loss:  0.09189315163414491\n",
      "Epoch:  30820 , step:  30820 , train loss:  0.06297293907403875 , test loss:  0.09188832894322074\n",
      "Epoch:  30830 , step:  30830 , train loss:  0.062964683312828 , test loss:  0.09188350951949711\n",
      "Epoch:  30840 , step:  30840 , train loss:  0.06295643143245896 , test loss:  0.09187869336007547\n",
      "Epoch:  30850 , step:  30850 , train loss:  0.06294818342984242 , test loss:  0.09187388046206081\n",
      "Epoch:  30860 , step:  30860 , train loss:  0.06293993930189265 , test loss:  0.09186907082256122\n",
      "Epoch:  30870 , step:  30870 , train loss:  0.06293169904552735 , test loss:  0.09186426443868874\n",
      "Epoch:  30880 , step:  30880 , train loss:  0.06292346265766768 , test loss:  0.09185946130755816\n",
      "Epoch:  30890 , step:  30890 , train loss:  0.06291523013523832 , test loss:  0.09185466142628815\n",
      "Epoch:  30900 , step:  30900 , train loss:  0.06290700147516727 , test loss:  0.09184986479200064\n",
      "Epoch:  30910 , step:  30910 , train loss:  0.06289877667438604 , test loss:  0.09184507140182087\n",
      "Epoch:  30920 , step:  30920 , train loss:  0.06289055572982959 , test loss:  0.09184028125287752\n",
      "Epoch:  30930 , step:  30930 , train loss:  0.06288233863843626 , test loss:  0.09183549434230272\n",
      "Epoch:  30940 , step:  30940 , train loss:  0.06287412539714785 , test loss:  0.09183071066723188\n",
      "Epoch:  30950 , step:  30950 , train loss:  0.06286591600290953 , test loss:  0.09182593022480386\n",
      "Epoch:  30960 , step:  30960 , train loss:  0.06285771045266991 , test loss:  0.09182115301216072\n",
      "Epoch:  30970 , step:  30970 , train loss:  0.06284950874338106 , test loss:  0.09181637902644817\n",
      "Epoch:  30980 , step:  30980 , train loss:  0.06284131087199837 , test loss:  0.09181160826481495\n",
      "Epoch:  30990 , step:  30990 , train loss:  0.06283311683548065 , test loss:  0.09180684072441332\n",
      "Epoch:  31000 , step:  31000 , train loss:  0.06282492663079013 , test loss:  0.09180207640239912\n",
      "Epoch:  31010 , step:  31010 , train loss:  0.06281674025489241 , test loss:  0.09179731529593108\n",
      "Epoch:  31020 , step:  31020 , train loss:  0.06280855770475649 , test loss:  0.09179255740217146\n",
      "Epoch:  31030 , step:  31030 , train loss:  0.06280037897735474 , test loss:  0.09178780271828593\n",
      "Epoch:  31040 , step:  31040 , train loss:  0.06279220406966286 , test loss:  0.09178305124144345\n",
      "Epoch:  31050 , step:  31050 , train loss:  0.06278403297866 , test loss:  0.09177830296881626\n",
      "Epoch:  31060 , step:  31060 , train loss:  0.06277586570132862 , test loss:  0.09177355789757986\n",
      "Epoch:  31070 , step:  31070 , train loss:  0.06276770223465455 , test loss:  0.09176881602491335\n",
      "Epoch:  31080 , step:  31080 , train loss:  0.06275954257562696 , test loss:  0.09176407734799874\n",
      "Epoch:  31090 , step:  31090 , train loss:  0.0627513867212384 , test loss:  0.09175934186402147\n",
      "Epoch:  31100 , step:  31100 , train loss:  0.06274323466848478 , test loss:  0.09175460957017048\n",
      "Epoch:  31110 , step:  31110 , train loss:  0.06273508641436529 , test loss:  0.09174988046363783\n",
      "Epoch:  31120 , step:  31120 , train loss:  0.0627269419558825 , test loss:  0.09174515454161902\n",
      "Epoch:  31130 , step:  31130 , train loss:  0.06271880129004229 , test loss:  0.09174043180131242\n",
      "Epoch:  31140 , step:  31140 , train loss:  0.06271066441385388 , test loss:  0.09173571223992023\n",
      "Epoch:  31150 , step:  31150 , train loss:  0.06270253132432979 , test loss:  0.09173099585464749\n",
      "Epoch:  31160 , step:  31160 , train loss:  0.06269440201848592 , test loss:  0.09172628264270277\n",
      "Epoch:  31170 , step:  31170 , train loss:  0.0626862764933414 , test loss:  0.09172157260129789\n",
      "Epoch:  31180 , step:  31180 , train loss:  0.06267815474591866 , test loss:  0.0917168657276477\n",
      "Epoch:  31190 , step:  31190 , train loss:  0.06267003677324355 , test loss:  0.09171216201897064\n",
      "Epoch:  31200 , step:  31200 , train loss:  0.0626619225723451 , test loss:  0.09170746147248808\n",
      "Epoch:  31210 , step:  31210 , train loss:  0.06265381214025567 , test loss:  0.09170276408542466\n",
      "Epoch:  31220 , step:  31220 , train loss:  0.06264570547401092 , test loss:  0.09169806985500877\n",
      "Epoch:  31230 , step:  31230 , train loss:  0.06263760257064979 , test loss:  0.09169337877847122\n",
      "Epoch:  31240 , step:  31240 , train loss:  0.06262950342721448 , test loss:  0.09168869085304673\n",
      "Epoch:  31250 , step:  31250 , train loss:  0.06262140804075049 , test loss:  0.09168400607597295\n",
      "Epoch:  31260 , step:  31260 , train loss:  0.06261331640830657 , test loss:  0.0916793244444908\n",
      "Epoch:  31270 , step:  31270 , train loss:  0.06260522852693477 , test loss:  0.09167464595584422\n",
      "Epoch:  31280 , step:  31280 , train loss:  0.06259714439369032 , test loss:  0.09166997060728088\n",
      "Epoch:  31290 , step:  31290 , train loss:  0.06258906400563183 , test loss:  0.09166529839605111\n",
      "Epoch:  31300 , step:  31300 , train loss:  0.062580987359821 , test loss:  0.09166062931940858\n",
      "Epoch:  31310 , step:  31310 , train loss:  0.06257291445332293 , test loss:  0.09165596337461056\n",
      "Epoch:  31320 , step:  31320 , train loss:  0.06256484528320588 , test loss:  0.09165130055891686\n",
      "Epoch:  31330 , step:  31330 , train loss:  0.06255677984654137 , test loss:  0.09164664086959104\n",
      "Epoch:  31340 , step:  31340 , train loss:  0.06254871814040415 , test loss:  0.09164198430389936\n",
      "Epoch:  31350 , step:  31350 , train loss:  0.06254066016187217 , test loss:  0.09163733085911177\n",
      "Epoch:  31360 , step:  31360 , train loss:  0.06253260590802662 , test loss:  0.09163268053250119\n",
      "Epoch:  31370 , step:  31370 , train loss:  0.06252455537595199 , test loss:  0.09162803332134338\n",
      "Epoch:  31380 , step:  31380 , train loss:  0.06251650856273581 , test loss:  0.09162338922291775\n",
      "Epoch:  31390 , step:  31390 , train loss:  0.062508465465469 , test loss:  0.09161874823450664\n",
      "Epoch:  31400 , step:  31400 , train loss:  0.06250042608124558 , test loss:  0.09161411035339545\n",
      "Epoch:  31410 , step:  31410 , train loss:  0.062492390407162784 , test loss:  0.09160947557687305\n",
      "Epoch:  31420 , step:  31420 , train loss:  0.06248435844032106 , test loss:  0.09160484390223113\n",
      "Epoch:  31430 , step:  31430 , train loss:  0.062476330177824065 , test loss:  0.09160021532676481\n",
      "Epoch:  31440 , step:  31440 , train loss:  0.062468305616778574 , test loss:  0.09159558984777197\n",
      "Epoch:  31450 , step:  31450 , train loss:  0.062460284754294645 , test loss:  0.091590967462554\n",
      "Epoch:  31460 , step:  31460 , train loss:  0.0624522675874854 , test loss:  0.0915863481684152\n",
      "Epoch:  31470 , step:  31470 , train loss:  0.06244425411346724 , test loss:  0.09158173196266313\n",
      "Epoch:  31480 , step:  31480 , train loss:  0.062436244329359666 , test loss:  0.0915771188426084\n",
      "Epoch:  31490 , step:  31490 , train loss:  0.062428238232285396 , test loss:  0.0915725088055647\n",
      "Epoch:  31500 , step:  31500 , train loss:  0.06242023581937024 , test loss:  0.09156790184884887\n",
      "Epoch:  31510 , step:  31510 , train loss:  0.062412237087743225 , test loss:  0.0915632979697808\n",
      "Epoch:  31520 , step:  31520 , train loss:  0.0624042420345365 , test loss:  0.09155869716568378\n",
      "Epoch:  31530 , step:  31530 , train loss:  0.06239625065688536 , test loss:  0.09155409943388368\n",
      "Epoch:  31540 , step:  31540 , train loss:  0.062388262951928265 , test loss:  0.09154950477170998\n",
      "Epoch:  31550 , step:  31550 , train loss:  0.06238027891680678 , test loss:  0.09154491317649473\n",
      "Epoch:  31560 , step:  31560 , train loss:  0.062372298548665626 , test loss:  0.09154032464557349\n",
      "Epoch:  31570 , step:  31570 , train loss:  0.06236432184465266 , test loss:  0.09153573917628485\n",
      "Epoch:  31580 , step:  31580 , train loss:  0.06235634880191883 , test loss:  0.09153115676597022\n",
      "Epoch:  31590 , step:  31590 , train loss:  0.06234837941761822 , test loss:  0.09152657741197436\n",
      "Epoch:  31600 , step:  31600 , train loss:  0.062340413688908074 , test loss:  0.09152200111164487\n",
      "Epoch:  31610 , step:  31610 , train loss:  0.06233245161294868 , test loss:  0.09151742786233262\n",
      "Epoch:  31620 , step:  31620 , train loss:  0.06232449318690348 , test loss:  0.09151285766139143\n",
      "Epoch:  31630 , step:  31630 , train loss:  0.062316538407938944 , test loss:  0.09150829050617812\n",
      "Epoch:  31640 , step:  31640 , train loss:  0.06230858727322479 , test loss:  0.09150372639405262\n",
      "Epoch:  31650 , step:  31650 , train loss:  0.06230063977993363 , test loss:  0.09149916532237785\n",
      "Epoch:  31660 , step:  31660 , train loss:  0.062292695925241344 , test loss:  0.09149460728851998\n",
      "Epoch:  31670 , step:  31670 , train loss:  0.06228475570632684 , test loss:  0.09149005228984794\n",
      "Epoch:  31680 , step:  31680 , train loss:  0.062276819120372015 , test loss:  0.09148550032373377\n",
      "Epoch:  31690 , step:  31690 , train loss:  0.06226888616456198 , test loss:  0.09148095138755281\n",
      "Epoch:  31700 , step:  31700 , train loss:  0.06226095683608483 , test loss:  0.09147640547868284\n",
      "Epoch:  31710 , step:  31710 , train loss:  0.06225303113213177 , test loss:  0.09147186259450524\n",
      "Epoch:  31720 , step:  31720 , train loss:  0.06224510904989706 , test loss:  0.09146732273240415\n",
      "Epoch:  31730 , step:  31730 , train loss:  0.062237190586578 , test loss:  0.09146278588976671\n",
      "Epoch:  31740 , step:  31740 , train loss:  0.06222927573937498 , test loss:  0.09145825206398316\n",
      "Epoch:  31750 , step:  31750 , train loss:  0.06222136450549137 , test loss:  0.09145372125244655\n",
      "Epoch:  31760 , step:  31760 , train loss:  0.0622134568821337 , test loss:  0.09144919345255313\n",
      "Epoch:  31770 , step:  31770 , train loss:  0.06220555286651142 , test loss:  0.09144466866170202\n",
      "Epoch:  31780 , step:  31780 , train loss:  0.06219765245583715 , test loss:  0.0914401468772955\n",
      "Epoch:  31790 , step:  31790 , train loss:  0.06218975564732636 , test loss:  0.09143562809673852\n",
      "Epoch:  31800 , step:  31800 , train loss:  0.062181862438197774 , test loss:  0.09143111231743932\n",
      "Epoch:  31810 , step:  31810 , train loss:  0.062173972825672934 , test loss:  0.09142659953680903\n",
      "Epoch:  31820 , step:  31820 , train loss:  0.06216608680697657 , test loss:  0.09142208975226158\n",
      "Epoch:  31830 , step:  31830 , train loss:  0.06215820437933633 , test loss:  0.09141758296121405\n",
      "Epoch:  31840 , step:  31840 , train loss:  0.06215032553998287 , test loss:  0.09141307916108644\n",
      "Epoch:  31850 , step:  31850 , train loss:  0.062142450286149904 , test loss:  0.09140857834930184\n",
      "Epoch:  31860 , step:  31860 , train loss:  0.06213457861507415 , test loss:  0.09140408052328595\n",
      "Epoch:  31870 , step:  31870 , train loss:  0.06212671052399528 , test loss:  0.09139958568046762\n",
      "Epoch:  31880 , step:  31880 , train loss:  0.06211884601015598 , test loss:  0.09139509381827865\n",
      "Epoch:  31890 , step:  31890 , train loss:  0.06211098507080196 , test loss:  0.09139060493415402\n",
      "Epoch:  31900 , step:  31900 , train loss:  0.06210312770318188 , test loss:  0.09138611902553122\n",
      "Epoch:  31910 , step:  31910 , train loss:  0.06209527390454738 , test loss:  0.09138163608985077\n",
      "Epoch:  31920 , step:  31920 , train loss:  0.062087423672153134 , test loss:  0.0913771561245564\n",
      "Epoch:  31930 , step:  31930 , train loss:  0.062079577003256725 , test loss:  0.0913726791270944\n",
      "Epoch:  31940 , step:  31940 , train loss:  0.06207173389511872 , test loss:  0.0913682050949143\n",
      "Epoch:  31950 , step:  31950 , train loss:  0.0620638943450027 , test loss:  0.09136373402546843\n",
      "Epoch:  31960 , step:  31960 , train loss:  0.06205605835017516 , test loss:  0.09135926591621173\n",
      "Epoch:  31970 , step:  31970 , train loss:  0.062048225907905576 , test loss:  0.0913548007646027\n",
      "Epoch:  31980 , step:  31980 , train loss:  0.062040397015466354 , test loss:  0.09135033856810201\n",
      "Epoch:  31990 , step:  31990 , train loss:  0.06203257167013286 , test loss:  0.09134587932417375\n",
      "Epoch:  32000 , step:  32000 , train loss:  0.062024749869183445 , test loss:  0.09134142303028484\n",
      "Epoch:  32010 , step:  32010 , train loss:  0.062016931609899344 , test loss:  0.0913369696839048\n",
      "Epoch:  32020 , step:  32020 , train loss:  0.062009116889564765 , test loss:  0.0913325192825062\n",
      "Epoch:  32030 , step:  32030 , train loss:  0.06200130570546683 , test loss:  0.09132807182356459\n",
      "Epoch:  32040 , step:  32040 , train loss:  0.06199349805489561 , test loss:  0.09132362730455847\n",
      "Epoch:  32050 , step:  32050 , train loss:  0.061985693935144116 , test loss:  0.09131918572296892\n",
      "Epoch:  32060 , step:  32060 , train loss:  0.061977893343508214 , test loss:  0.09131474707628011\n",
      "Epoch:  32070 , step:  32070 , train loss:  0.06197009627728676 , test loss:  0.0913103113619789\n",
      "Epoch:  32080 , step:  32080 , train loss:  0.06196230273378147 , test loss:  0.09130587857755518\n",
      "Epoch:  32090 , step:  32090 , train loss:  0.06195451271029702 , test loss:  0.09130144872050165\n",
      "Epoch:  32100 , step:  32100 , train loss:  0.061946726204140996 , test loss:  0.09129702178831405\n",
      "Epoch:  32110 , step:  32110 , train loss:  0.06193894321262377 , test loss:  0.09129259777849032\n",
      "Epoch:  32120 , step:  32120 , train loss:  0.061931163733058715 , test loss:  0.09128817668853209\n",
      "Epoch:  32130 , step:  32130 , train loss:  0.061923387762762155 , test loss:  0.09128375851594325\n",
      "Epoch:  32140 , step:  32140 , train loss:  0.06191561529905316 , test loss:  0.09127934325823087\n",
      "Epoch:  32150 , step:  32150 , train loss:  0.06190784633925377 , test loss:  0.09127493091290455\n",
      "Epoch:  32160 , step:  32160 , train loss:  0.061900080880688886 , test loss:  0.09127052147747688\n",
      "Epoch:  32170 , step:  32170 , train loss:  0.06189231892068631 , test loss:  0.09126611494946332\n",
      "Epoch:  32180 , step:  32180 , train loss:  0.06188456045657665 , test loss:  0.09126171132638213\n",
      "Epoch:  32190 , step:  32190 , train loss:  0.06187680548569346 , test loss:  0.09125731060575443\n",
      "Epoch:  32200 , step:  32200 , train loss:  0.0618690540053731 , test loss:  0.0912529127851039\n",
      "Epoch:  32210 , step:  32210 , train loss:  0.06186130601295489 , test loss:  0.0912485178619574\n",
      "Epoch:  32220 , step:  32220 , train loss:  0.06185356150578088 , test loss:  0.09124412583384424\n",
      "Epoch:  32230 , step:  32230 , train loss:  0.06184582048119605 , test loss:  0.09123973669829682\n",
      "Epoch:  32240 , step:  32240 , train loss:  0.061838082936548196 , test loss:  0.0912353504528501\n",
      "Epoch:  32250 , step:  32250 , train loss:  0.061830348869188 , test loss:  0.09123096709504215\n",
      "Epoch:  32260 , step:  32260 , train loss:  0.061822618276468934 , test loss:  0.09122658662241351\n",
      "Epoch:  32270 , step:  32270 , train loss:  0.06181489115574738 , test loss:  0.09122220903250762\n",
      "Epoch:  32280 , step:  32280 , train loss:  0.061807167504382456 , test loss:  0.09121783432287077\n",
      "Epoch:  32290 , step:  32290 , train loss:  0.06179944731973618 , test loss:  0.0912134624910518\n",
      "Epoch:  32300 , step:  32300 , train loss:  0.061791730599173395 , test loss:  0.09120909353460277\n",
      "Epoch:  32310 , step:  32310 , train loss:  0.06178401734006177 , test loss:  0.09120472745107812\n",
      "Epoch:  32320 , step:  32320 , train loss:  0.0617763075397717 , test loss:  0.09120036423803511\n",
      "Epoch:  32330 , step:  32330 , train loss:  0.06176860119567654 , test loss:  0.0911960038930338\n",
      "Epoch:  32340 , step:  32340 , train loss:  0.06176089830515236 , test loss:  0.09119164641363722\n",
      "Epoch:  32350 , step:  32350 , train loss:  0.061753198865578014 , test loss:  0.0911872917974108\n",
      "Epoch:  32360 , step:  32360 , train loss:  0.06174550287433527 , test loss:  0.09118294004192296\n",
      "Epoch:  32370 , step:  32370 , train loss:  0.06173781032880862 , test loss:  0.09117859114474475\n",
      "Epoch:  32380 , step:  32380 , train loss:  0.06173012122638532 , test loss:  0.09117424510345012\n",
      "Epoch:  32390 , step:  32390 , train loss:  0.06172243556445551 , test loss:  0.09116990191561551\n",
      "Epoch:  32400 , step:  32400 , train loss:  0.06171475334041203 , test loss:  0.09116556157882037\n",
      "Epoch:  32410 , step:  32410 , train loss:  0.06170707455165055 , test loss:  0.0911612240906466\n",
      "Epoch:  32420 , step:  32420 , train loss:  0.06169939919556951 , test loss:  0.09115688944867908\n",
      "Epoch:  32430 , step:  32430 , train loss:  0.06169172726957012 , test loss:  0.09115255765050537\n",
      "Epoch:  32440 , step:  32440 , train loss:  0.06168405877105638 , test loss:  0.09114822869371539\n",
      "Epoch:  32450 , step:  32450 , train loss:  0.06167639369743502 , test loss:  0.0911439025759023\n",
      "Epoch:  32460 , step:  32460 , train loss:  0.0616687320461156 , test loss:  0.09113957929466192\n",
      "Epoch:  32470 , step:  32470 , train loss:  0.061661073814510366 , test loss:  0.09113525884759224\n",
      "Epoch:  32480 , step:  32480 , train loss:  0.06165341900003438 , test loss:  0.09113094123229461\n",
      "Epoch:  32490 , step:  32490 , train loss:  0.061645767600105414 , test loss:  0.09112662644637266\n",
      "Epoch:  32500 , step:  32500 , train loss:  0.061638119612144035 , test loss:  0.09112231448743281\n",
      "Epoch:  32510 , step:  32510 , train loss:  0.06163047503357351 , test loss:  0.09111800535308431\n",
      "Epoch:  32520 , step:  32520 , train loss:  0.06162283386181987 , test loss:  0.09111369904093894\n",
      "Epoch:  32530 , step:  32530 , train loss:  0.06161519609431191 , test loss:  0.09110939554861133\n",
      "Epoch:  32540 , step:  32540 , train loss:  0.06160756172848109 , test loss:  0.09110509487371855\n",
      "Epoch:  32550 , step:  32550 , train loss:  0.061599930761761665 , test loss:  0.09110079701388049\n",
      "Epoch:  32560 , step:  32560 , train loss:  0.06159230319159058 , test loss:  0.09109650196671976\n",
      "Epoch:  32570 , step:  32570 , train loss:  0.06158467901540756 , test loss:  0.09109220972986172\n",
      "Epoch:  32580 , step:  32580 , train loss:  0.061577058230654944 , test loss:  0.09108792030093399\n",
      "Epoch:  32590 , step:  32590 , train loss:  0.0615694408347779 , test loss:  0.09108363367756743\n",
      "Epoch:  32600 , step:  32600 , train loss:  0.06156182682522425 , test loss:  0.09107934985739506\n",
      "Epoch:  32610 , step:  32610 , train loss:  0.061554216199444525 , test loss:  0.09107506883805282\n",
      "Epoch:  32620 , step:  32620 , train loss:  0.06154660895489195 , test loss:  0.09107079061717925\n",
      "Epoch:  32630 , step:  32630 , train loss:  0.06153900508902252 , test loss:  0.09106651519241558\n",
      "Epoch:  32640 , step:  32640 , train loss:  0.06153140459929481 , test loss:  0.09106224256140556\n",
      "Epoch:  32650 , step:  32650 , train loss:  0.06152380748317021 , test loss:  0.09105797272179562\n",
      "Epoch:  32660 , step:  32660 , train loss:  0.06151621373811274 , test loss:  0.09105370567123507\n",
      "Epoch:  32670 , step:  32670 , train loss:  0.06150862336158909 , test loss:  0.0910494414073753\n",
      "Epoch:  32680 , step:  32680 , train loss:  0.061501036351068655 , test loss:  0.09104517992787087\n",
      "Epoch:  32690 , step:  32690 , train loss:  0.06149345270402354 , test loss:  0.0910409212303789\n",
      "Epoch:  32700 , step:  32700 , train loss:  0.06148587241792846 , test loss:  0.09103666531255877\n",
      "Epoch:  32710 , step:  32710 , train loss:  0.06147829549026084 , test loss:  0.09103241217207285\n",
      "Epoch:  32720 , step:  32720 , train loss:  0.061470721918500816 , test loss:  0.09102816180658595\n",
      "Epoch:  32730 , step:  32730 , train loss:  0.06146315170013108 , test loss:  0.09102391421376552\n",
      "Epoch:  32740 , step:  32740 , train loss:  0.06145558483263707 , test loss:  0.0910196693912817\n",
      "Epoch:  32750 , step:  32750 , train loss:  0.06144802131350687 , test loss:  0.09101542733680693\n",
      "Epoch:  32760 , step:  32760 , train loss:  0.06144046114023117 , test loss:  0.09101118804801674\n",
      "Epoch:  32770 , step:  32770 , train loss:  0.06143290431030342 , test loss:  0.0910069515225889\n",
      "Epoch:  32780 , step:  32780 , train loss:  0.06142535082121955 , test loss:  0.09100271775820395\n",
      "Epoch:  32790 , step:  32790 , train loss:  0.06141780067047829 , test loss:  0.09099848675254475\n",
      "Epoch:  32800 , step:  32800 , train loss:  0.0614102538555809 , test loss:  0.09099425850329722\n",
      "Epoch:  32810 , step:  32810 , train loss:  0.06140271037403138 , test loss:  0.09099003300814928\n",
      "Epoch:  32820 , step:  32820 , train loss:  0.061395170223336244 , test loss:  0.090985810264792\n",
      "Epoch:  32830 , step:  32830 , train loss:  0.06138763340100472 , test loss:  0.09098159027091861\n",
      "Epoch:  32840 , step:  32840 , train loss:  0.06138009990454861 , test loss:  0.090977373024225\n",
      "Epoch:  32850 , step:  32850 , train loss:  0.0613725697314824 , test loss:  0.09097315852240973\n",
      "Epoch:  32860 , step:  32860 , train loss:  0.061365042879323116 , test loss:  0.09096894676317402\n",
      "Epoch:  32870 , step:  32870 , train loss:  0.06135751934559046 , test loss:  0.09096473774422136\n",
      "Epoch:  32880 , step:  32880 , train loss:  0.061349999127806705 , test loss:  0.09096053146325789\n",
      "Epoch:  32890 , step:  32890 , train loss:  0.06134248222349676 , test loss:  0.0909563279179925\n",
      "Epoch:  32900 , step:  32900 , train loss:  0.06133496863018813 , test loss:  0.09095212710613652\n",
      "Epoch:  32910 , step:  32910 , train loss:  0.061327458345410896 , test loss:  0.09094792902540376\n",
      "Epoch:  32920 , step:  32920 , train loss:  0.06131995136669775 , test loss:  0.09094373367351041\n",
      "Epoch:  32930 , step:  32930 , train loss:  0.06131244769158404 , test loss:  0.09093954104817559\n",
      "Epoch:  32940 , step:  32940 , train loss:  0.061304947317607585 , test loss:  0.09093535114712087\n",
      "Epoch:  32950 , step:  32950 , train loss:  0.06129745024230887 , test loss:  0.09093116396807006\n",
      "Epoch:  32960 , step:  32960 , train loss:  0.06128995646323095 , test loss:  0.09092697950874973\n",
      "Epoch:  32970 , step:  32970 , train loss:  0.06128246597791944 , test loss:  0.0909227977668889\n",
      "Epoch:  32980 , step:  32980 , train loss:  0.06127497878392255 , test loss:  0.09091861874021934\n",
      "Epoch:  32990 , step:  32990 , train loss:  0.061267494878791094 , test loss:  0.09091444242647502\n",
      "Epoch:  33000 , step:  33000 , train loss:  0.061260014260078356 , test loss:  0.09091026882339252\n",
      "Epoch:  33010 , step:  33010 , train loss:  0.06125253692534028 , test loss:  0.09090609792871097\n",
      "Epoch:  33020 , step:  33020 , train loss:  0.06124506287213531 , test loss:  0.09090192974017199\n",
      "Epoch:  33030 , step:  33030 , train loss:  0.06123759209802452 , test loss:  0.09089776425551975\n",
      "Epoch:  33040 , step:  33040 , train loss:  0.06123012460057146 , test loss:  0.0908936014725008\n",
      "Epoch:  33050 , step:  33050 , train loss:  0.06122266037734227 , test loss:  0.09088944138886451\n",
      "Epoch:  33060 , step:  33060 , train loss:  0.06121519942590565 , test loss:  0.09088528400236227\n",
      "Epoch:  33070 , step:  33070 , train loss:  0.06120774174383282 , test loss:  0.09088112931074821\n",
      "Epoch:  33080 , step:  33080 , train loss:  0.06120028732869754 , test loss:  0.09087697731177903\n",
      "Epoch:  33090 , step:  33090 , train loss:  0.06119283617807616 , test loss:  0.09087282800321386\n",
      "Epoch:  33100 , step:  33100 , train loss:  0.06118538828954746 , test loss:  0.09086868138281419\n",
      "Epoch:  33110 , step:  33110 , train loss:  0.06117794366069286 , test loss:  0.09086453744834398\n",
      "Epoch:  33120 , step:  33120 , train loss:  0.06117050228909623 , test loss:  0.09086039619756998\n",
      "Epoch:  33130 , step:  33130 , train loss:  0.061163064172344024 , test loss:  0.09085625762826091\n",
      "Epoch:  33140 , step:  33140 , train loss:  0.06115562930802518 , test loss:  0.09085212173818848\n",
      "Epoch:  33150 , step:  33150 , train loss:  0.061148197693731116 , test loss:  0.09084798852512647\n",
      "Epoch:  33160 , step:  33160 , train loss:  0.06114076932705588 , test loss:  0.09084385798685124\n",
      "Epoch:  33170 , step:  33170 , train loss:  0.06113334420559592 , test loss:  0.09083973012114185\n",
      "Epoch:  33180 , step:  33180 , train loss:  0.06112592232695022 , test loss:  0.09083560492577937\n",
      "Epoch:  33190 , step:  33190 , train loss:  0.06111850368872033 , test loss:  0.09083148239854764\n",
      "Epoch:  33200 , step:  33200 , train loss:  0.0611110882885102 , test loss:  0.09082736253723271\n",
      "Epoch:  33210 , step:  33210 , train loss:  0.06110367612392633 , test loss:  0.09082324533962328\n",
      "Epoch:  33220 , step:  33220 , train loss:  0.06109626719257772 , test loss:  0.09081913080351042\n",
      "Epoch:  33230 , step:  33230 , train loss:  0.06108886149207587 , test loss:  0.09081501892668765\n",
      "Epoch:  33240 , step:  33240 , train loss:  0.06108145902003468 , test loss:  0.09081090970695106\n",
      "Epoch:  33250 , step:  33250 , train loss:  0.06107405977407072 , test loss:  0.0908068031420988\n",
      "Epoch:  33260 , step:  33260 , train loss:  0.0610666637518028 , test loss:  0.09080269922993174\n",
      "Epoch:  33270 , step:  33270 , train loss:  0.061059270950852396 , test loss:  0.09079859796825304\n",
      "Epoch:  33280 , step:  33280 , train loss:  0.061051881368843375 , test loss:  0.09079449935486847\n",
      "Epoch:  33290 , step:  33290 , train loss:  0.061044495003402074 , test loss:  0.09079040338758593\n",
      "Epoch:  33300 , step:  33300 , train loss:  0.061037111852157336 , test loss:  0.09078631006421602\n",
      "Epoch:  33310 , step:  33310 , train loss:  0.06102973191274046 , test loss:  0.09078221938257161\n",
      "Epoch:  33320 , step:  33320 , train loss:  0.06102235518278515 , test loss:  0.09077813134046794\n",
      "Epoch:  33330 , step:  33330 , train loss:  0.06101498165992761 , test loss:  0.09077404593572262\n",
      "Epoch:  33340 , step:  33340 , train loss:  0.06100761134180652 , test loss:  0.09076996316615593\n",
      "Epoch:  33350 , step:  33350 , train loss:  0.061000244226063 , test loss:  0.09076588302959021\n",
      "Epoch:  33360 , step:  33360 , train loss:  0.060992880310340565 , test loss:  0.0907618055238503\n",
      "Epoch:  33370 , step:  33370 , train loss:  0.06098551959228524 , test loss:  0.09075773064676357\n",
      "Epoch:  33380 , step:  33380 , train loss:  0.06097816206954547 , test loss:  0.09075365839615965\n",
      "Epoch:  33390 , step:  33390 , train loss:  0.06097080773977209 , test loss:  0.09074958876987042\n",
      "Epoch:  33400 , step:  33400 , train loss:  0.060963456600618464 , test loss:  0.09074552176573054\n",
      "Epoch:  33410 , step:  33410 , train loss:  0.06095610864974031 , test loss:  0.09074145738157667\n",
      "Epoch:  33420 , step:  33420 , train loss:  0.06094876388479579 , test loss:  0.09073739561524795\n",
      "Epoch:  33430 , step:  33430 , train loss:  0.06094142230344552 , test loss:  0.09073333646458599\n",
      "Epoch:  33440 , step:  33440 , train loss:  0.0609340839033525 , test loss:  0.09072927992743471\n",
      "Epoch:  33450 , step:  33450 , train loss:  0.06092674868218217 , test loss:  0.09072522600164024\n",
      "Epoch:  33460 , step:  33460 , train loss:  0.060919416637602385 , test loss:  0.09072117468505138\n",
      "Epoch:  33470 , step:  33470 , train loss:  0.060912087767283414 , test loss:  0.09071712597551898\n",
      "Epoch:  33480 , step:  33480 , train loss:  0.06090476206889791 , test loss:  0.09071307987089645\n",
      "Epoch:  33490 , step:  33490 , train loss:  0.06089743954012098 , test loss:  0.09070903636903942\n",
      "Epoch:  33500 , step:  33500 , train loss:  0.06089012017863007 , test loss:  0.09070499546780594\n",
      "Epoch:  33510 , step:  33510 , train loss:  0.06088280398210504 , test loss:  0.09070095716505641\n",
      "Epoch:  33520 , step:  33520 , train loss:  0.060875490948228236 , test loss:  0.09069692145865346\n",
      "Epoch:  33530 , step:  33530 , train loss:  0.06086818107468425 , test loss:  0.09069288834646232\n",
      "Epoch:  33540 , step:  33540 , train loss:  0.06086087435916019 , test loss:  0.0906888578263503\n",
      "Epoch:  33550 , step:  33550 , train loss:  0.060853570799345454 , test loss:  0.09068482989618717\n",
      "Epoch:  33560 , step:  33560 , train loss:  0.060846270392931896 , test loss:  0.09068080455384503\n",
      "Epoch:  33570 , step:  33570 , train loss:  0.06083897313761371 , test loss:  0.09067678179719799\n",
      "Epoch:  33580 , step:  33580 , train loss:  0.06083167903108749 , test loss:  0.09067276162412298\n",
      "Epoch:  33590 , step:  33590 , train loss:  0.060824388071052164 , test loss:  0.09066874403249894\n",
      "Epoch:  33600 , step:  33600 , train loss:  0.06081710025520909 , test loss:  0.09066472902020747\n",
      "Epoch:  33610 , step:  33610 , train loss:  0.060809815581261965 , test loss:  0.09066071658513174\n",
      "Epoch:  33620 , step:  33620 , train loss:  0.0608025340469168 , test loss:  0.09065670672515802\n",
      "Epoch:  33630 , step:  33630 , train loss:  0.06079525564988207 , test loss:  0.09065269943817454\n",
      "Epoch:  33640 , step:  33640 , train loss:  0.060787980387868513 , test loss:  0.09064869472207186\n",
      "Epoch:  33650 , step:  33650 , train loss:  0.06078070825858927 , test loss:  0.09064469257474281\n",
      "Epoch:  33660 , step:  33660 , train loss:  0.06077343925975984 , test loss:  0.09064069299408252\n",
      "Epoch:  33670 , step:  33670 , train loss:  0.060766173389098024 , test loss:  0.0906366959779885\n",
      "Epoch:  33680 , step:  33680 , train loss:  0.06075891064432403 , test loss:  0.09063270152436041\n",
      "Epoch:  33690 , step:  33690 , train loss:  0.06075165102316035 , test loss:  0.09062870963110059\n",
      "Epoch:  33700 , step:  33700 , train loss:  0.060744394523331885 , test loss:  0.09062472029611307\n",
      "Epoch:  33710 , step:  33710 , train loss:  0.06073714114256579 , test loss:  0.09062073351730457\n",
      "Epoch:  33720 , step:  33720 , train loss:  0.060729890878591604 , test loss:  0.09061674929258384\n",
      "Epoch:  33730 , step:  33730 , train loss:  0.060722643729141206 , test loss:  0.09061276761986209\n",
      "Epoch:  33740 , step:  33740 , train loss:  0.060715399691948745 , test loss:  0.09060878849705276\n",
      "Epoch:  33750 , step:  33750 , train loss:  0.060708158764750775 , test loss:  0.09060481192207169\n",
      "Epoch:  33760 , step:  33760 , train loss:  0.06070092094528609 , test loss:  0.0906008378928366\n",
      "Epoch:  33770 , step:  33770 , train loss:  0.06069368623129586 , test loss:  0.0905968664072678\n",
      "Epoch:  33780 , step:  33780 , train loss:  0.06068645462052351 , test loss:  0.09059289746328794\n",
      "Epoch:  33790 , step:  33790 , train loss:  0.06067922611071487 , test loss:  0.09058893105882147\n",
      "Epoch:  33800 , step:  33800 , train loss:  0.060672000699618006 , test loss:  0.09058496719179551\n",
      "Epoch:  33810 , step:  33810 , train loss:  0.06066477838498327 , test loss:  0.09058100586013945\n",
      "Epoch:  33820 , step:  33820 , train loss:  0.060657559164563404 , test loss:  0.09057704706178464\n",
      "Epoch:  33830 , step:  33830 , train loss:  0.06065034303611337 , test loss:  0.09057309079466476\n",
      "Epoch:  33840 , step:  33840 , train loss:  0.060643129997390466 , test loss:  0.09056913705671596\n",
      "Epoch:  33850 , step:  33850 , train loss:  0.06063592004615427 , test loss:  0.09056518584587626\n",
      "Epoch:  33860 , step:  33860 , train loss:  0.060628713180166646 , test loss:  0.09056123716008621\n",
      "Epoch:  33870 , step:  33870 , train loss:  0.06062150939719178 , test loss:  0.09055729099728854\n",
      "Epoch:  33880 , step:  33880 , train loss:  0.06061430869499605 , test loss:  0.0905533473554281\n",
      "Epoch:  33890 , step:  33890 , train loss:  0.06060711107134829 , test loss:  0.0905494062324521\n",
      "Epoch:  33900 , step:  33900 , train loss:  0.0605999165240194 , test loss:  0.09054546762630973\n",
      "Epoch:  33910 , step:  33910 , train loss:  0.06059272505078271 , test loss:  0.09054153153495263\n",
      "Epoch:  33920 , step:  33920 , train loss:  0.06058553664941374 , test loss:  0.09053759795633469\n",
      "Epoch:  33930 , step:  33930 , train loss:  0.06057835131769036 , test loss:  0.09053366688841201\n",
      "Epoch:  33940 , step:  33940 , train loss:  0.06057116905339261 , test loss:  0.09052973832914267\n",
      "Epoch:  33950 , step:  33950 , train loss:  0.06056398985430287 , test loss:  0.09052581227648712\n",
      "Epoch:  33960 , step:  33960 , train loss:  0.060556813718205725 , test loss:  0.09052188872840793\n",
      "Epoch:  33970 , step:  33970 , train loss:  0.06054964064288807 , test loss:  0.09051796768287002\n",
      "Epoch:  33980 , step:  33980 , train loss:  0.06054247062613901 , test loss:  0.09051404913784032\n",
      "Epoch:  33990 , step:  33990 , train loss:  0.060535303665749905 , test loss:  0.09051013309128822\n",
      "Epoch:  34000 , step:  34000 , train loss:  0.06052813975951444 , test loss:  0.09050621954118512\n",
      "Epoch:  34010 , step:  34010 , train loss:  0.06052097890522841 , test loss:  0.09050230848550458\n",
      "Epoch:  34020 , step:  34020 , train loss:  0.06051382110068997 , test loss:  0.09049839992222253\n",
      "Epoch:  34030 , step:  34030 , train loss:  0.06050666634369948 , test loss:  0.09049449384931675\n",
      "Epoch:  34040 , step:  34040 , train loss:  0.0604995146320595 , test loss:  0.09049059026476755\n",
      "Epoch:  34050 , step:  34050 , train loss:  0.06049236596357488 , test loss:  0.09048668916655732\n",
      "Epoch:  34060 , step:  34060 , train loss:  0.06048522033605265 , test loss:  0.09048279055267042\n",
      "Epoch:  34070 , step:  34070 , train loss:  0.0604780777473021 , test loss:  0.09047889442109369\n",
      "Epoch:  34080 , step:  34080 , train loss:  0.06047093819513475 , test loss:  0.090475000769816\n",
      "Epoch:  34090 , step:  34090 , train loss:  0.060463801677364315 , test loss:  0.0904711095968283\n",
      "Epoch:  34100 , step:  34100 , train loss:  0.06045666819180674 , test loss:  0.09046722090012382\n",
      "Epoch:  34110 , step:  34110 , train loss:  0.060449537736280236 , test loss:  0.09046333467769793\n",
      "Epoch:  34120 , step:  34120 , train loss:  0.060442410308605116 , test loss:  0.09045945092754817\n",
      "Epoch:  34130 , step:  34130 , train loss:  0.060435285906604026 , test loss:  0.0904555696476742\n",
      "Epoch:  34140 , step:  34140 , train loss:  0.06042816452810175 , test loss:  0.09045169083607778\n",
      "Epoch:  34150 , step:  34150 , train loss:  0.060421046170925266 , test loss:  0.090447814490763\n",
      "Epoch:  34160 , step:  34160 , train loss:  0.06041393083290378 , test loss:  0.09044394060973585\n",
      "Epoch:  34170 , step:  34170 , train loss:  0.060406818511868736 , test loss:  0.09044006919100474\n",
      "Epoch:  34180 , step:  34180 , train loss:  0.06039970920565371 , test loss:  0.09043620023258007\n",
      "Epoch:  34190 , step:  34190 , train loss:  0.060392602912094494 , test loss:  0.09043233373247415\n",
      "Epoch:  34200 , step:  34200 , train loss:  0.06038549962902909 , test loss:  0.09042846968870193\n",
      "Epoch:  34210 , step:  34210 , train loss:  0.06037839935429766 , test loss:  0.09042460809928018\n",
      "Epoch:  34220 , step:  34220 , train loss:  0.060371302085742554 , test loss:  0.0904207489622276\n",
      "Epoch:  34230 , step:  34230 , train loss:  0.06036420782120834 , test loss:  0.09041689227556558\n",
      "Epoch:  34240 , step:  34240 , train loss:  0.060357116558541694 , test loss:  0.09041303803731705\n",
      "Epoch:  34250 , step:  34250 , train loss:  0.060350028295591544 , test loss:  0.09040918624550749\n",
      "Epoch:  34260 , step:  34260 , train loss:  0.06034294303020895 , test loss:  0.09040533689816421\n",
      "Epoch:  34270 , step:  34270 , train loss:  0.06033586076024718 , test loss:  0.09040148999331694\n",
      "Epoch:  34280 , step:  34280 , train loss:  0.06032878148356161 , test loss:  0.09039764552899728\n",
      "Epoch:  34290 , step:  34290 , train loss:  0.06032170519800982 , test loss:  0.09039380350323871\n",
      "Epoch:  34300 , step:  34300 , train loss:  0.06031463190145154 , test loss:  0.09038996391407748\n",
      "Epoch:  34310 , step:  34310 , train loss:  0.0603075615917487 , test loss:  0.09038612675955143\n",
      "Epoch:  34320 , step:  34320 , train loss:  0.060300494266765294 , test loss:  0.09038229203770048\n",
      "Epoch:  34330 , step:  34330 , train loss:  0.06029342992436757 , test loss:  0.09037845974656701\n",
      "Epoch:  34340 , step:  34340 , train loss:  0.060286368562423906 , test loss:  0.09037462988419513\n",
      "Epoch:  34350 , step:  34350 , train loss:  0.060279310178804724 , test loss:  0.09037080244863141\n",
      "Epoch:  34360 , step:  34360 , train loss:  0.06027225477138278 , test loss:  0.09036697743792416\n",
      "Epoch:  34370 , step:  34370 , train loss:  0.0602652023380328 , test loss:  0.09036315485012388\n",
      "Epoch:  34380 , step:  34380 , train loss:  0.06025815287663174 , test loss:  0.09035933468328333\n",
      "Epoch:  34390 , step:  34390 , train loss:  0.06025110638505867 , test loss:  0.09035551693545713\n",
      "Epoch:  34400 , step:  34400 , train loss:  0.06024406286119477 , test loss:  0.090351701604702\n",
      "Epoch:  34410 , step:  34410 , train loss:  0.06023702230292341 , test loss:  0.09034788868907698\n",
      "Epoch:  34420 , step:  34420 , train loss:  0.060229984708130055 , test loss:  0.09034407818664271\n",
      "Epoch:  34430 , step:  34430 , train loss:  0.06022295007470228 , test loss:  0.09034027009546239\n",
      "Epoch:  34440 , step:  34440 , train loss:  0.06021591840052982 , test loss:  0.09033646441360119\n",
      "Epoch:  34450 , step:  34450 , train loss:  0.06020888968350449 , test loss:  0.09033266113912615\n",
      "Epoch:  34460 , step:  34460 , train loss:  0.06020186392152025 , test loss:  0.09032886027010639\n",
      "Epoch:  34470 , step:  34470 , train loss:  0.060194841112473196 , test loss:  0.09032506180461317\n",
      "Epoch:  34480 , step:  34480 , train loss:  0.06018782125426148 , test loss:  0.09032126574072\n",
      "Epoch:  34490 , step:  34490 , train loss:  0.060180804344785385 , test loss:  0.09031747207650194\n",
      "Epoch:  34500 , step:  34500 , train loss:  0.06017379038194737 , test loss:  0.09031368081003667\n",
      "Epoch:  34510 , step:  34510 , train loss:  0.06016677936365187 , test loss:  0.09030989193940336\n",
      "Epoch:  34520 , step:  34520 , train loss:  0.060159771287805516 , test loss:  0.09030610546268389\n",
      "Epoch:  34530 , step:  34530 , train loss:  0.06015276615231701 , test loss:  0.09030232137796161\n",
      "Epoch:  34540 , step:  34540 , train loss:  0.06014576395509717 , test loss:  0.09029853968332205\n",
      "Epoch:  34550 , step:  34550 , train loss:  0.06013876469405887 , test loss:  0.09029476037685294\n",
      "Epoch:  34560 , step:  34560 , train loss:  0.06013176836711708 , test loss:  0.09029098345664394\n",
      "Epoch:  34570 , step:  34570 , train loss:  0.06012477497218893 , test loss:  0.09028720892078682\n",
      "Epoch:  34580 , step:  34580 , train loss:  0.06011778450719349 , test loss:  0.0902834367673751\n",
      "Epoch:  34590 , step:  34590 , train loss:  0.060110796970052074 , test loss:  0.09027966699450475\n",
      "Epoch:  34600 , step:  34600 , train loss:  0.060103812358687954 , test loss:  0.09027589960027341\n",
      "Epoch:  34610 , step:  34610 , train loss:  0.06009683067102655 , test loss:  0.09027213458278092\n",
      "Epoch:  34620 , step:  34620 , train loss:  0.06008985190499535 , test loss:  0.09026837194012921\n",
      "Epoch:  34630 , step:  34630 , train loss:  0.06008287605852386 , test loss:  0.09026461167042191\n",
      "Epoch:  34640 , step:  34640 , train loss:  0.06007590312954376 , test loss:  0.090260853771765\n",
      "Epoch:  34650 , step:  34650 , train loss:  0.06006893311598864 , test loss:  0.09025709824226624\n",
      "Epoch:  34660 , step:  34660 , train loss:  0.06006196601579429 , test loss:  0.09025334508003556\n",
      "Epoch:  34670 , step:  34670 , train loss:  0.06005500182689853 , test loss:  0.09024959428318473\n",
      "Epoch:  34680 , step:  34680 , train loss:  0.06004804054724119 , test loss:  0.09024584584982777\n",
      "Epoch:  34690 , step:  34690 , train loss:  0.060041082174764206 , test loss:  0.09024209977808056\n",
      "Epoch:  34700 , step:  34700 , train loss:  0.06003412670741157 , test loss:  0.09023835606606086\n",
      "Epoch:  34710 , step:  34710 , train loss:  0.060027174143129254 , test loss:  0.0902346147118886\n",
      "Epoch:  34720 , step:  34720 , train loss:  0.060020224479865394 , test loss:  0.09023087571368554\n",
      "Epoch:  34730 , step:  34730 , train loss:  0.060013277715570044 , test loss:  0.09022713906957568\n",
      "Epoch:  34740 , step:  34740 , train loss:  0.06000633384819544 , test loss:  0.09022340477768469\n",
      "Epoch:  34750 , step:  34750 , train loss:  0.0599993928756957 , test loss:  0.09021967283614055\n",
      "Epoch:  34760 , step:  34760 , train loss:  0.059992454796027134 , test loss:  0.09021594324307294\n",
      "Epoch:  34770 , step:  34770 , train loss:  0.059985519607147965 , test loss:  0.09021221599661357\n",
      "Epoch:  34780 , step:  34780 , train loss:  0.05997858730701852 , test loss:  0.09020849109489644\n",
      "Epoch:  34790 , step:  34790 , train loss:  0.059971657893601135 , test loss:  0.09020476853605706\n",
      "Epoch:  34800 , step:  34800 , train loss:  0.05996473136486015 , test loss:  0.09020104831823311\n",
      "Epoch:  34810 , step:  34810 , train loss:  0.05995780771876199 , test loss:  0.09019733043956431\n",
      "Epoch:  34820 , step:  34820 , train loss:  0.05995088695327505 , test loss:  0.09019361489819237\n",
      "Epoch:  34830 , step:  34830 , train loss:  0.05994396906636975 , test loss:  0.09018990169226095\n",
      "Epoch:  34840 , step:  34840 , train loss:  0.05993705405601854 , test loss:  0.09018619081991532\n",
      "Epoch:  34850 , step:  34850 , train loss:  0.059930141920195884 , test loss:  0.09018248227930317\n",
      "Epoch:  34860 , step:  34860 , train loss:  0.059923232656878246 , test loss:  0.09017877606857398\n",
      "Epoch:  34870 , step:  34870 , train loss:  0.059916326264044105 , test loss:  0.09017507218587914\n",
      "Epoch:  34880 , step:  34880 , train loss:  0.05990942273967398 , test loss:  0.09017137062937203\n",
      "Epoch:  34890 , step:  34890 , train loss:  0.059902522081750294 , test loss:  0.09016767139720795\n",
      "Epoch:  34900 , step:  34900 , train loss:  0.05989562428825762 , test loss:  0.09016397448754412\n",
      "Epoch:  34910 , step:  34910 , train loss:  0.059888729357182376 , test loss:  0.09016027989853975\n",
      "Epoch:  34920 , step:  34920 , train loss:  0.059881837286513105 , test loss:  0.09015658762835611\n",
      "Epoch:  34930 , step:  34930 , train loss:  0.05987494807424027 , test loss:  0.0901528976751562\n",
      "Epoch:  34940 , step:  34940 , train loss:  0.05986806171835633 , test loss:  0.09014921003710509\n",
      "Epoch:  34950 , step:  34950 , train loss:  0.05986117821685578 , test loss:  0.09014552471236967\n",
      "Epoch:  34960 , step:  34960 , train loss:  0.059854297567735026 , test loss:  0.09014184169911885\n",
      "Epoch:  34970 , step:  34970 , train loss:  0.059847419768992535 , test loss:  0.09013816099552345\n",
      "Epoch:  34980 , step:  34980 , train loss:  0.059840544818628674 , test loss:  0.09013448259975608\n",
      "Epoch:  34990 , step:  34990 , train loss:  0.059833672714645895 , test loss:  0.09013080650999146\n",
      "Epoch:  35000 , step:  35000 , train loss:  0.059826803455048524 , test loss:  0.09012713272440617\n",
      "Epoch:  35010 , step:  35010 , train loss:  0.059819937037842885 , test loss:  0.09012346124117872\n",
      "Epoch:  35020 , step:  35020 , train loss:  0.059813073461037335 , test loss:  0.0901197920584896\n",
      "Epoch:  35030 , step:  35030 , train loss:  0.05980621272264217 , test loss:  0.0901161251745211\n",
      "Epoch:  35040 , step:  35040 , train loss:  0.059799354820669566 , test loss:  0.09011246058745732\n",
      "Epoch:  35050 , step:  35050 , train loss:  0.05979249975313376 , test loss:  0.09010879829548447\n",
      "Epoch:  35060 , step:  35060 , train loss:  0.05978564751805095 , test loss:  0.09010513829679068\n",
      "Epoch:  35070 , step:  35070 , train loss:  0.05977879811343923 , test loss:  0.09010148058956569\n",
      "Epoch:  35080 , step:  35080 , train loss:  0.05977195153731873 , test loss:  0.09009782517200143\n",
      "Epoch:  35090 , step:  35090 , train loss:  0.05976510778771142 , test loss:  0.09009417204229168\n",
      "Epoch:  35100 , step:  35100 , train loss:  0.05975826686264135 , test loss:  0.09009052119863202\n",
      "Epoch:  35110 , step:  35110 , train loss:  0.05975142876013443 , test loss:  0.09008687263922004\n",
      "Epoch:  35120 , step:  35120 , train loss:  0.05974459347821855 , test loss:  0.09008322636225513\n",
      "Epoch:  35130 , step:  35130 , train loss:  0.059737761014923535 , test loss:  0.09007958236593859\n",
      "Epoch:  35140 , step:  35140 , train loss:  0.05973093136828117 , test loss:  0.09007594064847355\n",
      "Epoch:  35150 , step:  35150 , train loss:  0.059724104536325144 , test loss:  0.09007230120806514\n",
      "Epoch:  35160 , step:  35160 , train loss:  0.059717280517091104 , test loss:  0.09006866404292029\n",
      "Epoch:  35170 , step:  35170 , train loss:  0.059710459308616626 , test loss:  0.09006502915124785\n",
      "Epoch:  35180 , step:  35180 , train loss:  0.05970364090894124 , test loss:  0.09006139653125843\n",
      "Epoch:  35190 , step:  35190 , train loss:  0.05969682531610635 , test loss:  0.09005776618116464\n",
      "Epoch:  35200 , step:  35200 , train loss:  0.05969001252815536 , test loss:  0.09005413809918103\n",
      "Epoch:  35210 , step:  35210 , train loss:  0.059683202543133536 , test loss:  0.09005051228352381\n",
      "Epoch:  35220 , step:  35220 , train loss:  0.059676395359088086 , test loss:  0.09004688873241116\n",
      "Epoch:  35230 , step:  35230 , train loss:  0.05966959097406813 , test loss:  0.0900432674440632\n",
      "Epoch:  35240 , step:  35240 , train loss:  0.059662789386124744 , test loss:  0.09003964841670163\n",
      "Epoch:  35250 , step:  35250 , train loss:  0.059655990593310865 , test loss:  0.09003603164855051\n",
      "Epoch:  35260 , step:  35260 , train loss:  0.059649194593681364 , test loss:  0.09003241713783515\n",
      "Epoch:  35270 , step:  35270 , train loss:  0.05964240138529304 , test loss:  0.09002880488278311\n",
      "Epoch:  35280 , step:  35280 , train loss:  0.059635610966204566 , test loss:  0.09002519488162386\n",
      "Epoch:  35290 , step:  35290 , train loss:  0.05962882333447654 , test loss:  0.09002158713258832\n",
      "Epoch:  35300 , step:  35300 , train loss:  0.059622038488171464 , test loss:  0.09001798163390969\n",
      "Epoch:  35310 , step:  35310 , train loss:  0.05961525642535372 , test loss:  0.09001437838382272\n",
      "Epoch:  35320 , step:  35320 , train loss:  0.05960847714408961 , test loss:  0.09001077738056415\n",
      "Epoch:  35330 , step:  35330 , train loss:  0.05960170064244734 , test loss:  0.09000717862237245\n",
      "Epoch:  35340 , step:  35340 , train loss:  0.059594926918496965 , test loss:  0.09000358210748806\n",
      "Epoch:  35350 , step:  35350 , train loss:  0.059588155970310454 , test loss:  0.08999998783415321\n",
      "Epoch:  35360 , step:  35360 , train loss:  0.059581387795961664 , test loss:  0.08999639580061178\n",
      "Epoch:  35370 , step:  35370 , train loss:  0.05957462239352636 , test loss:  0.08999280600510981\n",
      "Epoch:  35380 , step:  35380 , train loss:  0.059567859761082156 , test loss:  0.0899892184458948\n",
      "Epoch:  35390 , step:  35390 , train loss:  0.05956109989670857 , test loss:  0.08998563312121635\n",
      "Epoch:  35400 , step:  35400 , train loss:  0.059554342798486995 , test loss:  0.08998205002932595\n",
      "Epoch:  35410 , step:  35410 , train loss:  0.059547588464500645 , test loss:  0.08997846916847647\n",
      "Epoch:  35420 , step:  35420 , train loss:  0.059540836892834716 , test loss:  0.08997489053692305\n",
      "Epoch:  35430 , step:  35430 , train loss:  0.05953408808157617 , test loss:  0.08997131413292252\n",
      "Epoch:  35440 , step:  35440 , train loss:  0.05952734202881392 , test loss:  0.08996773995473341\n",
      "Epoch:  35450 , step:  35450 , train loss:  0.05952059873263867 , test loss:  0.089964168000616\n",
      "Epoch:  35460 , step:  35460 , train loss:  0.05951385819114308 , test loss:  0.08996059826883267\n",
      "Epoch:  35470 , step:  35470 , train loss:  0.05950712040242155 , test loss:  0.08995703075764735\n",
      "Epoch:  35480 , step:  35480 , train loss:  0.05950038536457046 , test loss:  0.08995346546532598\n",
      "Epoch:  35490 , step:  35490 , train loss:  0.05949365307568798 , test loss:  0.08994990239013607\n",
      "Epoch:  35500 , step:  35500 , train loss:  0.059486923533874164 , test loss:  0.08994634153034703\n",
      "Epoch:  35510 , step:  35510 , train loss:  0.059480196737230887 , test loss:  0.08994278288423012\n",
      "Epoch:  35520 , step:  35520 , train loss:  0.059473472683861904 , test loss:  0.08993922645005839\n",
      "Epoch:  35530 , step:  35530 , train loss:  0.05946675137187278 , test loss:  0.08993567222610663\n",
      "Epoch:  35540 , step:  35540 , train loss:  0.059460032799371 , test loss:  0.08993212021065147\n",
      "Epoch:  35550 , step:  35550 , train loss:  0.059453316964465805 , test loss:  0.0899285704019711\n",
      "Epoch:  35560 , step:  35560 , train loss:  0.0594466038652683 , test loss:  0.08992502279834594\n",
      "Epoch:  35570 , step:  35570 , train loss:  0.0594398934998915 , test loss:  0.08992147739805766\n",
      "Epoch:  35580 , step:  35580 , train loss:  0.059433185866450175 , test loss:  0.08991793419939023\n",
      "Epoch:  35590 , step:  35590 , train loss:  0.05942648096306095 , test loss:  0.08991439320062902\n",
      "Epoch:  35600 , step:  35600 , train loss:  0.059419778787842305 , test loss:  0.08991085440006137\n",
      "Epoch:  35610 , step:  35610 , train loss:  0.0594130793389145 , test loss:  0.08990731779597627\n",
      "Epoch:  35620 , step:  35620 , train loss:  0.059406382614399685 , test loss:  0.0899037833866645\n",
      "Epoch:  35630 , step:  35630 , train loss:  0.05939968861242176 , test loss:  0.08990025117041876\n",
      "Epoch:  35640 , step:  35640 , train loss:  0.05939299733110654 , test loss:  0.08989672114553338\n",
      "Epoch:  35650 , step:  35650 , train loss:  0.059386308768581586 , test loss:  0.08989319331030447\n",
      "Epoch:  35660 , step:  35660 , train loss:  0.05937962292297632 , test loss:  0.08988966766302985\n",
      "Epoch:  35670 , step:  35670 , train loss:  0.05937293979242193 , test loss:  0.08988614420200924\n",
      "Epoch:  35680 , step:  35680 , train loss:  0.05936625937505148 , test loss:  0.08988262292554396\n",
      "Epoch:  35690 , step:  35690 , train loss:  0.05935958166899981 , test loss:  0.08987910383193722\n",
      "Epoch:  35700 , step:  35700 , train loss:  0.05935290667240355 , test loss:  0.08987558691949392\n",
      "Epoch:  35710 , step:  35710 , train loss:  0.05934623438340121 , test loss:  0.08987207218652069\n",
      "Epoch:  35720 , step:  35720 , train loss:  0.059339564800133 , test loss:  0.08986855963132594\n",
      "Epoch:  35730 , step:  35730 , train loss:  0.059332897920741014 , test loss:  0.08986504925221966\n",
      "Epoch:  35740 , step:  35740 , train loss:  0.059326233743369115 , test loss:  0.089861541047514\n",
      "Epoch:  35750 , step:  35750 , train loss:  0.05931957226616297 , test loss:  0.08985803501552253\n",
      "Epoch:  35760 , step:  35760 , train loss:  0.05931291348727005 , test loss:  0.08985453115456056\n",
      "Epoch:  35770 , step:  35770 , train loss:  0.05930625740483959 , test loss:  0.08985102946294528\n",
      "Epoch:  35780 , step:  35780 , train loss:  0.059299604017022625 , test loss:  0.08984752993899528\n",
      "Epoch:  35790 , step:  35790 , train loss:  0.05929295332197204 , test loss:  0.08984403258103134\n",
      "Epoch:  35800 , step:  35800 , train loss:  0.05928630531784238 , test loss:  0.08984053738737571\n",
      "Epoch:  35810 , step:  35810 , train loss:  0.05927966000279011 , test loss:  0.0898370443563525\n",
      "Epoch:  35820 , step:  35820 , train loss:  0.059273017374973376 , test loss:  0.0898335534862873\n",
      "Epoch:  35830 , step:  35830 , train loss:  0.059266377432552184 , test loss:  0.08983006477550774\n",
      "Epoch:  35840 , step:  35840 , train loss:  0.05925974017368825 , test loss:  0.08982657822234294\n",
      "Epoch:  35850 , step:  35850 , train loss:  0.059253105596545105 , test loss:  0.08982309382512393\n",
      "Epoch:  35860 , step:  35860 , train loss:  0.05924647369928804 , test loss:  0.08981961158218307\n",
      "Epoch:  35870 , step:  35870 , train loss:  0.05923984448008408 , test loss:  0.08981613149185505\n",
      "Epoch:  35880 , step:  35880 , train loss:  0.059233217937102144 , test loss:  0.0898126535524757\n",
      "Epoch:  35890 , step:  35890 , train loss:  0.059226594068512745 , test loss:  0.08980917776238284\n",
      "Epoch:  35900 , step:  35900 , train loss:  0.059219972872488286 , test loss:  0.089805704119916\n",
      "Epoch:  35910 , step:  35910 , train loss:  0.05921335434720288 , test loss:  0.08980223262341634\n",
      "Epoch:  35920 , step:  35920 , train loss:  0.05920673849083243 , test loss:  0.08979876327122668\n",
      "Epoch:  35930 , step:  35930 , train loss:  0.05920012530155456 , test loss:  0.08979529606169177\n",
      "Epoch:  35940 , step:  35940 , train loss:  0.05919351477754869 , test loss:  0.08979183099315778\n",
      "Epoch:  35950 , step:  35950 , train loss:  0.059186906916995956 , test loss:  0.08978836806397271\n",
      "Epoch:  35960 , step:  35960 , train loss:  0.05918030171807927 , test loss:  0.08978490727248614\n",
      "Epoch:  35970 , step:  35970 , train loss:  0.05917369917898327 , test loss:  0.08978144861704959\n",
      "Epoch:  35980 , step:  35980 , train loss:  0.0591670992978944 , test loss:  0.08977799209601607\n",
      "Epoch:  35990 , step:  35990 , train loss:  0.05916050207300075 , test loss:  0.08977453770774037\n",
      "Epoch:  36000 , step:  36000 , train loss:  0.05915390750249223 , test loss:  0.0897710854505789\n",
      "Epoch:  36010 , step:  36010 , train loss:  0.05914731558456046 , test loss:  0.0897676353228896\n",
      "Epoch:  36020 , step:  36020 , train loss:  0.05914072631739884 , test loss:  0.08976418732303251\n",
      "Epoch:  36030 , step:  36030 , train loss:  0.05913413969920244 , test loss:  0.08976074144936912\n",
      "Epoch:  36040 , step:  36040 , train loss:  0.059127555728168094 , test loss:  0.0897572977002624\n",
      "Epoch:  36050 , step:  36050 , train loss:  0.05912097440249441 , test loss:  0.08975385607407739\n",
      "Epoch:  36060 , step:  36060 , train loss:  0.05911439572038164 , test loss:  0.08975041656918038\n",
      "Epoch:  36070 , step:  36070 , train loss:  0.05910781968003185 , test loss:  0.08974697918393966\n",
      "Epoch:  36080 , step:  36080 , train loss:  0.05910124627964874 , test loss:  0.08974354391672512\n",
      "Epoch:  36090 , step:  36090 , train loss:  0.059094675517437835 , test loss:  0.08974011076590822\n",
      "Epoch:  36100 , step:  36100 , train loss:  0.05908810739160631 , test loss:  0.08973667972986206\n",
      "Epoch:  36110 , step:  36110 , train loss:  0.059081541900363056 , test loss:  0.0897332508069616\n",
      "Epoch:  36120 , step:  36120 , train loss:  0.05907497904191876 , test loss:  0.08972982399558338\n",
      "Epoch:  36130 , step:  36130 , train loss:  0.059068418814485754 , test loss:  0.0897263992941055\n",
      "Epoch:  36140 , step:  36140 , train loss:  0.05906186121627805 , test loss:  0.0897229767009077\n",
      "Epoch:  36150 , step:  36150 , train loss:  0.05905530624551148 , test loss:  0.08971955621437147\n",
      "Epoch:  36160 , step:  36160 , train loss:  0.059048753900403504 , test loss:  0.08971613783288\n",
      "Epoch:  36170 , step:  36170 , train loss:  0.05904220417917329 , test loss:  0.08971272155481803\n",
      "Epoch:  36180 , step:  36180 , train loss:  0.059035657080041715 , test loss:  0.08970930737857188\n",
      "Epoch:  36190 , step:  36190 , train loss:  0.05902911260123142 , test loss:  0.0897058953025297\n",
      "Epoch:  36200 , step:  36200 , train loss:  0.059022570740966666 , test loss:  0.0897024853250813\n",
      "Epoch:  36210 , step:  36210 , train loss:  0.05901603149747345 , test loss:  0.08969907744461786\n",
      "Epoch:  36220 , step:  36220 , train loss:  0.05900949486897946 , test loss:  0.08969567165953245\n",
      "Epoch:  36230 , step:  36230 , train loss:  0.05900296085371406 , test loss:  0.08969226796821966\n",
      "Epoch:  36240 , step:  36240 , train loss:  0.05899642944990836 , test loss:  0.08968886636907576\n",
      "Epoch:  36250 , step:  36250 , train loss:  0.058989900655795055 , test loss:  0.08968546686049864\n",
      "Epoch:  36260 , step:  36260 , train loss:  0.05898337446960867 , test loss:  0.08968206944088782\n",
      "Epoch:  36270 , step:  36270 , train loss:  0.05897685088958528 , test loss:  0.08967867410864444\n",
      "Epoch:  36280 , step:  36280 , train loss:  0.05897032991396276 , test loss:  0.08967528086217123\n",
      "Epoch:  36290 , step:  36290 , train loss:  0.05896381154098055 , test loss:  0.0896718896998728\n",
      "Epoch:  36300 , step:  36300 , train loss:  0.058957295768879876 , test loss:  0.08966850062015493\n",
      "Epoch:  36310 , step:  36310 , train loss:  0.05895078259590355 , test loss:  0.08966511362142539\n",
      "Epoch:  36320 , step:  36320 , train loss:  0.05894427202029613 , test loss:  0.08966172870209342\n",
      "Epoch:  36330 , step:  36330 , train loss:  0.05893776404030384 , test loss:  0.08965834586056998\n",
      "Epoch:  36340 , step:  36340 , train loss:  0.05893125865417452 , test loss:  0.08965496509526749\n",
      "Epoch:  36350 , step:  36350 , train loss:  0.05892475586015772 , test loss:  0.08965158640459994\n",
      "Epoch:  36360 , step:  36360 , train loss:  0.058918255656504666 , test loss:  0.0896482097869833\n",
      "Epoch:  36370 , step:  36370 , train loss:  0.05891175804146822 , test loss:  0.08964483524083469\n",
      "Epoch:  36380 , step:  36380 , train loss:  0.05890526301330292 , test loss:  0.08964146276457308\n",
      "Epoch:  36390 , step:  36390 , train loss:  0.05889877057026499 , test loss:  0.08963809235661901\n",
      "Epoch:  36400 , step:  36400 , train loss:  0.05889228071061225 , test loss:  0.0896347240153947\n",
      "Epoch:  36410 , step:  36410 , train loss:  0.05888579343260426 , test loss:  0.08963135773932387\n",
      "Epoch:  36420 , step:  36420 , train loss:  0.05887930873450216 , test loss:  0.08962799352683166\n",
      "Epoch:  36430 , step:  36430 , train loss:  0.058872826614568774 , test loss:  0.08962463137634546\n",
      "Epoch:  36440 , step:  36440 , train loss:  0.05886634707106859 , test loss:  0.08962127128629338\n",
      "Epoch:  36450 , step:  36450 , train loss:  0.05885987010226772 , test loss:  0.08961791325510561\n",
      "Epoch:  36460 , step:  36460 , train loss:  0.058853395706433935 , test loss:  0.08961455728121402\n",
      "Epoch:  36470 , step:  36470 , train loss:  0.058846923881836655 , test loss:  0.08961120336305174\n",
      "Epoch:  36480 , step:  36480 , train loss:  0.05884045462674693 , test loss:  0.08960785149905363\n",
      "Epoch:  36490 , step:  36490 , train loss:  0.05883398793943749 , test loss:  0.08960450168765637\n",
      "Epoch:  36500 , step:  36500 , train loss:  0.058827523818182584 , test loss:  0.08960115392729787\n",
      "Epoch:  36510 , step:  36510 , train loss:  0.058821062261258286 , test loss:  0.08959780821641776\n",
      "Epoch:  36520 , step:  36520 , train loss:  0.05881460326694214 , test loss:  0.08959446455345728\n",
      "Epoch:  36530 , step:  36530 , train loss:  0.0588081468335134 , test loss:  0.08959112293685904\n",
      "Epoch:  36540 , step:  36540 , train loss:  0.05880169295925294 , test loss:  0.08958778336506766\n",
      "Epoch:  36550 , step:  36550 , train loss:  0.05879524164244326 , test loss:  0.08958444583652901\n",
      "Epoch:  36560 , step:  36560 , train loss:  0.05878879288136848 , test loss:  0.0895811103496905\n",
      "Epoch:  36570 , step:  36570 , train loss:  0.05878234667431437 , test loss:  0.08957777690300114\n",
      "Epoch:  36580 , step:  36580 , train loss:  0.05877590301956829 , test loss:  0.08957444549491186\n",
      "Epoch:  36590 , step:  36590 , train loss:  0.05876946191541922 , test loss:  0.0895711161238745\n",
      "Epoch:  36600 , step:  36600 , train loss:  0.05876302336015779 , test loss:  0.08956778878834305\n",
      "Epoch:  36610 , step:  36610 , train loss:  0.05875658735207621 , test loss:  0.08956446348677262\n",
      "Epoch:  36620 , step:  36620 , train loss:  0.058750153889468325 , test loss:  0.08956114021762038\n",
      "Epoch:  36630 , step:  36630 , train loss:  0.058743722970629624 , test loss:  0.08955781897934453\n",
      "Epoch:  36640 , step:  36640 , train loss:  0.05873729459385716 , test loss:  0.08955449977040526\n",
      "Epoch:  36650 , step:  36650 , train loss:  0.0587308687574496 , test loss:  0.08955118258926391\n",
      "Epoch:  36660 , step:  36660 , train loss:  0.058724445459707224 , test loss:  0.08954786743438366\n",
      "Epoch:  36670 , step:  36670 , train loss:  0.058718024698931934 , test loss:  0.0895445543042291\n",
      "Epoch:  36680 , step:  36680 , train loss:  0.05871160647342721 , test loss:  0.0895412431972664\n",
      "Epoch:  36690 , step:  36690 , train loss:  0.05870519078149816 , test loss:  0.08953793411196342\n",
      "Epoch:  36700 , step:  36700 , train loss:  0.05869877762145147 , test loss:  0.08953462704678936\n",
      "Epoch:  36710 , step:  36710 , train loss:  0.058692366991595406 , test loss:  0.08953132200021496\n",
      "Epoch:  36720 , step:  36720 , train loss:  0.05868595889023989 , test loss:  0.08952801897071282\n",
      "Epoch:  36730 , step:  36730 , train loss:  0.05867955331569639 , test loss:  0.08952471795675654\n",
      "Epoch:  36740 , step:  36740 , train loss:  0.058673150266277944 , test loss:  0.08952141895682171\n",
      "Epoch:  36750 , step:  36750 , train loss:  0.058666749740299264 , test loss:  0.08951812196938516\n",
      "Epoch:  36760 , step:  36760 , train loss:  0.05866035173607653 , test loss:  0.08951482699292544\n",
      "Epoch:  36770 , step:  36770 , train loss:  0.05865395625192763 , test loss:  0.08951153402592274\n",
      "Epoch:  36780 , step:  36780 , train loss:  0.05864756328617197 , test loss:  0.08950824306685837\n",
      "Epoch:  36790 , step:  36790 , train loss:  0.05864117283713053 , test loss:  0.08950495411421536\n",
      "Epoch:  36800 , step:  36800 , train loss:  0.05863478490312589 , test loss:  0.08950166716647845\n",
      "Epoch:  36810 , step:  36810 , train loss:  0.058628399482482205 , test loss:  0.08949838222213381\n",
      "Epoch:  36820 , step:  36820 , train loss:  0.058622016573525206 , test loss:  0.08949509927966887\n",
      "Epoch:  36830 , step:  36830 , train loss:  0.05861563617458221 , test loss:  0.08949181833757294\n",
      "Epoch:  36840 , step:  36840 , train loss:  0.058609258283982085 , test loss:  0.08948853939433671\n",
      "Epoch:  36850 , step:  36850 , train loss:  0.058602882900055275 , test loss:  0.08948526244845216\n",
      "Epoch:  36860 , step:  36860 , train loss:  0.058596510021133794 , test loss:  0.0894819874984132\n",
      "Epoch:  36870 , step:  36870 , train loss:  0.05859013964555125 , test loss:  0.089478714542715\n",
      "Epoch:  36880 , step:  36880 , train loss:  0.05858377177164277 , test loss:  0.08947544357985421\n",
      "Epoch:  36890 , step:  36890 , train loss:  0.05857740639774505 , test loss:  0.08947217460832887\n",
      "Epoch:  36900 , step:  36900 , train loss:  0.05857104352219641 , test loss:  0.08946890762663906\n",
      "Epoch:  36910 , step:  36910 , train loss:  0.0585646831433366 , test loss:  0.08946564263328562\n",
      "Epoch:  36920 , step:  36920 , train loss:  0.05855832525950707 , test loss:  0.08946237962677162\n",
      "Epoch:  36930 , step:  36930 , train loss:  0.05855196986905077 , test loss:  0.08945911860560121\n",
      "Epoch:  36940 , step:  36940 , train loss:  0.05854561697031213 , test loss:  0.08945585956828\n",
      "Epoch:  36950 , step:  36950 , train loss:  0.05853926656163726 , test loss:  0.08945260251331522\n",
      "Epoch:  36960 , step:  36960 , train loss:  0.05853291864137372 , test loss:  0.08944934743921552\n",
      "Epoch:  36970 , step:  36970 , train loss:  0.05852657320787068 , test loss:  0.08944609434449127\n",
      "Epoch:  36980 , step:  36980 , train loss:  0.058520230259478785 , test loss:  0.08944284322765406\n",
      "Epoch:  36990 , step:  36990 , train loss:  0.05851388979455033 , test loss:  0.08943959408721709\n",
      "Epoch:  37000 , step:  37000 , train loss:  0.05850755181143905 , test loss:  0.08943634692169485\n",
      "Epoch:  37010 , step:  37010 , train loss:  0.058501216308500255 , test loss:  0.08943310172960381\n",
      "Epoch:  37020 , step:  37020 , train loss:  0.058494883284090825 , test loss:  0.08942985850946125\n",
      "Epoch:  37030 , step:  37030 , train loss:  0.05848855273656914 , test loss:  0.08942661725978654\n",
      "Epoch:  37040 , step:  37040 , train loss:  0.05848222466429513 , test loss:  0.08942337797910016\n",
      "Epoch:  37050 , step:  37050 , train loss:  0.058475899065630235 , test loss:  0.08942014066592431\n",
      "Epoch:  37060 , step:  37060 , train loss:  0.05846957593893748 , test loss:  0.08941690531878219\n",
      "Epoch:  37070 , step:  37070 , train loss:  0.05846325528258134 , test loss:  0.08941367193619913\n",
      "Epoch:  37080 , step:  37080 , train loss:  0.058456937094927885 , test loss:  0.08941044051670152\n",
      "Epoch:  37090 , step:  37090 , train loss:  0.058450621374344695 , test loss:  0.08940721105881719\n",
      "Epoch:  37100 , step:  37100 , train loss:  0.05844430811920083 , test loss:  0.08940398356107566\n",
      "Epoch:  37110 , step:  37110 , train loss:  0.058437997327866954 , test loss:  0.08940075802200788\n",
      "Epoch:  37120 , step:  37120 , train loss:  0.058431688998715155 , test loss:  0.08939753444014603\n",
      "Epoch:  37130 , step:  37130 , train loss:  0.058425383130119114 , test loss:  0.08939431281402407\n",
      "Epoch:  37140 , step:  37140 , train loss:  0.05841907972045402 , test loss:  0.08939109314217722\n",
      "Epoch:  37150 , step:  37150 , train loss:  0.058412778768096515 , test loss:  0.0893878754231421\n",
      "Epoch:  37160 , step:  37160 , train loss:  0.05840648027142481 , test loss:  0.08938465965545703\n",
      "Epoch:  37170 , step:  37170 , train loss:  0.05840018422881865 , test loss:  0.0893814458376616\n",
      "Epoch:  37180 , step:  37180 , train loss:  0.05839389063865921 , test loss:  0.0893782339682967\n",
      "Epoch:  37190 , step:  37190 , train loss:  0.058387599499329215 , test loss:  0.08937502404590512\n",
      "Epoch:  37200 , step:  37200 , train loss:  0.05838131080921294 , test loss:  0.08937181606903091\n",
      "Epoch:  37210 , step:  37210 , train loss:  0.058375024566696083 , test loss:  0.08936861003621925\n",
      "Epoch:  37220 , step:  37220 , train loss:  0.058368740770165864 , test loss:  0.08936540594601712\n",
      "Epoch:  37230 , step:  37230 , train loss:  0.058362459418011055 , test loss:  0.08936220379697289\n",
      "Epoch:  37240 , step:  37240 , train loss:  0.05835618050862189 , test loss:  0.08935900358763627\n",
      "Epoch:  37250 , step:  37250 , train loss:  0.05834990404039006 , test loss:  0.08935580531655848\n",
      "Epoch:  37260 , step:  37260 , train loss:  0.058343630011708836 , test loss:  0.08935260898229223\n",
      "Epoch:  37270 , step:  37270 , train loss:  0.0583373584209729 , test loss:  0.08934941458339146\n",
      "Epoch:  37280 , step:  37280 , train loss:  0.058331089266578484 , test loss:  0.08934622211841178\n",
      "Epoch:  37290 , step:  37290 , train loss:  0.05832482254692328 , test loss:  0.08934303158591016\n",
      "Epoch:  37300 , step:  37300 , train loss:  0.05831855826040647 , test loss:  0.08933984298444494\n",
      "Epoch:  37310 , step:  37310 , train loss:  0.05831229640542874 , test loss:  0.08933665631257588\n",
      "Epoch:  37320 , step:  37320 , train loss:  0.05830603698039222 , test loss:  0.08933347156886434\n",
      "Epoch:  37330 , step:  37330 , train loss:  0.05829977998370056 , test loss:  0.08933028875187289\n",
      "Epoch:  37340 , step:  37340 , train loss:  0.05829352541375889 , test loss:  0.08932710786016554\n",
      "Epoch:  37350 , step:  37350 , train loss:  0.058287273268973774 , test loss:  0.08932392889230792\n",
      "Epoch:  37360 , step:  37360 , train loss:  0.05828102354775332 , test loss:  0.08932075184686691\n",
      "Epoch:  37370 , step:  37370 , train loss:  0.05827477624850707 , test loss:  0.08931757672241089\n",
      "Epoch:  37380 , step:  37380 , train loss:  0.05826853136964603 , test loss:  0.08931440351750969\n",
      "Epoch:  37390 , step:  37390 , train loss:  0.058262288909582696 , test loss:  0.08931123223073431\n",
      "Epoch:  37400 , step:  37400 , train loss:  0.05825604886673106 , test loss:  0.08930806286065755\n",
      "Epoch:  37410 , step:  37410 , train loss:  0.058249811239506497 , test loss:  0.08930489540585325\n",
      "Epoch:  37420 , step:  37420 , train loss:  0.05824357602632596 , test loss:  0.0893017298648968\n",
      "Epoch:  37430 , step:  37430 , train loss:  0.058237343225607786 , test loss:  0.0892985662363652\n",
      "Epoch:  37440 , step:  37440 , train loss:  0.05823111283577181 , test loss:  0.08929540451883641\n",
      "Epoch:  37450 , step:  37450 , train loss:  0.05822488485523931 , test loss:  0.08929224471089027\n",
      "Epoch:  37460 , step:  37460 , train loss:  0.05821865928243299 , test loss:  0.08928908681110781\n",
      "Epoch:  37470 , step:  37470 , train loss:  0.05821243611577712 , test loss:  0.08928593081807144\n",
      "Epoch:  37480 , step:  37480 , train loss:  0.05820621535369734 , test loss:  0.089282776730365\n",
      "Epoch:  37490 , step:  37490 , train loss:  0.05819999699462078 , test loss:  0.08927962454657383\n",
      "Epoch:  37500 , step:  37500 , train loss:  0.05819378103697593 , test loss:  0.08927647426528434\n",
      "Epoch:  37510 , step:  37510 , train loss:  0.05818756747919289 , test loss:  0.08927332588508467\n",
      "Epoch:  37520 , step:  37520 , train loss:  0.05818135631970309 , test loss:  0.08927017940456432\n",
      "Epoch:  37530 , step:  37530 , train loss:  0.05817514755693945 , test loss:  0.08926703482231392\n",
      "Epoch:  37540 , step:  37540 , train loss:  0.0581689411893363 , test loss:  0.08926389213692588\n",
      "Epoch:  37550 , step:  37550 , train loss:  0.05816273721532946 , test loss:  0.08926075134699363\n",
      "Epoch:  37560 , step:  37560 , train loss:  0.058156535633356196 , test loss:  0.08925761245111222\n",
      "Epoch:  37570 , step:  37570 , train loss:  0.058150336441855154 , test loss:  0.08925447544787794\n",
      "Epoch:  37580 , step:  37580 , train loss:  0.058144139639266484 , test loss:  0.08925134033588852\n",
      "Epoch:  37590 , step:  37590 , train loss:  0.05813794522403172 , test loss:  0.08924820711374316\n",
      "Epoch:  37600 , step:  37600 , train loss:  0.05813175319459387 , test loss:  0.08924507578004232\n",
      "Epoch:  37610 , step:  37610 , train loss:  0.05812556354939739 , test loss:  0.08924194633338796\n",
      "Epoch:  37620 , step:  37620 , train loss:  0.05811937628688809 , test loss:  0.08923881877238322\n",
      "Epoch:  37630 , step:  37630 , train loss:  0.058113191405513284 , test loss:  0.08923569309563277\n",
      "Epoch:  37640 , step:  37640 , train loss:  0.058107008903721696 , test loss:  0.08923256930174255\n",
      "Epoch:  37650 , step:  37650 , train loss:  0.05810082877996345 , test loss:  0.08922944738932\n",
      "Epoch:  37660 , step:  37660 , train loss:  0.058094651032690146 , test loss:  0.08922632735697393\n",
      "Epoch:  37670 , step:  37670 , train loss:  0.05808847566035475 , test loss:  0.08922320920331422\n",
      "Epoch:  37680 , step:  37680 , train loss:  0.05808230266141168 , test loss:  0.08922009292695257\n",
      "Epoch:  37690 , step:  37690 , train loss:  0.0580761320343168 , test loss:  0.08921697852650168\n",
      "Epoch:  37700 , step:  37700 , train loss:  0.05806996377752733 , test loss:  0.08921386600057586\n",
      "Epoch:  37710 , step:  37710 , train loss:  0.058063797889501945 , test loss:  0.08921075534779041\n",
      "Epoch:  37720 , step:  37720 , train loss:  0.05805763436870073 , test loss:  0.08920764656676256\n",
      "Epoch:  37730 , step:  37730 , train loss:  0.05805147321358522 , test loss:  0.08920453965611054\n",
      "Epoch:  37740 , step:  37740 , train loss:  0.05804531442261828 , test loss:  0.08920143461445379\n",
      "Epoch:  37750 , step:  37750 , train loss:  0.05803915799426422 , test loss:  0.0891983314404134\n",
      "Epoch:  37760 , step:  37760 , train loss:  0.05803300392698883 , test loss:  0.08919523013261174\n",
      "Epoch:  37770 , step:  37770 , train loss:  0.05802685221925918 , test loss:  0.0891921306896725\n",
      "Epoch:  37780 , step:  37780 , train loss:  0.05802070286954384 , test loss:  0.08918903311022057\n",
      "Epoch:  37790 , step:  37790 , train loss:  0.058014555876312775 , test loss:  0.08918593739288257\n",
      "Epoch:  37800 , step:  37800 , train loss:  0.05800841123803728 , test loss:  0.08918284353628601\n",
      "Epoch:  37810 , step:  37810 , train loss:  0.05800226895319011 , test loss:  0.0891797515390601\n",
      "Epoch:  37820 , step:  37820 , train loss:  0.057996129020245445 , test loss:  0.08917666139983517\n",
      "Epoch:  37830 , step:  37830 , train loss:  0.057989991437678794 , test loss:  0.08917357311724304\n",
      "Epoch:  37840 , step:  37840 , train loss:  0.05798385620396707 , test loss:  0.08917048668991685\n",
      "Epoch:  37850 , step:  37850 , train loss:  0.057977723317588666 , test loss:  0.08916740211649077\n",
      "Epoch:  37860 , step:  37860 , train loss:  0.05797159277702324 , test loss:  0.08916431939560092\n",
      "Epoch:  37870 , step:  37870 , train loss:  0.05796546458075193 , test loss:  0.08916123852588412\n",
      "Epoch:  37880 , step:  37880 , train loss:  0.05795933872725723 , test loss:  0.08915815950597894\n",
      "Epoch:  37890 , step:  37890 , train loss:  0.05795321521502303 , test loss:  0.08915508233452514\n",
      "Epoch:  37900 , step:  37900 , train loss:  0.05794709404253458 , test loss:  0.08915200701016371\n",
      "Epoch:  37910 , step:  37910 , train loss:  0.05794097520827856 , test loss:  0.08914893353153727\n",
      "Epoch:  37920 , step:  37920 , train loss:  0.057934858710743 , test loss:  0.08914586189728944\n",
      "Epoch:  37930 , step:  37930 , train loss:  0.05792874454841728 , test loss:  0.08914279210606527\n",
      "Epoch:  37940 , step:  37940 , train loss:  0.05792263271979225 , test loss:  0.08913972415651132\n",
      "Epoch:  37950 , step:  37950 , train loss:  0.057916523223360036 , test loss:  0.08913665804727501\n",
      "Epoch:  37960 , step:  37960 , train loss:  0.0579104160576142 , test loss:  0.08913359377700551\n",
      "Epoch:  37970 , step:  37970 , train loss:  0.057904311221049665 , test loss:  0.0891305313443533\n",
      "Epoch:  37980 , step:  37980 , train loss:  0.057898208712162745 , test loss:  0.08912747074796996\n",
      "Epoch:  37990 , step:  37990 , train loss:  0.05789210852945108 , test loss:  0.08912441198650847\n",
      "Epoch:  38000 , step:  38000 , train loss:  0.057886010671413676 , test loss:  0.08912135505862323\n",
      "Epoch:  38010 , step:  38010 , train loss:  0.05787991513655099 , test loss:  0.08911829996296959\n",
      "Epoch:  38020 , step:  38020 , train loss:  0.057873821923364745 , test loss:  0.08911524669820466\n",
      "Epoch:  38030 , step:  38030 , train loss:  0.057867731030358124 , test loss:  0.0891121952629867\n",
      "Epoch:  38040 , step:  38040 , train loss:  0.057861642456035534 , test loss:  0.08910914565597508\n",
      "Epoch:  38050 , step:  38050 , train loss:  0.05785555619890292 , test loss:  0.08910609787583078\n",
      "Epoch:  38060 , step:  38060 , train loss:  0.05784947225746742 , test loss:  0.08910305192121577\n",
      "Epoch:  38070 , step:  38070 , train loss:  0.057843390630237634 , test loss:  0.08910000779079383\n",
      "Epoch:  38080 , step:  38080 , train loss:  0.057837311315723505 , test loss:  0.08909696548322933\n",
      "Epoch:  38090 , step:  38090 , train loss:  0.05783123431243629 , test loss:  0.08909392499718856\n",
      "Epoch:  38100 , step:  38100 , train loss:  0.05782515961888863 , test loss:  0.08909088633133871\n",
      "Epoch:  38110 , step:  38110 , train loss:  0.057819087233594524 , test loss:  0.08908784948434845\n",
      "Epoch:  38120 , step:  38120 , train loss:  0.05781301715506927 , test loss:  0.08908481445488793\n",
      "Epoch:  38130 , step:  38130 , train loss:  0.057806949381829634 , test loss:  0.08908178124162801\n",
      "Epoch:  38140 , step:  38140 , train loss:  0.05780088391239356 , test loss:  0.08907874984324145\n",
      "Epoch:  38150 , step:  38150 , train loss:  0.05779482074528043 , test loss:  0.08907572025840199\n",
      "Epoch:  38160 , step:  38160 , train loss:  0.05778875987901103 , test loss:  0.08907269248578475\n",
      "Epoch:  38170 , step:  38170 , train loss:  0.05778270131210734 , test loss:  0.08906966652406596\n",
      "Epoch:  38180 , step:  38180 , train loss:  0.057776645043092816 , test loss:  0.08906664237192348\n",
      "Epoch:  38190 , step:  38190 , train loss:  0.057770591070492185 , test loss:  0.08906362002803625\n",
      "Epoch:  38200 , step:  38200 , train loss:  0.05776453939283152 , test loss:  0.08906059949108434\n",
      "Epoch:  38210 , step:  38210 , train loss:  0.05775849000863826 , test loss:  0.08905758075974925\n",
      "Epoch:  38220 , step:  38220 , train loss:  0.057752442916441105 , test loss:  0.08905456383271396\n",
      "Epoch:  38230 , step:  38230 , train loss:  0.057746398114770156 , test loss:  0.08905154870866235\n",
      "Epoch:  38240 , step:  38240 , train loss:  0.057740355602156845 , test loss:  0.08904853538627995\n",
      "Epoch:  38250 , step:  38250 , train loss:  0.05773431537713389 , test loss:  0.08904552386425311\n",
      "Epoch:  38260 , step:  38260 , train loss:  0.05772827743823536 , test loss:  0.08904251414126996\n",
      "Epoch:  38270 , step:  38270 , train loss:  0.05772224178399663 , test loss:  0.08903950621601942\n",
      "Epoch:  38280 , step:  38280 , train loss:  0.05771620841295448 , test loss:  0.08903650008719215\n",
      "Epoch:  38290 , step:  38290 , train loss:  0.0577101773236469 , test loss:  0.08903349575347973\n",
      "Epoch:  38300 , step:  38300 , train loss:  0.05770414851461327 , test loss:  0.08903049321357501\n",
      "Epoch:  38310 , step:  38310 , train loss:  0.05769812198439428 , test loss:  0.0890274924661724\n",
      "Epoch:  38320 , step:  38320 , train loss:  0.05769209773153193 , test loss:  0.08902449350996736\n",
      "Epoch:  38330 , step:  38330 , train loss:  0.057686075754569514 , test loss:  0.08902149634365651\n",
      "Epoch:  38340 , step:  38340 , train loss:  0.057680056052051724 , test loss:  0.08901850096593793\n",
      "Epoch:  38350 , step:  38350 , train loss:  0.05767403862252449 , test loss:  0.08901550737551092\n",
      "Epoch:  38360 , step:  38360 , train loss:  0.057668023464535034 , test loss:  0.08901251557107595\n",
      "Epoch:  38370 , step:  38370 , train loss:  0.05766201057663197 , test loss:  0.08900952555133484\n",
      "Epoch:  38380 , step:  38380 , train loss:  0.057655999957365194 , test loss:  0.08900653731499056\n",
      "Epoch:  38390 , step:  38390 , train loss:  0.05764999160528585 , test loss:  0.08900355086074747\n",
      "Epoch:  38400 , step:  38400 , train loss:  0.05764398551894647 , test loss:  0.08900056618731103\n",
      "Epoch:  38410 , step:  38410 , train loss:  0.057637981696900875 , test loss:  0.0889975832933881\n",
      "Epoch:  38420 , step:  38420 , train loss:  0.05763198013770411 , test loss:  0.08899460217768669\n",
      "Epoch:  38430 , step:  38430 , train loss:  0.05762598083991262 , test loss:  0.08899162283891585\n",
      "Epoch:  38440 , step:  38440 , train loss:  0.057619983802084114 , test loss:  0.08898864527578647\n",
      "Epoch:  38450 , step:  38450 , train loss:  0.0576139890227776 , test loss:  0.08898566948701009\n",
      "Epoch:  38460 , step:  38460 , train loss:  0.057607996500553385 , test loss:  0.08898269547129983\n",
      "Epoch:  38470 , step:  38470 , train loss:  0.057602006233973053 , test loss:  0.08897972322736995\n",
      "Epoch:  38480 , step:  38480 , train loss:  0.05759601822159951 , test loss:  0.08897675275393582\n",
      "Epoch:  38490 , step:  38490 , train loss:  0.05759003246199694 , test loss:  0.08897378404971423\n",
      "Epoch:  38500 , step:  38500 , train loss:  0.05758404895373082 , test loss:  0.08897081711342315\n",
      "Epoch:  38510 , step:  38510 , train loss:  0.05757806769536794 , test loss:  0.08896785194378173\n",
      "Epoch:  38520 , step:  38520 , train loss:  0.05757208868547632 , test loss:  0.0889648885395105\n",
      "Epoch:  38530 , step:  38530 , train loss:  0.05756611192262534 , test loss:  0.08896192689933095\n",
      "Epoch:  38540 , step:  38540 , train loss:  0.05756013740538558 , test loss:  0.08895896702196607\n",
      "Epoch:  38550 , step:  38550 , train loss:  0.05755416513232903 , test loss:  0.08895600890614011\n",
      "Epoch:  38560 , step:  38560 , train loss:  0.05754819510202881 , test loss:  0.08895305255057827\n",
      "Epoch:  38570 , step:  38570 , train loss:  0.05754222731305945 , test loss:  0.08895009795400703\n",
      "Epoch:  38580 , step:  38580 , train loss:  0.05753626176399667 , test loss:  0.08894714511515436\n",
      "Epoch:  38590 , step:  38590 , train loss:  0.05753029845341751 , test loss:  0.08894419403274925\n",
      "Epoch:  38600 , step:  38600 , train loss:  0.05752433737990032 , test loss:  0.08894124470552181\n",
      "Epoch:  38610 , step:  38610 , train loss:  0.057518378542024635 , test loss:  0.08893829713220351\n",
      "Epoch:  38620 , step:  38620 , train loss:  0.057512421938371354 , test loss:  0.08893535131152729\n",
      "Epoch:  38630 , step:  38630 , train loss:  0.05750646756752256 , test loss:  0.08893240724222667\n",
      "Epoch:  38640 , step:  38640 , train loss:  0.05750051542806168 , test loss:  0.08892946492303702\n",
      "Epoch:  38650 , step:  38650 , train loss:  0.05749456551857338 , test loss:  0.08892652435269464\n",
      "Epoch:  38660 , step:  38660 , train loss:  0.05748861783764365 , test loss:  0.08892358552993694\n",
      "Epoch:  38670 , step:  38670 , train loss:  0.05748267238385962 , test loss:  0.0889206484535028\n",
      "Epoch:  38680 , step:  38680 , train loss:  0.057476729155809786 , test loss:  0.08891771312213195\n",
      "Epoch:  38690 , step:  38690 , train loss:  0.057470788152083896 , test loss:  0.08891477953456595\n",
      "Epoch:  38700 , step:  38700 , train loss:  0.057464849371272914 , test loss:  0.08891184768954681\n",
      "Epoch:  38710 , step:  38710 , train loss:  0.057458912811969146 , test loss:  0.08890891758581825\n",
      "Epoch:  38720 , step:  38720 , train loss:  0.05745297847276605 , test loss:  0.08890598922212498\n",
      "Epoch:  38730 , step:  38730 , train loss:  0.057447046352258405 , test loss:  0.08890306259721306\n",
      "Epoch:  38740 , step:  38740 , train loss:  0.057441116449042276 , test loss:  0.08890013770982969\n",
      "Epoch:  38750 , step:  38750 , train loss:  0.05743518876171492 , test loss:  0.08889721455872307\n",
      "Epoch:  38760 , step:  38760 , train loss:  0.05742926328887485 , test loss:  0.08889429314264302\n",
      "Epoch:  38770 , step:  38770 , train loss:  0.0574233400291219 , test loss:  0.08889137346034008\n",
      "Epoch:  38780 , step:  38780 , train loss:  0.05741741898105706 , test loss:  0.0888884555105665\n",
      "Epoch:  38790 , step:  38790 , train loss:  0.05741150014328266 , test loss:  0.08888553929207524\n",
      "Epoch:  38800 , step:  38800 , train loss:  0.0574055835144022 , test loss:  0.08888262480362069\n",
      "Epoch:  38810 , step:  38810 , train loss:  0.05739966909302048 , test loss:  0.08887971204395848\n",
      "Epoch:  38820 , step:  38820 , train loss:  0.05739375687774348 , test loss:  0.08887680101184516\n",
      "Epoch:  38830 , step:  38830 , train loss:  0.05738784686717853 , test loss:  0.0888738917060388\n",
      "Epoch:  38840 , step:  38840 , train loss:  0.057381939059934085 , test loss:  0.0888709841252985\n",
      "Epoch:  38850 , step:  38850 , train loss:  0.057376033454619925 , test loss:  0.08886807826838455\n",
      "Epoch:  38860 , step:  38860 , train loss:  0.057370130049847035 , test loss:  0.08886517413405841\n",
      "Epoch:  38870 , step:  38870 , train loss:  0.05736422884422761 , test loss:  0.08886227172108278\n",
      "Epoch:  38880 , step:  38880 , train loss:  0.05735832983637515 , test loss:  0.08885937102822147\n",
      "Epoch:  38890 , step:  38890 , train loss:  0.057352433024904316 , test loss:  0.08885647205423952\n",
      "Epoch:  38900 , step:  38900 , train loss:  0.05734653840843105 , test loss:  0.08885357479790298\n",
      "Epoch:  38910 , step:  38910 , train loss:  0.05734064598557251 , test loss:  0.08885067925797943\n",
      "Epoch:  38920 , step:  38920 , train loss:  0.057334755754947085 , test loss:  0.08884778543323733\n",
      "Epoch:  38930 , step:  38930 , train loss:  0.057328867715174384 , test loss:  0.08884489332244645\n",
      "Epoch:  38940 , step:  38940 , train loss:  0.05732298186487527 , test loss:  0.08884200292437772\n",
      "Epoch:  38950 , step:  38950 , train loss:  0.057317098202671835 , test loss:  0.08883911423780318\n",
      "Epoch:  38960 , step:  38960 , train loss:  0.057311216727187315 , test loss:  0.08883622726149601\n",
      "Epoch:  38970 , step:  38970 , train loss:  0.05730533743704628 , test loss:  0.08883334199423071\n",
      "Epoch:  38980 , step:  38980 , train loss:  0.05729946033087446 , test loss:  0.0888304584347827\n",
      "Epoch:  38990 , step:  38990 , train loss:  0.057293585407298814 , test loss:  0.08882757658192879\n",
      "Epoch:  39000 , step:  39000 , train loss:  0.05728771266494752 , test loss:  0.08882469643444693\n",
      "Epoch:  39010 , step:  39010 , train loss:  0.05728184210244996 , test loss:  0.08882181799111634\n",
      "Epoch:  39020 , step:  39020 , train loss:  0.05727597371843679 , test loss:  0.08881894125071689\n",
      "Epoch:  39030 , step:  39030 , train loss:  0.057270107511539804 , test loss:  0.08881606621203024\n",
      "Epoch:  39040 , step:  39040 , train loss:  0.05726424348039206 , test loss:  0.08881319287383865\n",
      "Epoch:  39050 , step:  39050 , train loss:  0.057258381623627806 , test loss:  0.0888103212349262\n",
      "Epoch:  39060 , step:  39060 , train loss:  0.057252521939882534 , test loss:  0.08880745129407758\n",
      "Epoch:  39070 , step:  39070 , train loss:  0.05724666442779288 , test loss:  0.08880458305007889\n",
      "Epoch:  39080 , step:  39080 , train loss:  0.057240809085996755 , test loss:  0.08880171650171705\n",
      "Epoch:  39090 , step:  39090 , train loss:  0.057234955913133226 , test loss:  0.0887988516477805\n",
      "Epoch:  39100 , step:  39100 , train loss:  0.057229104907842596 , test loss:  0.08879598848705879\n",
      "Epoch:  39110 , step:  39110 , train loss:  0.057223256068766395 , test loss:  0.08879312701834258\n",
      "Epoch:  39120 , step:  39120 , train loss:  0.05721740939454731 , test loss:  0.08879026724042333\n",
      "Epoch:  39130 , step:  39130 , train loss:  0.05721156488382919 , test loss:  0.08878740915209433\n",
      "Epoch:  39140 , step:  39140 , train loss:  0.0572057225352572 , test loss:  0.08878455275214944\n",
      "Epoch:  39150 , step:  39150 , train loss:  0.05719988234747763 , test loss:  0.08878169803938392\n",
      "Epoch:  39160 , step:  39160 , train loss:  0.057194044319137945 , test loss:  0.088778845012594\n",
      "Epoch:  39170 , step:  39170 , train loss:  0.057188208448886876 , test loss:  0.0887759936705773\n",
      "Epoch:  39180 , step:  39180 , train loss:  0.057182374735374304 , test loss:  0.08877314401213246\n",
      "Epoch:  39190 , step:  39190 , train loss:  0.057176543177251266 , test loss:  0.08877029603605915\n",
      "Epoch:  39200 , step:  39200 , train loss:  0.057170713773170075 , test loss:  0.08876744974115816\n",
      "Epoch:  39210 , step:  39210 , train loss:  0.05716488652178419 , test loss:  0.08876460512623174\n",
      "Epoch:  39220 , step:  39220 , train loss:  0.057159061421748245 , test loss:  0.08876176219008294\n",
      "Epoch:  39230 , step:  39230 , train loss:  0.0571532384717181 , test loss:  0.08875892093151606\n",
      "Epoch:  39240 , step:  39240 , train loss:  0.05714741767035074 , test loss:  0.08875608134933663\n",
      "Epoch:  39250 , step:  39250 , train loss:  0.05714159901630443 , test loss:  0.08875324344235103\n",
      "Epoch:  39260 , step:  39260 , train loss:  0.05713578250823851 , test loss:  0.08875040720936712\n",
      "Epoch:  39270 , step:  39270 , train loss:  0.057129968144813574 , test loss:  0.08874757264919361\n",
      "Epoch:  39280 , step:  39280 , train loss:  0.05712415592469138 , test loss:  0.08874473976064064\n",
      "Epoch:  39290 , step:  39290 , train loss:  0.05711834584653486 , test loss:  0.088741908542519\n",
      "Epoch:  39300 , step:  39300 , train loss:  0.057112537909008124 , test loss:  0.08873907899364093\n",
      "Epoch:  39310 , step:  39310 , train loss:  0.05710673211077646 , test loss:  0.08873625111281996\n",
      "Epoch:  39320 , step:  39320 , train loss:  0.057100928450506315 , test loss:  0.08873342489887026\n",
      "Epoch:  39330 , step:  39330 , train loss:  0.05709512692686536 , test loss:  0.08873060035060766\n",
      "Epoch:  39340 , step:  39340 , train loss:  0.0570893275385224 , test loss:  0.08872777746684861\n",
      "Epoch:  39350 , step:  39350 , train loss:  0.05708353028414738 , test loss:  0.08872495624641098\n",
      "Epoch:  39360 , step:  39360 , train loss:  0.057077735162411505 , test loss:  0.08872213668811367\n",
      "Epoch:  39370 , step:  39370 , train loss:  0.057071942171987064 , test loss:  0.08871931879077667\n",
      "Epoch:  39380 , step:  39380 , train loss:  0.057066151311547526 , test loss:  0.08871650255322112\n",
      "Epoch:  39390 , step:  39390 , train loss:  0.057060362579767585 , test loss:  0.0887136879742692\n",
      "Epoch:  39400 , step:  39400 , train loss:  0.057054575975323055 , test loss:  0.08871087505274447\n",
      "Epoch:  39410 , step:  39410 , train loss:  0.05704879149689088 , test loss:  0.08870806378747122\n",
      "Epoch:  39420 , step:  39420 , train loss:  0.057043009143149234 , test loss:  0.08870525417727507\n",
      "Epoch:  39430 , step:  39430 , train loss:  0.05703722891277744 , test loss:  0.0887024462209827\n",
      "Epoch:  39440 , step:  39440 , train loss:  0.05703145080445592 , test loss:  0.08869963991742173\n",
      "Epoch:  39450 , step:  39450 , train loss:  0.05702567481686633 , test loss:  0.0886968352654213\n",
      "Epoch:  39460 , step:  39460 , train loss:  0.057019900948691436 , test loss:  0.08869403226381133\n",
      "Epoch:  39470 , step:  39470 , train loss:  0.05701412919861518 , test loss:  0.08869123091142285\n",
      "Epoch:  39480 , step:  39480 , train loss:  0.057008359565322644 , test loss:  0.08868843120708794\n",
      "Epoch:  39490 , step:  39490 , train loss:  0.05700259204750005 , test loss:  0.08868563314964018\n",
      "Epoch:  39500 , step:  39500 , train loss:  0.05699682664383481 , test loss:  0.0886828367379136\n",
      "Epoch:  39510 , step:  39510 , train loss:  0.05699106335301548 , test loss:  0.08868004197074388\n",
      "Epoch:  39520 , step:  39520 , train loss:  0.056985302173731746 , test loss:  0.08867724884696758\n",
      "Epoch:  39530 , step:  39530 , train loss:  0.05697954310467442 , test loss:  0.08867445736542244\n",
      "Epoch:  39540 , step:  39540 , train loss:  0.05697378614453556 , test loss:  0.08867166752494703\n",
      "Epoch:  39550 , step:  39550 , train loss:  0.05696803129200821 , test loss:  0.08866887932438147\n",
      "Epoch:  39560 , step:  39560 , train loss:  0.05696227854578671 , test loss:  0.0886660927625664\n",
      "Epoch:  39570 , step:  39570 , train loss:  0.056956527904566456 , test loss:  0.0886633078383441\n",
      "Epoch:  39580 , step:  39580 , train loss:  0.05695077936704399 , test loss:  0.08866052455055737\n",
      "Epoch:  39590 , step:  39590 , train loss:  0.05694503293191704 , test loss:  0.08865774289805073\n",
      "Epoch:  39600 , step:  39600 , train loss:  0.05693928859788442 , test loss:  0.08865496287966937\n",
      "Epoch:  39610 , step:  39610 , train loss:  0.056933546363646136 , test loss:  0.08865218449425961\n",
      "Epoch:  39620 , step:  39620 , train loss:  0.05692780622790329 , test loss:  0.08864940774066903\n",
      "Epoch:  39630 , step:  39630 , train loss:  0.05692206818935811 , test loss:  0.08864663261774594\n",
      "Epoch:  39640 , step:  39640 , train loss:  0.056916332246713996 , test loss:  0.08864385912433993\n",
      "Epoch:  39650 , step:  39650 , train loss:  0.056910598398675426 , test loss:  0.08864108725930193\n",
      "Epoch:  39660 , step:  39660 , train loss:  0.056904866643948074 , test loss:  0.08863831702148356\n",
      "Epoch:  39670 , step:  39670 , train loss:  0.05689913698123871 , test loss:  0.08863554840973775\n",
      "Epoch:  39680 , step:  39680 , train loss:  0.05689340940925527 , test loss:  0.08863278142291839\n",
      "Epoch:  39690 , step:  39690 , train loss:  0.05688768392670668 , test loss:  0.08863001605988038\n",
      "Epoch:  39700 , step:  39700 , train loss:  0.05688196053230318 , test loss:  0.08862725231947972\n",
      "Epoch:  39710 , step:  39710 , train loss:  0.05687623922475602 , test loss:  0.0886244902005738\n",
      "Epoch:  39720 , step:  39720 , train loss:  0.056870520002777626 , test loss:  0.08862172970202051\n",
      "Epoch:  39730 , step:  39730 , train loss:  0.056864802865081505 , test loss:  0.08861897082267942\n",
      "Epoch:  39740 , step:  39740 , train loss:  0.056859087810382306 , test loss:  0.08861621356141071\n",
      "Epoch:  39750 , step:  39750 , train loss:  0.0568533748373958 , test loss:  0.08861345791707578\n",
      "Epoch:  39760 , step:  39760 , train loss:  0.05684766394483885 , test loss:  0.08861070388853712\n",
      "Epoch:  39770 , step:  39770 , train loss:  0.056841955131429485 , test loss:  0.0886079514746582\n",
      "Epoch:  39780 , step:  39780 , train loss:  0.056836248395886774 , test loss:  0.08860520067430364\n",
      "Epoch:  39790 , step:  39790 , train loss:  0.05683054373693101 , test loss:  0.08860245148633908\n",
      "Epoch:  39800 , step:  39800 , train loss:  0.05682484115328349 , test loss:  0.0885997039096313\n",
      "Epoch:  39810 , step:  39810 , train loss:  0.0568191406436667 , test loss:  0.08859695794304803\n",
      "Epoch:  39820 , step:  39820 , train loss:  0.05681344220680418 , test loss:  0.088594213585458\n",
      "Epoch:  39830 , step:  39830 , train loss:  0.056807745841420614 , test loss:  0.08859147083573128\n",
      "Epoch:  39840 , step:  39840 , train loss:  0.05680205154624182 , test loss:  0.08858872969273857\n",
      "Epoch:  39850 , step:  39850 , train loss:  0.05679635931999463 , test loss:  0.08858599015535185\n",
      "Epoch:  39860 , step:  39860 , train loss:  0.056790669161407105 , test loss:  0.08858325222244444\n",
      "Epoch:  39870 , step:  39870 , train loss:  0.0567849810692083 , test loss:  0.0885805158928903\n",
      "Epoch:  39880 , step:  39880 , train loss:  0.05677929504212846 , test loss:  0.08857778116556447\n",
      "Epoch:  39890 , step:  39890 , train loss:  0.05677361107889883 , test loss:  0.0885750480393432\n",
      "Epoch:  39900 , step:  39900 , train loss:  0.056767929178251894 , test loss:  0.08857231651310361\n",
      "Epoch:  39910 , step:  39910 , train loss:  0.05676224933892111 , test loss:  0.08856958658572422\n",
      "Epoch:  39920 , step:  39920 , train loss:  0.05675657155964111 , test loss:  0.08856685825608432\n",
      "Epoch:  39930 , step:  39930 , train loss:  0.056750895839147594 , test loss:  0.08856413152306403\n",
      "Epoch:  39940 , step:  39940 , train loss:  0.05674522217617737 , test loss:  0.0885614063855449\n",
      "Epoch:  39950 , step:  39950 , train loss:  0.056739550569468306 , test loss:  0.08855868284240946\n",
      "Epoch:  39960 , step:  39960 , train loss:  0.05673388101775939 , test loss:  0.08855596089254107\n",
      "Epoch:  39970 , step:  39970 , train loss:  0.056728213519790745 , test loss:  0.08855324053482432\n",
      "Epoch:  39980 , step:  39980 , train loss:  0.05672254807430351 , test loss:  0.08855052176814479\n",
      "Epoch:  39990 , step:  39990 , train loss:  0.05671688468003994 , test loss:  0.08854780459138913\n",
      "Epoch:  40000 , step:  40000 , train loss:  0.05671122333574339 , test loss:  0.0885450890034448\n",
      "Epoch:  40010 , step:  40010 , train loss:  0.05670556404015836 , test loss:  0.08854237500320059\n",
      "Epoch:  40020 , step:  40020 , train loss:  0.05669990679203028 , test loss:  0.08853966258954624\n",
      "Epoch:  40030 , step:  40030 , train loss:  0.056694251590105806 , test loss:  0.0885369517613724\n",
      "Epoch:  40040 , step:  40040 , train loss:  0.05668859843313263 , test loss:  0.08853424251757093\n",
      "Epoch:  40050 , step:  40050 , train loss:  0.05668294731985951 , test loss:  0.08853153485703455\n",
      "Epoch:  40060 , step:  40060 , train loss:  0.05667729824903633 , test loss:  0.088528828778657\n",
      "Epoch:  40070 , step:  40070 , train loss:  0.056671651219414004 , test loss:  0.08852612428133332\n",
      "Epoch:  40080 , step:  40080 , train loss:  0.05666600622974456 , test loss:  0.08852342136395923\n",
      "Epoch:  40090 , step:  40090 , train loss:  0.056660363278781094 , test loss:  0.08852072002543175\n",
      "Epoch:  40100 , step:  40100 , train loss:  0.056654722365277764 , test loss:  0.08851802026464868\n",
      "Epoch:  40110 , step:  40110 , train loss:  0.05664908348798983 , test loss:  0.08851532208050901\n",
      "Epoch:  40120 , step:  40120 , train loss:  0.05664344664567362 , test loss:  0.08851262547191266\n",
      "Epoch:  40130 , step:  40130 , train loss:  0.0566378118370865 , test loss:  0.0885099304377607\n",
      "Epoch:  40140 , step:  40140 , train loss:  0.05663217906098694 , test loss:  0.08850723697695505\n",
      "Epoch:  40150 , step:  40150 , train loss:  0.0566265483161345 , test loss:  0.08850454508839879\n",
      "Epoch:  40160 , step:  40160 , train loss:  0.05662091960128975 , test loss:  0.08850185477099594\n",
      "Epoch:  40170 , step:  40170 , train loss:  0.0566152929152144 , test loss:  0.08849916602365149\n",
      "Epoch:  40180 , step:  40180 , train loss:  0.05660966825667117 , test loss:  0.08849647884527155\n",
      "Epoch:  40190 , step:  40190 , train loss:  0.05660404562442387 , test loss:  0.0884937932347631\n",
      "Epoch:  40200 , step:  40200 , train loss:  0.05659842501723737 , test loss:  0.08849110919103446\n",
      "Epoch:  40210 , step:  40210 , train loss:  0.056592806433877596 , test loss:  0.08848842671299446\n",
      "Epoch:  40220 , step:  40220 , train loss:  0.05658718987311155 , test loss:  0.08848574579955325\n",
      "Epoch:  40230 , step:  40230 , train loss:  0.05658157533370731 , test loss:  0.08848306644962213\n",
      "Epoch:  40240 , step:  40240 , train loss:  0.05657596281443395 , test loss:  0.08848038866211308\n",
      "Epoch:  40250 , step:  40250 , train loss:  0.05657035231406169 , test loss:  0.08847771243593905\n",
      "Epoch:  40260 , step:  40260 , train loss:  0.05656474383136173 , test loss:  0.08847503777001454\n",
      "Epoch:  40270 , step:  40270 , train loss:  0.056559137365106385 , test loss:  0.08847236466325437\n",
      "Epoch:  40280 , step:  40280 , train loss:  0.05655353291406899 , test loss:  0.08846969311457488\n",
      "Epoch:  40290 , step:  40290 , train loss:  0.05654793047702398 , test loss:  0.08846702312289297\n",
      "Epoch:  40300 , step:  40300 , train loss:  0.056542330052746725 , test loss:  0.088464354687127\n",
      "Epoch:  40310 , step:  40310 , train loss:  0.05653673164001379 , test loss:  0.08846168780619604\n",
      "Epoch:  40320 , step:  40320 , train loss:  0.05653113523760272 , test loss:  0.08845902247902018\n",
      "Epoch:  40330 , step:  40330 , train loss:  0.05652554084429213 , test loss:  0.08845635870452043\n",
      "Epoch:  40340 , step:  40340 , train loss:  0.056519948458861664 , test loss:  0.08845369648161916\n",
      "Epoch:  40350 , step:  40350 , train loss:  0.05651435808009201 , test loss:  0.08845103580923921\n",
      "Epoch:  40360 , step:  40360 , train loss:  0.056508769706764925 , test loss:  0.08844837668630494\n",
      "Epoch:  40370 , step:  40370 , train loss:  0.05650318333766319 , test loss:  0.08844571911174137\n",
      "Epoch:  40380 , step:  40380 , train loss:  0.056497598971570646 , test loss:  0.08844306308447443\n",
      "Epoch:  40390 , step:  40390 , train loss:  0.05649201660727218 , test loss:  0.08844040860343143\n",
      "Epoch:  40400 , step:  40400 , train loss:  0.05648643624355369 , test loss:  0.08843775566754025\n",
      "Epoch:  40410 , step:  40410 , train loss:  0.056480857879202165 , test loss:  0.08843510427572994\n",
      "Epoch:  40420 , step:  40420 , train loss:  0.05647528151300557 , test loss:  0.08843245442693058\n",
      "Epoch:  40430 , step:  40430 , train loss:  0.05646970714375296 , test loss:  0.08842980612007333\n",
      "Epoch:  40440 , step:  40440 , train loss:  0.056464134770234375 , test loss:  0.0884271593540901\n",
      "Epoch:  40450 , step:  40450 , train loss:  0.05645856439124096 , test loss:  0.0884245141279136\n",
      "Epoch:  40460 , step:  40460 , train loss:  0.05645299600556485 , test loss:  0.08842187044047824\n",
      "Epoch:  40470 , step:  40470 , train loss:  0.056447429611999216 , test loss:  0.08841922829071867\n",
      "Epoch:  40480 , step:  40480 , train loss:  0.056441865209338256 , test loss:  0.08841658767757109\n",
      "Epoch:  40490 , step:  40490 , train loss:  0.05643630279637721 , test loss:  0.08841394859997215\n",
      "Epoch:  40500 , step:  40500 , train loss:  0.05643074237191233 , test loss:  0.08841131105685972\n",
      "Epoch:  40510 , step:  40510 , train loss:  0.05642518393474096 , test loss:  0.08840867504717281\n",
      "Epoch:  40520 , step:  40520 , train loss:  0.05641962748366141 , test loss:  0.08840604056985124\n",
      "Epoch:  40530 , step:  40530 , train loss:  0.056414073017472983 , test loss:  0.08840340762383576\n",
      "Epoch:  40540 , step:  40540 , train loss:  0.05640852053497612 , test loss:  0.08840077620806813\n",
      "Epoch:  40550 , step:  40550 , train loss:  0.05640297003497216 , test loss:  0.08839814632149123\n",
      "Epoch:  40560 , step:  40560 , train loss:  0.05639742151626358 , test loss:  0.08839551796304858\n",
      "Epoch:  40570 , step:  40570 , train loss:  0.056391874977653814 , test loss:  0.08839289113168496\n",
      "Epoch:  40580 , step:  40580 , train loss:  0.05638633041794731 , test loss:  0.08839026582634606\n",
      "Epoch:  40590 , step:  40590 , train loss:  0.056380787835949575 , test loss:  0.08838764204597857\n",
      "Epoch:  40600 , step:  40600 , train loss:  0.056375247230467086 , test loss:  0.08838501978952995\n",
      "Epoch:  40610 , step:  40610 , train loss:  0.056369708600307364 , test loss:  0.08838239905594861\n",
      "Epoch:  40620 , step:  40620 , train loss:  0.056364171944279035 , test loss:  0.08837977984418445\n",
      "Epoch:  40630 , step:  40630 , train loss:  0.056358637261191516 , test loss:  0.08837716215318764\n",
      "Epoch:  40640 , step:  40640 , train loss:  0.05635310454985547 , test loss:  0.08837454598190976\n",
      "Epoch:  40650 , step:  40650 , train loss:  0.056347573809082445 , test loss:  0.08837193132930309\n",
      "Epoch:  40660 , step:  40660 , train loss:  0.05634204503768503 , test loss:  0.08836931819432113\n",
      "Epoch:  40670 , step:  40670 , train loss:  0.05633651823447686 , test loss:  0.08836670657591814\n",
      "Epoch:  40680 , step:  40680 , train loss:  0.05633099339827251 , test loss:  0.08836409647304937\n",
      "Epoch:  40690 , step:  40690 , train loss:  0.05632547052788763 , test loss:  0.08836148788467099\n",
      "Epoch:  40700 , step:  40700 , train loss:  0.05631994962213883 , test loss:  0.0883588808097403\n",
      "Epoch:  40710 , step:  40710 , train loss:  0.056314430679843736 , test loss:  0.08835627524721544\n",
      "Epoch:  40720 , step:  40720 , train loss:  0.05630891369982104 , test loss:  0.08835367119605542\n",
      "Epoch:  40730 , step:  40730 , train loss:  0.05630339868089034 , test loss:  0.08835106865522041\n",
      "Epoch:  40740 , step:  40740 , train loss:  0.05629788562187228 , test loss:  0.08834846762367107\n",
      "Epoch:  40750 , step:  40750 , train loss:  0.056292374521588556 , test loss:  0.08834586810036975\n",
      "Epoch:  40760 , step:  40760 , train loss:  0.05628686537886176 , test loss:  0.088343270084279\n",
      "Epoch:  40770 , step:  40770 , train loss:  0.056281358192515604 , test loss:  0.08834067357436305\n",
      "Epoch:  40780 , step:  40780 , train loss:  0.05627585296137468 , test loss:  0.0883380785695863\n",
      "Epoch:  40790 , step:  40790 , train loss:  0.05627034968426469 , test loss:  0.08833548506891466\n",
      "Epoch:  40800 , step:  40800 , train loss:  0.056264848360012214 , test loss:  0.08833289307131484\n",
      "Epoch:  40810 , step:  40810 , train loss:  0.05625934898744496 , test loss:  0.08833030257575425\n",
      "Epoch:  40820 , step:  40820 , train loss:  0.056253851565391484 , test loss:  0.08832771358120159\n",
      "Epoch:  40830 , step:  40830 , train loss:  0.05624835609268151 , test loss:  0.08832512608662636\n",
      "Epoch:  40840 , step:  40840 , train loss:  0.05624286256814559 , test loss:  0.08832254009099898\n",
      "Epoch:  40850 , step:  40850 , train loss:  0.05623737099061533 , test loss:  0.08831995559329081\n",
      "Epoch:  40860 , step:  40860 , train loss:  0.05623188135892334 , test loss:  0.08831737259247417\n",
      "Epoch:  40870 , step:  40870 , train loss:  0.05622639367190321 , test loss:  0.0883147910875222\n",
      "Epoch:  40880 , step:  40880 , train loss:  0.056220907928389525 , test loss:  0.08831221107740928\n",
      "Epoch:  40890 , step:  40890 , train loss:  0.05621542412721785 , test loss:  0.08830963256111038\n",
      "Epoch:  40900 , step:  40900 , train loss:  0.056209942267224734 , test loss:  0.08830705553760156\n",
      "Epoch:  40910 , step:  40910 , train loss:  0.05620446234724768 , test loss:  0.08830448000585985\n",
      "Epoch:  40920 , step:  40920 , train loss:  0.05619898436612523 , test loss:  0.08830190596486323\n",
      "Epoch:  40930 , step:  40930 , train loss:  0.05619350832269688 , test loss:  0.08829933341359038\n",
      "Epoch:  40940 , step:  40940 , train loss:  0.05618803421580311 , test loss:  0.08829676235102112\n",
      "Epoch:  40950 , step:  40950 , train loss:  0.05618256204428539 , test loss:  0.08829419277613623\n",
      "Epoch:  40960 , step:  40960 , train loss:  0.05617709180698614 , test loss:  0.08829162468791728\n",
      "Epoch:  40970 , step:  40970 , train loss:  0.0561716235027488 , test loss:  0.08828905808534675\n",
      "Epoch:  40980 , step:  40980 , train loss:  0.05616615713041774 , test loss:  0.08828649296740813\n",
      "Epoch:  40990 , step:  40990 , train loss:  0.056160692688838346 , test loss:  0.08828392933308599\n",
      "Epoch:  41000 , step:  41000 , train loss:  0.056155230176856946 , test loss:  0.08828136718136552\n",
      "Epoch:  41010 , step:  41010 , train loss:  0.05614976959332089 , test loss:  0.0882788065112329\n",
      "Epoch:  41020 , step:  41020 , train loss:  0.056144310937078445 , test loss:  0.08827624732167527\n",
      "Epoch:  41030 , step:  41030 , train loss:  0.05613885420697888 , test loss:  0.08827368961168089\n",
      "Epoch:  41040 , step:  41040 , train loss:  0.05613339940187243 , test loss:  0.08827113338023879\n",
      "Epoch:  41050 , step:  41050 , train loss:  0.056127946520610324 , test loss:  0.08826857862633869\n",
      "Epoch:  41060 , step:  41060 , train loss:  0.05612249556204469 , test loss:  0.08826602534897146\n",
      "Epoch:  41070 , step:  41070 , train loss:  0.056117046525028726 , test loss:  0.088263473547129\n",
      "Epoch:  41080 , step:  41080 , train loss:  0.05611159940841647 , test loss:  0.08826092321980378\n",
      "Epoch:  41090 , step:  41090 , train loss:  0.05610615421106306 , test loss:  0.08825837436598936\n",
      "Epoch:  41100 , step:  41100 , train loss:  0.0561007109318245 , test loss:  0.08825582698468049\n",
      "Epoch:  41110 , step:  41110 , train loss:  0.05609526956955777 , test loss:  0.08825328107487235\n",
      "Epoch:  41120 , step:  41120 , train loss:  0.05608983012312086 , test loss:  0.08825073663556132\n",
      "Epoch:  41130 , step:  41130 , train loss:  0.05608439259137273 , test loss:  0.0882481936657447\n",
      "Epoch:  41140 , step:  41140 , train loss:  0.056078956973173194 , test loss:  0.08824565216442036\n",
      "Epoch:  41150 , step:  41150 , train loss:  0.05607352326738314 , test loss:  0.0882431121305877\n",
      "Epoch:  41160 , step:  41160 , train loss:  0.05606809147286434 , test loss:  0.08824057356324633\n",
      "Epoch:  41170 , step:  41170 , train loss:  0.05606266158847957 , test loss:  0.08823803646139723\n",
      "Epoch:  41180 , step:  41180 , train loss:  0.05605723361309257 , test loss:  0.08823550082404216\n",
      "Epoch:  41190 , step:  41190 , train loss:  0.05605180754556794 , test loss:  0.08823296665018364\n",
      "Epoch:  41200 , step:  41200 , train loss:  0.056046383384771366 , test loss:  0.08823043393882544\n",
      "Epoch:  41210 , step:  41210 , train loss:  0.05604096112956939 , test loss:  0.08822790268897181\n",
      "Epoch:  41220 , step:  41220 , train loss:  0.056035540778829565 , test loss:  0.08822537289962831\n",
      "Epoch:  41230 , step:  41230 , train loss:  0.05603012233142036 , test loss:  0.08822284456980097\n",
      "Epoch:  41240 , step:  41240 , train loss:  0.056024705786211174 , test loss:  0.08822031769849713\n",
      "Epoch:  41250 , step:  41250 , train loss:  0.05601929114207244 , test loss:  0.08821779228472465\n",
      "Epoch:  41260 , step:  41260 , train loss:  0.05601387839787545 , test loss:  0.08821526832749264\n",
      "Epoch:  41270 , step:  41270 , train loss:  0.05600846755249247 , test loss:  0.08821274582581085\n",
      "Epoch:  41280 , step:  41280 , train loss:  0.05600305860479674 , test loss:  0.0882102247786901\n",
      "Epoch:  41290 , step:  41290 , train loss:  0.05599765155366242 , test loss:  0.08820770518514195\n",
      "Epoch:  41300 , step:  41300 , train loss:  0.05599224639796459 , test loss:  0.08820518704417886\n",
      "Epoch:  41310 , step:  41310 , train loss:  0.05598684313657932 , test loss:  0.08820267035481433\n",
      "Epoch:  41320 , step:  41320 , train loss:  0.05598144176838362 , test loss:  0.08820015511606262\n",
      "Epoch:  41330 , step:  41330 , train loss:  0.055976042292255376 , test loss:  0.08819764132693883\n",
      "Epoch:  41340 , step:  41340 , train loss:  0.05597064470707348 , test loss:  0.08819512898645913\n",
      "Epoch:  41350 , step:  41350 , train loss:  0.05596524901171774 , test loss:  0.08819261809364053\n",
      "Epoch:  41360 , step:  41360 , train loss:  0.05595985520506892 , test loss:  0.08819010864750071\n",
      "Epoch:  41370 , step:  41370 , train loss:  0.05595446328600869 , test loss:  0.08818760064705854\n",
      "Epoch:  41380 , step:  41380 , train loss:  0.055949073253419676 , test loss:  0.08818509409133354\n",
      "Epoch:  41390 , step:  41390 , train loss:  0.0559436851061854 , test loss:  0.08818258897934621\n",
      "Epoch:  41400 , step:  41400 , train loss:  0.05593829884319038 , test loss:  0.08818008531011808\n",
      "Epoch:  41410 , step:  41410 , train loss:  0.05593291446332006 , test loss:  0.08817758308267108\n",
      "Epoch:  41420 , step:  41420 , train loss:  0.05592753196546072 , test loss:  0.0881750822960288\n",
      "Epoch:  41430 , step:  41430 , train loss:  0.0559221513484997 , test loss:  0.08817258294921478\n",
      "Epoch:  41440 , step:  41440 , train loss:  0.05591677261132517 , test loss:  0.08817008504125427\n",
      "Epoch:  41450 , step:  41450 , train loss:  0.055911395752826325 , test loss:  0.08816758857117288\n",
      "Epoch:  41460 , step:  41460 , train loss:  0.055906020771893196 , test loss:  0.08816509353799731\n",
      "Epoch:  41470 , step:  41470 , train loss:  0.055900647667416786 , test loss:  0.08816259994075505\n",
      "Epoch:  41480 , step:  41480 , train loss:  0.055895276438288996 , test loss:  0.08816010777847447\n",
      "Epoch:  41490 , step:  41490 , train loss:  0.0558899070834027 , test loss:  0.08815761705018502\n",
      "Epoch:  41500 , step:  41500 , train loss:  0.05588453960165162 , test loss:  0.08815512775491652\n",
      "Epoch:  41510 , step:  41510 , train loss:  0.05587917399193049 , test loss:  0.08815263989170014\n",
      "Epoch:  41520 , step:  41520 , train loss:  0.05587381025313491 , test loss:  0.0881501534595678\n",
      "Epoch:  41530 , step:  41530 , train loss:  0.055868448384161405 , test loss:  0.08814766845755234\n",
      "Epoch:  41540 , step:  41540 , train loss:  0.05586308838390741 , test loss:  0.08814518488468703\n",
      "Epoch:  41550 , step:  41550 , train loss:  0.055857730251271345 , test loss:  0.08814270274000668\n",
      "Epoch:  41560 , step:  41560 , train loss:  0.05585237398515248 , test loss:  0.08814022202254657\n",
      "Epoch:  41570 , step:  41570 , train loss:  0.05584701958445096 , test loss:  0.08813774273134274\n",
      "Epoch:  41580 , step:  41580 , train loss:  0.05584166704806797 , test loss:  0.08813526486543255\n",
      "Epoch:  41590 , step:  41590 , train loss:  0.05583631637490552 , test loss:  0.08813278842385375\n",
      "Epoch:  41600 , step:  41600 , train loss:  0.05583096756386659 , test loss:  0.08813031340564527\n",
      "Epoch:  41610 , step:  41610 , train loss:  0.05582562061385499 , test loss:  0.08812783980984666\n",
      "Epoch:  41620 , step:  41620 , train loss:  0.05582027552377554 , test loss:  0.08812536763549839\n",
      "Epoch:  41630 , step:  41630 , train loss:  0.05581493229253388 , test loss:  0.08812289688164195\n",
      "Epoch:  41640 , step:  41640 , train loss:  0.05580959091903662 , test loss:  0.08812042754731958\n",
      "Epoch:  41650 , step:  41650 , train loss:  0.05580425140219127 , test loss:  0.0881179596315744\n",
      "Epoch:  41660 , step:  41660 , train loss:  0.05579891374090623 , test loss:  0.08811549313345023\n",
      "Epoch:  41670 , step:  41670 , train loss:  0.05579357793409083 , test loss:  0.08811302805199209\n",
      "Epoch:  41680 , step:  41680 , train loss:  0.0557882439806553 , test loss:  0.08811056438624554\n",
      "Epoch:  41690 , step:  41690 , train loss:  0.055782911879510726 , test loss:  0.08810810213525697\n",
      "Epoch:  41700 , step:  41700 , train loss:  0.05577758162956919 , test loss:  0.08810564129807402\n",
      "Epoch:  41710 , step:  41710 , train loss:  0.05577225322974357 , test loss:  0.0881031818737447\n",
      "Epoch:  41720 , step:  41720 , train loss:  0.05576692667894773 , test loss:  0.08810072386131816\n",
      "Epoch:  41730 , step:  41730 , train loss:  0.055761601976096434 , test loss:  0.08809826725984432\n",
      "Epoch:  41740 , step:  41740 , train loss:  0.05575627912010526 , test loss:  0.08809581206837391\n",
      "Epoch:  41750 , step:  41750 , train loss:  0.055750958109890815 , test loss:  0.08809335828595867\n",
      "Epoch:  41760 , step:  41760 , train loss:  0.055745638944370464 , test loss:  0.08809090591165088\n",
      "Epoch:  41770 , step:  41770 , train loss:  0.055740321622462584 , test loss:  0.08808845494450405\n",
      "Epoch:  41780 , step:  41780 , train loss:  0.05573500614308636 , test loss:  0.08808600538357217\n",
      "Epoch:  41790 , step:  41790 , train loss:  0.05572969250516196 , test loss:  0.08808355722791031\n",
      "Epoch:  41800 , step:  41800 , train loss:  0.05572438070761037 , test loss:  0.08808111047657433\n",
      "Epoch:  41810 , step:  41810 , train loss:  0.05571907074935353 , test loss:  0.08807866512862096\n",
      "Epoch:  41820 , step:  41820 , train loss:  0.05571376262931423 , test loss:  0.08807622118310755\n",
      "Epoch:  41830 , step:  41830 , train loss:  0.05570845634641615 , test loss:  0.08807377863909258\n",
      "Epoch:  41840 , step:  41840 , train loss:  0.055703151899583865 , test loss:  0.08807133749563528\n",
      "Epoch:  41850 , step:  41850 , train loss:  0.05569784928774289 , test loss:  0.08806889775179569\n",
      "Epoch:  41860 , step:  41860 , train loss:  0.05569254850981954 , test loss:  0.08806645940663467\n",
      "Epoch:  41870 , step:  41870 , train loss:  0.055687249564741095 , test loss:  0.08806402245921403\n",
      "Epoch:  41880 , step:  41880 , train loss:  0.0556819524514357 , test loss:  0.08806158690859613\n",
      "Epoch:  41890 , step:  41890 , train loss:  0.05567665716883235 , test loss:  0.08805915275384442\n",
      "Epoch:  41900 , step:  41900 , train loss:  0.055671363715860955 , test loss:  0.08805671999402319\n",
      "Epoch:  41910 , step:  41910 , train loss:  0.05566607209145233 , test loss:  0.08805428862819759\n",
      "Epoch:  41920 , step:  41920 , train loss:  0.05566078229453813 , test loss:  0.08805185865543336\n",
      "Epoch:  41930 , step:  41930 , train loss:  0.05565549432405092 , test loss:  0.08804943007479718\n",
      "Epoch:  41940 , step:  41940 , train loss:  0.05565020817892412 , test loss:  0.0880470028853567\n",
      "Epoch:  41950 , step:  41950 , train loss:  0.05564492385809206 , test loss:  0.08804457708618019\n",
      "Epoch:  41960 , step:  41960 , train loss:  0.05563964136048994 , test loss:  0.08804215267633693\n",
      "Epoch:  41970 , step:  41970 , train loss:  0.05563436068505382 , test loss:  0.08803972965489704\n",
      "Epoch:  41980 , step:  41980 , train loss:  0.05562908183072067 , test loss:  0.08803730802093122\n",
      "Epoch:  41990 , step:  41990 , train loss:  0.05562380479642829 , test loss:  0.0880348877735113\n",
      "Epoch:  42000 , step:  42000 , train loss:  0.055618529581115426 , test loss:  0.08803246891170978\n",
      "Epoch:  42010 , step:  42010 , train loss:  0.05561325618372162 , test loss:  0.08803005143459983\n",
      "Epoch:  42020 , step:  42020 , train loss:  0.055607984603187305 , test loss:  0.08802763534125553\n",
      "Epoch:  42030 , step:  42030 , train loss:  0.055602714838453866 , test loss:  0.08802522063075209\n",
      "Epoch:  42040 , step:  42040 , train loss:  0.05559744688846346 , test loss:  0.08802280730216525\n",
      "Epoch:  42050 , step:  42050 , train loss:  0.05559218075215916 , test loss:  0.08802039535457175\n",
      "Epoch:  42060 , step:  42060 , train loss:  0.05558691642848489 , test loss:  0.08801798478704878\n",
      "Epoch:  42070 , step:  42070 , train loss:  0.05558165391638545 , test loss:  0.08801557559867466\n",
      "Epoch:  42080 , step:  42080 , train loss:  0.05557639321480655 , test loss:  0.08801316778852858\n",
      "Epoch:  42090 , step:  42090 , train loss:  0.055571134322694685 , test loss:  0.08801076135569035\n",
      "Epoch:  42100 , step:  42100 , train loss:  0.05556587723899732 , test loss:  0.08800835629924054\n",
      "Epoch:  42110 , step:  42110 , train loss:  0.055560621962662676 , test loss:  0.08800595261826079\n",
      "Epoch:  42120 , step:  42120 , train loss:  0.055555368492639895 , test loss:  0.08800355031183354\n",
      "Epoch:  42130 , step:  42130 , train loss:  0.05555011682787897 , test loss:  0.0880011493790418\n",
      "Epoch:  42140 , step:  42140 , train loss:  0.0555448669673308 , test loss:  0.08799874981896963\n",
      "Epoch:  42150 , step:  42150 , train loss:  0.05553961890994709 , test loss:  0.08799635163070164\n",
      "Epoch:  42160 , step:  42160 , train loss:  0.0555343726546804 , test loss:  0.08799395481332346\n",
      "Epoch:  42170 , step:  42170 , train loss:  0.0555291282004842 , test loss:  0.08799155936592153\n",
      "Epoch:  42180 , step:  42180 , train loss:  0.055523885546312775 , test loss:  0.08798916528758297\n",
      "Epoch:  42190 , step:  42190 , train loss:  0.0555186446911213 , test loss:  0.08798677257739583\n",
      "Epoch:  42200 , step:  42200 , train loss:  0.055513405633865774 , test loss:  0.08798438123444892\n",
      "Epoch:  42210 , step:  42210 , train loss:  0.055508168373503086 , test loss:  0.0879819912578318\n",
      "Epoch:  42220 , step:  42220 , train loss:  0.05550293290899096 , test loss:  0.08797960264663489\n",
      "Epoch:  42230 , step:  42230 , train loss:  0.05549769923928798 , test loss:  0.08797721539994952\n",
      "Epoch:  42240 , step:  42240 , train loss:  0.05549246736335359 , test loss:  0.08797482951686747\n",
      "Epoch:  42250 , step:  42250 , train loss:  0.05548723728014803 , test loss:  0.08797244499648187\n",
      "Epoch:  42260 , step:  42260 , train loss:  0.05548200898863252 , test loss:  0.08797006183788617\n",
      "Epoch:  42270 , step:  42270 , train loss:  0.05547678248776898 , test loss:  0.08796768004017486\n",
      "Epoch:  42280 , step:  42280 , train loss:  0.055471557776520304 , test loss:  0.08796529960244309\n",
      "Epoch:  42290 , step:  42290 , train loss:  0.05546633485385014 , test loss:  0.08796292052378686\n",
      "Epoch:  42300 , step:  42300 , train loss:  0.05546111371872304 , test loss:  0.0879605428033031\n",
      "Epoch:  42310 , step:  42310 , train loss:  0.05545589437010439 , test loss:  0.08795816644008939\n",
      "Epoch:  42320 , step:  42320 , train loss:  0.055450676806960426 , test loss:  0.08795579143324406\n",
      "Epoch:  42330 , step:  42330 , train loss:  0.05544546102825823 , test loss:  0.08795341778186647\n",
      "Epoch:  42340 , step:  42340 , train loss:  0.055440247032965705 , test loss:  0.08795104548505649\n",
      "Epoch:  42350 , step:  42350 , train loss:  0.055435034820051604 , test loss:  0.08794867454191493\n",
      "Epoch:  42360 , step:  42360 , train loss:  0.05542982438848557 , test loss:  0.08794630495154343\n",
      "Epoch:  42370 , step:  42370 , train loss:  0.055424615737238045 , test loss:  0.08794393671304442\n",
      "Epoch:  42380 , step:  42380 , train loss:  0.055419408865280274 , test loss:  0.08794156982552095\n",
      "Epoch:  42390 , step:  42390 , train loss:  0.05541420377158442 , test loss:  0.0879392042880771\n",
      "Epoch:  42400 , step:  42400 , train loss:  0.055409000455123446 , test loss:  0.08793684009981738\n",
      "Epoch:  42410 , step:  42410 , train loss:  0.055403798914871176 , test loss:  0.08793447725984747\n",
      "Epoch:  42420 , step:  42420 , train loss:  0.05539859914980222 , test loss:  0.08793211576727386\n",
      "Epoch:  42430 , step:  42430 , train loss:  0.05539340115889209 , test loss:  0.08792975562120343\n",
      "Epoch:  42440 , step:  42440 , train loss:  0.055388204941117083 , test loss:  0.08792739682074417\n",
      "Epoch:  42450 , step:  42450 , train loss:  0.05538301049545435 , test loss:  0.08792503936500468\n",
      "Epoch:  42460 , step:  42460 , train loss:  0.055377817820881865 , test loss:  0.0879226832530945\n",
      "Epoch:  42470 , step:  42470 , train loss:  0.05537262691637851 , test loss:  0.0879203284841239\n",
      "Epoch:  42480 , step:  42480 , train loss:  0.055367437780923824 , test loss:  0.08791797505720383\n",
      "Epoch:  42490 , step:  42490 , train loss:  0.05536225041349837 , test loss:  0.08791562297144606\n",
      "Epoch:  42500 , step:  42500 , train loss:  0.05535706481308344 , test loss:  0.08791327222596322\n",
      "Epoch:  42510 , step:  42510 , train loss:  0.055351880978661144 , test loss:  0.08791092281986879\n",
      "Epoch:  42520 , step:  42520 , train loss:  0.05534669890921448 , test loss:  0.08790857475227688\n",
      "Epoch:  42530 , step:  42530 , train loss:  0.05534151860372725 , test loss:  0.08790622802230207\n",
      "Epoch:  42540 , step:  42540 , train loss:  0.05533634006118405 , test loss:  0.0879038826290605\n",
      "Epoch:  42550 , step:  42550 , train loss:  0.05533116328057035 , test loss:  0.08790153857166842\n",
      "Epoch:  42560 , step:  42560 , train loss:  0.05532598826087243 , test loss:  0.08789919584924327\n",
      "Epoch:  42570 , step:  42570 , train loss:  0.055320815001077356 , test loss:  0.08789685446090284\n",
      "Epoch:  42580 , step:  42580 , train loss:  0.055315643500173095 , test loss:  0.08789451440576622\n",
      "Epoch:  42590 , step:  42590 , train loss:  0.05531047375714835 , test loss:  0.08789217568295252\n",
      "Epoch:  42600 , step:  42600 , train loss:  0.05530530577099269 , test loss:  0.08788983829158255\n",
      "Epoch:  42610 , step:  42610 , train loss:  0.05530013954069653 , test loss:  0.08788750223077706\n",
      "Epoch:  42620 , step:  42620 , train loss:  0.05529497506525105 , test loss:  0.0878851674996581\n",
      "Epoch:  42630 , step:  42630 , train loss:  0.05528981234364828 , test loss:  0.08788283409734836\n",
      "Epoch:  42640 , step:  42640 , train loss:  0.05528465137488107 , test loss:  0.08788050202297111\n",
      "Epoch:  42650 , step:  42650 , train loss:  0.05527949215794307 , test loss:  0.0878781712756507\n",
      "Epoch:  42660 , step:  42660 , train loss:  0.05527433469182878 , test loss:  0.08787584185451205\n",
      "Epoch:  42670 , step:  42670 , train loss:  0.055269178975533476 , test loss:  0.08787351375868087\n",
      "Epoch:  42680 , step:  42680 , train loss:  0.05526402500805327 , test loss:  0.08787118698728362\n",
      "Epoch:  42690 , step:  42690 , train loss:  0.055258872788385066 , test loss:  0.08786886153944748\n",
      "Epoch:  42700 , step:  42700 , train loss:  0.05525372231552662 , test loss:  0.08786653741430049\n",
      "Epoch:  42710 , step:  42710 , train loss:  0.055248573588476464 , test loss:  0.08786421461097149\n",
      "Epoch:  42720 , step:  42720 , train loss:  0.05524342660623398 , test loss:  0.08786189312858998\n",
      "Epoch:  42730 , step:  42730 , train loss:  0.055238281367799305 , test loss:  0.08785957296628617\n",
      "Epoch:  42740 , step:  42740 , train loss:  0.05523313787217343 , test loss:  0.08785725412319129\n",
      "Epoch:  42750 , step:  42750 , train loss:  0.055227996118358144 , test loss:  0.08785493659843707\n",
      "Epoch:  42760 , step:  42760 , train loss:  0.05522285610535606 , test loss:  0.08785262039115606\n",
      "Epoch:  42770 , step:  42770 , train loss:  0.05521771783217055 , test loss:  0.08785030550048152\n",
      "Epoch:  42780 , step:  42780 , train loss:  0.05521258129780584 , test loss:  0.08784799192554774\n",
      "Epoch:  42790 , step:  42790 , train loss:  0.055207446501266924 , test loss:  0.08784567966548928\n",
      "Epoch:  42800 , step:  42800 , train loss:  0.055202313441559646 , test loss:  0.087843368719442\n",
      "Epoch:  42810 , step:  42810 , train loss:  0.055197182117690605 , test loss:  0.08784105908654206\n",
      "Epoch:  42820 , step:  42820 , train loss:  0.05519205252866724 , test loss:  0.08783875076592668\n",
      "Epoch:  42830 , step:  42830 , train loss:  0.05518692467349778 , test loss:  0.08783644375673363\n",
      "Epoch:  42840 , step:  42840 , train loss:  0.05518179855119125 , test loss:  0.0878341380581017\n",
      "Epoch:  42850 , step:  42850 , train loss:  0.055176674160757465 , test loss:  0.08783183366917004\n",
      "Epoch:  42860 , step:  42860 , train loss:  0.05517155150120707 , test loss:  0.08782953058907879\n",
      "Epoch:  42870 , step:  42870 , train loss:  0.05516643057155149 , test loss:  0.08782722881696876\n",
      "Epoch:  42880 , step:  42880 , train loss:  0.05516131137080295 , test loss:  0.0878249283519818\n",
      "Epoch:  42890 , step:  42890 , train loss:  0.05515619389797445 , test loss:  0.0878226291932601\n",
      "Epoch:  42900 , step:  42900 , train loss:  0.055151078152079855 , test loss:  0.08782033133994666\n",
      "Epoch:  42910 , step:  42910 , train loss:  0.05514596413213374 , test loss:  0.0878180347911855\n",
      "Epoch:  42920 , step:  42920 , train loss:  0.055140851837151506 , test loss:  0.08781573954612111\n",
      "Epoch:  42930 , step:  42930 , train loss:  0.05513574126614938 , test loss:  0.08781344560389884\n",
      "Epoch:  42940 , step:  42940 , train loss:  0.05513063241814435 , test loss:  0.08781115296366494\n",
      "Epoch:  42950 , step:  42950 , train loss:  0.05512552529215421 , test loss:  0.08780886162456585\n",
      "Epoch:  42960 , step:  42960 , train loss:  0.055120419887197535 , test loss:  0.08780657158574949\n",
      "Epoch:  42970 , step:  42970 , train loss:  0.05511531620229367 , test loss:  0.08780428284636384\n",
      "Epoch:  42980 , step:  42980 , train loss:  0.055110214236462805 , test loss:  0.08780199540555826\n",
      "Epoch:  42990 , step:  42990 , train loss:  0.05510511398872588 , test loss:  0.08779970926248248\n",
      "Epoch:  43000 , step:  43000 , train loss:  0.055100015458104605 , test loss:  0.08779742441628682\n",
      "Epoch:  43010 , step:  43010 , train loss:  0.055094918643621524 , test loss:  0.08779514086612261\n",
      "Epoch:  43020 , step:  43020 , train loss:  0.05508982354429995 , test loss:  0.08779285861114207\n",
      "Epoch:  43030 , step:  43030 , train loss:  0.05508473015916397 , test loss:  0.0877905776504976\n",
      "Epoch:  43040 , step:  43040 , train loss:  0.05507963848723846 , test loss:  0.08778829798334292\n",
      "Epoch:  43050 , step:  43050 , train loss:  0.05507454852754909 , test loss:  0.08778601960883209\n",
      "Epoch:  43060 , step:  43060 , train loss:  0.0550694602791223 , test loss:  0.08778374252611999\n",
      "Epoch:  43070 , step:  43070 , train loss:  0.05506437374098533 , test loss:  0.0877814667343626\n",
      "Epoch:  43080 , step:  43080 , train loss:  0.05505928891216617 , test loss:  0.08777919223271592\n",
      "Epoch:  43090 , step:  43090 , train loss:  0.0550542057916936 , test loss:  0.08777691902033748\n",
      "Epoch:  43100 , step:  43100 , train loss:  0.055049124378597235 , test loss:  0.0877746470963848\n",
      "Epoch:  43110 , step:  43110 , train loss:  0.055044044671907405 , test loss:  0.08777237646001675\n",
      "Epoch:  43120 , step:  43120 , train loss:  0.05503896667065523 , test loss:  0.08777010711039239\n",
      "Epoch:  43130 , step:  43130 , train loss:  0.05503389037387261 , test loss:  0.08776783904667196\n",
      "Epoch:  43140 , step:  43140 , train loss:  0.05502881578059224 , test loss:  0.08776557226801608\n",
      "Epoch:  43150 , step:  43150 , train loss:  0.055023742889847564 , test loss:  0.08776330677358628\n",
      "Epoch:  43160 , step:  43160 , train loss:  0.0550186717006728 , test loss:  0.08776104256254504\n",
      "Epoch:  43170 , step:  43170 , train loss:  0.05501360221210301 , test loss:  0.08775877963405508\n",
      "Epoch:  43180 , step:  43180 , train loss:  0.05500853442317394 , test loss:  0.08775651798728014\n",
      "Epoch:  43190 , step:  43190 , train loss:  0.05500346833292212 , test loss:  0.08775425762138463\n",
      "Epoch:  43200 , step:  43200 , train loss:  0.05499840394038493 , test loss:  0.08775199853553364\n",
      "Epoch:  43210 , step:  43210 , train loss:  0.054993341244600404 , test loss:  0.08774974072889312\n",
      "Epoch:  43220 , step:  43220 , train loss:  0.05498828024460743 , test loss:  0.08774748420062944\n",
      "Epoch:  43230 , step:  43230 , train loss:  0.054983220939445694 , test loss:  0.08774522894990999\n",
      "Epoch:  43240 , step:  43240 , train loss:  0.05497816332815552 , test loss:  0.08774297497590279\n",
      "Epoch:  43250 , step:  43250 , train loss:  0.0549731074097781 , test loss:  0.0877407222777765\n",
      "Epoch:  43260 , step:  43260 , train loss:  0.05496805318335542 , test loss:  0.08773847085470085\n",
      "Epoch:  43270 , step:  43270 , train loss:  0.054963000647930145 , test loss:  0.0877362207058458\n",
      "Epoch:  43280 , step:  43280 , train loss:  0.054957949802545744 , test loss:  0.08773397183038209\n",
      "Epoch:  43290 , step:  43290 , train loss:  0.054952900646246465 , test loss:  0.08773172422748145\n",
      "Epoch:  43300 , step:  43300 , train loss:  0.05494785317807731 , test loss:  0.08772947789631622\n",
      "Epoch:  43310 , step:  43310 , train loss:  0.054942807397084015 , test loss:  0.08772723283605946\n",
      "Epoch:  43320 , step:  43320 , train loss:  0.05493776330231314 , test loss:  0.08772498904588474\n",
      "Epoch:  43330 , step:  43330 , train loss:  0.05493272089281195 , test loss:  0.08772274652496662\n",
      "Epoch:  43340 , step:  43340 , train loss:  0.05492768016762848 , test loss:  0.08772050527248028\n",
      "Epoch:  43350 , step:  43350 , train loss:  0.05492264112581158 , test loss:  0.08771826528760159\n",
      "Epoch:  43360 , step:  43360 , train loss:  0.05491760376641075 , test loss:  0.08771602656950701\n",
      "Epoch:  43370 , step:  43370 , train loss:  0.05491256808847638 , test loss:  0.08771378911737403\n",
      "Epoch:  43380 , step:  43380 , train loss:  0.054907534091059515 , test loss:  0.08771155293038052\n",
      "Epoch:  43390 , step:  43390 , train loss:  0.054902501773212 , test loss:  0.0877093180077052\n",
      "Epoch:  43400 , step:  43400 , train loss:  0.05489747113398645 , test loss:  0.08770708434852763\n",
      "Epoch:  43410 , step:  43410 , train loss:  0.05489244217243617 , test loss:  0.08770485195202772\n",
      "Epoch:  43420 , step:  43420 , train loss:  0.05488741488761528 , test loss:  0.08770262081738636\n",
      "Epoch:  43430 , step:  43430 , train loss:  0.05488238927857868 , test loss:  0.08770039094378523\n",
      "Epoch:  43440 , step:  43440 , train loss:  0.05487736534438194 , test loss:  0.08769816233040638\n",
      "Epoch:  43450 , step:  43450 , train loss:  0.05487234308408145 , test loss:  0.08769593497643277\n",
      "Epoch:  43460 , step:  43460 , train loss:  0.054867322496734254 , test loss:  0.08769370888104829\n",
      "Epoch:  43470 , step:  43470 , train loss:  0.05486230358139831 , test loss:  0.08769148404343685\n",
      "Epoch:  43480 , step:  43480 , train loss:  0.05485728633713219 , test loss:  0.0876892604627839\n",
      "Epoch:  43490 , step:  43490 , train loss:  0.05485227076299525 , test loss:  0.08768703813827504\n",
      "Epoch:  43500 , step:  43500 , train loss:  0.05484725685804763 , test loss:  0.08768481706909677\n",
      "Epoch:  43510 , step:  43510 , train loss:  0.05484224462135015 , test loss:  0.08768259725443617\n",
      "Epoch:  43520 , step:  43520 , train loss:  0.05483723405196445 , test loss:  0.08768037869348105\n",
      "Epoch:  43530 , step:  43530 , train loss:  0.05483222514895287 , test loss:  0.08767816138542005\n",
      "Epoch:  43540 , step:  43540 , train loss:  0.05482721791137852 , test loss:  0.08767594532944227\n",
      "Epoch:  43550 , step:  43550 , train loss:  0.05482221233830523 , test loss:  0.08767373052473783\n",
      "Epoch:  43560 , step:  43560 , train loss:  0.054817208428797594 , test loss:  0.08767151697049728\n",
      "Epoch:  43570 , step:  43570 , train loss:  0.05481220618192092 , test loss:  0.08766930466591187\n",
      "Epoch:  43580 , step:  43580 , train loss:  0.0548072055967413 , test loss:  0.08766709361017375\n",
      "Epoch:  43590 , step:  43590 , train loss:  0.05480220667232553 , test loss:  0.08766488380247552\n",
      "Epoch:  43600 , step:  43600 , train loss:  0.0547972094077412 , test loss:  0.08766267524201057\n",
      "Epoch:  43610 , step:  43610 , train loss:  0.054792213802056566 , test loss:  0.08766046792797312\n",
      "Epoch:  43620 , step:  43620 , train loss:  0.05478721985434066 , test loss:  0.08765826185955795\n",
      "Epoch:  43630 , step:  43630 , train loss:  0.054782227563663285 , test loss:  0.08765605703596041\n",
      "Epoch:  43640 , step:  43640 , train loss:  0.05477723692909491 , test loss:  0.08765385345637686\n",
      "Epoch:  43650 , step:  43650 , train loss:  0.05477224794970683 , test loss:  0.08765165112000384\n",
      "Epoch:  43660 , step:  43660 , train loss:  0.05476726062457095 , test loss:  0.08764945002603915\n",
      "Epoch:  43670 , step:  43670 , train loss:  0.05476227495276006 , test loss:  0.08764725017368108\n",
      "Epoch:  43680 , step:  43680 , train loss:  0.05475729093334757 , test loss:  0.08764505156212829\n",
      "Epoch:  43690 , step:  43690 , train loss:  0.05475230856540769 , test loss:  0.08764285419058049\n",
      "Epoch:  43700 , step:  43700 , train loss:  0.05474732784801534 , test loss:  0.08764065805823808\n",
      "Epoch:  43710 , step:  43710 , train loss:  0.054742348780246156 , test loss:  0.08763846316430182\n",
      "Epoch:  43720 , step:  43720 , train loss:  0.054737371361176484 , test loss:  0.08763626950797343\n",
      "Epoch:  43730 , step:  43730 , train loss:  0.05473239558988352 , test loss:  0.08763407708845543\n",
      "Epoch:  43740 , step:  43740 , train loss:  0.05472742146544504 , test loss:  0.08763188590495068\n",
      "Epoch:  43750 , step:  43750 , train loss:  0.05472244898693964 , test loss:  0.08762969595666295\n",
      "Epoch:  43760 , step:  43760 , train loss:  0.05471747815344663 , test loss:  0.08762750724279653\n",
      "Epoch:  43770 , step:  43770 , train loss:  0.05471250896404603 , test loss:  0.08762531976255636\n",
      "Epoch:  43780 , step:  43780 , train loss:  0.05470754141781858 , test loss:  0.08762313351514844\n",
      "Epoch:  43790 , step:  43790 , train loss:  0.05470257551384578 , test loss:  0.08762094849977907\n",
      "Epoch:  43800 , step:  43800 , train loss:  0.05469761125120984 , test loss:  0.0876187647156555\n",
      "Epoch:  43810 , step:  43810 , train loss:  0.05469264862899371 , test loss:  0.08761658216198517\n",
      "Epoch:  43820 , step:  43820 , train loss:  0.05468768764628103 , test loss:  0.08761440083797675\n",
      "Epoch:  43830 , step:  43830 , train loss:  0.05468272830215614 , test loss:  0.0876122207428394\n",
      "Epoch:  43840 , step:  43840 , train loss:  0.05467777059570422 , test loss:  0.08761004187578274\n",
      "Epoch:  43850 , step:  43850 , train loss:  0.05467281452601102 , test loss:  0.08760786423601744\n",
      "Epoch:  43860 , step:  43860 , train loss:  0.05466786009216316 , test loss:  0.0876056878227544\n",
      "Epoch:  43870 , step:  43870 , train loss:  0.05466290729324785 , test loss:  0.08760351263520566\n",
      "Epoch:  43880 , step:  43880 , train loss:  0.05465795612835311 , test loss:  0.08760133867258363\n",
      "Epoch:  43890 , step:  43890 , train loss:  0.054653006596567646 , test loss:  0.08759916593410146\n",
      "Epoch:  43900 , step:  43900 , train loss:  0.05464805869698085 , test loss:  0.08759699441897299\n",
      "Epoch:  43910 , step:  43910 , train loss:  0.0546431124286829 , test loss:  0.08759482412641272\n",
      "Epoch:  43920 , step:  43920 , train loss:  0.05463816779076465 , test loss:  0.08759265505563599\n",
      "Epoch:  43930 , step:  43930 , train loss:  0.054633224782317656 , test loss:  0.08759048720585824\n",
      "Epoch:  43940 , step:  43940 , train loss:  0.05462828340243423 , test loss:  0.0875883205762963\n",
      "Epoch:  43950 , step:  43950 , train loss:  0.05462334365020735 , test loss:  0.08758615516616719\n",
      "Epoch:  43960 , step:  43960 , train loss:  0.05461840552473075 , test loss:  0.08758399097468875\n",
      "Epoch:  43970 , step:  43970 , train loss:  0.054613469025098865 , test loss:  0.08758182800107947\n",
      "Epoch:  43980 , step:  43980 , train loss:  0.05460853415040684 , test loss:  0.08757966624455868\n",
      "Epoch:  43990 , step:  43990 , train loss:  0.054603600899750526 , test loss:  0.08757750570434607\n",
      "Epoch:  44000 , step:  44000 , train loss:  0.054598669272226494 , test loss:  0.08757534637966216\n",
      "Epoch:  44010 , step:  44010 , train loss:  0.054593739266932007 , test loss:  0.0875731882697282\n",
      "Epoch:  44020 , step:  44020 , train loss:  0.0545888108829651 , test loss:  0.08757103137376578\n",
      "Epoch:  44030 , step:  44030 , train loss:  0.054583884119424414 , test loss:  0.08756887569099757\n",
      "Epoch:  44040 , step:  44040 , train loss:  0.05457895897540938 , test loss:  0.0875667212206467\n",
      "Epoch:  44050 , step:  44050 , train loss:  0.0545740354500201 , test loss:  0.08756456796193687\n",
      "Epoch:  44060 , step:  44060 , train loss:  0.0545691135423574 , test loss:  0.08756241591409265\n",
      "Epoch:  44070 , step:  44070 , train loss:  0.0545641932515228 , test loss:  0.08756026507633907\n",
      "Epoch:  44080 , step:  44080 , train loss:  0.05455927457661854 , test loss:  0.08755811544790201\n",
      "Epoch:  44090 , step:  44090 , train loss:  0.05455435751674753 , test loss:  0.08755596702800782\n",
      "Epoch:  44100 , step:  44100 , train loss:  0.054549442071013435 , test loss:  0.08755381981588352\n",
      "Epoch:  44110 , step:  44110 , train loss:  0.05454452823852059 , test loss:  0.08755167381075694\n",
      "Epoch:  44120 , step:  44120 , train loss:  0.05453961601837403 , test loss:  0.08754952901185648\n",
      "Epoch:  44130 , step:  44130 , train loss:  0.05453470540967949 , test loss:  0.08754738541841119\n",
      "Epoch:  44140 , step:  44140 , train loss:  0.054529796411543444 , test loss:  0.08754524302965072\n",
      "Epoch:  44150 , step:  44150 , train loss:  0.05452488902307303 , test loss:  0.08754310184480553\n",
      "Epoch:  44160 , step:  44160 , train loss:  0.0545199832433761 , test loss:  0.08754096186310663\n",
      "Epoch:  44170 , step:  44170 , train loss:  0.05451507907156117 , test loss:  0.08753882308378551\n",
      "Epoch:  44180 , step:  44180 , train loss:  0.054510176506737486 , test loss:  0.0875366855060746\n",
      "Epoch:  44190 , step:  44190 , train loss:  0.054505275548015035 , test loss:  0.08753454912920687\n",
      "Epoch:  44200 , step:  44200 , train loss:  0.05450037619450442 , test loss:  0.087532413952416\n",
      "Epoch:  44210 , step:  44210 , train loss:  0.05449547844531698 , test loss:  0.08753027997493608\n",
      "Epoch:  44220 , step:  44220 , train loss:  0.05449058229956473 , test loss:  0.08752814719600219\n",
      "Epoch:  44230 , step:  44230 , train loss:  0.05448568775636042 , test loss:  0.08752601561484978\n",
      "Epoch:  44240 , step:  44240 , train loss:  0.054480794814817476 , test loss:  0.0875238852307151\n",
      "Epoch:  44250 , step:  44250 , train loss:  0.054475903474049965 , test loss:  0.08752175604283492\n",
      "Epoch:  44260 , step:  44260 , train loss:  0.05447101373317272 , test loss:  0.08751962805044666\n",
      "Epoch:  44270 , step:  44270 , train loss:  0.05446612559130122 , test loss:  0.08751750125278876\n",
      "Epoch:  44280 , step:  44280 , train loss:  0.05446123904755168 , test loss:  0.08751537564909986\n",
      "Epoch:  44290 , step:  44290 , train loss:  0.05445635410104094 , test loss:  0.08751325123861926\n",
      "Epoch:  44300 , step:  44300 , train loss:  0.0544514707508866 , test loss:  0.0875111280205872\n",
      "Epoch:  44310 , step:  44310 , train loss:  0.0544465889962069 , test loss:  0.08750900599424431\n",
      "Epoch:  44320 , step:  44320 , train loss:  0.05444170883612077 , test loss:  0.08750688515883187\n",
      "Epoch:  44330 , step:  44330 , train loss:  0.05443683026974789 , test loss:  0.08750476551359189\n",
      "Epoch:  44340 , step:  44340 , train loss:  0.05443195329620853 , test loss:  0.0875026470577671\n",
      "Epoch:  44350 , step:  44350 , train loss:  0.05442707791462371 , test loss:  0.0875005297906006\n",
      "Epoch:  44360 , step:  44360 , train loss:  0.05442220412411511 , test loss:  0.08749841371133656\n",
      "Epoch:  44370 , step:  44370 , train loss:  0.054417331923805115 , test loss:  0.08749629881921946\n",
      "Epoch:  44380 , step:  44380 , train loss:  0.05441246131281682 , test loss:  0.08749418511349413\n",
      "Epoch:  44390 , step:  44390 , train loss:  0.054407592290273914 , test loss:  0.08749207259340674\n",
      "Epoch:  44400 , step:  44400 , train loss:  0.05440272485530083 , test loss:  0.08748996125820369\n",
      "Epoch:  44410 , step:  44410 , train loss:  0.0543978590070227 , test loss:  0.08748785110713211\n",
      "Epoch:  44420 , step:  44420 , train loss:  0.05439299474456531 , test loss:  0.0874857421394397\n",
      "Epoch:  44430 , step:  44430 , train loss:  0.05438813206705514 , test loss:  0.0874836343543747\n",
      "Epoch:  44440 , step:  44440 , train loss:  0.05438327097361929 , test loss:  0.08748152775118605\n",
      "Epoch:  44450 , step:  44450 , train loss:  0.054378411463385644 , test loss:  0.08747942232912365\n",
      "Epoch:  44460 , step:  44460 , train loss:  0.054373553535482705 , test loss:  0.08747731808743762\n",
      "Epoch:  44470 , step:  44470 , train loss:  0.05436869718903963 , test loss:  0.08747521502537872\n",
      "Epoch:  44480 , step:  44480 , train loss:  0.05436384242318631 , test loss:  0.08747311314219858\n",
      "Epoch:  44490 , step:  44490 , train loss:  0.054358989237053276 , test loss:  0.08747101243714944\n",
      "Epoch:  44500 , step:  44500 , train loss:  0.054354137629771725 , test loss:  0.08746891290948375\n",
      "Epoch:  44510 , step:  44510 , train loss:  0.0543492876004736 , test loss:  0.08746681455845533\n",
      "Epoch:  44520 , step:  44520 , train loss:  0.054344439148291415 , test loss:  0.08746471738331797\n",
      "Epoch:  44530 , step:  44530 , train loss:  0.05433959227235844 , test loss:  0.0874626213833264\n",
      "Epoch:  44540 , step:  44540 , train loss:  0.054334746971808595 , test loss:  0.0874605265577358\n",
      "Epoch:  44550 , step:  44550 , train loss:  0.054329903245776426 , test loss:  0.08745843290580209\n",
      "Epoch:  44560 , step:  44560 , train loss:  0.05432506109339725 , test loss:  0.08745634042678194\n",
      "Epoch:  44570 , step:  44570 , train loss:  0.05432022051380696 , test loss:  0.08745424911993246\n",
      "Epoch:  44580 , step:  44580 , train loss:  0.054315381506142166 , test loss:  0.08745215898451139\n",
      "Epoch:  44590 , step:  44590 , train loss:  0.05431054406954012 , test loss:  0.08745007001977712\n",
      "Epoch:  44600 , step:  44600 , train loss:  0.05430570820313878 , test loss:  0.08744798222498872\n",
      "Epoch:  44610 , step:  44610 , train loss:  0.054300873906076755 , test loss:  0.08744589559940595\n",
      "Epoch:  44620 , step:  44620 , train loss:  0.05429604117749332 , test loss:  0.08744381014228887\n",
      "Epoch:  44630 , step:  44630 , train loss:  0.054291210016528424 , test loss:  0.08744172585289851\n",
      "Epoch:  44640 , step:  44640 , train loss:  0.054286380422322644 , test loss:  0.08743964273049615\n",
      "Epoch:  44650 , step:  44650 , train loss:  0.05428155239401731 , test loss:  0.08743756077434439\n",
      "Epoch:  44660 , step:  44660 , train loss:  0.05427672593075431 , test loss:  0.08743547998370554\n",
      "Epoch:  44670 , step:  44670 , train loss:  0.054271901031676284 , test loss:  0.0874334003578432\n",
      "Epoch:  44680 , step:  44680 , train loss:  0.0542670776959265 , test loss:  0.08743132189602117\n",
      "Epoch:  44690 , step:  44690 , train loss:  0.05426225592264888 , test loss:  0.08742924459750424\n",
      "Epoch:  44700 , step:  44700 , train loss:  0.05425743571098803 , test loss:  0.08742716846155756\n",
      "Epoch:  44710 , step:  44710 , train loss:  0.05425261706008917 , test loss:  0.08742509348744688\n",
      "Epoch:  44720 , step:  44720 , train loss:  0.05424779996909828 , test loss:  0.0874230196744388\n",
      "Epoch:  44730 , step:  44730 , train loss:  0.05424298443716189 , test loss:  0.08742094702180032\n",
      "Epoch:  44740 , step:  44740 , train loss:  0.05423817046342727 , test loss:  0.08741887552879904\n",
      "Epoch:  44750 , step:  44750 , train loss:  0.0542333580470423 , test loss:  0.08741680519470331\n",
      "Epoch:  44760 , step:  44760 , train loss:  0.054228547187155564 , test loss:  0.08741473601878201\n",
      "Epoch:  44770 , step:  44770 , train loss:  0.054223737882916234 , test loss:  0.08741266800030456\n",
      "Epoch:  44780 , step:  44780 , train loss:  0.054218930133474195 , test loss:  0.08741060113854127\n",
      "Epoch:  44790 , step:  44790 , train loss:  0.05421412393798 , test loss:  0.08740853543276286\n",
      "Epoch:  44800 , step:  44800 , train loss:  0.05420931929558482 , test loss:  0.08740647088224049\n",
      "Epoch:  44810 , step:  44810 , train loss:  0.0542045162054405 , test loss:  0.08740440748624624\n",
      "Epoch:  44820 , step:  44820 , train loss:  0.054199714666699525 , test loss:  0.08740234524405265\n",
      "Epoch:  44830 , step:  44830 , train loss:  0.054194914678515047 , test loss:  0.0874002841549329\n",
      "Epoch:  44840 , step:  44840 , train loss:  0.054190116240040886 , test loss:  0.08739822421816072\n",
      "Epoch:  44850 , step:  44850 , train loss:  0.054185319350431485 , test loss:  0.0873961654330105\n",
      "Epoch:  44860 , step:  44860 , train loss:  0.05418052400884193 , test loss:  0.08739410779875727\n",
      "Epoch:  44870 , step:  44870 , train loss:  0.05417573021442801 , test loss:  0.08739205131467662\n",
      "Epoch:  44880 , step:  44880 , train loss:  0.05417093796634613 , test loss:  0.08738999598004464\n",
      "Epoch:  44890 , step:  44890 , train loss:  0.05416614726375332 , test loss:  0.0873879417941381\n",
      "Epoch:  44900 , step:  44900 , train loss:  0.05416135810580735 , test loss:  0.08738588875623463\n",
      "Epoch:  44910 , step:  44910 , train loss:  0.054156570491666514 , test loss:  0.08738383686561192\n",
      "Epoch:  44920 , step:  44920 , train loss:  0.05415178442048984 , test loss:  0.08738178612154897\n",
      "Epoch:  44930 , step:  44930 , train loss:  0.05414699989143699 , test loss:  0.08737973652332472\n",
      "Epoch:  44940 , step:  44940 , train loss:  0.054142216903668286 , test loss:  0.087377688070219\n",
      "Epoch:  44950 , step:  44950 , train loss:  0.05413743545634464 , test loss:  0.08737564076151233\n",
      "Epoch:  44960 , step:  44960 , train loss:  0.05413265554862765 , test loss:  0.08737359459648554\n",
      "Epoch:  44970 , step:  44970 , train loss:  0.05412787717967957 , test loss:  0.08737154957442038\n",
      "Epoch:  44980 , step:  44980 , train loss:  0.054123100348663265 , test loss:  0.08736950569459899\n",
      "Epoch:  44990 , step:  44990 , train loss:  0.05411832505474228 , test loss:  0.08736746295630417\n",
      "Epoch:  45000 , step:  45000 , train loss:  0.054113551297080766 , test loss:  0.08736542135881935\n",
      "Epoch:  45010 , step:  45010 , train loss:  0.054108779074843516 , test loss:  0.08736338090142863\n",
      "Epoch:  45020 , step:  45020 , train loss:  0.05410400838719605 , test loss:  0.08736134158341627\n",
      "Epoch:  45030 , step:  45030 , train loss:  0.054099239233304376 , test loss:  0.08735930340406783\n",
      "Epoch:  45040 , step:  45040 , train loss:  0.054094471612335306 , test loss:  0.0873572663626689\n",
      "Epoch:  45050 , step:  45050 , train loss:  0.05408970552345617 , test loss:  0.08735523045850589\n",
      "Epoch:  45060 , step:  45060 , train loss:  0.05408494096583497 , test loss:  0.08735319569086558\n",
      "Epoch:  45070 , step:  45070 , train loss:  0.054080177938640395 , test loss:  0.08735116205903584\n",
      "Epoch:  45080 , step:  45080 , train loss:  0.05407541644104173 , test loss:  0.08734912956230473\n",
      "Epoch:  45090 , step:  45090 , train loss:  0.05407065647220889 , test loss:  0.0873470981999609\n",
      "Epoch:  45100 , step:  45100 , train loss:  0.05406589803131243 , test loss:  0.08734506797129375\n",
      "Epoch:  45110 , step:  45110 , train loss:  0.054061141117523574 , test loss:  0.08734303887559322\n",
      "Epoch:  45120 , step:  45120 , train loss:  0.05405638573001413 , test loss:  0.08734101091214973\n",
      "Epoch:  45130 , step:  45130 , train loss:  0.05405163186795662 , test loss:  0.08733898408025441\n",
      "Epoch:  45140 , step:  45140 , train loss:  0.0540468795305241 , test loss:  0.08733695837919915\n",
      "Epoch:  45150 , step:  45150 , train loss:  0.054042128716890334 , test loss:  0.08733493380827602\n",
      "Epoch:  45160 , step:  45160 , train loss:  0.05403737942622965 , test loss:  0.08733291036677793\n",
      "Epoch:  45170 , step:  45170 , train loss:  0.054032631657717135 , test loss:  0.08733088805399826\n",
      "Epoch:  45180 , step:  45180 , train loss:  0.054027885410528324 , test loss:  0.08732886686923116\n",
      "Epoch:  45190 , step:  45190 , train loss:  0.05402314068383958 , test loss:  0.08732684681177116\n",
      "Epoch:  45200 , step:  45200 , train loss:  0.05401839747682774 , test loss:  0.0873248278809136\n",
      "Epoch:  45210 , step:  45210 , train loss:  0.054013655788670324 , test loss:  0.08732281007595419\n",
      "Epoch:  45220 , step:  45220 , train loss:  0.05400891561854555 , test loss:  0.08732079339618921\n",
      "Epoch:  45230 , step:  45230 , train loss:  0.054004176965632124 , test loss:  0.08731877784091575\n",
      "Epoch:  45240 , step:  45240 , train loss:  0.05399943982910953 , test loss:  0.08731676340943136\n",
      "Epoch:  45250 , step:  45250 , train loss:  0.053994704208157746 , test loss:  0.087314750101034\n",
      "Epoch:  45260 , step:  45260 , train loss:  0.05398997010195748 , test loss:  0.08731273791502252\n",
      "Epoch:  45270 , step:  45270 , train loss:  0.05398523750969003 , test loss:  0.08731072685069624\n",
      "Epoch:  45280 , step:  45280 , train loss:  0.05398050643053729 , test loss:  0.0873087169073549\n",
      "Epoch:  45290 , step:  45290 , train loss:  0.053975776863681796 , test loss:  0.08730670808429913\n",
      "Epoch:  45300 , step:  45300 , train loss:  0.053971048808306735 , test loss:  0.08730470038082963\n",
      "Epoch:  45310 , step:  45310 , train loss:  0.05396632226359589 , test loss:  0.08730269379624815\n",
      "Epoch:  45320 , step:  45320 , train loss:  0.05396159722873367 , test loss:  0.08730068832985703\n",
      "Epoch:  45330 , step:  45330 , train loss:  0.053956873702905146 , test loss:  0.08729868398095882\n",
      "Epoch:  45340 , step:  45340 , train loss:  0.053952151685295935 , test loss:  0.08729668074885695\n",
      "Epoch:  45350 , step:  45350 , train loss:  0.05394743117509233 , test loss:  0.08729467863285525\n",
      "Epoch:  45360 , step:  45360 , train loss:  0.05394271217148123 , test loss:  0.0872926776322583\n",
      "Epoch:  45370 , step:  45370 , train loss:  0.05393799467365015 , test loss:  0.08729067774637103\n",
      "Epoch:  45380 , step:  45380 , train loss:  0.053933278680787226 , test loss:  0.0872886789744992\n",
      "Epoch:  45390 , step:  45390 , train loss:  0.05392856419208125 , test loss:  0.08728668131594894\n",
      "Epoch:  45400 , step:  45400 , train loss:  0.053923851206721544 , test loss:  0.08728468477002695\n",
      "Epoch:  45410 , step:  45410 , train loss:  0.05391913972389815 , test loss:  0.08728268933604072\n",
      "Epoch:  45420 , step:  45420 , train loss:  0.053914429742801674 , test loss:  0.08728069501329808\n",
      "Epoch:  45430 , step:  45430 , train loss:  0.053909721262623306 , test loss:  0.08727870180110753\n",
      "Epoch:  45440 , step:  45440 , train loss:  0.05390501428255491 , test loss:  0.08727670969877814\n",
      "Epoch:  45450 , step:  45450 , train loss:  0.05390030880178894 , test loss:  0.0872747187056196\n",
      "Epoch:  45460 , step:  45460 , train loss:  0.05389560481951849 , test loss:  0.08727272882094197\n",
      "Epoch:  45470 , step:  45470 , train loss:  0.05389090233493721 , test loss:  0.08727074004405608\n",
      "Epoch:  45480 , step:  45480 , train loss:  0.05388620134723944 , test loss:  0.08726875237427324\n",
      "Epoch:  45490 , step:  45490 , train loss:  0.05388150185562005 , test loss:  0.08726676581090531\n",
      "Epoch:  45500 , step:  45500 , train loss:  0.05387680385927457 , test loss:  0.08726478035326492\n",
      "Epoch:  45510 , step:  45510 , train loss:  0.05387210735739917 , test loss:  0.08726279600066505\n",
      "Epoch:  45520 , step:  45520 , train loss:  0.05386741234919054 , test loss:  0.08726081275241925\n",
      "Epoch:  45530 , step:  45530 , train loss:  0.053862718833846114 , test loss:  0.08725883060784166\n",
      "Epoch:  45540 , step:  45540 , train loss:  0.0538580268105638 , test loss:  0.08725684956624705\n",
      "Epoch:  45550 , step:  45550 , train loss:  0.053853336278542184 , test loss:  0.08725486962695066\n",
      "Epoch:  45560 , step:  45560 , train loss:  0.05384864723698045 , test loss:  0.08725289078926836\n",
      "Epoch:  45570 , step:  45570 , train loss:  0.05384395968507839 , test loss:  0.08725091305251666\n",
      "Epoch:  45580 , step:  45580 , train loss:  0.05383927362203642 , test loss:  0.08724893641601253\n",
      "Epoch:  45590 , step:  45590 , train loss:  0.05383458904705552 , test loss:  0.08724696087907333\n",
      "Epoch:  45600 , step:  45600 , train loss:  0.05382990595933732 , test loss:  0.08724498644101734\n",
      "Epoch:  45610 , step:  45610 , train loss:  0.05382522435808404 , test loss:  0.08724301310116309\n",
      "Epoch:  45620 , step:  45620 , train loss:  0.05382054424249845 , test loss:  0.08724104085883007\n",
      "Epoch:  45630 , step:  45630 , train loss:  0.05381586561178405 , test loss:  0.08723906971333775\n",
      "Epoch:  45640 , step:  45640 , train loss:  0.05381118846514484 , test loss:  0.08723709966400667\n",
      "Epoch:  45650 , step:  45650 , train loss:  0.053806512801785444 , test loss:  0.08723513071015745\n",
      "Epoch:  45660 , step:  45660 , train loss:  0.0538018386209111 , test loss:  0.08723316285111195\n",
      "Epoch:  45670 , step:  45670 , train loss:  0.05379716592172766 , test loss:  0.08723119608619198\n",
      "Epoch:  45680 , step:  45680 , train loss:  0.053792494703441544 , test loss:  0.08722923041471997\n",
      "Epoch:  45690 , step:  45690 , train loss:  0.05378782496525982 , test loss:  0.08722726583601936\n",
      "Epoch:  45700 , step:  45700 , train loss:  0.053783156706390105 , test loss:  0.08722530234941367\n",
      "Epoch:  45710 , step:  45710 , train loss:  0.05377848992604065 , test loss:  0.08722333995422704\n",
      "Epoch:  45720 , step:  45720 , train loss:  0.05377382462342029 , test loss:  0.0872213786497844\n",
      "Epoch:  45730 , step:  45730 , train loss:  0.053769160797738434 , test loss:  0.08721941843541094\n",
      "Epoch:  45740 , step:  45740 , train loss:  0.053764498448205154 , test loss:  0.08721745931043265\n",
      "Epoch:  45750 , step:  45750 , train loss:  0.053759837574031064 , test loss:  0.08721550127417599\n",
      "Epoch:  45760 , step:  45760 , train loss:  0.053755178174427404 , test loss:  0.08721354432596799\n",
      "Epoch:  45770 , step:  45770 , train loss:  0.05375052024860601 , test loss:  0.08721158846513619\n",
      "Epoch:  45780 , step:  45780 , train loss:  0.053745863795779246 , test loss:  0.08720963369100863\n",
      "Epoch:  45790 , step:  45790 , train loss:  0.0537412088151602 , test loss:  0.08720768000291405\n",
      "Epoch:  45800 , step:  45800 , train loss:  0.053736555305962434 , test loss:  0.08720572740018145\n",
      "Epoch:  45810 , step:  45810 , train loss:  0.053731903267400176 , test loss:  0.08720377588214089\n",
      "Epoch:  45820 , step:  45820 , train loss:  0.0537272526986882 , test loss:  0.08720182544812245\n",
      "Epoch:  45830 , step:  45830 , train loss:  0.05372260359904192 , test loss:  0.08719987609745701\n",
      "Epoch:  45840 , step:  45840 , train loss:  0.0537179559676773 , test loss:  0.087197927829476\n",
      "Epoch:  45850 , step:  45850 , train loss:  0.0537133098038109 , test loss:  0.08719598064351133\n",
      "Epoch:  45860 , step:  45860 , train loss:  0.05370866510665994 , test loss:  0.08719403453889547\n",
      "Epoch:  45870 , step:  45870 , train loss:  0.05370402187544211 , test loss:  0.08719208951496148\n",
      "Epoch:  45880 , step:  45880 , train loss:  0.05369938010937577 , test loss:  0.08719014557104297\n",
      "Epoch:  45890 , step:  45890 , train loss:  0.05369473980767988 , test loss:  0.08718820270647401\n",
      "Epoch:  45900 , step:  45900 , train loss:  0.05369010096957395 , test loss:  0.08718626092058926\n",
      "Epoch:  45910 , step:  45910 , train loss:  0.05368546359427808 , test loss:  0.08718432021272403\n",
      "Epoch:  45920 , step:  45920 , train loss:  0.053680827681012955 , test loss:  0.08718238058221389\n",
      "Epoch:  45930 , step:  45930 , train loss:  0.053676193228999886 , test loss:  0.08718044202839524\n",
      "Epoch:  45940 , step:  45940 , train loss:  0.053671560237460704 , test loss:  0.087178504550605\n",
      "Epoch:  45950 , step:  45950 , train loss:  0.053666928705617914 , test loss:  0.08717656814818049\n",
      "Epoch:  45960 , step:  45960 , train loss:  0.05366229863269451 , test loss:  0.08717463282045954\n",
      "Epoch:  45970 , step:  45970 , train loss:  0.053657670017914154 , test loss:  0.08717269856678071\n",
      "Epoch:  45980 , step:  45980 , train loss:  0.053653042860501064 , test loss:  0.08717076538648298\n",
      "Epoch:  45990 , step:  45990 , train loss:  0.053648417159679995 , test loss:  0.08716883327890575\n",
      "Epoch:  46000 , step:  46000 , train loss:  0.05364379291467632 , test loss:  0.08716690224338929\n",
      "Epoch:  46010 , step:  46010 , train loss:  0.053639170124716036 , test loss:  0.08716497227927396\n",
      "Epoch:  46020 , step:  46020 , train loss:  0.05363454878902568 , test loss:  0.08716304338590121\n",
      "Epoch:  46030 , step:  46030 , train loss:  0.05362992890683232 , test loss:  0.08716111556261258\n",
      "Epoch:  46040 , step:  46040 , train loss:  0.05362531047736371 , test loss:  0.08715918880875026\n",
      "Epoch:  46050 , step:  46050 , train loss:  0.0536206934998481 , test loss:  0.08715726312365706\n",
      "Epoch:  46060 , step:  46060 , train loss:  0.053616077973514364 , test loss:  0.08715533850667619\n",
      "Epoch:  46070 , step:  46070 , train loss:  0.05361146389759196 , test loss:  0.0871534149571517\n",
      "Epoch:  46080 , step:  46080 , train loss:  0.053606851271310894 , test loss:  0.08715149247442765\n",
      "Epoch:  46090 , step:  46090 , train loss:  0.05360224009390172 , test loss:  0.08714957105784917\n",
      "Epoch:  46100 , step:  46100 , train loss:  0.05359763036459569 , test loss:  0.08714765070676161\n",
      "Epoch:  46110 , step:  46110 , train loss:  0.05359302208262449 , test loss:  0.08714573142051091\n",
      "Epoch:  46120 , step:  46120 , train loss:  0.05358841524722048 , test loss:  0.08714381319844357\n",
      "Epoch:  46130 , step:  46130 , train loss:  0.05358380985761655 , test loss:  0.08714189603990659\n",
      "Epoch:  46140 , step:  46140 , train loss:  0.05357920591304617 , test loss:  0.08713997994424767\n",
      "Epoch:  46150 , step:  46150 , train loss:  0.053574603412743414 , test loss:  0.08713806491081465\n",
      "Epoch:  46160 , step:  46160 , train loss:  0.05357000235594288 , test loss:  0.08713615093895631\n",
      "Epoch:  46170 , step:  46170 , train loss:  0.05356540274187979 , test loss:  0.08713423802802178\n",
      "Epoch:  46180 , step:  46180 , train loss:  0.05356080456978991 , test loss:  0.08713232617736066\n",
      "Epoch:  46190 , step:  46190 , train loss:  0.05355620783890957 , test loss:  0.08713041538632318\n",
      "Epoch:  46200 , step:  46200 , train loss:  0.053551612548475694 , test loss:  0.08712850565426013\n",
      "Epoch:  46210 , step:  46210 , train loss:  0.05354701869772577 , test loss:  0.08712659698052269\n",
      "Epoch:  46220 , step:  46220 , train loss:  0.053542426285897865 , test loss:  0.08712468936446276\n",
      "Epoch:  46230 , step:  46230 , train loss:  0.05353783531223059 , test loss:  0.08712278280543244\n",
      "Epoch:  46240 , step:  46240 , train loss:  0.05353324577596313 , test loss:  0.08712087730278471\n",
      "Epoch:  46250 , step:  46250 , train loss:  0.05352865767633527 , test loss:  0.08711897285587299\n",
      "Epoch:  46260 , step:  46260 , train loss:  0.05352407101258734 , test loss:  0.08711706946405094\n",
      "Epoch:  46270 , step:  46270 , train loss:  0.05351948578396025 , test loss:  0.08711516712667308\n",
      "Epoch:  46280 , step:  46280 , train loss:  0.05351490198969545 , test loss:  0.0871132658430944\n",
      "Epoch:  46290 , step:  46290 , train loss:  0.053510319629035 , test loss:  0.08711136561267023\n",
      "Epoch:  46300 , step:  46300 , train loss:  0.053505738701221474 , test loss:  0.08710946643475664\n",
      "Epoch:  46310 , step:  46310 , train loss:  0.05350115920549807 , test loss:  0.0871075683087101\n",
      "Epoch:  46320 , step:  46320 , train loss:  0.05349658114110848 , test loss:  0.08710567123388777\n",
      "Epoch:  46330 , step:  46330 , train loss:  0.05349200450729705 , test loss:  0.08710377520964692\n",
      "Epoch:  46340 , step:  46340 , train loss:  0.053487429303308597 , test loss:  0.0871018802353457\n",
      "Epoch:  46350 , step:  46350 , train loss:  0.05348285552838857 , test loss:  0.08709998631034267\n",
      "Epoch:  46360 , step:  46360 , train loss:  0.05347828318178296 , test loss:  0.08709809343399705\n",
      "Epoch:  46370 , step:  46370 , train loss:  0.05347371226273829 , test loss:  0.08709620160566839\n",
      "Epoch:  46380 , step:  46380 , train loss:  0.0534691427705017 , test loss:  0.0870943108247168\n",
      "Epoch:  46390 , step:  46390 , train loss:  0.05346457470432085 , test loss:  0.08709242109050289\n",
      "Epoch:  46400 , step:  46400 , train loss:  0.05346000806344397 , test loss:  0.08709053240238773\n",
      "Epoch:  46410 , step:  46410 , train loss:  0.053455442847119866 , test loss:  0.08708864475973328\n",
      "Epoch:  46420 , step:  46420 , train loss:  0.053450879054597895 , test loss:  0.08708675816190158\n",
      "Epoch:  46430 , step:  46430 , train loss:  0.05344631668512795 , test loss:  0.08708487260825534\n",
      "Epoch:  46440 , step:  46440 , train loss:  0.05344175573796052 , test loss:  0.0870829880981579\n",
      "Epoch:  46450 , step:  46450 , train loss:  0.053437196212346605 , test loss:  0.08708110463097282\n",
      "Epoch:  46460 , step:  46460 , train loss:  0.053432638107537844 , test loss:  0.0870792222060645\n",
      "Epoch:  46470 , step:  46470 , train loss:  0.05342808142278631 , test loss:  0.08707734082279756\n",
      "Epoch:  46480 , step:  46480 , train loss:  0.05342352615734478 , test loss:  0.08707546048053749\n",
      "Epoch:  46490 , step:  46490 , train loss:  0.05341897231046647 , test loss:  0.08707358117864993\n",
      "Epoch:  46500 , step:  46500 , train loss:  0.05341441988140517 , test loss:  0.08707170291650124\n",
      "Epoch:  46510 , step:  46510 , train loss:  0.05340986886941529 , test loss:  0.0870698256934582\n",
      "Epoch:  46520 , step:  46520 , train loss:  0.053405319273751736 , test loss:  0.08706794950888819\n",
      "Epoch:  46530 , step:  46530 , train loss:  0.053400771093669974 , test loss:  0.08706607436215898\n",
      "Epoch:  46540 , step:  46540 , train loss:  0.05339622432842604 , test loss:  0.08706420025263907\n",
      "Epoch:  46550 , step:  46550 , train loss:  0.05339167897727649 , test loss:  0.08706232717969711\n",
      "Epoch:  46560 , step:  46560 , train loss:  0.0533871350394785 , test loss:  0.08706045514270251\n",
      "Epoch:  46570 , step:  46570 , train loss:  0.053382592514289715 , test loss:  0.08705858414102526\n",
      "Epoch:  46580 , step:  46580 , train loss:  0.05337805140096839 , test loss:  0.08705671417403564\n",
      "Epoch:  46590 , step:  46590 , train loss:  0.05337351169877333 , test loss:  0.08705484524110473\n",
      "Epoch:  46600 , step:  46600 , train loss:  0.05336897340696382 , test loss:  0.0870529773416037\n",
      "Epoch:  46610 , step:  46610 , train loss:  0.05336443652479979 , test loss:  0.08705111047490452\n",
      "Epoch:  46620 , step:  46620 , train loss:  0.05335990105154163 , test loss:  0.08704924464037953\n",
      "Epoch:  46630 , step:  46630 , train loss:  0.053355366986450395 , test loss:  0.0870473798374019\n",
      "Epoch:  46640 , step:  46640 , train loss:  0.053350834328787566 , test loss:  0.08704551606534466\n",
      "Epoch:  46650 , step:  46650 , train loss:  0.053346303077815224 , test loss:  0.08704365332358205\n",
      "Epoch:  46660 , step:  46660 , train loss:  0.05334177323279597 , test loss:  0.08704179161148833\n",
      "Epoch:  46670 , step:  46670 , train loss:  0.05333724479299303 , test loss:  0.08703993092843851\n",
      "Epoch:  46680 , step:  46680 , train loss:  0.053332717757670095 , test loss:  0.08703807127380793\n",
      "Epoch:  46690 , step:  46690 , train loss:  0.053328192126091446 , test loss:  0.08703621264697259\n",
      "Epoch:  46700 , step:  46700 , train loss:  0.05332366789752186 , test loss:  0.08703435504730885\n",
      "Epoch:  46710 , step:  46710 , train loss:  0.053319145071226676 , test loss:  0.08703249847419378\n",
      "Epoch:  46720 , step:  46720 , train loss:  0.05331462364647185 , test loss:  0.08703064292700469\n",
      "Epoch:  46730 , step:  46730 , train loss:  0.05331010362252378 , test loss:  0.08702878840511946\n",
      "Epoch:  46740 , step:  46740 , train loss:  0.05330558499864945 , test loss:  0.0870269349079167\n",
      "Epoch:  46750 , step:  46750 , train loss:  0.053301067774116404 , test loss:  0.08702508243477515\n",
      "Epoch:  46760 , step:  46760 , train loss:  0.053296551948192675 , test loss:  0.08702323098507461\n",
      "Epoch:  46770 , step:  46770 , train loss:  0.05329203752014692 , test loss:  0.08702138055819474\n",
      "Epoch:  46780 , step:  46780 , train loss:  0.05328752448924821 , test loss:  0.08701953115351581\n",
      "Epoch:  46790 , step:  46790 , train loss:  0.05328301285476631 , test loss:  0.08701768277041899\n",
      "Epoch:  46800 , step:  46800 , train loss:  0.053278502615971404 , test loss:  0.08701583540828557\n",
      "Epoch:  46810 , step:  46810 , train loss:  0.05327399377213428 , test loss:  0.08701398906649756\n",
      "Epoch:  46820 , step:  46820 , train loss:  0.05326948632252622 , test loss:  0.0870121437444374\n",
      "Epoch:  46830 , step:  46830 , train loss:  0.05326498026641908 , test loss:  0.08701029944148786\n",
      "Epoch:  46840 , step:  46840 , train loss:  0.05326047560308525 , test loss:  0.08700845615703237\n",
      "Epoch:  46850 , step:  46850 , train loss:  0.05325597233179766 , test loss:  0.08700661389045483\n",
      "Epoch:  46860 , step:  46860 , train loss:  0.0532514704518297 , test loss:  0.0870047726411397\n",
      "Epoch:  46870 , step:  46870 , train loss:  0.05324696996245543 , test loss:  0.08700293240847194\n",
      "Epoch:  46880 , step:  46880 , train loss:  0.053242470862949344 , test loss:  0.0870010931918367\n",
      "Epoch:  46890 , step:  46890 , train loss:  0.05323797315258651 , test loss:  0.08699925499061995\n",
      "Epoch:  46900 , step:  46900 , train loss:  0.053233476830642515 , test loss:  0.08699741780420803\n",
      "Epoch:  46910 , step:  46910 , train loss:  0.05322898189639352 , test loss:  0.0869955816319879\n",
      "Epoch:  46920 , step:  46920 , train loss:  0.053224488349116125 , test loss:  0.08699374647334678\n",
      "Epoch:  46930 , step:  46930 , train loss:  0.05321999618808759 , test loss:  0.08699191232767246\n",
      "Epoch:  46940 , step:  46940 , train loss:  0.053215505412585615 , test loss:  0.0869900791943533\n",
      "Epoch:  46950 , step:  46950 , train loss:  0.05321101602188847 , test loss:  0.08698824707277822\n",
      "Epoch:  46960 , step:  46960 , train loss:  0.05320652801527493 , test loss:  0.08698641596233639\n",
      "Epoch:  46970 , step:  46970 , train loss:  0.05320204139202432 , test loss:  0.08698458586241778\n",
      "Epoch:  46980 , step:  46980 , train loss:  0.05319755615141654 , test loss:  0.08698275677241234\n",
      "Epoch:  46990 , step:  46990 , train loss:  0.05319307229273192 , test loss:  0.08698092869171088\n",
      "Epoch:  47000 , step:  47000 , train loss:  0.053188589815251375 , test loss:  0.08697910161970496\n",
      "Epoch:  47010 , step:  47010 , train loss:  0.05318410871825636 , test loss:  0.08697727555578594\n",
      "Epoch:  47020 , step:  47020 , train loss:  0.05317962900102884 , test loss:  0.0869754504993464\n",
      "Epoch:  47030 , step:  47030 , train loss:  0.05317515066285134 , test loss:  0.08697362644977878\n",
      "Epoch:  47040 , step:  47040 , train loss:  0.053170673703006835 , test loss:  0.08697180340647642\n",
      "Epoch:  47050 , step:  47050 , train loss:  0.053166198120778926 , test loss:  0.08696998136883281\n",
      "Epoch:  47060 , step:  47060 , train loss:  0.05316172391545166 , test loss:  0.08696816033624226\n",
      "Epoch:  47070 , step:  47070 , train loss:  0.05315725108630966 , test loss:  0.08696634030809951\n",
      "Epoch:  47080 , step:  47080 , train loss:  0.05315277963263804 , test loss:  0.08696452128379924\n",
      "Epoch:  47090 , step:  47090 , train loss:  0.05314830955372247 , test loss:  0.08696270326273733\n",
      "Epoch:  47100 , step:  47100 , train loss:  0.05314384084884913 , test loss:  0.08696088624431009\n",
      "Epoch:  47110 , step:  47110 , train loss:  0.05313937351730471 , test loss:  0.0869590702279138\n",
      "Epoch:  47120 , step:  47120 , train loss:  0.05313490755837644 , test loss:  0.08695725521294556\n",
      "Epoch:  47130 , step:  47130 , train loss:  0.05313044297135208 , test loss:  0.08695544119880298\n",
      "Epoch:  47140 , step:  47140 , train loss:  0.05312597975551991 , test loss:  0.08695362818488395\n",
      "Epoch:  47150 , step:  47150 , train loss:  0.05312151791016868 , test loss:  0.08695181617058696\n",
      "Epoch:  47160 , step:  47160 , train loss:  0.053117057434587764 , test loss:  0.08695000515531107\n",
      "Epoch:  47170 , step:  47170 , train loss:  0.05311259832806695 , test loss:  0.08694819513845563\n",
      "Epoch:  47180 , step:  47180 , train loss:  0.05310814058989662 , test loss:  0.08694638611942065\n",
      "Epoch:  47190 , step:  47190 , train loss:  0.05310368421936765 , test loss:  0.0869445780976065\n",
      "Epoch:  47200 , step:  47200 , train loss:  0.05309922921577143 , test loss:  0.08694277107241405\n",
      "Epoch:  47210 , step:  47210 , train loss:  0.053094775578399894 , test loss:  0.08694096504324467\n",
      "Epoch:  47220 , step:  47220 , train loss:  0.053090323306545444 , test loss:  0.08693916000950018\n",
      "Epoch:  47230 , step:  47230 , train loss:  0.05308587239950106 , test loss:  0.08693735597058276\n",
      "Epoch:  47240 , step:  47240 , train loss:  0.053081422856560215 , test loss:  0.08693555292589542\n",
      "Epoch:  47250 , step:  47250 , train loss:  0.053076974677016865 , test loss:  0.08693375087484125\n",
      "Epoch:  47260 , step:  47260 , train loss:  0.05307252786016551 , test loss:  0.08693194981682394\n",
      "Epoch:  47270 , step:  47270 , train loss:  0.053068082405301226 , test loss:  0.08693014975124773\n",
      "Epoch:  47280 , step:  47280 , train loss:  0.053063638311719466 , test loss:  0.08692835067751739\n",
      "Epoch:  47290 , step:  47290 , train loss:  0.05305919557871635 , test loss:  0.0869265525950381\n",
      "Epoch:  47300 , step:  47300 , train loss:  0.05305475420558843 , test loss:  0.0869247555032154\n",
      "Epoch:  47310 , step:  47310 , train loss:  0.053050314191632746 , test loss:  0.08692295940145525\n",
      "Epoch:  47320 , step:  47320 , train loss:  0.05304587553614693 , test loss:  0.08692116428916437\n",
      "Epoch:  47330 , step:  47330 , train loss:  0.05304143823842909 , test loss:  0.0869193701657497\n",
      "Epoch:  47340 , step:  47340 , train loss:  0.053037002297777795 , test loss:  0.08691757703061898\n",
      "Epoch:  47350 , step:  47350 , train loss:  0.05303256771349222 , test loss:  0.08691578488317989\n",
      "Epoch:  47360 , step:  47360 , train loss:  0.053028134484872015 , test loss:  0.08691399372284096\n",
      "Epoch:  47370 , step:  47370 , train loss:  0.053023702611217284 , test loss:  0.08691220354901126\n",
      "Epoch:  47380 , step:  47380 , train loss:  0.05301927209182874 , test loss:  0.08691041436109999\n",
      "Epoch:  47390 , step:  47390 , train loss:  0.05301484292600753 , test loss:  0.08690862615851702\n",
      "Epoch:  47400 , step:  47400 , train loss:  0.05301041511305534 , test loss:  0.08690683894067269\n",
      "Epoch:  47410 , step:  47410 , train loss:  0.05300598865227436 , test loss:  0.08690505270697778\n",
      "Epoch:  47420 , step:  47420 , train loss:  0.053001563542967325 , test loss:  0.0869032674568436\n",
      "Epoch:  47430 , step:  47430 , train loss:  0.05299713978443741 , test loss:  0.08690148318968173\n",
      "Epoch:  47440 , step:  47440 , train loss:  0.052992717375988325 , test loss:  0.08689969990490451\n",
      "Epoch:  47450 , step:  47450 , train loss:  0.05298829631692434 , test loss:  0.08689791760192453\n",
      "Epoch:  47460 , step:  47460 , train loss:  0.05298387660655012 , test loss:  0.08689613628015486\n",
      "Epoch:  47470 , step:  47470 , train loss:  0.052979458244170975 , test loss:  0.08689435593900893\n",
      "Epoch:  47480 , step:  47480 , train loss:  0.05297504122909261 , test loss:  0.08689257657790121\n",
      "Epoch:  47490 , step:  47490 , train loss:  0.052970625560621286 , test loss:  0.08689079819624593\n",
      "Epoch:  47500 , step:  47500 , train loss:  0.052966211238063755 , test loss:  0.08688902079345795\n",
      "Epoch:  47510 , step:  47510 , train loss:  0.05296179826072728 , test loss:  0.08688724436895279\n",
      "Epoch:  47520 , step:  47520 , train loss:  0.05295738662791961 , test loss:  0.08688546892214652\n",
      "Epoch:  47530 , step:  47530 , train loss:  0.05295297633894903 , test loss:  0.08688369445245521\n",
      "Epoch:  47540 , step:  47540 , train loss:  0.0529485673931243 , test loss:  0.08688192095929598\n",
      "Epoch:  47550 , step:  47550 , train loss:  0.05294415978975471 , test loss:  0.08688014844208575\n",
      "Epoch:  47560 , step:  47560 , train loss:  0.052939753528150016 , test loss:  0.08687837690024255\n",
      "Epoch:  47570 , step:  47570 , train loss:  0.052935348607620496 , test loss:  0.08687660633318453\n",
      "Epoch:  47580 , step:  47580 , train loss:  0.05293094502747693 , test loss:  0.08687483674033018\n",
      "Epoch:  47590 , step:  47590 , train loss:  0.052926542787030594 , test loss:  0.08687306812109867\n",
      "Epoch:  47600 , step:  47600 , train loss:  0.05292214188559329 , test loss:  0.0868713004749096\n",
      "Epoch:  47610 , step:  47610 , train loss:  0.05291774232247724 , test loss:  0.08686953380118315\n",
      "Epoch:  47620 , step:  47620 , train loss:  0.0529133440969953 , test loss:  0.0868677680993396\n",
      "Epoch:  47630 , step:  47630 , train loss:  0.05290894720846068 , test loss:  0.08686600336879986\n",
      "Epoch:  47640 , step:  47640 , train loss:  0.052904551656187196 , test loss:  0.08686423960898555\n",
      "Epoch:  47650 , step:  47650 , train loss:  0.052900157439489094 , test loss:  0.08686247681931832\n",
      "Epoch:  47660 , step:  47660 , train loss:  0.05289576455768117 , test loss:  0.0868607149992207\n",
      "Epoch:  47670 , step:  47670 , train loss:  0.05289137301007865 , test loss:  0.08685895414811536\n",
      "Epoch:  47680 , step:  47680 , train loss:  0.052886982795997364 , test loss:  0.08685719426542539\n",
      "Epoch:  47690 , step:  47690 , train loss:  0.052882593914753494 , test loss:  0.08685543535057459\n",
      "Epoch:  47700 , step:  47700 , train loss:  0.05287820636566388 , test loss:  0.08685367740298706\n",
      "Epoch:  47710 , step:  47710 , train loss:  0.052873820148045696 , test loss:  0.08685192042208736\n",
      "Epoch:  47720 , step:  47720 , train loss:  0.05286943526121672 , test loss:  0.08685016440730056\n",
      "Epoch:  47730 , step:  47730 , train loss:  0.052865051704495195 , test loss:  0.08684840935805212\n",
      "Epoch:  47740 , step:  47740 , train loss:  0.05286066947719986 , test loss:  0.08684665527376809\n",
      "Epoch:  47750 , step:  47750 , train loss:  0.052856288578649914 , test loss:  0.08684490215387472\n",
      "Epoch:  47760 , step:  47760 , train loss:  0.05285190900816511 , test loss:  0.08684314999779884\n",
      "Epoch:  47770 , step:  47770 , train loss:  0.05284753076506564 , test loss:  0.08684139880496783\n",
      "Epoch:  47780 , step:  47780 , train loss:  0.052843153848672185 , test loss:  0.0868396485748095\n",
      "Epoch:  47790 , step:  47790 , train loss:  0.052838778258306 , test loss:  0.08683789930675188\n",
      "Epoch:  47800 , step:  47800 , train loss:  0.05283440399328872 , test loss:  0.08683615100022386\n",
      "Epoch:  47810 , step:  47810 , train loss:  0.05283003105294253 , test loss:  0.08683440365465425\n",
      "Epoch:  47820 , step:  47820 , train loss:  0.05282565943659012 , test loss:  0.08683265726947274\n",
      "Epoch:  47830 , step:  47830 , train loss:  0.052821289143554634 , test loss:  0.08683091184410943\n",
      "Epoch:  47840 , step:  47840 , train loss:  0.052816920173159705 , test loss:  0.08682916737799443\n",
      "Epoch:  47850 , step:  47850 , train loss:  0.05281255252472946 , test loss:  0.08682742387055892\n",
      "Epoch:  47860 , step:  47860 , train loss:  0.05280818619758854 , test loss:  0.08682568132123408\n",
      "Epoch:  47870 , step:  47870 , train loss:  0.05280382119106207 , test loss:  0.08682393972945174\n",
      "Epoch:  47880 , step:  47880 , train loss:  0.052799457504475615 , test loss:  0.0868221990946441\n",
      "Epoch:  47890 , step:  47890 , train loss:  0.05279509513715527 , test loss:  0.08682045941624393\n",
      "Epoch:  47900 , step:  47900 , train loss:  0.05279073408842762 , test loss:  0.0868187206936842\n",
      "Epoch:  47910 , step:  47910 , train loss:  0.052786374357619695 , test loss:  0.08681698292639853\n",
      "Epoch:  47920 , step:  47920 , train loss:  0.05278201594405905 , test loss:  0.08681524611382094\n",
      "Epoch:  47930 , step:  47930 , train loss:  0.05277765884707372 , test loss:  0.08681351025538589\n",
      "Epoch:  47940 , step:  47940 , train loss:  0.052773303065992205 , test loss:  0.08681177535052823\n",
      "Epoch:  47950 , step:  47950 , train loss:  0.05276894860014348 , test loss:  0.08681004139868333\n",
      "Epoch:  47960 , step:  47960 , train loss:  0.05276459544885707 , test loss:  0.08680830839928687\n",
      "Epoch:  47970 , step:  47970 , train loss:  0.052760243611462906 , test loss:  0.08680657635177519\n",
      "Epoch:  47980 , step:  47980 , train loss:  0.05275589308729146 , test loss:  0.08680484525558499\n",
      "Epoch:  47990 , step:  47990 , train loss:  0.05275154387567363 , test loss:  0.08680311511015337\n",
      "Epoch:  48000 , step:  48000 , train loss:  0.05274719597594082 , test loss:  0.08680138591491761\n",
      "Epoch:  48010 , step:  48010 , train loss:  0.05274284938742496 , test loss:  0.08679965766931586\n",
      "Epoch:  48020 , step:  48020 , train loss:  0.052738504109458385 , test loss:  0.08679793037278653\n",
      "Epoch:  48030 , step:  48030 , train loss:  0.05273416014137398 , test loss:  0.0867962040247684\n",
      "Epoch:  48040 , step:  48040 , train loss:  0.05272981748250504 , test loss:  0.08679447862470108\n",
      "Epoch:  48050 , step:  48050 , train loss:  0.05272547613218538 , test loss:  0.08679275417202398\n",
      "Epoch:  48060 , step:  48060 , train loss:  0.05272113608974934 , test loss:  0.0867910306661774\n",
      "Epoch:  48070 , step:  48070 , train loss:  0.05271679735453165 , test loss:  0.08678930810660186\n",
      "Epoch:  48080 , step:  48080 , train loss:  0.05271245992586756 , test loss:  0.08678758649273871\n",
      "Epoch:  48090 , step:  48090 , train loss:  0.05270812380309279 , test loss:  0.08678586582402899\n",
      "Epoch:  48100 , step:  48100 , train loss:  0.05270378898554357 , test loss:  0.08678414609991499\n",
      "Epoch:  48110 , step:  48110 , train loss:  0.05269945547255655 , test loss:  0.08678242731983887\n",
      "Epoch:  48120 , step:  48120 , train loss:  0.05269512326346893 , test loss:  0.08678070948324348\n",
      "Epoch:  48130 , step:  48130 , train loss:  0.0526907923576183 , test loss:  0.08677899258957211\n",
      "Epoch:  48140 , step:  48140 , train loss:  0.052686462754342786 , test loss:  0.08677727663826834\n",
      "Epoch:  48150 , step:  48150 , train loss:  0.052682134452980985 , test loss:  0.08677556162877632\n",
      "Epoch:  48160 , step:  48160 , train loss:  0.05267780745287194 , test loss:  0.08677384756054055\n",
      "Epoch:  48170 , step:  48170 , train loss:  0.05267348175335518 , test loss:  0.08677213443300617\n",
      "Epoch:  48180 , step:  48180 , train loss:  0.05266915735377072 , test loss:  0.08677042224561816\n",
      "Epoch:  48190 , step:  48190 , train loss:  0.052664834253459025 , test loss:  0.0867687109978227\n",
      "Epoch:  48200 , step:  48200 , train loss:  0.05266051245176106 , test loss:  0.086767000689066\n",
      "Epoch:  48210 , step:  48210 , train loss:  0.05265619194801825 , test loss:  0.08676529131879467\n",
      "Epoch:  48220 , step:  48220 , train loss:  0.0526518727415725 , test loss:  0.0867635828864559\n",
      "Epoch:  48230 , step:  48230 , train loss:  0.052647554831766176 , test loss:  0.08676187539149721\n",
      "Epoch:  48240 , step:  48240 , train loss:  0.05264323821794213 , test loss:  0.0867601688333667\n",
      "Epoch:  48250 , step:  48250 , train loss:  0.052638922899443656 , test loss:  0.08675846321151273\n",
      "Epoch:  48260 , step:  48260 , train loss:  0.052634608875614566 , test loss:  0.08675675852538409\n",
      "Epoch:  48270 , step:  48270 , train loss:  0.052630296145799046 , test loss:  0.08675505477443018\n",
      "Epoch:  48280 , step:  48280 , train loss:  0.05262598470934188 , test loss:  0.08675335195810036\n",
      "Epoch:  48290 , step:  48290 , train loss:  0.052621674565588264 , test loss:  0.08675165007584527\n",
      "Epoch:  48300 , step:  48300 , train loss:  0.05261736571388378 , test loss:  0.08674994912711516\n",
      "Epoch:  48310 , step:  48310 , train loss:  0.052613058153574636 , test loss:  0.08674824911136099\n",
      "Epoch:  48320 , step:  48320 , train loss:  0.052608751884007404 , test loss:  0.08674655002803439\n",
      "Epoch:  48330 , step:  48330 , train loss:  0.05260444690452916 , test loss:  0.08674485187658701\n",
      "Epoch:  48340 , step:  48340 , train loss:  0.05260014321448736 , test loss:  0.0867431546564712\n",
      "Epoch:  48350 , step:  48350 , train loss:  0.0525958408132301 , test loss:  0.08674145836713965\n",
      "Epoch:  48360 , step:  48360 , train loss:  0.052591539700105806 , test loss:  0.08673976300804548\n",
      "Epoch:  48370 , step:  48370 , train loss:  0.05258723987446337 , test loss:  0.08673806857864222\n",
      "Epoch:  48380 , step:  48380 , train loss:  0.052582941335652236 , test loss:  0.08673637507838394\n",
      "Epoch:  48390 , step:  48390 , train loss:  0.05257864408302221 , test loss:  0.08673468250672484\n",
      "Epoch:  48400 , step:  48400 , train loss:  0.05257434811592366 , test loss:  0.08673299086311992\n",
      "Epoch:  48410 , step:  48410 , train loss:  0.05257005343370735 , test loss:  0.0867313001470244\n",
      "Epoch:  48420 , step:  48420 , train loss:  0.05256576003572455 , test loss:  0.0867296103578939\n",
      "Epoch:  48430 , step:  48430 , train loss:  0.052561467921326974 , test loss:  0.08672792149518442\n",
      "Epoch:  48440 , step:  48440 , train loss:  0.05255717708986674 , test loss:  0.0867262335583525\n",
      "Epoch:  48450 , step:  48450 , train loss:  0.052552887540696556 , test loss:  0.0867245465468553\n",
      "Epoch:  48460 , step:  48460 , train loss:  0.052548599273169475 , test loss:  0.08672286046014986\n",
      "Epoch:  48470 , step:  48470 , train loss:  0.052544312286639076 , test loss:  0.08672117529769419\n",
      "Epoch:  48480 , step:  48480 , train loss:  0.05254002658045938 , test loss:  0.08671949105894629\n",
      "Epoch:  48490 , step:  48490 , train loss:  0.05253574215398486 , test loss:  0.0867178077433649\n",
      "Epoch:  48500 , step:  48500 , train loss:  0.052531459006570455 , test loss:  0.08671612535040896\n",
      "Epoch:  48510 , step:  48510 , train loss:  0.05252717713757159 , test loss:  0.08671444387953801\n",
      "Epoch:  48520 , step:  48520 , train loss:  0.05252289654634412 , test loss:  0.08671276333021197\n",
      "Epoch:  48530 , step:  48530 , train loss:  0.05251861723224433 , test loss:  0.08671108370189104\n",
      "Epoch:  48540 , step:  48540 , train loss:  0.05251433919462903 , test loss:  0.08670940499403601\n",
      "Epoch:  48550 , step:  48550 , train loss:  0.05251006243285546 , test loss:  0.08670772720610787\n",
      "Epoch:  48560 , step:  48560 , train loss:  0.05250578694628132 , test loss:  0.08670605033756822\n",
      "Epoch:  48570 , step:  48570 , train loss:  0.052501512734264726 , test loss:  0.08670437438787908\n",
      "Epoch:  48580 , step:  48580 , train loss:  0.05249723979616431 , test loss:  0.08670269935650268\n",
      "Epoch:  48590 , step:  48590 , train loss:  0.05249296813133911 , test loss:  0.08670102524290206\n",
      "Epoch:  48600 , step:  48600 , train loss:  0.05248869773914867 , test loss:  0.08669935204654022\n",
      "Epoch:  48610 , step:  48610 , train loss:  0.05248442861895295 , test loss:  0.08669767976688085\n",
      "Epoch:  48620 , step:  48620 , train loss:  0.05248016077011241 , test loss:  0.08669600840338795\n",
      "Epoch:  48630 , step:  48630 , train loss:  0.0524758941919879 , test loss:  0.08669433795552615\n",
      "Epoch:  48640 , step:  48640 , train loss:  0.05247162888394077 , test loss:  0.08669266842276009\n",
      "Epoch:  48650 , step:  48650 , train loss:  0.052467364845332806 , test loss:  0.08669099980455504\n",
      "Epoch:  48660 , step:  48660 , train loss:  0.05246310207552628 , test loss:  0.08668933210037687\n",
      "Epoch:  48670 , step:  48670 , train loss:  0.05245884057388384 , test loss:  0.08668766530969163\n",
      "Epoch:  48680 , step:  48680 , train loss:  0.05245458033976869 , test loss:  0.08668599943196571\n",
      "Epoch:  48690 , step:  48690 , train loss:  0.052450321372544406 , test loss:  0.08668433446666625\n",
      "Epoch:  48700 , step:  48700 , train loss:  0.052446063671575074 , test loss:  0.08668267041326046\n",
      "Epoch:  48710 , step:  48710 , train loss:  0.05244180723622513 , test loss:  0.0866810072712161\n",
      "Epoch:  48720 , step:  48720 , train loss:  0.052437552065859615 , test loss:  0.08667934504000138\n",
      "Epoch:  48730 , step:  48730 , train loss:  0.05243329815984387 , test loss:  0.08667768371908488\n",
      "Epoch:  48740 , step:  48740 , train loss:  0.05242904551754378 , test loss:  0.08667602330793542\n",
      "Epoch:  48750 , step:  48750 , train loss:  0.05242479413832564 , test loss:  0.08667436380602254\n",
      "Epoch:  48760 , step:  48760 , train loss:  0.05242054402155625 , test loss:  0.0866727052128162\n",
      "Epoch:  48770 , step:  48770 , train loss:  0.05241629516660274 , test loss:  0.0866710475277862\n",
      "Epoch:  48780 , step:  48780 , train loss:  0.05241204757283284 , test loss:  0.08666939075040367\n",
      "Epoch:  48790 , step:  48790 , train loss:  0.052407801239614596 , test loss:  0.08666773488013925\n",
      "Epoch:  48800 , step:  48800 , train loss:  0.05240355616631656 , test loss:  0.0866660799164644\n",
      "Epoch:  48810 , step:  48810 , train loss:  0.05239931235230778 , test loss:  0.08666442585885127\n",
      "Epoch:  48820 , step:  48820 , train loss:  0.05239506979695766 , test loss:  0.08666277270677167\n",
      "Epoch:  48830 , step:  48830 , train loss:  0.052390828499636055 , test loss:  0.08666112045969869\n",
      "Epoch:  48840 , step:  48840 , train loss:  0.05238658845971336 , test loss:  0.08665946911710501\n",
      "Epoch:  48850 , step:  48850 , train loss:  0.05238234967656034 , test loss:  0.08665781867846435\n",
      "Epoch:  48860 , step:  48860 , train loss:  0.0523781121495482 , test loss:  0.08665616914325044\n",
      "Epoch:  48870 , step:  48870 , train loss:  0.05237387587804863 , test loss:  0.08665452051093779\n",
      "Epoch:  48880 , step:  48880 , train loss:  0.05236964086143374 , test loss:  0.08665287278100084\n",
      "Epoch:  48890 , step:  48890 , train loss:  0.05236540709907606 , test loss:  0.0866512259529148\n",
      "Epoch:  48900 , step:  48900 , train loss:  0.05236117459034864 , test loss:  0.0866495800261551\n",
      "Epoch:  48910 , step:  48910 , train loss:  0.05235694333462492 , test loss:  0.08664793500019768\n",
      "Epoch:  48920 , step:  48920 , train loss:  0.05235271333127875 , test loss:  0.08664629087451885\n",
      "Epoch:  48930 , step:  48930 , train loss:  0.05234848457968448 , test loss:  0.08664464764859532\n",
      "Epoch:  48940 , step:  48940 , train loss:  0.05234425707921689 , test loss:  0.08664300532190407\n",
      "Epoch:  48950 , step:  48950 , train loss:  0.05234003082925119 , test loss:  0.08664136389392263\n",
      "Epoch:  48960 , step:  48960 , train loss:  0.05233580582916302 , test loss:  0.08663972336412894\n",
      "Epoch:  48970 , step:  48970 , train loss:  0.0523315820783285 , test loss:  0.08663808373200127\n",
      "Epoch:  48980 , step:  48980 , train loss:  0.05232735957612411 , test loss:  0.08663644499701856\n",
      "Epoch:  48990 , step:  48990 , train loss:  0.05232313832192691 , test loss:  0.08663480715865966\n",
      "Epoch:  49000 , step:  49000 , train loss:  0.05231891831511424 , test loss:  0.08663317021640418\n",
      "Epoch:  49010 , step:  49010 , train loss:  0.05231469955506398 , test loss:  0.08663153416973197\n",
      "Epoch:  49020 , step:  49020 , train loss:  0.05231048204115444 , test loss:  0.08662989901812335\n",
      "Epoch:  49030 , step:  49030 , train loss:  0.052306265772764315 , test loss:  0.08662826476105918\n",
      "Epoch:  49040 , step:  49040 , train loss:  0.05230205074927284 , test loss:  0.08662663139802035\n",
      "Epoch:  49050 , step:  49050 , train loss:  0.052297836970059526 , test loss:  0.08662499892848828\n",
      "Epoch:  49060 , step:  49060 , train loss:  0.052293624434504485 , test loss:  0.08662336735194516\n",
      "Epoch:  49070 , step:  49070 , train loss:  0.05228941314198818 , test loss:  0.08662173666787319\n",
      "Epoch:  49080 , step:  49080 , train loss:  0.05228520309189149 , test loss:  0.08662010687575496\n",
      "Epoch:  49090 , step:  49090 , train loss:  0.05228099428359583 , test loss:  0.08661847797507369\n",
      "Epoch:  49100 , step:  49100 , train loss:  0.05227678671648292 , test loss:  0.08661684996531262\n",
      "Epoch:  49110 , step:  49110 , train loss:  0.05227258038993506 , test loss:  0.08661522284595595\n",
      "Epoch:  49120 , step:  49120 , train loss:  0.052268375303334856 , test loss:  0.0866135966164878\n",
      "Epoch:  49130 , step:  49130 , train loss:  0.052264171456065364 , test loss:  0.08661197127639275\n",
      "Epoch:  49140 , step:  49140 , train loss:  0.05225996884751018 , test loss:  0.08661034682515593\n",
      "Epoch:  49150 , step:  49150 , train loss:  0.05225576747705323 , test loss:  0.08660872326226295\n",
      "Epoch:  49160 , step:  49160 , train loss:  0.052251567344078924 , test loss:  0.0866071005871996\n",
      "Epoch:  49170 , step:  49170 , train loss:  0.05224736844797204 , test loss:  0.08660547879945205\n",
      "Epoch:  49180 , step:  49180 , train loss:  0.05224317078811789 , test loss:  0.08660385789850694\n",
      "Epoch:  49190 , step:  49190 , train loss:  0.05223897436390212 , test loss:  0.08660223788385119\n",
      "Epoch:  49200 , step:  49200 , train loss:  0.05223477917471088 , test loss:  0.08660061875497245\n",
      "Epoch:  49210 , step:  49210 , train loss:  0.05223058521993069 , test loss:  0.08659900051135842\n",
      "Epoch:  49220 , step:  49220 , train loss:  0.052226392498948546 , test loss:  0.08659738315249732\n",
      "Epoch:  49230 , step:  49230 , train loss:  0.052222201011151895 , test loss:  0.08659576667787781\n",
      "Epoch:  49240 , step:  49240 , train loss:  0.052218010755928516 , test loss:  0.08659415108698881\n",
      "Epoch:  49250 , step:  49250 , train loss:  0.052213821732666726 , test loss:  0.0865925363793197\n",
      "Epoch:  49260 , step:  49260 , train loss:  0.052209633940755215 , test loss:  0.08659092255436021\n",
      "Epoch:  49270 , step:  49270 , train loss:  0.05220544737958309 , test loss:  0.08658930961160059\n",
      "Epoch:  49280 , step:  49280 , train loss:  0.052201262048539934 , test loss:  0.08658769755053139\n",
      "Epoch:  49290 , step:  49290 , train loss:  0.052197077947015724 , test loss:  0.08658608637064352\n",
      "Epoch:  49300 , step:  49300 , train loss:  0.05219289507440089 , test loss:  0.08658447607142832\n",
      "Epoch:  49310 , step:  49310 , train loss:  0.05218871343008625 , test loss:  0.08658286665237731\n",
      "Epoch:  49320 , step:  49320 , train loss:  0.05218453301346306 , test loss:  0.08658125811298298\n",
      "Epoch:  49330 , step:  49330 , train loss:  0.05218035382392307 , test loss:  0.0865796504527375\n",
      "Epoch:  49340 , step:  49340 , train loss:  0.05217617586085834 , test loss:  0.08657804367113384\n",
      "Epoch:  49350 , step:  49350 , train loss:  0.05217199912366143 , test loss:  0.0865764377676652\n",
      "Epoch:  49360 , step:  49360 , train loss:  0.05216782361172535 , test loss:  0.08657483274182531\n",
      "Epoch:  49370 , step:  49370 , train loss:  0.05216364932444343 , test loss:  0.08657322859310829\n",
      "Epoch:  49380 , step:  49380 , train loss:  0.05215947626120956 , test loss:  0.0865716253210084\n",
      "Epoch:  49390 , step:  49390 , train loss:  0.052155304421417935 , test loss:  0.08657002292502046\n",
      "Epoch:  49400 , step:  49400 , train loss:  0.052151133804463234 , test loss:  0.08656842140463976\n",
      "Epoch:  49410 , step:  49410 , train loss:  0.05214696440974055 , test loss:  0.08656682075936181\n",
      "Epoch:  49420 , step:  49420 , train loss:  0.05214279623664541 , test loss:  0.08656522098868258\n",
      "Epoch:  49430 , step:  49430 , train loss:  0.05213862928457372 , test loss:  0.08656362209209842\n",
      "Epoch:  49440 , step:  49440 , train loss:  0.05213446355292188 , test loss:  0.08656202406910601\n",
      "Epoch:  49450 , step:  49450 , train loss:  0.052130299041086674 , test loss:  0.08656042691920261\n",
      "Epoch:  49460 , step:  49460 , train loss:  0.05212613574846525 , test loss:  0.08655883064188552\n",
      "Epoch:  49470 , step:  49470 , train loss:  0.05212197367445527 , test loss:  0.08655723523665264\n",
      "Epoch:  49480 , step:  49480 , train loss:  0.05211781281845477 , test loss:  0.0865556407030023\n",
      "Epoch:  49490 , step:  49490 , train loss:  0.052113653179862246 , test loss:  0.08655404704043325\n",
      "Epoch:  49500 , step:  49500 , train loss:  0.052109494758076524 , test loss:  0.08655245424844424\n",
      "Epoch:  49510 , step:  49510 , train loss:  0.05210533755249697 , test loss:  0.08655086232653478\n",
      "Epoch:  49520 , step:  49520 , train loss:  0.052101181562523254 , test loss:  0.08654927127420477\n",
      "Epoch:  49530 , step:  49530 , train loss:  0.052097026787555574 , test loss:  0.08654768109095423\n",
      "Epoch:  49540 , step:  49540 , train loss:  0.05209287322699444 , test loss:  0.08654609177628375\n",
      "Epoch:  49550 , step:  49550 , train loss:  0.052088720880240844 , test loss:  0.08654450332969411\n",
      "Epoch:  49560 , step:  49560 , train loss:  0.05208456974669618 , test loss:  0.08654291575068683\n",
      "Epoch:  49570 , step:  49570 , train loss:  0.052080419825762295 , test loss:  0.08654132903876345\n",
      "Epoch:  49580 , step:  49580 , train loss:  0.05207627111684138 , test loss:  0.08653974319342617\n",
      "Epoch:  49590 , step:  49590 , train loss:  0.0520721236193361 , test loss:  0.08653815821417711\n",
      "Epoch:  49600 , step:  49600 , train loss:  0.0520679773326495 , test loss:  0.08653657410051944\n",
      "Epoch:  49610 , step:  49610 , train loss:  0.0520638322561851 , test loss:  0.08653499085195612\n",
      "Epoch:  49620 , step:  49620 , train loss:  0.05205968838934676 , test loss:  0.08653340846799093\n",
      "Epoch:  49630 , step:  49630 , train loss:  0.05205554573153881 , test loss:  0.0865318269481275\n",
      "Epoch:  49640 , step:  49640 , train loss:  0.05205140428216596 , test loss:  0.08653024629187038\n",
      "Epoch:  49650 , step:  49650 , train loss:  0.05204726404063334 , test loss:  0.08652866649872416\n",
      "Epoch:  49660 , step:  49660 , train loss:  0.052043125006346516 , test loss:  0.08652708756819392\n",
      "Epoch:  49670 , step:  49670 , train loss:  0.05203898717871148 , test loss:  0.08652550949978517\n",
      "Epoch:  49680 , step:  49680 , train loss:  0.05203485055713457 , test loss:  0.08652393229300379\n",
      "Epoch:  49690 , step:  49690 , train loss:  0.05203071514102258 , test loss:  0.08652235594735572\n",
      "Epoch:  49700 , step:  49700 , train loss:  0.05202658092978277 , test loss:  0.08652078046234779\n",
      "Epoch:  49710 , step:  49710 , train loss:  0.05202244792282268 , test loss:  0.08651920583748664\n",
      "Epoch:  49720 , step:  49720 , train loss:  0.052018316119550385 , test loss:  0.08651763207227987\n",
      "Epoch:  49730 , step:  49730 , train loss:  0.0520141855193743 , test loss:  0.08651605916623513\n",
      "Epoch:  49740 , step:  49740 , train loss:  0.05201005612170332 , test loss:  0.0865144871188603\n",
      "Epoch:  49750 , step:  49750 , train loss:  0.05200592792594663 , test loss:  0.0865129159296639\n",
      "Epoch:  49760 , step:  49760 , train loss:  0.05200180093151399 , test loss:  0.08651134559815471\n",
      "Epoch:  49770 , step:  49770 , train loss:  0.05199767513781539 , test loss:  0.08650977612384197\n",
      "Epoch:  49780 , step:  49780 , train loss:  0.05199355054426142 , test loss:  0.0865082075062353\n",
      "Epoch:  49790 , step:  49790 , train loss:  0.051989427150262925 , test loss:  0.08650663974484435\n",
      "Epoch:  49800 , step:  49800 , train loss:  0.05198530495523119 , test loss:  0.08650507283917952\n",
      "Epoch:  49810 , step:  49810 , train loss:  0.05198118395857797 , test loss:  0.08650350678875156\n",
      "Epoch:  49820 , step:  49820 , train loss:  0.05197706415971538 , test loss:  0.08650194159307155\n",
      "Epoch:  49830 , step:  49830 , train loss:  0.051972945558055955 , test loss:  0.08650037725165058\n",
      "Epoch:  49840 , step:  49840 , train loss:  0.051968828153012646 , test loss:  0.08649881376400065\n",
      "Epoch:  49850 , step:  49850 , train loss:  0.051964711943998786 , test loss:  0.08649725112963394\n",
      "Epoch:  49860 , step:  49860 , train loss:  0.051960596930428114 , test loss:  0.08649568934806282\n",
      "Epoch:  49870 , step:  49870 , train loss:  0.05195648311171483 , test loss:  0.08649412841880016\n",
      "Epoch:  49880 , step:  49880 , train loss:  0.051952370487273455 , test loss:  0.08649256834135934\n",
      "Epoch:  49890 , step:  49890 , train loss:  0.051948259056518974 , test loss:  0.08649100911525386\n",
      "Epoch:  49900 , step:  49900 , train loss:  0.05194414881886679 , test loss:  0.0864894507399976\n",
      "Epoch:  49910 , step:  49910 , train loss:  0.05194003977373265 , test loss:  0.08648789321510507\n",
      "Epoch:  49920 , step:  49920 , train loss:  0.05193593192053278 , test loss:  0.08648633654009073\n",
      "Epoch:  49930 , step:  49930 , train loss:  0.051931825258683714 , test loss:  0.08648478071447\n",
      "Epoch:  49940 , step:  49940 , train loss:  0.051927719787602473 , test loss:  0.08648322573775803\n",
      "Epoch:  49950 , step:  49950 , train loss:  0.051923615506706454 , test loss:  0.08648167160947065\n",
      "Epoch:  49960 , step:  49960 , train loss:  0.05191951241541349 , test loss:  0.0864801183291242\n",
      "Epoch:  49970 , step:  49970 , train loss:  0.051915410513141705 , test loss:  0.08647856589623501\n",
      "Epoch:  49980 , step:  49980 , train loss:  0.051911309799309774 , test loss:  0.08647701431032011\n",
      "Epoch:  49990 , step:  49990 , train loss:  0.05190721027333669 , test loss:  0.08647546357089668\n",
      "Epoch:  50000 , step:  50000 , train loss:  0.05190311193464181 , test loss:  0.08647391367748236\n",
      "Epoch:  50010 , step:  50010 , train loss:  0.051899014782645 , test loss:  0.08647236462959525\n",
      "Epoch:  50020 , step:  50020 , train loss:  0.051894918816766444 , test loss:  0.08647081642675342\n",
      "Epoch:  50030 , step:  50030 , train loss:  0.05189082403642675 , test loss:  0.08646926906847581\n",
      "Epoch:  50040 , step:  50040 , train loss:  0.05188673044104694 , test loss:  0.08646772255428141\n",
      "Epoch:  50050 , step:  50050 , train loss:  0.05188263803004841 , test loss:  0.08646617688368959\n",
      "Epoch:  50060 , step:  50060 , train loss:  0.05187854680285298 , test loss:  0.08646463205622038\n",
      "Epoch:  50070 , step:  50070 , train loss:  0.05187445675888285 , test loss:  0.08646308807139362\n",
      "Epoch:  50080 , step:  50080 , train loss:  0.05187036789756065 , test loss:  0.08646154492873015\n",
      "Epoch:  50090 , step:  50090 , train loss:  0.05186628021830934 , test loss:  0.0864600026277506\n",
      "Epoch:  50100 , step:  50100 , train loss:  0.05186219372055237 , test loss:  0.08645846116797634\n",
      "Epoch:  50110 , step:  50110 , train loss:  0.05185810840371351 , test loss:  0.08645692054892883\n",
      "Epoch:  50120 , step:  50120 , train loss:  0.05185402426721697 , test loss:  0.08645538077013012\n",
      "Epoch:  50130 , step:  50130 , train loss:  0.05184994131048732 , test loss:  0.0864538418311025\n",
      "Epoch:  50140 , step:  50140 , train loss:  0.05184585953294957 , test loss:  0.08645230373136863\n",
      "Epoch:  50150 , step:  50150 , train loss:  0.051841778934029116 , test loss:  0.08645076647045163\n",
      "Epoch:  50160 , step:  50160 , train loss:  0.05183769951315174 , test loss:  0.08644923004787478\n",
      "Epoch:  50170 , step:  50170 , train loss:  0.05183362126974359 , test loss:  0.08644769446316196\n",
      "Epoch:  50180 , step:  50180 , train loss:  0.05182954420323128 , test loss:  0.08644615971583719\n",
      "Epoch:  50190 , step:  50190 , train loss:  0.051825468313041745 , test loss:  0.08644462580542497\n",
      "Epoch:  50200 , step:  50200 , train loss:  0.051821393598602376 , test loss:  0.08644309273144998\n",
      "Epoch:  50210 , step:  50210 , train loss:  0.051817320059340914 , test loss:  0.0864415604934375\n",
      "Epoch:  50220 , step:  50220 , train loss:  0.051813247694685496 , test loss:  0.08644002909091308\n",
      "Epoch:  50230 , step:  50230 , train loss:  0.051809176504064684 , test loss:  0.08643849852340257\n",
      "Epoch:  50240 , step:  50240 , train loss:  0.051805106486907414 , test loss:  0.08643696879043229\n",
      "Epoch:  50250 , step:  50250 , train loss:  0.05180103764264302 , test loss:  0.08643543989152867\n",
      "Epoch:  50260 , step:  50260 , train loss:  0.05179696997070121 , test loss:  0.08643391182621885\n",
      "Epoch:  50270 , step:  50270 , train loss:  0.051792903470512096 , test loss:  0.08643238459402995\n",
      "Epoch:  50280 , step:  50280 , train loss:  0.05178883814150619 , test loss:  0.08643085819448983\n",
      "Epoch:  50290 , step:  50290 , train loss:  0.051784773983114374 , test loss:  0.08642933262712629\n",
      "Epoch:  50300 , step:  50300 , train loss:  0.051780710994767984 , test loss:  0.08642780789146798\n",
      "Epoch:  50310 , step:  50310 , train loss:  0.05177664917589864 , test loss:  0.08642628398704336\n",
      "Epoch:  50320 , step:  50320 , train loss:  0.05177258852593844 , test loss:  0.08642476091338165\n",
      "Epoch:  50330 , step:  50330 , train loss:  0.05176852904431979 , test loss:  0.08642323867001232\n",
      "Epoch:  50340 , step:  50340 , train loss:  0.05176447073047566 , test loss:  0.08642171725646484\n",
      "Epoch:  50350 , step:  50350 , train loss:  0.05176041358383915 , test loss:  0.08642019667226951\n",
      "Epoch:  50360 , step:  50360 , train loss:  0.05175635760384394 , test loss:  0.08641867691695694\n",
      "Epoch:  50370 , step:  50370 , train loss:  0.051752302789924105 , test loss:  0.08641715799005785\n",
      "Epoch:  50380 , step:  50380 , train loss:  0.05174824914151392 , test loss:  0.08641563989110339\n",
      "Epoch:  50390 , step:  50390 , train loss:  0.051744196658048254 , test loss:  0.08641412261962525\n",
      "Epoch:  50400 , step:  50400 , train loss:  0.05174014533896229 , test loss:  0.08641260617515521\n",
      "Epoch:  50410 , step:  50410 , train loss:  0.05173609518369158 , test loss:  0.08641109055722547\n",
      "Epoch:  50420 , step:  50420 , train loss:  0.05173204619167206 , test loss:  0.08640957576536859\n",
      "Epoch:  50430 , step:  50430 , train loss:  0.05172799836234009 , test loss:  0.08640806179911749\n",
      "Epoch:  50440 , step:  50440 , train loss:  0.051723951695132345 , test loss:  0.08640654865800555\n",
      "Epoch:  50450 , step:  50450 , train loss:  0.051719906189486006 , test loss:  0.08640503634156642\n",
      "Epoch:  50460 , step:  50460 , train loss:  0.05171586184483852 , test loss:  0.08640352484933403\n",
      "Epoch:  50470 , step:  50470 , train loss:  0.05171181866062779 , test loss:  0.0864020141808426\n",
      "Epoch:  50480 , step:  50480 , train loss:  0.051707776636292044 , test loss:  0.08640050433562693\n",
      "Epoch:  50490 , step:  50490 , train loss:  0.05170373577126996 , test loss:  0.08639899531322194\n",
      "Epoch:  50500 , step:  50500 , train loss:  0.0516996960650006 , test loss:  0.08639748711316321\n",
      "Epoch:  50510 , step:  50510 , train loss:  0.05169565751692331 , test loss:  0.08639597973498615\n",
      "Epoch:  50520 , step:  50520 , train loss:  0.051691620126477926 , test loss:  0.08639447317822699\n",
      "Epoch:  50530 , step:  50530 , train loss:  0.05168758389310463 , test loss:  0.08639296744242206\n",
      "Epoch:  50540 , step:  50540 , train loss:  0.05168354881624399 , test loss:  0.0863914625271082\n",
      "Epoch:  50550 , step:  50550 , train loss:  0.05167951489533698 , test loss:  0.08638995843182246\n",
      "Epoch:  50560 , step:  50560 , train loss:  0.05167548212982486 , test loss:  0.0863884551561021\n",
      "Epoch:  50570 , step:  50570 , train loss:  0.0516714505191494 , test loss:  0.08638695269948517\n",
      "Epoch:  50580 , step:  50580 , train loss:  0.051667420062752656 , test loss:  0.08638545106150977\n",
      "Epoch:  50590 , step:  50590 , train loss:  0.05166339076007716 , test loss:  0.0863839502417143\n",
      "Epoch:  50600 , step:  50600 , train loss:  0.0516593626105657 , test loss:  0.08638245023963759\n",
      "Epoch:  50610 , step:  50610 , train loss:  0.05165533561366152 , test loss:  0.08638095105481866\n",
      "Epoch:  50620 , step:  50620 , train loss:  0.051651309768808294 , test loss:  0.08637945268679723\n",
      "Epoch:  50630 , step:  50630 , train loss:  0.05164728507544995 , test loss:  0.08637795513511311\n",
      "Epoch:  50640 , step:  50640 , train loss:  0.05164326153303091 , test loss:  0.08637645839930633\n",
      "Epoch:  50650 , step:  50650 , train loss:  0.05163923914099586 , test loss:  0.08637496247891742\n",
      "Epoch:  50660 , step:  50660 , train loss:  0.05163521789879001 , test loss:  0.0863734673734876\n",
      "Epoch:  50670 , step:  50670 , train loss:  0.05163119780585883 , test loss:  0.08637197308255769\n",
      "Epoch:  50680 , step:  50680 , train loss:  0.05162717886164819 , test loss:  0.08637047960566939\n",
      "Epoch:  50690 , step:  50690 , train loss:  0.0516231610656044 , test loss:  0.0863689869423645\n",
      "Epoch:  50700 , step:  50700 , train loss:  0.05161914441717405 , test loss:  0.08636749509218544\n",
      "Epoch:  50710 , step:  50710 , train loss:  0.05161512891580421 , test loss:  0.08636600405467465\n",
      "Epoch:  50720 , step:  50720 , train loss:  0.051611114560942244 , test loss:  0.086364513829375\n",
      "Epoch:  50730 , step:  50730 , train loss:  0.05160710135203594 , test loss:  0.0863630244158299\n",
      "Epoch:  50740 , step:  50740 , train loss:  0.051603089288533445 , test loss:  0.08636153581358277\n",
      "Epoch:  50750 , step:  50750 , train loss:  0.05159907836988326 , test loss:  0.0863600480221776\n",
      "Epoch:  50760 , step:  50760 , train loss:  0.0515950685955343 , test loss:  0.08635856104115873\n",
      "Epoch:  50770 , step:  50770 , train loss:  0.05159105996493586 , test loss:  0.0863570748700708\n",
      "Epoch:  50780 , step:  50780 , train loss:  0.051587052477537566 , test loss:  0.08635558950845856\n",
      "Epoch:  50790 , step:  50790 , train loss:  0.05158304613278943 , test loss:  0.08635410495586754\n",
      "Epoch:  50800 , step:  50800 , train loss:  0.05157904093014189 , test loss:  0.08635262121184317\n",
      "Epoch:  50810 , step:  50810 , train loss:  0.051575036869045666 , test loss:  0.0863511382759314\n",
      "Epoch:  50820 , step:  50820 , train loss:  0.05157103394895194 , test loss:  0.0863496561476786\n",
      "Epoch:  50830 , step:  50830 , train loss:  0.05156703216931221 , test loss:  0.08634817482663126\n",
      "Epoch:  50840 , step:  50840 , train loss:  0.05156303152957838 , test loss:  0.08634669431233664\n",
      "Epoch:  50850 , step:  50850 , train loss:  0.051559032029202666 , test loss:  0.08634521460434175\n",
      "Epoch:  50860 , step:  50860 , train loss:  0.05155503366763779 , test loss:  0.08634373570219445\n",
      "Epoch:  50870 , step:  50870 , train loss:  0.05155103644433669 , test loss:  0.08634225760544256\n",
      "Epoch:  50880 , step:  50880 , train loss:  0.051547040358752766 , test loss:  0.08634078031363443\n",
      "Epoch:  50890 , step:  50890 , train loss:  0.05154304541033975 , test loss:  0.08633930382631864\n",
      "Epoch:  50900 , step:  50900 , train loss:  0.05153905159855181 , test loss:  0.08633782814304432\n",
      "Epoch:  50910 , step:  50910 , train loss:  0.05153505892284336 , test loss:  0.08633635326336059\n",
      "Epoch:  50920 , step:  50920 , train loss:  0.05153106738266931 , test loss:  0.08633487918681719\n",
      "Epoch:  50930 , step:  50930 , train loss:  0.051527076977484855 , test loss:  0.08633340591296416\n",
      "Epoch:  50940 , step:  50940 , train loss:  0.05152308770674564 , test loss:  0.0863319334413517\n",
      "Epoch:  50950 , step:  50950 , train loss:  0.05151909956990762 , test loss:  0.08633046177153048\n",
      "Epoch:  50960 , step:  50960 , train loss:  0.0515151125664271 , test loss:  0.0863289909030514\n",
      "Epoch:  50970 , step:  50970 , train loss:  0.05151112669576081 , test loss:  0.08632752083546595\n",
      "Epoch:  50980 , step:  50980 , train loss:  0.051507141957365804 , test loss:  0.08632605156832561\n",
      "Epoch:  50990 , step:  50990 , train loss:  0.05150315835069958 , test loss:  0.08632458310118234\n",
      "Epoch:  51000 , step:  51000 , train loss:  0.05149917587521988 , test loss:  0.08632311543358862\n",
      "Epoch:  51010 , step:  51010 , train loss:  0.0514951945303849 , test loss:  0.08632164856509693\n",
      "Epoch:  51020 , step:  51020 , train loss:  0.051491214315653185 , test loss:  0.08632018249526013\n",
      "Epoch:  51030 , step:  51030 , train loss:  0.051487235230483645 , test loss:  0.08631871722363164\n",
      "Epoch:  51040 , step:  51040 , train loss:  0.051483257274335545 , test loss:  0.086317252749765\n",
      "Epoch:  51050 , step:  51050 , train loss:  0.051479280446668545 , test loss:  0.08631578907321438\n",
      "Epoch:  51060 , step:  51060 , train loss:  0.051475304746942646 , test loss:  0.08631432619353385\n",
      "Epoch:  51070 , step:  51070 , train loss:  0.05147133017461821 , test loss:  0.08631286411027812\n",
      "Epoch:  51080 , step:  51080 , train loss:  0.05146735672915597 , test loss:  0.08631140282300184\n",
      "Epoch:  51090 , step:  51090 , train loss:  0.05146338441001705 , test loss:  0.08630994233126078\n",
      "Epoch:  51100 , step:  51100 , train loss:  0.051459413216662905 , test loss:  0.08630848263461013\n",
      "Epoch:  51110 , step:  51110 , train loss:  0.051455443148555366 , test loss:  0.0863070237326059\n",
      "Epoch:  51120 , step:  51120 , train loss:  0.05145147420515659 , test loss:  0.08630556562480445\n",
      "Epoch:  51130 , step:  51130 , train loss:  0.051447506385929216 , test loss:  0.08630410831076232\n",
      "Epoch:  51140 , step:  51140 , train loss:  0.051443539690336076 , test loss:  0.08630265179003625\n",
      "Epoch:  51150 , step:  51150 , train loss:  0.051439574117840475 , test loss:  0.08630119606218364\n",
      "Epoch:  51160 , step:  51160 , train loss:  0.05143560966790612 , test loss:  0.08629974112676189\n",
      "Epoch:  51170 , step:  51170 , train loss:  0.051431646339996924 , test loss:  0.08629828698332911\n",
      "Epoch:  51180 , step:  51180 , train loss:  0.05142768413357735 , test loss:  0.08629683363144326\n",
      "Epoch:  51190 , step:  51190 , train loss:  0.05142372304811205 , test loss:  0.08629538107066309\n",
      "Epoch:  51200 , step:  51200 , train loss:  0.05141976308306614 , test loss:  0.08629392930054723\n",
      "Epoch:  51210 , step:  51210 , train loss:  0.05141580423790508 , test loss:  0.08629247832065498\n",
      "Epoch:  51220 , step:  51220 , train loss:  0.051411846512094656 , test loss:  0.08629102813054582\n",
      "Epoch:  51230 , step:  51230 , train loss:  0.05140788990510109 , test loss:  0.08628957872977967\n",
      "Epoch:  51240 , step:  51240 , train loss:  0.05140393441639088 , test loss:  0.08628813011791647\n",
      "Epoch:  51250 , step:  51250 , train loss:  0.051399980045430914 , test loss:  0.08628668229451694\n",
      "Epoch:  51260 , step:  51260 , train loss:  0.051396026791688455 , test loss:  0.08628523525914172\n",
      "Epoch:  51270 , step:  51270 , train loss:  0.05139207465463111 , test loss:  0.08628378901135209\n",
      "Epoch:  51280 , step:  51280 , train loss:  0.051388123633726863 , test loss:  0.08628234355070918\n",
      "Epoch:  51290 , step:  51290 , train loss:  0.05138417372844399 , test loss:  0.086280898876775\n",
      "Epoch:  51300 , step:  51300 , train loss:  0.05138022493825122 , test loss:  0.08627945498911159\n",
      "Epoch:  51310 , step:  51310 , train loss:  0.051376277262617606 , test loss:  0.08627801188728136\n",
      "Epoch:  51320 , step:  51320 , train loss:  0.05137233070101252 , test loss:  0.08627656957084712\n",
      "Epoch:  51330 , step:  51330 , train loss:  0.05136838525290572 , test loss:  0.08627512803937164\n",
      "Epoch:  51340 , step:  51340 , train loss:  0.05136444091776734 , test loss:  0.0862736872924186\n",
      "Epoch:  51350 , step:  51350 , train loss:  0.05136049769506782 , test loss:  0.0862722473295517\n",
      "Epoch:  51360 , step:  51360 , train loss:  0.051356555584278006 , test loss:  0.08627080815033471\n",
      "Epoch:  51370 , step:  51370 , train loss:  0.051352614584869075 , test loss:  0.08626936975433205\n",
      "Epoch:  51380 , step:  51380 , train loss:  0.051348674696312574 , test loss:  0.08626793214110834\n",
      "Epoch:  51390 , step:  51390 , train loss:  0.05134473591808037 , test loss:  0.08626649531022874\n",
      "Epoch:  51400 , step:  51400 , train loss:  0.05134079824964474 , test loss:  0.08626505926125835\n",
      "Epoch:  51410 , step:  51410 , train loss:  0.05133686169047828 , test loss:  0.08626362399376293\n",
      "Epoch:  51420 , step:  51420 , train loss:  0.051332926240053915 , test loss:  0.08626218950730831\n",
      "Epoch:  51430 , step:  51430 , train loss:  0.051328991897845 , test loss:  0.08626075580146064\n",
      "Epoch:  51440 , step:  51440 , train loss:  0.05132505866332516 , test loss:  0.08625932287578662\n",
      "Epoch:  51450 , step:  51450 , train loss:  0.05132112653596846 , test loss:  0.08625789072985318\n",
      "Epoch:  51460 , step:  51460 , train loss:  0.05131719551524921 , test loss:  0.08625645936322737\n",
      "Epoch:  51470 , step:  51470 , train loss:  0.05131326560064218 , test loss:  0.08625502877547682\n",
      "Epoch:  51480 , step:  51480 , train loss:  0.05130933679162241 , test loss:  0.0862535989661693\n",
      "Epoch:  51490 , step:  51490 , train loss:  0.05130540908766535 , test loss:  0.08625216993487292\n",
      "Epoch:  51500 , step:  51500 , train loss:  0.05130148248824678 , test loss:  0.0862507416811563\n",
      "Epoch:  51510 , step:  51510 , train loss:  0.0512975569928428 , test loss:  0.08624931420458788\n",
      "Epoch:  51520 , step:  51520 , train loss:  0.05129363260092995 , test loss:  0.08624788750473726\n",
      "Epoch:  51530 , step:  51530 , train loss:  0.051289709311985 , test loss:  0.08624646158117351\n",
      "Epoch:  51540 , step:  51540 , train loss:  0.0512857871254852 , test loss:  0.08624503643346634\n",
      "Epoch:  51550 , step:  51550 , train loss:  0.05128186604090803 , test loss:  0.08624361206118591\n",
      "Epoch:  51560 , step:  51560 , train loss:  0.05127794605773137 , test loss:  0.08624218846390268\n",
      "Epoch:  51570 , step:  51570 , train loss:  0.05127402717543349 , test loss:  0.08624076564118709\n",
      "Epoch:  51580 , step:  51580 , train loss:  0.05127010939349295 , test loss:  0.08623934359261028\n",
      "Epoch:  51590 , step:  51590 , train loss:  0.051266192711388686 , test loss:  0.08623792231774328\n",
      "Epoch:  51600 , step:  51600 , train loss:  0.051262277128599994 , test loss:  0.08623650181615806\n",
      "Epoch:  51610 , step:  51610 , train loss:  0.05125836264460648 , test loss:  0.08623508208742636\n",
      "Epoch:  51620 , step:  51620 , train loss:  0.05125444925888811 , test loss:  0.08623366313112032\n",
      "Epoch:  51630 , step:  51630 , train loss:  0.051250536970925235 , test loss:  0.08623224494681267\n",
      "Epoch:  51640 , step:  51640 , train loss:  0.05124662578019852 , test loss:  0.08623082753407602\n",
      "Epoch:  51650 , step:  51650 , train loss:  0.051242715686189 , test loss:  0.0862294108924841\n",
      "Epoch:  51660 , step:  51660 , train loss:  0.05123880668837802 , test loss:  0.08622799502160981\n",
      "Epoch:  51670 , step:  51670 , train loss:  0.05123489878624729 , test loss:  0.08622657992102722\n",
      "Epoch:  51680 , step:  51680 , train loss:  0.051230991979278884 , test loss:  0.08622516559031024\n",
      "Epoch:  51690 , step:  51690 , train loss:  0.0512270862669552 , test loss:  0.0862237520290334\n",
      "Epoch:  51700 , step:  51700 , train loss:  0.051223181648759004 , test loss:  0.08622233923677144\n",
      "Epoch:  51710 , step:  51710 , train loss:  0.05121927812417337 , test loss:  0.08622092721309944\n",
      "Epoch:  51720 , step:  51720 , train loss:  0.05121537569268177 , test loss:  0.08621951595759274\n",
      "Epoch:  51730 , step:  51730 , train loss:  0.05121147435376795 , test loss:  0.08621810546982685\n",
      "Epoch:  51740 , step:  51740 , train loss:  0.05120757410691609 , test loss:  0.08621669574937785\n",
      "Epoch:  51750 , step:  51750 , train loss:  0.05120367495161061 , test loss:  0.08621528679582209\n",
      "Epoch:  51760 , step:  51760 , train loss:  0.05119977688733639 , test loss:  0.08621387860873587\n",
      "Epoch:  51770 , step:  51770 , train loss:  0.051195879913578546 , test loss:  0.0862124711876963\n",
      "Epoch:  51780 , step:  51780 , train loss:  0.0511919840298226 , test loss:  0.08621106453228053\n",
      "Epoch:  51790 , step:  51790 , train loss:  0.05118808923555439 , test loss:  0.08620965864206605\n",
      "Epoch:  51800 , step:  51800 , train loss:  0.05118419553026016 , test loss:  0.08620825351663079\n",
      "Epoch:  51810 , step:  51810 , train loss:  0.05118030291342637 , test loss:  0.08620684915555267\n",
      "Epoch:  51820 , step:  51820 , train loss:  0.05117641138453996 , test loss:  0.08620544555841025\n",
      "Epoch:  51830 , step:  51830 , train loss:  0.05117252094308811 , test loss:  0.08620404272478228\n",
      "Epoch:  51840 , step:  51840 , train loss:  0.051168631588558396 , test loss:  0.08620264065424775\n",
      "Epoch:  51850 , step:  51850 , train loss:  0.051164743320438716 , test loss:  0.08620123934638592\n",
      "Epoch:  51860 , step:  51860 , train loss:  0.05116085613821732 , test loss:  0.08619983880077657\n",
      "Epoch:  51870 , step:  51870 , train loss:  0.05115697004138278 , test loss:  0.08619843901699975\n",
      "Epoch:  51880 , step:  51880 , train loss:  0.05115308502942405 , test loss:  0.08619703999463532\n",
      "Epoch:  51890 , step:  51890 , train loss:  0.051149201101830376 , test loss:  0.08619564173326434\n",
      "Epoch:  51900 , step:  51900 , train loss:  0.05114531825809132 , test loss:  0.0861942442324674\n",
      "Epoch:  51910 , step:  51910 , train loss:  0.0511414364976969 , test loss:  0.08619284749182579\n",
      "Epoch:  51920 , step:  51920 , train loss:  0.051137555820137355 , test loss:  0.08619145151092081\n",
      "Epoch:  51930 , step:  51930 , train loss:  0.0511336762249033 , test loss:  0.08619005628933447\n",
      "Epoch:  51940 , step:  51940 , train loss:  0.05112979771148574 , test loss:  0.08618866182664872\n",
      "Epoch:  51950 , step:  51950 , train loss:  0.051125920279375923 , test loss:  0.08618726812244606\n",
      "Epoch:  51960 , step:  51960 , train loss:  0.051122043928065526 , test loss:  0.08618587517630896\n",
      "Epoch:  51970 , step:  51970 , train loss:  0.0511181686570465 , test loss:  0.08618448298782069\n",
      "Epoch:  51980 , step:  51980 , train loss:  0.05111429446581117 , test loss:  0.08618309155656434\n",
      "Epoch:  51990 , step:  51990 , train loss:  0.05111042135385217 , test loss:  0.08618170088212376\n",
      "Epoch:  52000 , step:  52000 , train loss:  0.051106549320662505 , test loss:  0.08618031096408262\n",
      "Epoch:  52010 , step:  52010 , train loss:  0.05110267836573548 , test loss:  0.08617892180202516\n",
      "Epoch:  52020 , step:  52020 , train loss:  0.05109880848856479 , test loss:  0.08617753339553602\n",
      "Epoch:  52030 , step:  52030 , train loss:  0.05109493968864439 , test loss:  0.08617614574419992\n",
      "Epoch:  52040 , step:  52040 , train loss:  0.051091071965468624 , test loss:  0.08617475884760187\n",
      "Epoch:  52050 , step:  52050 , train loss:  0.051087205318532176 , test loss:  0.0861733727053276\n",
      "Epoch:  52060 , step:  52060 , train loss:  0.05108333974733002 , test loss:  0.08617198731696235\n",
      "Epoch:  52070 , step:  52070 , train loss:  0.05107947525135751 , test loss:  0.08617060268209262\n",
      "Epoch:  52080 , step:  52080 , train loss:  0.051075611830110335 , test loss:  0.08616921880030431\n",
      "Epoch:  52090 , step:  52090 , train loss:  0.051071749483084454 , test loss:  0.0861678356711841\n",
      "Epoch:  52100 , step:  52100 , train loss:  0.05106788820977626 , test loss:  0.08616645329431913\n",
      "Epoch:  52110 , step:  52110 , train loss:  0.051064028009682384 , test loss:  0.08616507166929642\n",
      "Epoch:  52120 , step:  52120 , train loss:  0.05106016888229986 , test loss:  0.08616369079570368\n",
      "Epoch:  52130 , step:  52130 , train loss:  0.05105631082712603 , test loss:  0.08616231067312843\n",
      "Epoch:  52140 , step:  52140 , train loss:  0.05105245384365853 , test loss:  0.08616093130115894\n",
      "Epoch:  52150 , step:  52150 , train loss:  0.05104859793139542 , test loss:  0.08615955267938362\n",
      "Epoch:  52160 , step:  52160 , train loss:  0.051044743089835 , test loss:  0.0861581748073911\n",
      "Epoch:  52170 , step:  52170 , train loss:  0.051040889318475946 , test loss:  0.0861567976847704\n",
      "Epoch:  52180 , step:  52180 , train loss:  0.05103703661681728 , test loss:  0.08615542131111083\n",
      "Epoch:  52190 , step:  52190 , train loss:  0.05103318498435832 , test loss:  0.08615404568600205\n",
      "Epoch:  52200 , step:  52200 , train loss:  0.05102933442059872 , test loss:  0.08615267080903374\n",
      "Epoch:  52210 , step:  52210 , train loss:  0.05102548492503851 , test loss:  0.08615129667979633\n",
      "Epoch:  52220 , step:  52220 , train loss:  0.05102163649717799 , test loss:  0.08614992329788018\n",
      "Epoch:  52230 , step:  52230 , train loss:  0.051017789136517805 , test loss:  0.08614855066287581\n",
      "Epoch:  52240 , step:  52240 , train loss:  0.05101394284255896 , test loss:  0.08614717877437475\n",
      "Epoch:  52250 , step:  52250 , train loss:  0.051010097614802775 , test loss:  0.08614580763196802\n",
      "Epoch:  52260 , step:  52260 , train loss:  0.051006253452750915 , test loss:  0.08614443723524759\n",
      "Epoch:  52270 , step:  52270 , train loss:  0.05100241035590534 , test loss:  0.08614306758380523\n",
      "Epoch:  52280 , step:  52280 , train loss:  0.050998568323768305 , test loss:  0.08614169867723301\n",
      "Epoch:  52290 , step:  52290 , train loss:  0.05099472735584252 , test loss:  0.08614033051512375\n",
      "Epoch:  52300 , step:  52300 , train loss:  0.050990887451630884 , test loss:  0.08613896309707024\n",
      "Epoch:  52310 , step:  52310 , train loss:  0.050987048610636757 , test loss:  0.08613759642266544\n",
      "Epoch:  52320 , step:  52320 , train loss:  0.05098321083236367 , test loss:  0.08613623049150303\n",
      "Epoch:  52330 , step:  52330 , train loss:  0.05097937411631564 , test loss:  0.08613486530317646\n",
      "Epoch:  52340 , step:  52340 , train loss:  0.05097553846199689 , test loss:  0.08613350085727992\n",
      "Epoch:  52350 , step:  52350 , train loss:  0.050971703868912054 , test loss:  0.08613213715340774\n",
      "Epoch:  52360 , step:  52360 , train loss:  0.05096787033656605 , test loss:  0.08613077419115431\n",
      "Epoch:  52370 , step:  52370 , train loss:  0.05096403786446411 , test loss:  0.08612941197011453\n",
      "Epoch:  52380 , step:  52380 , train loss:  0.050960206452111814 , test loss:  0.08612805048988391\n",
      "Epoch:  52390 , step:  52390 , train loss:  0.0509563760990151 , test loss:  0.08612668975005759\n",
      "Epoch:  52400 , step:  52400 , train loss:  0.05095254680468017 , test loss:  0.08612532975023146\n",
      "Epoch:  52410 , step:  52410 , train loss:  0.05094871856861358 , test loss:  0.08612397049000167\n",
      "Epoch:  52420 , step:  52420 , train loss:  0.05094489139032224 , test loss:  0.08612261196896431\n",
      "Epoch:  52430 , step:  52430 , train loss:  0.0509410652693133 , test loss:  0.08612125418671612\n",
      "Epoch:  52440 , step:  52440 , train loss:  0.05093724020509434 , test loss:  0.08611989714285406\n",
      "Epoch:  52450 , step:  52450 , train loss:  0.05093341619717318 , test loss:  0.08611854083697539\n",
      "Epoch:  52460 , step:  52460 , train loss:  0.05092959324505801 , test loss:  0.08611718526867748\n",
      "Epoch:  52470 , step:  52470 , train loss:  0.05092577134825735 , test loss:  0.08611583043755823\n",
      "Epoch:  52480 , step:  52480 , train loss:  0.05092195050628 , test loss:  0.08611447634321558\n",
      "Epoch:  52490 , step:  52490 , train loss:  0.0509181307186351 , test loss:  0.08611312298524804\n",
      "Epoch:  52500 , step:  52500 , train loss:  0.05091431198483215 , test loss:  0.08611177036325422\n",
      "Epoch:  52510 , step:  52510 , train loss:  0.0509104943043809 , test loss:  0.08611041847683314\n",
      "Epoch:  52520 , step:  52520 , train loss:  0.050906677676791544 , test loss:  0.08610906732558389\n",
      "Epoch:  52530 , step:  52530 , train loss:  0.05090286210157445 , test loss:  0.08610771690910615\n",
      "Epoch:  52540 , step:  52540 , train loss:  0.05089904757824039 , test loss:  0.08610636722699962\n",
      "Epoch:  52550 , step:  52550 , train loss:  0.05089523410630044 , test loss:  0.08610501827886424\n",
      "Epoch:  52560 , step:  52560 , train loss:  0.05089142168526604 , test loss:  0.08610367006430075\n",
      "Epoch:  52570 , step:  52570 , train loss:  0.05088761031464885 , test loss:  0.08610232258290955\n",
      "Epoch:  52580 , step:  52580 , train loss:  0.05088379999396097 , test loss:  0.0861009758342918\n",
      "Epoch:  52590 , step:  52590 , train loss:  0.05087999072271474 , test loss:  0.08609962981804863\n",
      "Epoch:  52600 , step:  52600 , train loss:  0.05087618250042283 , test loss:  0.08609828453378147\n",
      "Epoch:  52610 , step:  52610 , train loss:  0.05087237532659827 , test loss:  0.08609693998109232\n",
      "Epoch:  52620 , step:  52620 , train loss:  0.050868569200754374 , test loss:  0.08609559615958311\n",
      "Epoch:  52630 , step:  52630 , train loss:  0.050864764122404774 , test loss:  0.08609425306885646\n",
      "Epoch:  52640 , step:  52640 , train loss:  0.05086096009106346 , test loss:  0.08609291070851471\n",
      "Epoch:  52650 , step:  52650 , train loss:  0.05085715710624466 , test loss:  0.08609156907816108\n",
      "Epoch:  52660 , step:  52660 , train loss:  0.05085335516746302 , test loss:  0.08609022817739867\n",
      "Epoch:  52670 , step:  52670 , train loss:  0.050849554274233454 , test loss:  0.08608888800583107\n",
      "Epoch:  52680 , step:  52680 , train loss:  0.050845754426071164 , test loss:  0.08608754856306222\n",
      "Epoch:  52690 , step:  52690 , train loss:  0.05084195562249172 , test loss:  0.08608620984869597\n",
      "Epoch:  52700 , step:  52700 , train loss:  0.05083815786301101 , test loss:  0.08608487186233675\n",
      "Epoch:  52710 , step:  52710 , train loss:  0.0508343611471452 , test loss:  0.08608353460358932\n",
      "Epoch:  52720 , step:  52720 , train loss:  0.0508305654744108 , test loss:  0.08608219807205854\n",
      "Epoch:  52730 , step:  52730 , train loss:  0.050826770844324604 , test loss:  0.08608086226734954\n",
      "Epoch:  52740 , step:  52740 , train loss:  0.05082297725640381 , test loss:  0.0860795271890681\n",
      "Epoch:  52750 , step:  52750 , train loss:  0.050819184710165825 , test loss:  0.08607819283681978\n",
      "Epoch:  52760 , step:  52760 , train loss:  0.0508153932051284 , test loss:  0.08607685921021074\n",
      "Epoch:  52770 , step:  52770 , train loss:  0.050811602740809675 , test loss:  0.0860755263088474\n",
      "Epoch:  52780 , step:  52780 , train loss:  0.050807813316727984 , test loss:  0.0860741941323362\n",
      "Epoch:  52790 , step:  52790 , train loss:  0.0508040249324021 , test loss:  0.08607286268028412\n",
      "Epoch:  52800 , step:  52800 , train loss:  0.05080023758735103 , test loss:  0.08607153195229846\n",
      "Epoch:  52810 , step:  52810 , train loss:  0.05079645128109411 , test loss:  0.0860702019479865\n",
      "Epoch:  52820 , step:  52820 , train loss:  0.05079266601315099 , test loss:  0.08606887266695616\n",
      "Epoch:  52830 , step:  52830 , train loss:  0.05078888178304167 , test loss:  0.08606754410881551\n",
      "Epoch:  52840 , step:  52840 , train loss:  0.0507850985902864 , test loss:  0.08606621627317271\n",
      "Epoch:  52850 , step:  52850 , train loss:  0.05078131643440581 , test loss:  0.08606488915963655\n",
      "Epoch:  52860 , step:  52860 , train loss:  0.05077753531492081 , test loss:  0.08606356276781583\n",
      "Epoch:  52870 , step:  52870 , train loss:  0.0507737552313526 , test loss:  0.08606223709731949\n",
      "Epoch:  52880 , step:  52880 , train loss:  0.050769976183222744 , test loss:  0.08606091214775725\n",
      "Epoch:  52890 , step:  52890 , train loss:  0.050766198170053047 , test loss:  0.08605958791873873\n",
      "Epoch:  52900 , step:  52900 , train loss:  0.050762421191365724 , test loss:  0.08605826440987385\n",
      "Epoch:  52910 , step:  52910 , train loss:  0.05075864524668323 , test loss:  0.08605694162077296\n",
      "Epoch:  52920 , step:  52920 , train loss:  0.05075487033552833 , test loss:  0.08605561955104654\n",
      "Epoch:  52930 , step:  52930 , train loss:  0.050751096457424136 , test loss:  0.0860542982003055\n",
      "Epoch:  52940 , step:  52940 , train loss:  0.05074732361189405 , test loss:  0.08605297756816083\n",
      "Epoch:  52950 , step:  52950 , train loss:  0.0507435517984618 , test loss:  0.08605165765422405\n",
      "Epoch:  52960 , step:  52960 , train loss:  0.05073978101665139 , test loss:  0.08605033845810664\n",
      "Epoch:  52970 , step:  52970 , train loss:  0.050736011265987195 , test loss:  0.08604901997942069\n",
      "Epoch:  52980 , step:  52980 , train loss:  0.050732242545993844 , test loss:  0.08604770221777827\n",
      "Epoch:  52990 , step:  52990 , train loss:  0.05072847485619628 , test loss:  0.08604638517279202\n",
      "Epoch:  53000 , step:  53000 , train loss:  0.0507247081961198 , test loss:  0.08604506884407452\n",
      "Epoch:  53010 , step:  53010 , train loss:  0.05072094256528995 , test loss:  0.08604375323123901\n",
      "Epoch:  53020 , step:  53020 , train loss:  0.050717177963232656 , test loss:  0.08604243833389874\n",
      "Epoch:  53030 , step:  53030 , train loss:  0.050713414389474055 , test loss:  0.08604112415166712\n",
      "Epoch:  53040 , step:  53040 , train loss:  0.05070965184354072 , test loss:  0.08603981068415828\n",
      "Epoch:  53050 , step:  53050 , train loss:  0.050705890324959396 , test loss:  0.08603849793098603\n",
      "Epoch:  53060 , step:  53060 , train loss:  0.05070212983325724 , test loss:  0.0860371858917651\n",
      "Epoch:  53070 , step:  53070 , train loss:  0.05069837036796165 , test loss:  0.08603587456610996\n",
      "Epoch:  53080 , step:  53080 , train loss:  0.05069461192860041 , test loss:  0.08603456395363564\n",
      "Epoch:  53090 , step:  53090 , train loss:  0.05069085451470151 , test loss:  0.08603325405395751\n",
      "Epoch:  53100 , step:  53100 , train loss:  0.0506870981257933 , test loss:  0.08603194486669087\n",
      "Epoch:  53110 , step:  53110 , train loss:  0.05068334276140446 , test loss:  0.08603063639145166\n",
      "Epoch:  53120 , step:  53120 , train loss:  0.050679588421063945 , test loss:  0.08602932862785574\n",
      "Epoch:  53130 , step:  53130 , train loss:  0.05067583510430102 , test loss:  0.08602802157551956\n",
      "Epoch:  53140 , step:  53140 , train loss:  0.050672082810645246 , test loss:  0.08602671523405964\n",
      "Epoch:  53150 , step:  53150 , train loss:  0.05066833153962651 , test loss:  0.08602540960309295\n",
      "Epoch:  53160 , step:  53160 , train loss:  0.05066458129077499 , test loss:  0.08602410468223672\n",
      "Epoch:  53170 , step:  53170 , train loss:  0.050660832063621195 , test loss:  0.08602280047110807\n",
      "Epoch:  53180 , step:  53180 , train loss:  0.05065708385769588 , test loss:  0.08602149696932489\n",
      "Epoch:  53190 , step:  53190 , train loss:  0.05065333667253018 , test loss:  0.08602019417650501\n",
      "Epoch:  53200 , step:  53200 , train loss:  0.05064959050765545 , test loss:  0.08601889209226689\n",
      "Epoch:  53210 , step:  53210 , train loss:  0.05064584536260346 , test loss:  0.08601759071622896\n",
      "Epoch:  53220 , step:  53220 , train loss:  0.05064210123690617 , test loss:  0.08601629004800979\n",
      "Epoch:  53230 , step:  53230 , train loss:  0.05063835813009591 , test loss:  0.0860149900872284\n",
      "Epoch:  53240 , step:  53240 , train loss:  0.05063461604170531 , test loss:  0.08601369083350442\n",
      "Epoch:  53250 , step:  53250 , train loss:  0.05063087497126725 , test loss:  0.08601239228645707\n",
      "Epoch:  53260 , step:  53260 , train loss:  0.050627134918315005 , test loss:  0.0860110944457065\n",
      "Epoch:  53270 , step:  53270 , train loss:  0.05062339588238204 , test loss:  0.08600979731087262\n",
      "Epoch:  53280 , step:  53280 , train loss:  0.05061965786300224 , test loss:  0.08600850088157594\n",
      "Epoch:  53290 , step:  53290 , train loss:  0.05061592085970969 , test loss:  0.08600720515743702\n",
      "Epoch:  53300 , step:  53300 , train loss:  0.05061218487203882 , test loss:  0.08600591013807676\n",
      "Epoch:  53310 , step:  53310 , train loss:  0.0506084498995244 , test loss:  0.08600461582311648\n",
      "Epoch:  53320 , step:  53320 , train loss:  0.05060471594170145 , test loss:  0.08600332221217769\n",
      "Epoch:  53330 , step:  53330 , train loss:  0.05060098299810529 , test loss:  0.08600202930488182\n",
      "Epoch:  53340 , step:  53340 , train loss:  0.050597251068271565 , test loss:  0.08600073710085106\n",
      "Epoch:  53350 , step:  53350 , train loss:  0.05059352015173619 , test loss:  0.08599944559970785\n",
      "Epoch:  53360 , step:  53360 , train loss:  0.05058979024803543 , test loss:  0.08599815480107434\n",
      "Epoch:  53370 , step:  53370 , train loss:  0.050586061356705785 , test loss:  0.08599686470457375\n",
      "Epoch:  53380 , step:  53380 , train loss:  0.05058233347728414 , test loss:  0.0859955753098287\n",
      "Epoch:  53390 , step:  53390 , train loss:  0.050578606609307596 , test loss:  0.08599428661646286\n",
      "Epoch:  53400 , step:  53400 , train loss:  0.050574880752313575 , test loss:  0.08599299862409962\n",
      "Epoch:  53410 , step:  53410 , train loss:  0.050571155905839854 , test loss:  0.08599171133236302\n",
      "Epoch:  53420 , step:  53420 , train loss:  0.05056743206942442 , test loss:  0.08599042474087715\n",
      "Epoch:  53430 , step:  53430 , train loss:  0.05056370924260562 , test loss:  0.08598913884926616\n",
      "Epoch:  53440 , step:  53440 , train loss:  0.0505599874249221 , test loss:  0.08598785365715496\n",
      "Epoch:  53450 , step:  53450 , train loss:  0.05055626661591276 , test loss:  0.08598656916416848\n",
      "Epoch:  53460 , step:  53460 , train loss:  0.05055254681511683 , test loss:  0.08598528536993175\n",
      "Epoch:  53470 , step:  53470 , train loss:  0.050548828022073854 , test loss:  0.08598400227407055\n",
      "Epoch:  53480 , step:  53480 , train loss:  0.05054511023632362 , test loss:  0.08598271987621015\n",
      "Epoch:  53490 , step:  53490 , train loss:  0.05054139345740626 , test loss:  0.08598143817597675\n",
      "Epoch:  53500 , step:  53500 , train loss:  0.05053767768486217 , test loss:  0.08598015717299656\n",
      "Epoch:  53510 , step:  53510 , train loss:  0.050533962918232096 , test loss:  0.08597887686689619\n",
      "Epoch:  53520 , step:  53520 , train loss:  0.050530249157057 , test loss:  0.08597759725730231\n",
      "Epoch:  53530 , step:  53530 , train loss:  0.050526536400878196 , test loss:  0.08597631834384199\n",
      "Epoch:  53540 , step:  53540 , train loss:  0.05052282464923728 , test loss:  0.08597504012614257\n",
      "Epoch:  53550 , step:  53550 , train loss:  0.050519113901676176 , test loss:  0.08597376260383169\n",
      "Epoch:  53560 , step:  53560 , train loss:  0.050515404157737025 , test loss:  0.08597248577653686\n",
      "Epoch:  53570 , step:  53570 , train loss:  0.05051169541696234 , test loss:  0.0859712096438864\n",
      "Epoch:  53580 , step:  53580 , train loss:  0.050507987678894875 , test loss:  0.08596993420550868\n",
      "Epoch:  53590 , step:  53590 , train loss:  0.050504280943077726 , test loss:  0.08596865946103223\n",
      "Epoch:  53600 , step:  53600 , train loss:  0.05050057520905424 , test loss:  0.08596738541008592\n",
      "Epoch:  53610 , step:  53610 , train loss:  0.050496870476368104 , test loss:  0.08596611205229891\n",
      "Epoch:  53620 , step:  53620 , train loss:  0.05049316674456323 , test loss:  0.08596483938730066\n",
      "Epoch:  53630 , step:  53630 , train loss:  0.05048946401318389 , test loss:  0.08596356741472069\n",
      "Epoch:  53640 , step:  53640 , train loss:  0.05048576228177462 , test loss:  0.0859622961341891\n",
      "Epoch:  53650 , step:  53650 , train loss:  0.05048206154988025 , test loss:  0.08596102554533586\n",
      "Epoch:  53660 , step:  53660 , train loss:  0.05047836181704594 , test loss:  0.08595975564779147\n",
      "Epoch:  53670 , step:  53670 , train loss:  0.05047466308281707 , test loss:  0.08595848644118669\n",
      "Epoch:  53680 , step:  53680 , train loss:  0.05047096534673936 , test loss:  0.08595721792515246\n",
      "Epoch:  53690 , step:  53690 , train loss:  0.050467268608358797 , test loss:  0.08595595009931986\n",
      "Epoch:  53700 , step:  53700 , train loss:  0.050463572867221695 , test loss:  0.08595468296332057\n",
      "Epoch:  53710 , step:  53710 , train loss:  0.05045987812287467 , test loss:  0.08595341651678629\n",
      "Epoch:  53720 , step:  53720 , train loss:  0.05045618437486454 , test loss:  0.08595215075934894\n",
      "Epoch:  53730 , step:  53730 , train loss:  0.05045249162273851 , test loss:  0.08595088569064073\n",
      "Epoch:  53740 , step:  53740 , train loss:  0.05044879986604405 , test loss:  0.08594962131029439\n",
      "Epoch:  53750 , step:  53750 , train loss:  0.05044510910432889 , test loss:  0.08594835761794241\n",
      "Epoch:  53760 , step:  53760 , train loss:  0.05044141933714106 , test loss:  0.08594709461321788\n",
      "Epoch:  53770 , step:  53770 , train loss:  0.05043773056402893 , test loss:  0.08594583229575425\n",
      "Epoch:  53780 , step:  53780 , train loss:  0.05043404278454106 , test loss:  0.08594457066518485\n",
      "Epoch:  53790 , step:  53790 , train loss:  0.050430355998226435 , test loss:  0.08594330972114363\n",
      "Epoch:  53800 , step:  53800 , train loss:  0.050426670204634184 , test loss:  0.08594204946326453\n",
      "Epoch:  53810 , step:  53810 , train loss:  0.05042298540331386 , test loss:  0.085940789891182\n",
      "Epoch:  53820 , step:  53820 , train loss:  0.05041930159381519 , test loss:  0.08593953100453057\n",
      "Epoch:  53830 , step:  53830 , train loss:  0.05041561877568827 , test loss:  0.08593827280294496\n",
      "Epoch:  53840 , step:  53840 , train loss:  0.05041193694848345 , test loss:  0.08593701528606033\n",
      "Epoch:  53850 , step:  53850 , train loss:  0.050408256111751364 , test loss:  0.08593575845351202\n",
      "Epoch:  53860 , step:  53860 , train loss:  0.050404576265042945 , test loss:  0.08593450230493566\n",
      "Epoch:  53870 , step:  53870 , train loss:  0.050400897407909415 , test loss:  0.08593324683996693\n",
      "Epoch:  53880 , step:  53880 , train loss:  0.050397219539902276 , test loss:  0.08593199205824208\n",
      "Epoch:  53890 , step:  53890 , train loss:  0.05039354266057334 , test loss:  0.08593073795939743\n",
      "Epoch:  53900 , step:  53900 , train loss:  0.05038986676947466 , test loss:  0.08592948454306967\n",
      "Epoch:  53910 , step:  53910 , train loss:  0.05038619186615864 , test loss:  0.08592823180889567\n",
      "Epoch:  53920 , step:  53920 , train loss:  0.050382517950177896 , test loss:  0.0859269797565124\n",
      "Epoch:  53930 , step:  53930 , train loss:  0.05037884502108538 , test loss:  0.08592572838555722\n",
      "Epoch:  53940 , step:  53940 , train loss:  0.05037517307843434 , test loss:  0.08592447769566791\n",
      "Epoch:  53950 , step:  53950 , train loss:  0.05037150212177825 , test loss:  0.08592322768648238\n",
      "Epoch:  53960 , step:  53960 , train loss:  0.05036783215067094 , test loss:  0.08592197835763855\n",
      "Epoch:  53970 , step:  53970 , train loss:  0.05036416316466649 , test loss:  0.08592072970877497\n",
      "Epoch:  53980 , step:  53980 , train loss:  0.05036049516331924 , test loss:  0.08591948173953029\n",
      "Epoch:  53990 , step:  53990 , train loss:  0.05035682814618387 , test loss:  0.08591823444954332\n",
      "Epoch:  54000 , step:  54000 , train loss:  0.05035316211281533 , test loss:  0.08591698783845345\n",
      "Epoch:  54010 , step:  54010 , train loss:  0.050349497062768804 , test loss:  0.08591574190589966\n",
      "Epoch:  54020 , step:  54020 , train loss:  0.05034583299559981 , test loss:  0.08591449665152204\n",
      "Epoch:  54030 , step:  54030 , train loss:  0.05034216991086419 , test loss:  0.08591325207496021\n",
      "Epoch:  54040 , step:  54040 , train loss:  0.05033850780811793 , test loss:  0.08591200817585422\n",
      "Epoch:  54050 , step:  54050 , train loss:  0.05033484668691743 , test loss:  0.08591076495384486\n",
      "Epoch:  54060 , step:  54060 , train loss:  0.050331186546819354 , test loss:  0.0859095224085725\n",
      "Epoch:  54070 , step:  54070 , train loss:  0.05032752738738059 , test loss:  0.08590828053967811\n",
      "Epoch:  54080 , step:  54080 , train loss:  0.05032386920815835 , test loss:  0.085907039346803\n",
      "Epoch:  54090 , step:  54090 , train loss:  0.050320212008710154 , test loss:  0.08590579882958839\n",
      "Epoch:  54100 , step:  54100 , train loss:  0.05031655578859374 , test loss:  0.08590455898767595\n",
      "Epoch:  54110 , step:  54110 , train loss:  0.050312900547367166 , test loss:  0.08590331982070766\n",
      "Epoch:  54120 , step:  54120 , train loss:  0.0503092462845888 , test loss:  0.08590208132832573\n",
      "Epoch:  54130 , step:  54130 , train loss:  0.05030559299981718 , test loss:  0.08590084351017245\n",
      "Epoch:  54140 , step:  54140 , train loss:  0.050301940692611284 , test loss:  0.08589960636589053\n",
      "Epoch:  54150 , step:  54150 , train loss:  0.050298289362530266 , test loss:  0.08589836989512296\n",
      "Epoch:  54160 , step:  54160 , train loss:  0.050294639009133564 , test loss:  0.0858971340975128\n",
      "Epoch:  54170 , step:  54170 , train loss:  0.050290989631980976 , test loss:  0.08589589897270355\n",
      "Epoch:  54180 , step:  54180 , train loss:  0.05028734123063246 , test loss:  0.08589466452033864\n",
      "Epoch:  54190 , step:  54190 , train loss:  0.05028369380464834 , test loss:  0.08589343074006224\n",
      "Epoch:  54200 , step:  54200 , train loss:  0.05028004735358919 , test loss:  0.0858921976315182\n",
      "Epoch:  54210 , step:  54210 , train loss:  0.05027640187701591 , test loss:  0.08589096519435128\n",
      "Epoch:  54220 , step:  54220 , train loss:  0.0502727573744896 , test loss:  0.08588973342820601\n",
      "Epoch:  54230 , step:  54230 , train loss:  0.050269113845571686 , test loss:  0.08588850233272714\n",
      "Epoch:  54240 , step:  54240 , train loss:  0.050265471289823865 , test loss:  0.08588727190755995\n",
      "Epoch:  54250 , step:  54250 , train loss:  0.050261829706808134 , test loss:  0.08588604215234973\n",
      "Epoch:  54260 , step:  54260 , train loss:  0.05025818909608674 , test loss:  0.08588481306674225\n",
      "Epoch:  54270 , step:  54270 , train loss:  0.050254549457222214 , test loss:  0.08588358465038315\n",
      "Epoch:  54280 , step:  54280 , train loss:  0.05025091078977737 , test loss:  0.08588235690291886\n",
      "Epoch:  54290 , step:  54290 , train loss:  0.05024727309331531 , test loss:  0.08588112982399557\n",
      "Epoch:  54300 , step:  54300 , train loss:  0.05024363636739938 , test loss:  0.08587990341325992\n",
      "Epoch:  54310 , step:  54310 , train loss:  0.05024000061159322 , test loss:  0.08587867767035884\n",
      "Epoch:  54320 , step:  54320 , train loss:  0.050236365825460816 , test loss:  0.08587745259493944\n",
      "Epoch:  54330 , step:  54330 , train loss:  0.050232732008566296 , test loss:  0.08587622818664904\n",
      "Epoch:  54340 , step:  54340 , train loss:  0.05022909916047416 , test loss:  0.08587500444513513\n",
      "Epoch:  54350 , step:  54350 , train loss:  0.050225467280749184 , test loss:  0.08587378137004575\n",
      "Epoch:  54360 , step:  54360 , train loss:  0.05022183636895637 , test loss:  0.08587255896102888\n",
      "Epoch:  54370 , step:  54370 , train loss:  0.05021820642466105 , test loss:  0.08587133721773284\n",
      "Epoch:  54380 , step:  54380 , train loss:  0.05021457744742877 , test loss:  0.08587011613980637\n",
      "Epoch:  54390 , step:  54390 , train loss:  0.050210949436825414 , test loss:  0.08586889572689808\n",
      "Epoch:  54400 , step:  54400 , train loss:  0.050207322392417125 , test loss:  0.08586767597865726\n",
      "Epoch:  54410 , step:  54410 , train loss:  0.050203696313770266 , test loss:  0.08586645689473303\n",
      "Epoch:  54420 , step:  54420 , train loss:  0.05020007120045157 , test loss:  0.08586523847477505\n",
      "Epoch:  54430 , step:  54430 , train loss:  0.05019644705202797 , test loss:  0.08586402071843309\n",
      "Epoch:  54440 , step:  54440 , train loss:  0.05019282386806669 , test loss:  0.08586280362535727\n",
      "Epoch:  54450 , step:  54450 , train loss:  0.05018920164813528 , test loss:  0.0858615871951977\n",
      "Epoch:  54460 , step:  54460 , train loss:  0.05018558039180145 , test loss:  0.08586037142760497\n",
      "Epoch:  54470 , step:  54470 , train loss:  0.05018196009863332 , test loss:  0.08585915632222994\n",
      "Epoch:  54480 , step:  54480 , train loss:  0.050178340768199166 , test loss:  0.08585794187872356\n",
      "Epoch:  54490 , step:  54490 , train loss:  0.05017472240006763 , test loss:  0.08585672809673713\n",
      "Epoch:  54500 , step:  54500 , train loss:  0.05017110499380757 , test loss:  0.08585551497592205\n",
      "Epoch:  54510 , step:  54510 , train loss:  0.05016748854898815 , test loss:  0.0858543025159302\n",
      "Epoch:  54520 , step:  54520 , train loss:  0.05016387306517876 , test loss:  0.08585309071641338\n",
      "Epoch:  54530 , step:  54530 , train loss:  0.05016025854194912 , test loss:  0.08585187957702389\n",
      "Epoch:  54540 , step:  54540 , train loss:  0.05015664497886917 , test loss:  0.08585066909741418\n",
      "Epoch:  54550 , step:  54550 , train loss:  0.050153032375509166 , test loss:  0.08584945927723703\n",
      "Epoch:  54560 , step:  54560 , train loss:  0.0501494207314396 , test loss:  0.08584825011614539\n",
      "Epoch:  54570 , step:  54570 , train loss:  0.05014581004623128 , test loss:  0.08584704161379231\n",
      "Epoch:  54580 , step:  54580 , train loss:  0.05014220031945523 , test loss:  0.08584583376983114\n",
      "Epoch:  54590 , step:  54590 , train loss:  0.05013859155068278 , test loss:  0.08584462658391567\n",
      "Epoch:  54600 , step:  54600 , train loss:  0.050134983739485536 , test loss:  0.08584342005569995\n",
      "Epoch:  54610 , step:  54610 , train loss:  0.050131376885435334 , test loss:  0.08584221418483791\n",
      "Epoch:  54620 , step:  54620 , train loss:  0.05012777098810436 , test loss:  0.08584100897098398\n",
      "Epoch:  54630 , step:  54630 , train loss:  0.05012416604706497 , test loss:  0.08583980441379283\n",
      "Epoch:  54640 , step:  54640 , train loss:  0.050120562061889855 , test loss:  0.08583860051291929\n",
      "Epoch:  54650 , step:  54650 , train loss:  0.050116959032151985 , test loss:  0.08583739726801838\n",
      "Epoch:  54660 , step:  54660 , train loss:  0.05011335695742454 , test loss:  0.08583619467874554\n",
      "Epoch:  54670 , step:  54670 , train loss:  0.050109755837281014 , test loss:  0.08583499274475626\n",
      "Epoch:  54680 , step:  54680 , train loss:  0.05010615567129516 , test loss:  0.08583379146570637\n",
      "Epoch:  54690 , step:  54690 , train loss:  0.05010255645904101 , test loss:  0.08583259084125207\n",
      "Epoch:  54700 , step:  54700 , train loss:  0.050098958200092844 , test loss:  0.0858313908710494\n",
      "Epoch:  54710 , step:  54710 , train loss:  0.05009536089402523 , test loss:  0.08583019155475498\n",
      "Epoch:  54720 , step:  54720 , train loss:  0.05009176454041298 , test loss:  0.08582899289202564\n",
      "Epoch:  54730 , step:  54730 , train loss:  0.050088169138831226 , test loss:  0.08582779488251828\n",
      "Epoch:  54740 , step:  54740 , train loss:  0.05008457468885527 , test loss:  0.0858265975258901\n",
      "Epoch:  54750 , step:  54750 , train loss:  0.050080981190060825 , test loss:  0.08582540082179876\n",
      "Epoch:  54760 , step:  54760 , train loss:  0.050077388642023726 , test loss:  0.0858242047699019\n",
      "Epoch:  54770 , step:  54770 , train loss:  0.050073797044320155 , test loss:  0.08582300936985725\n",
      "Epoch:  54780 , step:  54780 , train loss:  0.05007020639652654 , test loss:  0.08582181462132328\n",
      "Epoch:  54790 , step:  54790 , train loss:  0.050066616698219615 , test loss:  0.08582062052395822\n",
      "Epoch:  54800 , step:  54800 , train loss:  0.0500630279489763 , test loss:  0.08581942707742075\n",
      "Epoch:  54810 , step:  54810 , train loss:  0.05005944014837384 , test loss:  0.08581823428136988\n",
      "Epoch:  54820 , step:  54820 , train loss:  0.050055853295989765 , test loss:  0.08581704213546468\n",
      "Epoch:  54830 , step:  54830 , train loss:  0.050052267391401804 , test loss:  0.08581585063936431\n",
      "Epoch:  54840 , step:  54840 , train loss:  0.05004868243418802 , test loss:  0.08581465979272856\n",
      "Epoch:  54850 , step:  54850 , train loss:  0.05004509842392665 , test loss:  0.08581346959521713\n",
      "Epoch:  54860 , step:  54860 , train loss:  0.050041515360196326 , test loss:  0.08581228004649011\n",
      "Epoch:  54870 , step:  54870 , train loss:  0.05003793324257582 , test loss:  0.08581109114620789\n",
      "Epoch:  54880 , step:  54880 , train loss:  0.05003435207064424 , test loss:  0.08580990289403101\n",
      "Epoch:  54890 , step:  54890 , train loss:  0.050030771843980956 , test loss:  0.08580871528962011\n",
      "Epoch:  54900 , step:  54900 , train loss:  0.050027192562165576 , test loss:  0.08580752833263619\n",
      "Epoch:  54910 , step:  54910 , train loss:  0.05002361422477799 , test loss:  0.08580634202274057\n",
      "Epoch:  54920 , step:  54920 , train loss:  0.05002003683139833 , test loss:  0.08580515635959457\n",
      "Epoch:  54930 , step:  54930 , train loss:  0.050016460381607016 , test loss:  0.08580397134285998\n",
      "Epoch:  54940 , step:  54940 , train loss:  0.050012884874984745 , test loss:  0.08580278697219879\n",
      "Epoch:  54950 , step:  54950 , train loss:  0.05000931031111243 , test loss:  0.085801603247273\n",
      "Epoch:  54960 , step:  54960 , train loss:  0.05000573668957127 , test loss:  0.08580042016774515\n",
      "Epoch:  54970 , step:  54970 , train loss:  0.05000216400994274 , test loss:  0.08579923773327769\n",
      "Epoch:  54980 , step:  54980 , train loss:  0.049998592271808535 , test loss:  0.08579805594353353\n",
      "Epoch:  54990 , step:  54990 , train loss:  0.04999502147475071 , test loss:  0.0857968747981759\n",
      "Epoch:  55000 , step:  55000 , train loss:  0.04999145161835148 , test loss:  0.08579569429686797\n",
      "Epoch:  55010 , step:  55010 , train loss:  0.04998788270219335 , test loss:  0.08579451443927341\n",
      "Epoch:  55020 , step:  55020 , train loss:  0.04998431472585908 , test loss:  0.0857933352250558\n",
      "Epoch:  55030 , step:  55030 , train loss:  0.04998074768893177 , test loss:  0.08579215665387929\n",
      "Epoch:  55040 , step:  55040 , train loss:  0.04997718159099467 , test loss:  0.08579097872540811\n",
      "Epoch:  55050 , step:  55050 , train loss:  0.04997361643163133 , test loss:  0.08578980143930671\n",
      "Epoch:  55060 , step:  55060 , train loss:  0.04997005221042562 , test loss:  0.08578862479523978\n",
      "Epoch:  55070 , step:  55070 , train loss:  0.04996648892696157 , test loss:  0.08578744879287234\n",
      "Epoch:  55080 , step:  55080 , train loss:  0.04996292658082356 , test loss:  0.0857862734318695\n",
      "Epoch:  55090 , step:  55090 , train loss:  0.049959365171596166 , test loss:  0.0857850987118967\n",
      "Epoch:  55100 , step:  55100 , train loss:  0.04995580469886426 , test loss:  0.08578392463261932\n",
      "Epoch:  55110 , step:  55110 , train loss:  0.049952245162212965 , test loss:  0.0857827511937034\n",
      "Epoch:  55120 , step:  55120 , train loss:  0.049948686561227675 , test loss:  0.08578157839481501\n",
      "Epoch:  55130 , step:  55130 , train loss:  0.049945128895494 , test loss:  0.0857804062356205\n",
      "Epoch:  55140 , step:  55140 , train loss:  0.04994157216459787 , test loss:  0.08577923471578634\n",
      "Epoch:  55150 , step:  55150 , train loss:  0.04993801636812544 , test loss:  0.08577806383497942\n",
      "Epoch:  55160 , step:  55160 , train loss:  0.0499344615056631 , test loss:  0.08577689359286662\n",
      "Epoch:  55170 , step:  55170 , train loss:  0.049930907576797565 , test loss:  0.08577572398911507\n",
      "Epoch:  55180 , step:  55180 , train loss:  0.04992735458111576 , test loss:  0.08577455502339233\n",
      "Epoch:  55190 , step:  55190 , train loss:  0.049923802518204885 , test loss:  0.08577338669536604\n",
      "Epoch:  55200 , step:  55200 , train loss:  0.04992025138765236 , test loss:  0.08577221900470414\n",
      "Epoch:  55210 , step:  55210 , train loss:  0.049916701189045946 , test loss:  0.08577105195107469\n",
      "Epoch:  55220 , step:  55220 , train loss:  0.04991315192197357 , test loss:  0.08576988553414615\n",
      "Epoch:  55230 , step:  55230 , train loss:  0.04990960358602346 , test loss:  0.08576871975358707\n",
      "Epoch:  55240 , step:  55240 , train loss:  0.04990605618078411 , test loss:  0.08576755460906621\n",
      "Epoch:  55250 , step:  55250 , train loss:  0.04990250970584426 , test loss:  0.08576639010025267\n",
      "Epoch:  55260 , step:  55260 , train loss:  0.0498989641607929 , test loss:  0.0857652262268157\n",
      "Epoch:  55270 , step:  55270 , train loss:  0.04989541954521929 , test loss:  0.08576406298842479\n",
      "Epoch:  55280 , step:  55280 , train loss:  0.049891875858712915 , test loss:  0.08576290038474965\n",
      "Epoch:  55290 , step:  55290 , train loss:  0.04988833310086356 , test loss:  0.08576173841546013\n",
      "Epoch:  55300 , step:  55300 , train loss:  0.049884791271261254 , test loss:  0.08576057708022666\n",
      "Epoch:  55310 , step:  55310 , train loss:  0.04988125036949625 , test loss:  0.08575941637871937\n",
      "Epoch:  55320 , step:  55320 , train loss:  0.049877710395159106 , test loss:  0.08575825631060897\n",
      "Epoch:  55330 , step:  55330 , train loss:  0.049874171347840585 , test loss:  0.08575709687556633\n",
      "Epoch:  55340 , step:  55340 , train loss:  0.049870633227131755 , test loss:  0.08575593807326258\n",
      "Epoch:  55350 , step:  55350 , train loss:  0.049867096032623875 , test loss:  0.08575477990336883\n",
      "Epoch:  55360 , step:  55360 , train loss:  0.049863559763908535 , test loss:  0.0857536223655568\n",
      "Epoch:  55370 , step:  55370 , train loss:  0.04986002442057755 , test loss:  0.08575246545949804\n",
      "Epoch:  55380 , step:  55380 , train loss:  0.04985649000222291 , test loss:  0.08575130918486472\n",
      "Epoch:  55390 , step:  55390 , train loss:  0.04985295650843702 , test loss:  0.08575015354132877\n",
      "Epoch:  55400 , step:  55400 , train loss:  0.049849423938812386 , test loss:  0.08574899852856277\n",
      "Epoch:  55410 , step:  55410 , train loss:  0.049845892292941854 , test loss:  0.08574784414623933\n",
      "Epoch:  55420 , step:  55420 , train loss:  0.04984236157041851 , test loss:  0.08574669039403138\n",
      "Epoch:  55430 , step:  55430 , train loss:  0.049838831770835666 , test loss:  0.08574553727161187\n",
      "Epoch:  55440 , step:  55440 , train loss:  0.04983530289378692 , test loss:  0.08574438477865427\n",
      "Epoch:  55450 , step:  55450 , train loss:  0.049831774938866064 , test loss:  0.08574323291483188\n",
      "Epoch:  55460 , step:  55460 , train loss:  0.04982824790566725 , test loss:  0.08574208167981857\n",
      "Epoch:  55470 , step:  55470 , train loss:  0.049824721793784756 , test loss:  0.08574093107328855\n",
      "Epoch:  55480 , step:  55480 , train loss:  0.04982119660281322 , test loss:  0.08573978109491548\n",
      "Epoch:  55490 , step:  55490 , train loss:  0.049817672332347476 , test loss:  0.08573863174437435\n",
      "Epoch:  55500 , step:  55500 , train loss:  0.049814148981982616 , test loss:  0.08573748302133949\n",
      "Epoch:  55510 , step:  55510 , train loss:  0.049810626551313984 , test loss:  0.08573633492548573\n",
      "Epoch:  55520 , step:  55520 , train loss:  0.049807105039937176 , test loss:  0.08573518745648823\n",
      "Epoch:  55530 , step:  55530 , train loss:  0.04980358444744808 , test loss:  0.08573404061402233\n",
      "Epoch:  55540 , step:  55540 , train loss:  0.04980006477344275 , test loss:  0.08573289439776348\n",
      "Epoch:  55550 , step:  55550 , train loss:  0.049796546017517554 , test loss:  0.08573174880738747\n",
      "Epoch:  55560 , step:  55560 , train loss:  0.049793028179269096 , test loss:  0.08573060384257025\n",
      "Epoch:  55570 , step:  55570 , train loss:  0.04978951125829423 , test loss:  0.085729459502988\n",
      "Epoch:  55580 , step:  55580 , train loss:  0.04978599525419007 , test loss:  0.08572831578831717\n",
      "Epoch:  55590 , step:  55590 , train loss:  0.04978248016655398 , test loss:  0.0857271726982344\n",
      "Epoch:  55600 , step:  55600 , train loss:  0.049778965994983514 , test loss:  0.08572603023241632\n",
      "Epoch:  55610 , step:  55610 , train loss:  0.04977545273907656 , test loss:  0.08572488839054038\n",
      "Epoch:  55620 , step:  55620 , train loss:  0.04977194039843125 , test loss:  0.08572374717228355\n",
      "Epoch:  55630 , step:  55630 , train loss:  0.04976842897264587 , test loss:  0.08572260657732343\n",
      "Epoch:  55640 , step:  55640 , train loss:  0.04976491846131906 , test loss:  0.0857214666053377\n",
      "Epoch:  55650 , step:  55650 , train loss:  0.04976140886404969 , test loss:  0.08572032725600444\n",
      "Epoch:  55660 , step:  55660 , train loss:  0.049757900180436805 , test loss:  0.08571918852900161\n",
      "Epoch:  55670 , step:  55670 , train loss:  0.0497543924100798 , test loss:  0.0857180504240077\n",
      "Epoch:  55680 , step:  55680 , train loss:  0.04975088555257824 , test loss:  0.08571691294070137\n",
      "Epoch:  55690 , step:  55690 , train loss:  0.04974737960753197 , test loss:  0.08571577607876141\n",
      "Epoch:  55700 , step:  55700 , train loss:  0.04974387457454113 , test loss:  0.08571463983786672\n",
      "Epoch:  55710 , step:  55710 , train loss:  0.04974037045320597 , test loss:  0.08571350421769669\n",
      "Epoch:  55720 , step:  55720 , train loss:  0.04973686724312716 , test loss:  0.08571236921793081\n",
      "Epoch:  55730 , step:  55730 , train loss:  0.04973336494390548 , test loss:  0.08571123483824854\n",
      "Epoch:  55740 , step:  55740 , train loss:  0.049729863555142056 , test loss:  0.08571010107833005\n",
      "Epoch:  55750 , step:  55750 , train loss:  0.04972636307643816 , test loss:  0.08570896793785539\n",
      "Epoch:  55760 , step:  55760 , train loss:  0.0497228635073954 , test loss:  0.0857078354165048\n",
      "Epoch:  55770 , step:  55770 , train loss:  0.049719364847615626 , test loss:  0.08570670351395905\n",
      "Epoch:  55780 , step:  55780 , train loss:  0.049715867096700825 , test loss:  0.08570557222989857\n",
      "Epoch:  55790 , step:  55790 , train loss:  0.04971237025425337 , test loss:  0.08570444156400454\n",
      "Epoch:  55800 , step:  55800 , train loss:  0.04970887431987581 , test loss:  0.08570331151595831\n",
      "Epoch:  55810 , step:  55810 , train loss:  0.049705379293170954 , test loss:  0.08570218208544116\n",
      "Epoch:  55820 , step:  55820 , train loss:  0.0497018851737418 , test loss:  0.08570105327213452\n",
      "Epoch:  55830 , step:  55830 , train loss:  0.049698391961191725 , test loss:  0.0856999250757205\n",
      "Epoch:  55840 , step:  55840 , train loss:  0.04969489965512422 , test loss:  0.08569879749588102\n",
      "Epoch:  55850 , step:  55850 , train loss:  0.04969140825514307 , test loss:  0.08569767053229856\n",
      "Epoch:  55860 , step:  55860 , train loss:  0.04968791776085232 , test loss:  0.08569654418465546\n",
      "Epoch:  55870 , step:  55870 , train loss:  0.04968442817185623 , test loss:  0.08569541845263455\n",
      "Epoch:  55880 , step:  55880 , train loss:  0.04968093948775933 , test loss:  0.08569429333591866\n",
      "Epoch:  55890 , step:  55890 , train loss:  0.04967745170816636 , test loss:  0.08569316883419077\n",
      "Epoch:  55900 , step:  55900 , train loss:  0.049673964832682375 , test loss:  0.08569204494713455\n",
      "Epoch:  55910 , step:  55910 , train loss:  0.049670478860912574 , test loss:  0.08569092167443333\n",
      "Epoch:  55920 , step:  55920 , train loss:  0.04966699379246247 , test loss:  0.08568979901577098\n",
      "Epoch:  55930 , step:  55930 , train loss:  0.0496635096269378 , test loss:  0.0856886769708316\n",
      "Epoch:  55940 , step:  55940 , train loss:  0.049660026363944544 , test loss:  0.0856875555392993\n",
      "Epoch:  55950 , step:  55950 , train loss:  0.04965654400308892 , test loss:  0.08568643472085831\n",
      "Epoch:  55960 , step:  55960 , train loss:  0.049653062543977416 , test loss:  0.08568531451519366\n",
      "Epoch:  55970 , step:  55970 , train loss:  0.049649581986216704 , test loss:  0.08568419492198996\n",
      "Epoch:  55980 , step:  55980 , train loss:  0.049646102329413765 , test loss:  0.08568307594093236\n",
      "Epoch:  55990 , step:  55990 , train loss:  0.04964262357317576 , test loss:  0.08568195757170607\n",
      "Epoch:  56000 , step:  56000 , train loss:  0.04963914571711019 , test loss:  0.08568083981399661\n",
      "Epoch:  56010 , step:  56010 , train loss:  0.049635668760824654 , test loss:  0.0856797226674896\n",
      "Epoch:  56020 , step:  56020 , train loss:  0.04963219270392711 , test loss:  0.085678606131871\n",
      "Epoch:  56030 , step:  56030 , train loss:  0.04962871754602571 , test loss:  0.08567749020682702\n",
      "Epoch:  56040 , step:  56040 , train loss:  0.04962524328672883 , test loss:  0.08567637489204386\n",
      "Epoch:  56050 , step:  56050 , train loss:  0.04962176992564515 , test loss:  0.08567526018720807\n",
      "Epoch:  56060 , step:  56060 , train loss:  0.049618297462383526 , test loss:  0.08567414609200645\n",
      "Epoch:  56070 , step:  56070 , train loss:  0.04961482589655311 , test loss:  0.0856730326061261\n",
      "Epoch:  56080 , step:  56080 , train loss:  0.04961135522776325 , test loss:  0.08567191972925405\n",
      "Epoch:  56090 , step:  56090 , train loss:  0.04960788545562353 , test loss:  0.08567080746107773\n",
      "Epoch:  56100 , step:  56100 , train loss:  0.049604416579743814 , test loss:  0.08566969580128474\n",
      "Epoch:  56110 , step:  56110 , train loss:  0.04960094859973418 , test loss:  0.08566858474956293\n",
      "Epoch:  56120 , step:  56120 , train loss:  0.049597481515204954 , test loss:  0.0856674743056003\n",
      "Epoch:  56130 , step:  56130 , train loss:  0.049594015325766704 , test loss:  0.08566636446908493\n",
      "Epoch:  56140 , step:  56140 , train loss:  0.04959055003103022 , test loss:  0.08566525523970538\n",
      "Epoch:  56150 , step:  56150 , train loss:  0.04958708563060654 , test loss:  0.08566414661715051\n",
      "Epoch:  56160 , step:  56160 , train loss:  0.04958362212410696 , test loss:  0.08566303860110903\n",
      "Epoch:  56170 , step:  56170 , train loss:  0.04958015951114299 , test loss:  0.08566193119126973\n",
      "Epoch:  56180 , step:  56180 , train loss:  0.049576697791326396 , test loss:  0.08566082438732227\n",
      "Epoch:  56190 , step:  56190 , train loss:  0.04957323696426915 , test loss:  0.08565971818895596\n",
      "Epoch:  56200 , step:  56200 , train loss:  0.04956977702958352 , test loss:  0.08565861259586059\n",
      "Epoch:  56210 , step:  56210 , train loss:  0.04956631798688194 , test loss:  0.08565750760772599\n",
      "Epoch:  56220 , step:  56220 , train loss:  0.04956285983577715 , test loss:  0.08565640322424234\n",
      "Epoch:  56230 , step:  56230 , train loss:  0.04955940257588205 , test loss:  0.08565529944509992\n",
      "Epoch:  56240 , step:  56240 , train loss:  0.04955594620680989 , test loss:  0.08565419626998931\n",
      "Epoch:  56250 , step:  56250 , train loss:  0.049552490728174046 , test loss:  0.08565309369860119\n",
      "Epoch:  56260 , step:  56260 , train loss:  0.049549036139588214 , test loss:  0.08565199173062658\n",
      "Epoch:  56270 , step:  56270 , train loss:  0.049545582440666265 , test loss:  0.08565089036575659\n",
      "Epoch:  56280 , step:  56280 , train loss:  0.049542129631022315 , test loss:  0.08564978960368251\n",
      "Epoch:  56290 , step:  56290 , train loss:  0.04953867771027072 , test loss:  0.08564868944409586\n",
      "Epoch:  56300 , step:  56300 , train loss:  0.04953522667802614 , test loss:  0.08564758988668872\n",
      "Epoch:  56310 , step:  56310 , train loss:  0.04953177653390338 , test loss:  0.08564649093115295\n",
      "Epoch:  56320 , step:  56320 , train loss:  0.04952832727751754 , test loss:  0.08564539257718067\n",
      "Epoch:  56330 , step:  56330 , train loss:  0.0495248789084839 , test loss:  0.08564429482446427\n",
      "Epoch:  56340 , step:  56340 , train loss:  0.04952143142641803 , test loss:  0.08564319767269638\n",
      "Epoch:  56350 , step:  56350 , train loss:  0.04951798483093573 , test loss:  0.08564210112156977\n",
      "Epoch:  56360 , step:  56360 , train loss:  0.04951453912165296 , test loss:  0.08564100517077751\n",
      "Epoch:  56370 , step:  56370 , train loss:  0.049511094298186 , test loss:  0.08563990982001282\n",
      "Epoch:  56380 , step:  56380 , train loss:  0.049507650360151395 , test loss:  0.08563881506896916\n",
      "Epoch:  56390 , step:  56390 , train loss:  0.04950420730716579 , test loss:  0.08563772091734013\n",
      "Epoch:  56400 , step:  56400 , train loss:  0.04950076513884615 , test loss:  0.08563662736481957\n",
      "Epoch:  56410 , step:  56410 , train loss:  0.04949732385480971 , test loss:  0.0856355344111016\n",
      "Epoch:  56420 , step:  56420 , train loss:  0.04949388345467388 , test loss:  0.08563444205588025\n",
      "Epoch:  56430 , step:  56430 , train loss:  0.049490443938056274 , test loss:  0.08563335029885026\n",
      "Epoch:  56440 , step:  56440 , train loss:  0.049487005304574845 , test loss:  0.08563225913970612\n",
      "Epoch:  56450 , step:  56450 , train loss:  0.049483567553847696 , test loss:  0.0856311685781428\n",
      "Epoch:  56460 , step:  56460 , train loss:  0.04948013068549319 , test loss:  0.08563007861385516\n",
      "Epoch:  56470 , step:  56470 , train loss:  0.0494766946991299 , test loss:  0.08562898924653867\n",
      "Epoch:  56480 , step:  56480 , train loss:  0.04947325959437663 , test loss:  0.08562790047588878\n",
      "Epoch:  56490 , step:  56490 , train loss:  0.04946982537085253 , test loss:  0.08562681230160109\n",
      "Epoch:  56500 , step:  56500 , train loss:  0.04946639202817683 , test loss:  0.0856257247233716\n",
      "Epoch:  56510 , step:  56510 , train loss:  0.04946295956596902 , test loss:  0.08562463774089621\n",
      "Epoch:  56520 , step:  56520 , train loss:  0.049459527983848915 , test loss:  0.08562355135387138\n",
      "Epoch:  56530 , step:  56530 , train loss:  0.049456097281436455 , test loss:  0.08562246556199352\n",
      "Epoch:  56540 , step:  56540 , train loss:  0.04945266745835192 , test loss:  0.08562138036495939\n",
      "Epoch:  56550 , step:  56550 , train loss:  0.04944923851421569 , test loss:  0.08562029576246585\n",
      "Epoch:  56560 , step:  56560 , train loss:  0.049445810448648486 , test loss:  0.08561921175421003\n",
      "Epoch:  56570 , step:  56570 , train loss:  0.04944238326127122 , test loss:  0.0856181283398892\n",
      "Epoch:  56580 , step:  56580 , train loss:  0.049438956951705026 , test loss:  0.08561704551920102\n",
      "Epoch:  56590 , step:  56590 , train loss:  0.049435531519571266 , test loss:  0.08561596329184294\n",
      "Epoch:  56600 , step:  56600 , train loss:  0.049432106964491594 , test loss:  0.08561488165751306\n",
      "Epoch:  56610 , step:  56610 , train loss:  0.049428683286087786 , test loss:  0.08561380061590945\n",
      "Epoch:  56620 , step:  56620 , train loss:  0.04942526048398196 , test loss:  0.08561272016673045\n",
      "Epoch:  56630 , step:  56630 , train loss:  0.049421838557796374 , test loss:  0.08561164030967446\n",
      "Epoch:  56640 , step:  56640 , train loss:  0.04941841750715356 , test loss:  0.0856105610444403\n",
      "Epoch:  56650 , step:  56650 , train loss:  0.049414997331676314 , test loss:  0.08560948237072696\n",
      "Epoch:  56660 , step:  56660 , train loss:  0.04941157803098759 , test loss:  0.08560840428823333\n",
      "Epoch:  56670 , step:  56670 , train loss:  0.0494081596047106 , test loss:  0.08560732679665908\n",
      "Epoch:  56680 , step:  56680 , train loss:  0.049404742052468816 , test loss:  0.08560624989570334\n",
      "Epoch:  56690 , step:  56690 , train loss:  0.04940132537388586 , test loss:  0.08560517358506597\n",
      "Epoch:  56700 , step:  56700 , train loss:  0.04939790956858569 , test loss:  0.08560409786444703\n",
      "Epoch:  56710 , step:  56710 , train loss:  0.04939449463619239 , test loss:  0.08560302273354652\n",
      "Epoch:  56720 , step:  56720 , train loss:  0.049391080576330376 , test loss:  0.08560194819206483\n",
      "Epoch:  56730 , step:  56730 , train loss:  0.04938766738862417 , test loss:  0.08560087423970232\n",
      "Epoch:  56740 , step:  56740 , train loss:  0.04938425507269865 , test loss:  0.08559980087615998\n",
      "Epoch:  56750 , step:  56750 , train loss:  0.049380843628178836 , test loss:  0.08559872810113868\n",
      "Epoch:  56760 , step:  56760 , train loss:  0.04937743305468997 , test loss:  0.08559765591433936\n",
      "Epoch:  56770 , step:  56770 , train loss:  0.04937402335185762 , test loss:  0.08559658431546337\n",
      "Epoch:  56780 , step:  56780 , train loss:  0.04937061451930745 , test loss:  0.08559551330421238\n",
      "Epoch:  56790 , step:  56790 , train loss:  0.049367206556665454 , test loss:  0.08559444288028797\n",
      "Epoch:  56800 , step:  56800 , train loss:  0.04936379946355778 , test loss:  0.085593373043392\n",
      "Epoch:  56810 , step:  56810 , train loss:  0.04936039323961087 , test loss:  0.08559230379322681\n",
      "Epoch:  56820 , step:  56820 , train loss:  0.049356987884451364 , test loss:  0.08559123512949462\n",
      "Epoch:  56830 , step:  56830 , train loss:  0.0493535833977061 , test loss:  0.08559016705189784\n",
      "Epoch:  56840 , step:  56840 , train loss:  0.04935017977900214 , test loss:  0.08558909956013931\n",
      "Epoch:  56850 , step:  56850 , train loss:  0.04934677702796688 , test loss:  0.08558803265392174\n",
      "Epoch:  56860 , step:  56860 , train loss:  0.04934337514422778 , test loss:  0.08558696633294854\n",
      "Epoch:  56870 , step:  56870 , train loss:  0.04933997412741265 , test loss:  0.08558590059692282\n",
      "Epoch:  56880 , step:  56880 , train loss:  0.04933657397714948 , test loss:  0.08558483544554808\n",
      "Epoch:  56890 , step:  56890 , train loss:  0.049333174693066466 , test loss:  0.08558377087852796\n",
      "Epoch:  56900 , step:  56900 , train loss:  0.04932977627479207 , test loss:  0.0855827068955666\n",
      "Epoch:  56910 , step:  56910 , train loss:  0.04932637872195496 , test loss:  0.08558164349636786\n",
      "Epoch:  56920 , step:  56920 , train loss:  0.04932298203418401 , test loss:  0.08558058068063605\n",
      "Epoch:  56930 , step:  56930 , train loss:  0.04931958621110838 , test loss:  0.08557951844807565\n",
      "Epoch:  56940 , step:  56940 , train loss:  0.049316191252357366 , test loss:  0.08557845679839134\n",
      "Epoch:  56950 , step:  56950 , train loss:  0.04931279715756055 , test loss:  0.08557739573128828\n",
      "Epoch:  56960 , step:  56960 , train loss:  0.04930940392634775 , test loss:  0.08557633524647115\n",
      "Epoch:  56970 , step:  56970 , train loss:  0.049306011558348954 , test loss:  0.08557527534364542\n",
      "Epoch:  56980 , step:  56980 , train loss:  0.04930262005319441 , test loss:  0.08557421602251652\n",
      "Epoch:  56990 , step:  56990 , train loss:  0.049299229410514574 , test loss:  0.08557315728279007\n",
      "Epoch:  57000 , step:  57000 , train loss:  0.04929583962994016 , test loss:  0.08557209912417181\n",
      "Epoch:  57010 , step:  57010 , train loss:  0.04929245071110201 , test loss:  0.08557104154636812\n",
      "Epoch:  57020 , step:  57020 , train loss:  0.04928906265363131 , test loss:  0.08556998454908495\n",
      "Epoch:  57030 , step:  57030 , train loss:  0.04928567545715943 , test loss:  0.08556892813202888\n",
      "Epoch:  57040 , step:  57040 , train loss:  0.04928228912131792 , test loss:  0.08556787229490653\n",
      "Epoch:  57050 , step:  57050 , train loss:  0.04927890364573862 , test loss:  0.0855668170374247\n",
      "Epoch:  57060 , step:  57060 , train loss:  0.04927551903005349 , test loss:  0.08556576235929039\n",
      "Epoch:  57070 , step:  57070 , train loss:  0.04927213527389484 , test loss:  0.08556470826021091\n",
      "Epoch:  57080 , step:  57080 , train loss:  0.04926875237689509 , test loss:  0.08556365473989358\n",
      "Epoch:  57090 , step:  57090 , train loss:  0.04926537033868697 , test loss:  0.08556260179804599\n",
      "Epoch:  57100 , step:  57100 , train loss:  0.049261989158903376 , test loss:  0.08556154943437601\n",
      "Epoch:  57110 , step:  57110 , train loss:  0.04925860883717742 , test loss:  0.08556049764859154\n",
      "Epoch:  57120 , step:  57120 , train loss:  0.04925522937314251 , test loss:  0.0855594464404009\n",
      "Epoch:  57130 , step:  57130 , train loss:  0.04925185076643219 , test loss:  0.08555839580951242\n",
      "Epoch:  57140 , step:  57140 , train loss:  0.049248473016680255 , test loss:  0.0855573457556346\n",
      "Epoch:  57150 , step:  57150 , train loss:  0.049245096123520744 , test loss:  0.08555629627847619\n",
      "Epoch:  57160 , step:  57160 , train loss:  0.04924172008658788 , test loss:  0.08555524737774624\n",
      "Epoch:  57170 , step:  57170 , train loss:  0.04923834490551612 , test loss:  0.08555419905315384\n",
      "Epoch:  57180 , step:  57180 , train loss:  0.04923497057994015 , test loss:  0.08555315130440849\n",
      "Epoch:  57190 , step:  57190 , train loss:  0.04923159710949488 , test loss:  0.08555210413121944\n",
      "Epoch:  57200 , step:  57200 , train loss:  0.04922822449381543 , test loss:  0.08555105753329673\n",
      "Epoch:  57210 , step:  57210 , train loss:  0.04922485273253714 , test loss:  0.08555001151035002\n",
      "Epoch:  57220 , step:  57220 , train loss:  0.049221481825295565 , test loss:  0.08554896606208962\n",
      "Epoch:  57230 , step:  57230 , train loss:  0.0492181117717265 , test loss:  0.08554792118822568\n",
      "Epoch:  57240 , step:  57240 , train loss:  0.049214742571465936 , test loss:  0.08554687688846876\n",
      "Epoch:  57250 , step:  57250 , train loss:  0.049211374224150096 , test loss:  0.08554583316252948\n",
      "Epoch:  57260 , step:  57260 , train loss:  0.0492080067294154 , test loss:  0.0855447900101188\n",
      "Epoch:  57270 , step:  57270 , train loss:  0.049204640086898524 , test loss:  0.0855437474309477\n",
      "Epoch:  57280 , step:  57280 , train loss:  0.04920127429623635 , test loss:  0.08554270542472754\n",
      "Epoch:  57290 , step:  57290 , train loss:  0.049197909357065965 , test loss:  0.08554166399116976\n",
      "Epoch:  57300 , step:  57300 , train loss:  0.04919454526902469 , test loss:  0.08554062312998593\n",
      "Epoch:  57310 , step:  57310 , train loss:  0.04919118203175004 , test loss:  0.08553958284088797\n",
      "Epoch:  57320 , step:  57320 , train loss:  0.049187819644879754 , test loss:  0.08553854312358788\n",
      "Epoch:  57330 , step:  57330 , train loss:  0.049184458108051846 , test loss:  0.08553750397779779\n",
      "Epoch:  57340 , step:  57340 , train loss:  0.04918109742090448 , test loss:  0.08553646540323008\n",
      "Epoch:  57350 , step:  57350 , train loss:  0.04917773758307603 , test loss:  0.08553542739959737\n",
      "Epoch:  57360 , step:  57360 , train loss:  0.04917437859420514 , test loss:  0.08553438996661254\n",
      "Epoch:  57370 , step:  57370 , train loss:  0.04917102045393065 , test loss:  0.08553335310398864\n",
      "Epoch:  57380 , step:  57380 , train loss:  0.04916766316189159 , test loss:  0.08553231681143843\n",
      "Epoch:  57390 , step:  57390 , train loss:  0.04916430671772728 , test loss:  0.08553128108867561\n",
      "Epoch:  57400 , step:  57400 , train loss:  0.049160951121077164 , test loss:  0.08553024593541358\n",
      "Epoch:  57410 , step:  57410 , train loss:  0.049157596371580965 , test loss:  0.0855292113513661\n",
      "Epoch:  57420 , step:  57420 , train loss:  0.049154242468878576 , test loss:  0.08552817733624704\n",
      "Epoch:  57430 , step:  57430 , train loss:  0.04915088941261017 , test loss:  0.0855271438897707\n",
      "Epoch:  57440 , step:  57440 , train loss:  0.049147537202416096 , test loss:  0.08552611101165113\n",
      "Epoch:  57450 , step:  57450 , train loss:  0.04914418583793691 , test loss:  0.08552507870160284\n",
      "Epoch:  57460 , step:  57460 , train loss:  0.04914083531881339 , test loss:  0.08552404695934052\n",
      "Epoch:  57470 , step:  57470 , train loss:  0.04913748564468656 , test loss:  0.08552301578457898\n",
      "Epoch:  57480 , step:  57480 , train loss:  0.04913413681519763 , test loss:  0.08552198517703351\n",
      "Epoch:  57490 , step:  57490 , train loss:  0.049130788829987976 , test loss:  0.08552095513641912\n",
      "Epoch:  57500 , step:  57500 , train loss:  0.04912744168869932 , test loss:  0.08551992566245128\n",
      "Epoch:  57510 , step:  57510 , train loss:  0.049124095390973496 , test loss:  0.08551889675484553\n",
      "Epoch:  57520 , step:  57520 , train loss:  0.04912074993645257 , test loss:  0.08551786841331767\n",
      "Epoch:  57530 , step:  57530 , train loss:  0.04911740532477884 , test loss:  0.0855168406375838\n",
      "Epoch:  57540 , step:  57540 , train loss:  0.04911406155559479 , test loss:  0.08551581342735996\n",
      "Epoch:  57550 , step:  57550 , train loss:  0.04911071862854315 , test loss:  0.08551478678236256\n",
      "Epoch:  57560 , step:  57560 , train loss:  0.04910737654326688 , test loss:  0.08551376070230801\n",
      "Epoch:  57570 , step:  57570 , train loss:  0.049104035299409085 , test loss:  0.08551273518691324\n",
      "Epoch:  57580 , step:  57580 , train loss:  0.04910069489661315 , test loss:  0.08551171023589513\n",
      "Epoch:  57590 , step:  57590 , train loss:  0.04909735533452262 , test loss:  0.08551068584897067\n",
      "Epoch:  57600 , step:  57600 , train loss:  0.04909401661278133 , test loss:  0.08550966202585716\n",
      "Epoch:  57610 , step:  57610 , train loss:  0.04909067873103323 , test loss:  0.08550863876627207\n",
      "Epoch:  57620 , step:  57620 , train loss:  0.04908734168892256 , test loss:  0.08550761606993312\n",
      "Epoch:  57630 , step:  57630 , train loss:  0.04908400548609373 , test loss:  0.08550659393655806\n",
      "Epoch:  57640 , step:  57640 , train loss:  0.04908067012219139 , test loss:  0.08550557236586494\n",
      "Epoch:  57650 , step:  57650 , train loss:  0.049077335596860394 , test loss:  0.08550455135757212\n",
      "Epoch:  57660 , step:  57660 , train loss:  0.0490740019097458 , test loss:  0.08550353091139772\n",
      "Epoch:  57670 , step:  57670 , train loss:  0.04907066906049287 , test loss:  0.08550251102706058\n",
      "Epoch:  57680 , step:  57680 , train loss:  0.04906733704874711 , test loss:  0.08550149170427929\n",
      "Epoch:  57690 , step:  57690 , train loss:  0.04906400587415422 , test loss:  0.08550047294277283\n",
      "Epoch:  57700 , step:  57700 , train loss:  0.04906067553636008 , test loss:  0.08549945474226033\n",
      "Epoch:  57710 , step:  57710 , train loss:  0.049057346035010864 , test loss:  0.08549843710246119\n",
      "Epoch:  57720 , step:  57720 , train loss:  0.049054017369752866 , test loss:  0.08549742002309485\n",
      "Epoch:  57730 , step:  57730 , train loss:  0.049050689540232645 , test loss:  0.0854964035038809\n",
      "Epoch:  57740 , step:  57740 , train loss:  0.04904736254609696 , test loss:  0.08549538754453928\n",
      "Epoch:  57750 , step:  57750 , train loss:  0.04904403638699276 , test loss:  0.08549437214479017\n",
      "Epoch:  57760 , step:  57760 , train loss:  0.04904071106256722 , test loss:  0.08549335730435362\n",
      "Epoch:  57770 , step:  57770 , train loss:  0.0490373865724678 , test loss:  0.08549234302295011\n",
      "Epoch:  57780 , step:  57780 , train loss:  0.049034062916342 , test loss:  0.08549132930030033\n",
      "Epoch:  57790 , step:  57790 , train loss:  0.04903074009383767 , test loss:  0.08549031613612493\n",
      "Epoch:  57800 , step:  57800 , train loss:  0.04902741810460286 , test loss:  0.08548930353014497\n",
      "Epoch:  57810 , step:  57810 , train loss:  0.04902409694828574 , test loss:  0.08548829148208154\n",
      "Epoch:  57820 , step:  57820 , train loss:  0.049020776624534794 , test loss:  0.08548727999165598\n",
      "Epoch:  57830 , step:  57830 , train loss:  0.04901745713299866 , test loss:  0.08548626905858984\n",
      "Epoch:  57840 , step:  57840 , train loss:  0.04901413847332619 , test loss:  0.08548525868260483\n",
      "Epoch:  57850 , step:  57850 , train loss:  0.049010820645166445 , test loss:  0.08548424886342285\n",
      "Epoch:  57860 , step:  57860 , train loss:  0.0490075036481687 , test loss:  0.08548323960076591\n",
      "Epoch:  57870 , step:  57870 , train loss:  0.04900418748198245 , test loss:  0.08548223089435622\n",
      "Epoch:  57880 , step:  57880 , train loss:  0.049000872146257396 , test loss:  0.08548122274391622\n",
      "Epoch:  57890 , step:  57890 , train loss:  0.048997557640643445 , test loss:  0.08548021514916858\n",
      "Epoch:  57900 , step:  57900 , train loss:  0.048994243964790685 , test loss:  0.08547920810983596\n",
      "Epoch:  57910 , step:  57910 , train loss:  0.04899093111834944 , test loss:  0.08547820162564136\n",
      "Epoch:  57920 , step:  57920 , train loss:  0.04898761910097023 , test loss:  0.08547719569630799\n",
      "Epoch:  57930 , step:  57930 , train loss:  0.04898430791230382 , test loss:  0.0854761903215593\n",
      "Epoch:  57940 , step:  57940 , train loss:  0.04898099755200113 , test loss:  0.08547518550111857\n",
      "Epoch:  57950 , step:  57950 , train loss:  0.04897768801971331 , test loss:  0.08547418123470957\n",
      "Epoch:  57960 , step:  57960 , train loss:  0.048974379315091736 , test loss:  0.08547317752205623\n",
      "Epoch:  57970 , step:  57970 , train loss:  0.04897107143778798 , test loss:  0.08547217436288243\n",
      "Epoch:  57980 , step:  57980 , train loss:  0.048967764387453754 , test loss:  0.08547117175691257\n",
      "Epoch:  57990 , step:  57990 , train loss:  0.04896445816374111 , test loss:  0.08547016970387121\n",
      "Epoch:  58000 , step:  58000 , train loss:  0.04896115276630222 , test loss:  0.08546916820348273\n",
      "Epoch:  58010 , step:  58010 , train loss:  0.048957848194789426 , test loss:  0.08546816725547177\n",
      "Epoch:  58020 , step:  58020 , train loss:  0.048954544448855365 , test loss:  0.08546716685956361\n",
      "Epoch:  58030 , step:  58030 , train loss:  0.0489512415281529 , test loss:  0.08546616701548307\n",
      "Epoch:  58040 , step:  58040 , train loss:  0.04894793943233495 , test loss:  0.08546516772295566\n",
      "Epoch:  58050 , step:  58050 , train loss:  0.048944638161054765 , test loss:  0.08546416898170676\n",
      "Epoch:  58060 , step:  58060 , train loss:  0.04894133771396579 , test loss:  0.08546317079146207\n",
      "Epoch:  58070 , step:  58070 , train loss:  0.04893803809072163 , test loss:  0.08546217315194747\n",
      "Epoch:  58080 , step:  58080 , train loss:  0.04893473929097614 , test loss:  0.08546117606288897\n",
      "Epoch:  58090 , step:  58090 , train loss:  0.04893144131438335 , test loss:  0.08546017952401265\n",
      "Epoch:  58100 , step:  58100 , train loss:  0.04892814416059751 , test loss:  0.0854591835350452\n",
      "Epoch:  58110 , step:  58110 , train loss:  0.048924847829273074 , test loss:  0.08545818809571296\n",
      "Epoch:  58120 , step:  58120 , train loss:  0.048921552320064704 , test loss:  0.08545719320574276\n",
      "Epoch:  58130 , step:  58130 , train loss:  0.04891825763262725 , test loss:  0.08545619886486132\n",
      "Epoch:  58140 , step:  58140 , train loss:  0.048914963766615784 , test loss:  0.08545520507279586\n",
      "Epoch:  58150 , step:  58150 , train loss:  0.04891167072168557 , test loss:  0.08545421182927367\n",
      "Epoch:  58160 , step:  58160 , train loss:  0.048908378497492085 , test loss:  0.08545321913402217\n",
      "Epoch:  58170 , step:  58170 , train loss:  0.048905087093691034 , test loss:  0.08545222698676914\n",
      "Epoch:  58180 , step:  58180 , train loss:  0.04890179650993824 , test loss:  0.08545123538724216\n",
      "Epoch:  58190 , step:  58190 , train loss:  0.04889850674588986 , test loss:  0.08545024433516936\n",
      "Epoch:  58200 , step:  58200 , train loss:  0.04889521780120216 , test loss:  0.08544925383027863\n",
      "Epoch:  58210 , step:  58210 , train loss:  0.04889192967553159 , test loss:  0.08544826387229855\n",
      "Epoch:  58220 , step:  58220 , train loss:  0.04888864236853491 , test loss:  0.08544727446095784\n",
      "Epoch:  58230 , step:  58230 , train loss:  0.04888535587986898 , test loss:  0.0854462855959848\n",
      "Epoch:  58240 , step:  58240 , train loss:  0.04888207020919095 , test loss:  0.08544529727710846\n",
      "Epoch:  58250 , step:  58250 , train loss:  0.048878785356158055 , test loss:  0.08544430950405778\n",
      "Epoch:  58260 , step:  58260 , train loss:  0.04887550132042789 , test loss:  0.08544332227656203\n",
      "Epoch:  58270 , step:  58270 , train loss:  0.04887221810165811 , test loss:  0.08544233559435067\n",
      "Epoch:  58280 , step:  58280 , train loss:  0.04886893569950664 , test loss:  0.08544134945715316\n",
      "Epoch:  58290 , step:  58290 , train loss:  0.0488656541136316 , test loss:  0.0854403638646993\n",
      "Epoch:  58300 , step:  58300 , train loss:  0.04886237334369131 , test loss:  0.08543937881671891\n",
      "Epoch:  58310 , step:  58310 , train loss:  0.048859093389344285 , test loss:  0.08543839431294227\n",
      "Epoch:  58320 , step:  58320 , train loss:  0.04885581425024928 , test loss:  0.08543741035309955\n",
      "Epoch:  58330 , step:  58330 , train loss:  0.048852535926065165 , test loss:  0.08543642693692106\n",
      "Epoch:  58340 , step:  58340 , train loss:  0.048849258416451116 , test loss:  0.08543544406413768\n",
      "Epoch:  58350 , step:  58350 , train loss:  0.04884598172106644 , test loss:  0.08543446173447991\n",
      "Epoch:  58360 , step:  58360 , train loss:  0.04884270583957066 , test loss:  0.08543347994767898\n",
      "Epoch:  58370 , step:  58370 , train loss:  0.04883943077162352 , test loss:  0.08543249870346602\n",
      "Epoch:  58380 , step:  58380 , train loss:  0.04883615651688495 , test loss:  0.08543151800157216\n",
      "Epoch:  58390 , step:  58390 , train loss:  0.04883288307501504 , test loss:  0.08543053784172898\n",
      "Epoch:  58400 , step:  58400 , train loss:  0.048829610445674215 , test loss:  0.08542955822366821\n",
      "Epoch:  58410 , step:  58410 , train loss:  0.048826338628522904 , test loss:  0.08542857914712168\n",
      "Epoch:  58420 , step:  58420 , train loss:  0.04882306762322191 , test loss:  0.08542760061182127\n",
      "Epoch:  58430 , step:  58430 , train loss:  0.048819797429432155 , test loss:  0.08542662261749931\n",
      "Epoch:  58440 , step:  58440 , train loss:  0.04881652804681473 , test loss:  0.08542564516388805\n",
      "Epoch:  58450 , step:  58450 , train loss:  0.04881325947503101 , test loss:  0.08542466825072006\n",
      "Epoch:  58460 , step:  58460 , train loss:  0.048809991713742513 , test loss:  0.08542369187772823\n",
      "Epoch:  58470 , step:  58470 , train loss:  0.048806724762611 , test loss:  0.08542271604464508\n",
      "Epoch:  58480 , step:  58480 , train loss:  0.04880345862129835 , test loss:  0.08542174075120398\n",
      "Epoch:  58490 , step:  58490 , train loss:  0.048800193289466734 , test loss:  0.08542076599713795\n",
      "Epoch:  58500 , step:  58500 , train loss:  0.04879692876677846 , test loss:  0.08541979178218055\n",
      "Epoch:  58510 , step:  58510 , train loss:  0.048793665052896104 , test loss:  0.08541881810606523\n",
      "Epoch:  58520 , step:  58520 , train loss:  0.04879040214748231 , test loss:  0.08541784496852578\n",
      "Epoch:  58530 , step:  58530 , train loss:  0.04878714005020006 , test loss:  0.08541687236929618\n",
      "Epoch:  58540 , step:  58540 , train loss:  0.04878387876071249 , test loss:  0.08541590030811048\n",
      "Epoch:  58550 , step:  58550 , train loss:  0.048780618278682894 , test loss:  0.08541492878470275\n",
      "Epoch:  58560 , step:  58560 , train loss:  0.04877735860377481 , test loss:  0.08541395779880759\n",
      "Epoch:  58570 , step:  58570 , train loss:  0.048774099735651937 , test loss:  0.08541298735015967\n",
      "Epoch:  58580 , step:  58580 , train loss:  0.048770841673978216 , test loss:  0.08541201743849372\n",
      "Epoch:  58590 , step:  58590 , train loss:  0.048767584418417734 , test loss:  0.08541104806354465\n",
      "Epoch:  58600 , step:  58600 , train loss:  0.04876432796863484 , test loss:  0.0854100792250475\n",
      "Epoch:  58610 , step:  58610 , train loss:  0.04876107232429403 , test loss:  0.08540911092273767\n",
      "Epoch:  58620 , step:  58620 , train loss:  0.048757817485059994 , test loss:  0.08540814315635069\n",
      "Epoch:  58630 , step:  58630 , train loss:  0.04875456345059766 , test loss:  0.08540717592562194\n",
      "Epoch:  58640 , step:  58640 , train loss:  0.0487513102205721 , test loss:  0.08540620923028738\n",
      "Epoch:  58650 , step:  58650 , train loss:  0.048748057794648636 , test loss:  0.08540524307008296\n",
      "Epoch:  58660 , step:  58660 , train loss:  0.048744806172492765 , test loss:  0.0854042774447449\n",
      "Epoch:  58670 , step:  58670 , train loss:  0.04874155535377017 , test loss:  0.0854033123540094\n",
      "Epoch:  58680 , step:  58680 , train loss:  0.04873830533814673 , test loss:  0.08540234779761299\n",
      "Epoch:  58690 , step:  58690 , train loss:  0.04873505612528855 , test loss:  0.08540138377529238\n",
      "Epoch:  58700 , step:  58700 , train loss:  0.048731807714861894 , test loss:  0.08540042028678424\n",
      "Epoch:  58710 , step:  58710 , train loss:  0.04872856010653325 , test loss:  0.08539945733182562\n",
      "Epoch:  58720 , step:  58720 , train loss:  0.048725313299969264 , test loss:  0.08539849491015367\n",
      "Epoch:  58730 , step:  58730 , train loss:  0.04872206729483684 , test loss:  0.08539753302150599\n",
      "Epoch:  58740 , step:  58740 , train loss:  0.04871882209080302 , test loss:  0.08539657166561972\n",
      "Epoch:  58750 , step:  58750 , train loss:  0.04871557768753507 , test loss:  0.08539561084223272\n",
      "Epoch:  58760 , step:  58760 , train loss:  0.048712334084700444 , test loss:  0.08539465055108275\n",
      "Epoch:  58770 , step:  58770 , train loss:  0.04870909128196678 , test loss:  0.0853936907919078\n",
      "Epoch:  58780 , step:  58780 , train loss:  0.048705849279001914 , test loss:  0.08539273156444607\n",
      "Epoch:  58790 , step:  58790 , train loss:  0.0487026080754739 , test loss:  0.08539177286843593\n",
      "Epoch:  58800 , step:  58800 , train loss:  0.04869936767105097 , test loss:  0.08539081470361584\n",
      "Epoch:  58810 , step:  58810 , train loss:  0.04869612806540156 , test loss:  0.08538985706972446\n",
      "Epoch:  58820 , step:  58820 , train loss:  0.04869288925819426 , test loss:  0.08538889996650074\n",
      "Epoch:  58830 , step:  58830 , train loss:  0.04868965124909792 , test loss:  0.08538794339368366\n",
      "Epoch:  58840 , step:  58840 , train loss:  0.048686414037781525 , test loss:  0.08538698735101234\n",
      "Epoch:  58850 , step:  58850 , train loss:  0.048683177623914296 , test loss:  0.08538603183822638\n",
      "Epoch:  58860 , step:  58860 , train loss:  0.04867994200716562 , test loss:  0.08538507685506491\n",
      "Epoch:  58870 , step:  58870 , train loss:  0.04867670718720509 , test loss:  0.08538412240126791\n",
      "Epoch:  58880 , step:  58880 , train loss:  0.04867347316370249 , test loss:  0.08538316847657514\n",
      "Epoch:  58890 , step:  58890 , train loss:  0.0486702399363278 , test loss:  0.08538221508072667\n",
      "Epoch:  58900 , step:  58900 , train loss:  0.048667007504751204 , test loss:  0.08538126221346255\n",
      "Epoch:  58910 , step:  58910 , train loss:  0.04866377586864302 , test loss:  0.08538030987452327\n",
      "Epoch:  58920 , step:  58920 , train loss:  0.048660545027673856 , test loss:  0.08537935806364942\n",
      "Epoch:  58930 , step:  58930 , train loss:  0.048657314981514425 , test loss:  0.08537840678058142\n",
      "Epoch:  58940 , step:  58940 , train loss:  0.048654085729835685 , test loss:  0.08537745602506033\n",
      "Epoch:  58950 , step:  58950 , train loss:  0.048650857272308785 , test loss:  0.08537650579682723\n",
      "Epoch:  58960 , step:  58960 , train loss:  0.048647629608605025 , test loss:  0.0853755560956232\n",
      "Epoch:  58970 , step:  58970 , train loss:  0.048644402738395945 , test loss:  0.08537460692118948\n",
      "Epoch:  58980 , step:  58980 , train loss:  0.04864117666135319 , test loss:  0.08537365827326777\n",
      "Epoch:  58990 , step:  58990 , train loss:  0.04863795137714875 , test loss:  0.08537271015159974\n",
      "Epoch:  59000 , step:  59000 , train loss:  0.04863472688545469 , test loss:  0.08537176255592738\n",
      "Epoch:  59010 , step:  59010 , train loss:  0.04863150318594326 , test loss:  0.08537081548599228\n",
      "Epoch:  59020 , step:  59020 , train loss:  0.04862828027828698 , test loss:  0.08536986894153692\n",
      "Epoch:  59030 , step:  59030 , train loss:  0.04862505816215852 , test loss:  0.08536892292230379\n",
      "Epoch:  59040 , step:  59040 , train loss:  0.048621836837230696 , test loss:  0.08536797742803524\n",
      "Epoch:  59050 , step:  59050 , train loss:  0.04861861630317659 , test loss:  0.0853670324584739\n",
      "Epoch:  59060 , step:  59060 , train loss:  0.048615396559669434 , test loss:  0.08536608801336262\n",
      "Epoch:  59070 , step:  59070 , train loss:  0.048612177606382666 , test loss:  0.08536514409244464\n",
      "Epoch:  59080 , step:  59080 , train loss:  0.04860895944298992 , test loss:  0.08536420069546283\n",
      "Epoch:  59090 , step:  59090 , train loss:  0.048605742069164945 , test loss:  0.08536325782216046\n",
      "Epoch:  59100 , step:  59100 , train loss:  0.04860252548458183 , test loss:  0.08536231547228142\n",
      "Epoch:  59110 , step:  59110 , train loss:  0.048599309688914724 , test loss:  0.08536137364556927\n",
      "Epoch:  59120 , step:  59120 , train loss:  0.048596094681838 , test loss:  0.08536043234176759\n",
      "Epoch:  59130 , step:  59130 , train loss:  0.04859288046302624 , test loss:  0.08535949156062046\n",
      "Epoch:  59140 , step:  59140 , train loss:  0.04858966703215421 , test loss:  0.08535855130187224\n",
      "Epoch:  59150 , step:  59150 , train loss:  0.04858645438889686 , test loss:  0.08535761156526722\n",
      "Epoch:  59160 , step:  59160 , train loss:  0.04858324253292933 , test loss:  0.08535667235054964\n",
      "Epoch:  59170 , step:  59170 , train loss:  0.048580031463926934 , test loss:  0.08535573365746445\n",
      "Epoch:  59180 , step:  59180 , train loss:  0.048576821181565255 , test loss:  0.08535479548575632\n",
      "Epoch:  59190 , step:  59190 , train loss:  0.048573611685519914 , test loss:  0.08535385783517024\n",
      "Epoch:  59200 , step:  59200 , train loss:  0.04857040297546685 , test loss:  0.08535292070545131\n",
      "Epoch:  59210 , step:  59210 , train loss:  0.04856719505108213 , test loss:  0.08535198409634485\n",
      "Epoch:  59220 , step:  59220 , train loss:  0.04856398791204209 , test loss:  0.08535104800759652\n",
      "Epoch:  59230 , step:  59230 , train loss:  0.0485607815580231 , test loss:  0.08535011243895166\n",
      "Epoch:  59240 , step:  59240 , train loss:  0.04855757598870188 , test loss:  0.08534917739015628\n",
      "Epoch:  59250 , step:  59250 , train loss:  0.04855437120375524 , test loss:  0.08534824286095614\n",
      "Epoch:  59260 , step:  59260 , train loss:  0.048551167202860214 , test loss:  0.08534730885109738\n",
      "Epoch:  59270 , step:  59270 , train loss:  0.04854796398569399 , test loss:  0.08534637536032655\n",
      "Epoch:  59280 , step:  59280 , train loss:  0.048544761551934004 , test loss:  0.08534544238838977\n",
      "Epoch:  59290 , step:  59290 , train loss:  0.04854155990125783 , test loss:  0.08534450993503377\n",
      "Epoch:  59300 , step:  59300 , train loss:  0.048538359033343276 , test loss:  0.08534357800000537\n",
      "Epoch:  59310 , step:  59310 , train loss:  0.04853515894786825 , test loss:  0.08534264658305155\n",
      "Epoch:  59320 , step:  59320 , train loss:  0.04853195964451093 , test loss:  0.08534171568391911\n",
      "Epoch:  59330 , step:  59330 , train loss:  0.04852876112294967 , test loss:  0.08534078530235556\n",
      "Epoch:  59340 , step:  59340 , train loss:  0.04852556338286297 , test loss:  0.0853398554381083\n",
      "Epoch:  59350 , step:  59350 , train loss:  0.04852236642392957 , test loss:  0.08533892609092492\n",
      "Epoch:  59360 , step:  59360 , train loss:  0.048519170245828344 , test loss:  0.085337997260553\n",
      "Epoch:  59370 , step:  59370 , train loss:  0.04851597484823838 , test loss:  0.08533706894674042\n",
      "Epoch:  59380 , step:  59380 , train loss:  0.04851278023083894 , test loss:  0.08533614114923539\n",
      "Epoch:  59390 , step:  59390 , train loss:  0.04850958639330952 , test loss:  0.08533521386778606\n",
      "Epoch:  59400 , step:  59400 , train loss:  0.04850639333532971 , test loss:  0.0853342871021408\n",
      "Epoch:  59410 , step:  59410 , train loss:  0.04850320105657938 , test loss:  0.08533336085204825\n",
      "Epoch:  59420 , step:  59420 , train loss:  0.04850000955673852 , test loss:  0.08533243511725701\n",
      "Epoch:  59430 , step:  59430 , train loss:  0.048496818835487315 , test loss:  0.08533150989751584\n",
      "Epoch:  59440 , step:  59440 , train loss:  0.04849362889250622 , test loss:  0.0853305851925739\n",
      "Epoch:  59450 , step:  59450 , train loss:  0.04849043972747572 , test loss:  0.08532966100218037\n",
      "Epoch:  59460 , step:  59460 , train loss:  0.04848725134007663 , test loss:  0.08532873732608448\n",
      "Epoch:  59470 , step:  59470 , train loss:  0.04848406372998988 , test loss:  0.08532781416403591\n",
      "Epoch:  59480 , step:  59480 , train loss:  0.04848087689689658 , test loss:  0.08532689151578407\n",
      "Epoch:  59490 , step:  59490 , train loss:  0.04847769084047803 , test loss:  0.08532596938107903\n",
      "Epoch:  59500 , step:  59500 , train loss:  0.04847450556041576 , test loss:  0.08532504775967047\n",
      "Epoch:  59510 , step:  59510 , train loss:  0.04847132105639142 , test loss:  0.08532412665130881\n",
      "Epoch:  59520 , step:  59520 , train loss:  0.048468137328086885 , test loss:  0.0853232060557442\n",
      "Epoch:  59530 , step:  59530 , train loss:  0.04846495437518421 , test loss:  0.08532228597272729\n",
      "Epoch:  59540 , step:  59540 , train loss:  0.0484617721973656 , test loss:  0.08532136640200833\n",
      "Epoch:  59550 , step:  59550 , train loss:  0.048458590794313514 , test loss:  0.0853204473433383\n",
      "Epoch:  59560 , step:  59560 , train loss:  0.04845541016571052 , test loss:  0.08531952879646809\n",
      "Epoch:  59570 , step:  59570 , train loss:  0.048452230311239394 , test loss:  0.0853186107611489\n",
      "Epoch:  59580 , step:  59580 , train loss:  0.048449051230583115 , test loss:  0.08531769323713176\n",
      "Epoch:  59590 , step:  59590 , train loss:  0.048445872923424844 , test loss:  0.08531677622416825\n",
      "Epoch:  59600 , step:  59600 , train loss:  0.0484426953894479 , test loss:  0.08531585972200988\n",
      "Epoch:  59610 , step:  59610 , train loss:  0.04843951862833582 , test loss:  0.08531494373040835\n",
      "Epoch:  59620 , step:  59620 , train loss:  0.04843634263977226 , test loss:  0.08531402824911559\n",
      "Epoch:  59630 , step:  59630 , train loss:  0.048433167423441136 , test loss:  0.08531311327788364\n",
      "Epoch:  59640 , step:  59640 , train loss:  0.04842999297902652 , test loss:  0.08531219881646462\n",
      "Epoch:  59650 , step:  59650 , train loss:  0.048426819306212635 , test loss:  0.08531128486461079\n",
      "Epoch:  59660 , step:  59660 , train loss:  0.04842364640468391 , test loss:  0.0853103714220748\n",
      "Epoch:  59670 , step:  59670 , train loss:  0.04842047427412498 , test loss:  0.08530945848860924\n",
      "Epoch:  59680 , step:  59680 , train loss:  0.048417302914220604 , test loss:  0.08530854606396707\n",
      "Epoch:  59690 , step:  59690 , train loss:  0.04841413232465579 , test loss:  0.08530763414790117\n",
      "Epoch:  59700 , step:  59700 , train loss:  0.0484109625051157 , test loss:  0.08530672274016456\n",
      "Epoch:  59710 , step:  59710 , train loss:  0.048407793455285626 , test loss:  0.08530581184051073\n",
      "Epoch:  59720 , step:  59720 , train loss:  0.04840462517485114 , test loss:  0.0853049014486931\n",
      "Epoch:  59730 , step:  59730 , train loss:  0.048401457663497904 , test loss:  0.0853039915644652\n",
      "Epoch:  59740 , step:  59740 , train loss:  0.04839829092091182 , test loss:  0.08530308218758074\n",
      "Epoch:  59750 , step:  59750 , train loss:  0.04839512494677898 , test loss:  0.08530217331779356\n",
      "Epoch:  59760 , step:  59760 , train loss:  0.0483919597407856 , test loss:  0.08530126495485812\n",
      "Epoch:  59770 , step:  59770 , train loss:  0.0483887953026181 , test loss:  0.08530035709852829\n",
      "Epoch:  59780 , step:  59780 , train loss:  0.04838563163196309 , test loss:  0.08529944974855858\n",
      "Epoch:  59790 , step:  59790 , train loss:  0.04838246872850737 , test loss:  0.08529854290470348\n",
      "Epoch:  59800 , step:  59800 , train loss:  0.048379306591937905 , test loss:  0.08529763656671768\n",
      "Epoch:  59810 , step:  59810 , train loss:  0.04837614522194188 , test loss:  0.08529673073435615\n",
      "Epoch:  59820 , step:  59820 , train loss:  0.04837298461820653 , test loss:  0.08529582540737371\n",
      "Epoch:  59830 , step:  59830 , train loss:  0.048369824780419436 , test loss:  0.08529492058552575\n",
      "Epoch:  59840 , step:  59840 , train loss:  0.04836666570826829 , test loss:  0.08529401626856747\n",
      "Epoch:  59850 , step:  59850 , train loss:  0.04836350740144092 , test loss:  0.08529311245625427\n",
      "Epoch:  59860 , step:  59860 , train loss:  0.048360349859625396 , test loss:  0.08529220914834174\n",
      "Epoch:  59870 , step:  59870 , train loss:  0.04835719308250998 , test loss:  0.08529130634458566\n",
      "Epoch:  59880 , step:  59880 , train loss:  0.04835403706978303 , test loss:  0.08529040404474236\n",
      "Epoch:  59890 , step:  59890 , train loss:  0.04835088182113314 , test loss:  0.08528950224856736\n",
      "Epoch:  59900 , step:  59900 , train loss:  0.048347727336249084 , test loss:  0.08528860095581728\n",
      "Epoch:  59910 , step:  59910 , train loss:  0.04834457361481983 , test loss:  0.0852877001662484\n",
      "Epoch:  59920 , step:  59920 , train loss:  0.048341420656534455 , test loss:  0.08528679987961708\n",
      "Epoch:  59930 , step:  59930 , train loss:  0.04833826846108229 , test loss:  0.0852859000956803\n",
      "Epoch:  59940 , step:  59940 , train loss:  0.04833511702815282 , test loss:  0.08528500081419495\n",
      "Epoch:  59950 , step:  59950 , train loss:  0.048331966357435675 , test loss:  0.08528410203491792\n",
      "Epoch:  59960 , step:  59960 , train loss:  0.048328816448620755 , test loss:  0.08528320375760633\n",
      "Epoch:  59970 , step:  59970 , train loss:  0.04832566730139801 , test loss:  0.08528230598201736\n",
      "Epoch:  59980 , step:  59980 , train loss:  0.04832251891545768 , test loss:  0.0852814087079089\n",
      "Epoch:  59990 , step:  59990 , train loss:  0.0483193712904901 , test loss:  0.08528051193503819\n",
      "Epoch:  60000 , step:  60000 , train loss:  0.04831622442618582 , test loss:  0.08527961566316329\n",
      "Epoch:  60010 , step:  60010 , train loss:  0.048313078322235616 , test loss:  0.08527871989204185\n",
      "Epoch:  60020 , step:  60020 , train loss:  0.04830993297833034 , test loss:  0.0852778246214321\n",
      "Epoch:  60030 , step:  60030 , train loss:  0.048306788394161124 , test loss:  0.08527692985109245\n",
      "Epoch:  60040 , step:  60040 , train loss:  0.04830364456941917 , test loss:  0.08527603558078103\n",
      "Epoch:  60050 , step:  60050 , train loss:  0.04830050150379595 , test loss:  0.08527514181025651\n",
      "Epoch:  60060 , step:  60060 , train loss:  0.048297359196983095 , test loss:  0.08527424853927751\n",
      "Epoch:  60070 , step:  60070 , train loss:  0.04829421764867236 , test loss:  0.0852733557676029\n",
      "Epoch:  60080 , step:  60080 , train loss:  0.04829107685855574 , test loss:  0.08527246349499172\n",
      "Epoch:  60090 , step:  60090 , train loss:  0.04828793682632535 , test loss:  0.08527157172120299\n",
      "Epoch:  60100 , step:  60100 , train loss:  0.04828479755167353 , test loss:  0.0852706804459961\n",
      "Epoch:  60110 , step:  60110 , train loss:  0.0482816590342928 , test loss:  0.08526978966913056\n",
      "Epoch:  60120 , step:  60120 , train loss:  0.04827852127387576 , test loss:  0.08526889939036596\n",
      "Epoch:  60130 , step:  60130 , train loss:  0.04827538427011534 , test loss:  0.08526800960946199\n",
      "Epoch:  60140 , step:  60140 , train loss:  0.04827224802270454 , test loss:  0.08526712032617864\n",
      "Epoch:  60150 , step:  60150 , train loss:  0.04826911253133654 , test loss:  0.08526623154027604\n",
      "Epoch:  60160 , step:  60160 , train loss:  0.04826597779570474 , test loss:  0.08526534325151403\n",
      "Epoch:  60170 , step:  60170 , train loss:  0.04826284381550267 , test loss:  0.08526445545965335\n",
      "Epoch:  60180 , step:  60180 , train loss:  0.048259710590424075 , test loss:  0.08526356816445435\n",
      "Epoch:  60190 , step:  60190 , train loss:  0.04825657812016287 , test loss:  0.0852626813656777\n",
      "Epoch:  60200 , step:  60200 , train loss:  0.0482534464044131 , test loss:  0.08526179506308423\n",
      "Epoch:  60210 , step:  60210 , train loss:  0.04825031544286906 , test loss:  0.08526090925643504\n",
      "Epoch:  60220 , step:  60220 , train loss:  0.04824718523522515 , test loss:  0.08526002394549105\n",
      "Epoch:  60230 , step:  60230 , train loss:  0.048244055781175965 , test loss:  0.08525913913001357\n",
      "Epoch:  60240 , step:  60240 , train loss:  0.04824092708041631 , test loss:  0.08525825480976405\n",
      "Epoch:  60250 , step:  60250 , train loss:  0.048237799132641125 , test loss:  0.08525737098450413\n",
      "Epoch:  60260 , step:  60260 , train loss:  0.04823467193754553 , test loss:  0.08525648765399532\n",
      "Epoch:  60270 , step:  60270 , train loss:  0.048231545494824835 , test loss:  0.08525560481799963\n",
      "Epoch:  60280 , step:  60280 , train loss:  0.048228419804174534 , test loss:  0.08525472247627904\n",
      "Epoch:  60290 , step:  60290 , train loss:  0.04822529486529025 , test loss:  0.08525384062859573\n",
      "Epoch:  60300 , step:  60300 , train loss:  0.0482221706778678 , test loss:  0.085252959274712\n",
      "Epoch:  60310 , step:  60310 , train loss:  0.048219047241603204 , test loss:  0.08525207841439028\n",
      "Epoch:  60320 , step:  60320 , train loss:  0.04821592455619263 , test loss:  0.08525119804739324\n",
      "Epoch:  60330 , step:  60330 , train loss:  0.04821280262133241 , test loss:  0.08525031817348364\n",
      "Epoch:  60340 , step:  60340 , train loss:  0.04820968143671907 , test loss:  0.08524943879242432\n",
      "Epoch:  60350 , step:  60350 , train loss:  0.04820656100204927 , test loss:  0.0852485599039785\n",
      "Epoch:  60360 , step:  60360 , train loss:  0.048203441317019924 , test loss:  0.08524768150790916\n",
      "Epoch:  60370 , step:  60370 , train loss:  0.04820032238132807 , test loss:  0.08524680360397974\n",
      "Epoch:  60380 , step:  60380 , train loss:  0.048197204194670856 , test loss:  0.08524592619195366\n",
      "Epoch:  60390 , step:  60390 , train loss:  0.04819408675674573 , test loss:  0.08524504927159506\n",
      "Epoch:  60400 , step:  60400 , train loss:  0.04819097006725023 , test loss:  0.08524417284266714\n",
      "Epoch:  60410 , step:  60410 , train loss:  0.04818785412588206 , test loss:  0.08524329690493418\n",
      "Epoch:  60420 , step:  60420 , train loss:  0.048184738932339134 , test loss:  0.08524242145816012\n",
      "Epoch:  60430 , step:  60430 , train loss:  0.04818162448631954 , test loss:  0.08524154650210927\n",
      "Epoch:  60440 , step:  60440 , train loss:  0.0481785107875215 , test loss:  0.08524067203654583\n",
      "Epoch:  60450 , step:  60450 , train loss:  0.048175397835643424 , test loss:  0.08523979806123468\n",
      "Epoch:  60460 , step:  60460 , train loss:  0.04817228563038392 , test loss:  0.08523892457594037\n",
      "Epoch:  60470 , step:  60470 , train loss:  0.048169174171441745 , test loss:  0.08523805158042765\n",
      "Epoch:  60480 , step:  60480 , train loss:  0.04816606345851584 , test loss:  0.08523717907446161\n",
      "Epoch:  60490 , step:  60490 , train loss:  0.04816295349130531 , test loss:  0.08523630705780738\n",
      "Epoch:  60500 , step:  60500 , train loss:  0.048159844269509394 , test loss:  0.08523543553023025\n",
      "Epoch:  60510 , step:  60510 , train loss:  0.04815673579282757 , test loss:  0.0852345644914954\n",
      "Epoch:  60520 , step:  60520 , train loss:  0.04815362806095943 , test loss:  0.0852336939413687\n",
      "Epoch:  60530 , step:  60530 , train loss:  0.04815052107360481 , test loss:  0.08523282387961574\n",
      "Epoch:  60540 , step:  60540 , train loss:  0.04814741483046361 , test loss:  0.08523195430600243\n",
      "Epoch:  60550 , step:  60550 , train loss:  0.04814430933123598 , test loss:  0.08523108522029473\n",
      "Epoch:  60560 , step:  60560 , train loss:  0.048141204575622244 , test loss:  0.0852302166222588\n",
      "Epoch:  60570 , step:  60570 , train loss:  0.04813810056332286 , test loss:  0.0852293485116612\n",
      "Epoch:  60580 , step:  60580 , train loss:  0.04813499729403844 , test loss:  0.085228480888268\n",
      "Epoch:  60590 , step:  60590 , train loss:  0.048131894767469825 , test loss:  0.0852276137518458\n",
      "Epoch:  60600 , step:  60600 , train loss:  0.04812879298331798 , test loss:  0.08522674710216159\n",
      "Epoch:  60610 , step:  60610 , train loss:  0.04812569194128408 , test loss:  0.08522588093898208\n",
      "Epoch:  60620 , step:  60620 , train loss:  0.04812259164106942 , test loss:  0.08522501526207454\n",
      "Epoch:  60630 , step:  60630 , train loss:  0.04811949208237551 , test loss:  0.08522415007120589\n",
      "Epoch:  60640 , step:  60640 , train loss:  0.048116393264904016 , test loss:  0.08522328536614357\n",
      "Epoch:  60650 , step:  60650 , train loss:  0.04811329518835672 , test loss:  0.08522242114665521\n",
      "Epoch:  60660 , step:  60660 , train loss:  0.048110197852435685 , test loss:  0.08522155741250795\n",
      "Epoch:  60670 , step:  60670 , train loss:  0.04810710125684303 , test loss:  0.08522069416346997\n",
      "Epoch:  60680 , step:  60680 , train loss:  0.048104005401281104 , test loss:  0.08521983139930903\n",
      "Epoch:  60690 , step:  60690 , train loss:  0.04810091028545241 , test loss:  0.08521896911979322\n",
      "Epoch:  60700 , step:  60700 , train loss:  0.04809781590905966 , test loss:  0.08521810732469073\n",
      "Epoch:  60710 , step:  60710 , train loss:  0.04809472227180567 , test loss:  0.08521724601376997\n",
      "Epoch:  60720 , step:  60720 , train loss:  0.048091629373393445 , test loss:  0.08521638518679919\n",
      "Epoch:  60730 , step:  60730 , train loss:  0.04808853721352618 , test loss:  0.08521552484354734\n",
      "Epoch:  60740 , step:  60740 , train loss:  0.0480854457919072 , test loss:  0.08521466498378283\n",
      "Epoch:  60750 , step:  60750 , train loss:  0.04808235510824005 , test loss:  0.08521380560727485\n",
      "Epoch:  60760 , step:  60760 , train loss:  0.04807926516222839 , test loss:  0.08521294671379244\n",
      "Epoch:  60770 , step:  60770 , train loss:  0.04807617595357611 , test loss:  0.08521208830310473\n",
      "Epoch:  60780 , step:  60780 , train loss:  0.04807308748198721 , test loss:  0.08521123037498107\n",
      "Epoch:  60790 , step:  60790 , train loss:  0.04806999974716588 , test loss:  0.08521037292919109\n",
      "Epoch:  60800 , step:  60800 , train loss:  0.04806691274881647 , test loss:  0.0852095159655042\n",
      "Epoch:  60810 , step:  60810 , train loss:  0.04806382648664352 , test loss:  0.08520865948369037\n",
      "Epoch:  60820 , step:  60820 , train loss:  0.0480607409603517 , test loss:  0.08520780348351947\n",
      "Epoch:  60830 , step:  60830 , train loss:  0.04805765616964589 , test loss:  0.08520694796476143\n",
      "Epoch:  60840 , step:  60840 , train loss:  0.04805457211423111 , test loss:  0.08520609292718659\n",
      "Epoch:  60850 , step:  60850 , train loss:  0.04805148879381255 , test loss:  0.08520523837056533\n",
      "Epoch:  60860 , step:  60860 , train loss:  0.04804840620809557 , test loss:  0.08520438429466806\n",
      "Epoch:  60870 , step:  60870 , train loss:  0.0480453243567857 , test loss:  0.08520353069926555\n",
      "Epoch:  60880 , step:  60880 , train loss:  0.04804224323958862 , test loss:  0.08520267758412847\n",
      "Epoch:  60890 , step:  60890 , train loss:  0.04803916285621019 , test loss:  0.08520182494902773\n",
      "Epoch:  60900 , step:  60900 , train loss:  0.04803608320635648 , test loss:  0.08520097279373444\n",
      "Epoch:  60910 , step:  60910 , train loss:  0.048033004289733616 , test loss:  0.08520012111801989\n",
      "Epoch:  60920 , step:  60920 , train loss:  0.048029926106048004 , test loss:  0.08519926992165547\n",
      "Epoch:  60930 , step:  60930 , train loss:  0.04802684865500616 , test loss:  0.08519841920441235\n",
      "Epoch:  60940 , step:  60940 , train loss:  0.04802377193631474 , test loss:  0.08519756896606244\n",
      "Epoch:  60950 , step:  60950 , train loss:  0.04802069594968065 , test loss:  0.08519671920637738\n",
      "Epoch:  60960 , step:  60960 , train loss:  0.04801762069481088 , test loss:  0.08519586992512923\n",
      "Epoch:  60970 , step:  60970 , train loss:  0.04801454617141262 , test loss:  0.0851950211220899\n",
      "Epoch:  60980 , step:  60980 , train loss:  0.04801147237919326 , test loss:  0.08519417279703187\n",
      "Epoch:  60990 , step:  60990 , train loss:  0.048008399317860245 , test loss:  0.08519332494972712\n",
      "Epoch:  61000 , step:  61000 , train loss:  0.04800532698712132 , test loss:  0.0851924775799485\n",
      "Epoch:  61010 , step:  61010 , train loss:  0.04800225538668433 , test loss:  0.08519163068746835\n",
      "Epoch:  61020 , step:  61020 , train loss:  0.04799918451625725 , test loss:  0.08519078427205953\n",
      "Epoch:  61030 , step:  61030 , train loss:  0.04799611437554828 , test loss:  0.08518993833349497\n",
      "Epoch:  61040 , step:  61040 , train loss:  0.04799304496426577 , test loss:  0.08518909287154769\n",
      "Epoch:  61050 , step:  61050 , train loss:  0.04798997628211822 , test loss:  0.0851882478859911\n",
      "Epoch:  61060 , step:  61060 , train loss:  0.047986908328814304 , test loss:  0.08518740337659814\n",
      "Epoch:  61070 , step:  61070 , train loss:  0.04798384110406285 , test loss:  0.0851865593431426\n",
      "Epoch:  61080 , step:  61080 , train loss:  0.047980774607572864 , test loss:  0.0851857157853979\n",
      "Epoch:  61090 , step:  61090 , train loss:  0.04797770883905353 , test loss:  0.08518487270313795\n",
      "Epoch:  61100 , step:  61100 , train loss:  0.04797464379821412 , test loss:  0.0851840300961365\n",
      "Epoch:  61110 , step:  61110 , train loss:  0.04797157948476418 , test loss:  0.0851831879641677\n",
      "Epoch:  61120 , step:  61120 , train loss:  0.04796851589841334 , test loss:  0.08518234630700575\n",
      "Epoch:  61130 , step:  61130 , train loss:  0.04796545303887143 , test loss:  0.08518150512442488\n",
      "Epoch:  61140 , step:  61140 , train loss:  0.04796239090584844 , test loss:  0.0851806644161995\n",
      "Epoch:  61150 , step:  61150 , train loss:  0.04795932949905451 , test loss:  0.0851798241821043\n",
      "Epoch:  61160 , step:  61160 , train loss:  0.047956268818199935 , test loss:  0.08517898442191409\n",
      "Epoch:  61170 , step:  61170 , train loss:  0.0479532088629952 , test loss:  0.08517814513540356\n",
      "Epoch:  61180 , step:  61180 , train loss:  0.04795014963315095 , test loss:  0.08517730632234777\n",
      "Epoch:  61190 , step:  61190 , train loss:  0.047947091128377965 , test loss:  0.08517646798252206\n",
      "Epoch:  61200 , step:  61200 , train loss:  0.04794403334838721 , test loss:  0.08517563011570145\n",
      "Epoch:  61210 , step:  61210 , train loss:  0.04794097629288982 , test loss:  0.08517479272166155\n",
      "Epoch:  61220 , step:  61220 , train loss:  0.04793791996159709 , test loss:  0.08517395580017792\n",
      "Epoch:  61230 , step:  61230 , train loss:  0.04793486435422043 , test loss:  0.08517311935102621\n",
      "Epoch:  61240 , step:  61240 , train loss:  0.047931809470471494 , test loss:  0.08517228337398212\n",
      "Epoch:  61250 , step:  61250 , train loss:  0.04792875531006203 , test loss:  0.08517144786882194\n",
      "Epoch:  61260 , step:  61260 , train loss:  0.04792570187270397 , test loss:  0.08517061283532162\n",
      "Epoch:  61270 , step:  61270 , train loss:  0.04792264915810943 , test loss:  0.08516977827325765\n",
      "Epoch:  61280 , step:  61280 , train loss:  0.04791959716599066 , test loss:  0.08516894418240607\n",
      "Epoch:  61290 , step:  61290 , train loss:  0.04791654589606011 , test loss:  0.08516811056254367\n",
      "Epoch:  61300 , step:  61300 , train loss:  0.047913495348030306 , test loss:  0.08516727741344719\n",
      "Epoch:  61310 , step:  61310 , train loss:  0.047910445521614026 , test loss:  0.08516644473489314\n",
      "Epoch:  61320 , step:  61320 , train loss:  0.047907396416524174 , test loss:  0.08516561252665886\n",
      "Epoch:  61330 , step:  61330 , train loss:  0.04790434803247382 , test loss:  0.0851647807885211\n",
      "Epoch:  61340 , step:  61340 , train loss:  0.047901300369176184 , test loss:  0.08516394952025712\n",
      "Epoch:  61350 , step:  61350 , train loss:  0.04789825342634465 , test loss:  0.08516311872164456\n",
      "Epoch:  61360 , step:  61360 , train loss:  0.04789520720369278 , test loss:  0.08516228839246069\n",
      "Epoch:  61370 , step:  61370 , train loss:  0.04789216170093427 , test loss:  0.08516145853248343\n",
      "Epoch:  61380 , step:  61380 , train loss:  0.047889116917783005 , test loss:  0.0851606291414902\n",
      "Epoch:  61390 , step:  61390 , train loss:  0.04788607285395298 , test loss:  0.08515980021925906\n",
      "Epoch:  61400 , step:  61400 , train loss:  0.04788302950915846 , test loss:  0.08515897176556812\n",
      "Epoch:  61410 , step:  61410 , train loss:  0.04787998688311374 , test loss:  0.08515814378019558\n",
      "Epoch:  61420 , step:  61420 , train loss:  0.04787694497553337 , test loss:  0.08515731626291975\n",
      "Epoch:  61430 , step:  61430 , train loss:  0.04787390378613198 , test loss:  0.08515648921351905\n",
      "Epoch:  61440 , step:  61440 , train loss:  0.0478708633146244 , test loss:  0.08515566263177216\n",
      "Epoch:  61450 , step:  61450 , train loss:  0.0478678235607257 , test loss:  0.08515483651745775\n",
      "Epoch:  61460 , step:  61460 , train loss:  0.04786478452415093 , test loss:  0.08515401087035464\n",
      "Epoch:  61470 , step:  61470 , train loss:  0.04786174620461548 , test loss:  0.08515318569024212\n",
      "Epoch:  61480 , step:  61480 , train loss:  0.047858708601834804 , test loss:  0.08515236097689896\n",
      "Epoch:  61490 , step:  61490 , train loss:  0.047855671715524475 , test loss:  0.08515153673010463\n",
      "Epoch:  61500 , step:  61500 , train loss:  0.047852635545400375 , test loss:  0.08515071294963857\n",
      "Epoch:  61510 , step:  61510 , train loss:  0.047849600091178376 , test loss:  0.08514988963528035\n",
      "Epoch:  61520 , step:  61520 , train loss:  0.047846565352574616 , test loss:  0.08514906678680963\n",
      "Epoch:  61530 , step:  61530 , train loss:  0.047843531329305376 , test loss:  0.0851482444040062\n",
      "Epoch:  61540 , step:  61540 , train loss:  0.047840498021087065 , test loss:  0.08514742248665019\n",
      "Epoch:  61550 , step:  61550 , train loss:  0.04783746542763626 , test loss:  0.08514660103452129\n",
      "Epoch:  61560 , step:  61560 , train loss:  0.04783443354866973 , test loss:  0.08514578004740025\n",
      "Epoch:  61570 , step:  61570 , train loss:  0.04783140238390436 , test loss:  0.08514495952506718\n",
      "Epoch:  61580 , step:  61580 , train loss:  0.047828371933057195 , test loss:  0.08514413946730257\n",
      "Epoch:  61590 , step:  61590 , train loss:  0.047825342195845486 , test loss:  0.08514331987388712\n",
      "Epoch:  61600 , step:  61600 , train loss:  0.0478223131719866 , test loss:  0.08514250074460174\n",
      "Epoch:  61610 , step:  61610 , train loss:  0.04781928486119803 , test loss:  0.08514168207922695\n",
      "Epoch:  61620 , step:  61620 , train loss:  0.04781625726319753 , test loss:  0.0851408638775441\n",
      "Epoch:  61630 , step:  61630 , train loss:  0.04781323037770294 , test loss:  0.08514004613933433\n",
      "Epoch:  61640 , step:  61640 , train loss:  0.0478102042044322 , test loss:  0.08513922886437887\n",
      "Epoch:  61650 , step:  61650 , train loss:  0.047807178743103564 , test loss:  0.08513841205245931\n",
      "Epoch:  61660 , step:  61660 , train loss:  0.047804153993435294 , test loss:  0.08513759570335708\n",
      "Epoch:  61670 , step:  61670 , train loss:  0.047801129955145905 , test loss:  0.08513677981685408\n",
      "Epoch:  61680 , step:  61680 , train loss:  0.04779810662795401 , test loss:  0.085135964392732\n",
      "Epoch:  61690 , step:  61690 , train loss:  0.04779508401157842 , test loss:  0.08513514943077274\n",
      "Epoch:  61700 , step:  61700 , train loss:  0.04779206210573809 , test loss:  0.08513433493075868\n",
      "Epoch:  61710 , step:  61710 , train loss:  0.04778904091015208 , test loss:  0.085133520892472\n",
      "Epoch:  61720 , step:  61720 , train loss:  0.04778602042453971 , test loss:  0.08513270731569508\n",
      "Epoch:  61730 , step:  61730 , train loss:  0.047783000648620386 , test loss:  0.08513189420021026\n",
      "Epoch:  61740 , step:  61740 , train loss:  0.04777998158211368 , test loss:  0.08513108154580029\n",
      "Epoch:  61750 , step:  61750 , train loss:  0.04777696322473933 , test loss:  0.08513026935224816\n",
      "Epoch:  61760 , step:  61760 , train loss:  0.04777394557621723 , test loss:  0.0851294576193367\n",
      "Epoch:  61770 , step:  61770 , train loss:  0.04777092863626741 , test loss:  0.08512864634684884\n",
      "Epoch:  61780 , step:  61780 , train loss:  0.04776791240461011 , test loss:  0.08512783553456782\n",
      "Epoch:  61790 , step:  61790 , train loss:  0.04776489688096566 , test loss:  0.08512702518227697\n",
      "Epoch:  61800 , step:  61800 , train loss:  0.047761882065054564 , test loss:  0.08512621528975968\n",
      "Epoch:  61810 , step:  61810 , train loss:  0.04775886795659751 , test loss:  0.08512540585679963\n",
      "Epoch:  61820 , step:  61820 , train loss:  0.04775585455531532 , test loss:  0.08512459688318058\n",
      "Epoch:  61830 , step:  61830 , train loss:  0.047752841860928964 , test loss:  0.08512378836868624\n",
      "Epoch:  61840 , step:  61840 , train loss:  0.04774982987315961 , test loss:  0.08512298031310074\n",
      "Epoch:  61850 , step:  61850 , train loss:  0.047746818591728524 , test loss:  0.08512217271620798\n",
      "Epoch:  61860 , step:  61860 , train loss:  0.04774380801635717 , test loss:  0.08512136557779253\n",
      "Epoch:  61870 , step:  61870 , train loss:  0.04774079814676713 , test loss:  0.0851205588976385\n",
      "Epoch:  61880 , step:  61880 , train loss:  0.04773778898268018 , test loss:  0.0851197526755305\n",
      "Epoch:  61890 , step:  61890 , train loss:  0.04773478052381823 , test loss:  0.08511894691125334\n",
      "Epoch:  61900 , step:  61900 , train loss:  0.04773177276990331 , test loss:  0.08511814160459154\n",
      "Epoch:  61910 , step:  61910 , train loss:  0.04772876572065768 , test loss:  0.08511733675532994\n",
      "Epoch:  61920 , step:  61920 , train loss:  0.047725759375803735 , test loss:  0.08511653236325399\n",
      "Epoch:  61930 , step:  61930 , train loss:  0.047722753735063954 , test loss:  0.08511572842814846\n",
      "Epoch:  61940 , step:  61940 , train loss:  0.04771974879816106 , test loss:  0.0851149249497989\n",
      "Epoch:  61950 , step:  61950 , train loss:  0.04771674456481787 , test loss:  0.08511412192799069\n",
      "Epoch:  61960 , step:  61960 , train loss:  0.047713741034757375 , test loss:  0.08511331936250931\n",
      "Epoch:  61970 , step:  61970 , train loss:  0.04771073820770276 , test loss:  0.08511251725314063\n",
      "Epoch:  61980 , step:  61980 , train loss:  0.047707736083377285 , test loss:  0.08511171559967022\n",
      "Epoch:  61990 , step:  61990 , train loss:  0.047704734661504405 , test loss:  0.08511091440188422\n",
      "Epoch:  62000 , step:  62000 , train loss:  0.04770173394180775 , test loss:  0.08511011365956878\n",
      "Epoch:  62010 , step:  62010 , train loss:  0.04769873392401107 , test loss:  0.08510931337251\n",
      "Epoch:  62020 , step:  62020 , train loss:  0.04769573460783828 , test loss:  0.08510851354049416\n",
      "Epoch:  62030 , step:  62030 , train loss:  0.04769273599301344 , test loss:  0.08510771416330791\n",
      "Epoch:  62040 , step:  62040 , train loss:  0.04768973807926078 , test loss:  0.08510691524073781\n",
      "Epoch:  62050 , step:  62050 , train loss:  0.047686740866304686 , test loss:  0.08510611677257057\n",
      "Epoch:  62060 , step:  62060 , train loss:  0.047683744353869656 , test loss:  0.08510531875859313\n",
      "Epoch:  62070 , step:  62070 , train loss:  0.04768074854168042 , test loss:  0.08510452119859238\n",
      "Epoch:  62080 , step:  62080 , train loss:  0.047677753429461754 , test loss:  0.08510372409235557\n",
      "Epoch:  62090 , step:  62090 , train loss:  0.04767475901693866 , test loss:  0.08510292743966992\n",
      "Epoch:  62100 , step:  62100 , train loss:  0.04767176530383628 , test loss:  0.08510213124032293\n",
      "Epoch:  62110 , step:  62110 , train loss:  0.04766877228987995 , test loss:  0.08510133549410195\n",
      "Epoch:  62120 , step:  62120 , train loss:  0.047665779974795035 , test loss:  0.0851005402007947\n",
      "Epoch:  62130 , step:  62130 , train loss:  0.04766278835830718 , test loss:  0.08509974536018898\n",
      "Epoch:  62140 , step:  62140 , train loss:  0.04765979744014212 , test loss:  0.08509895097207282\n",
      "Epoch:  62150 , step:  62150 , train loss:  0.04765680722002573 , test loss:  0.08509815703623401\n",
      "Epoch:  62160 , step:  62160 , train loss:  0.047653817697684085 , test loss:  0.08509736355246086\n",
      "Epoch:  62170 , step:  62170 , train loss:  0.047650828872843415 , test loss:  0.08509657052054173\n",
      "Epoch:  62180 , step:  62180 , train loss:  0.04764784074523005 , test loss:  0.08509577794026503\n",
      "Epoch:  62190 , step:  62190 , train loss:  0.047644853314570454 , test loss:  0.08509498581141921\n",
      "Epoch:  62200 , step:  62200 , train loss:  0.04764186658059134 , test loss:  0.085094194133793\n",
      "Epoch:  62210 , step:  62210 , train loss:  0.0476388805430195 , test loss:  0.08509340290717521\n",
      "Epoch:  62220 , step:  62220 , train loss:  0.04763589520158188 , test loss:  0.08509261213135481\n",
      "Epoch:  62230 , step:  62230 , train loss:  0.047632910556005596 , test loss:  0.08509182180612093\n",
      "Epoch:  62240 , step:  62240 , train loss:  0.04762992660601792 , test loss:  0.08509103193126277\n",
      "Epoch:  62250 , step:  62250 , train loss:  0.04762694335134627 , test loss:  0.08509024250656956\n",
      "Epoch:  62260 , step:  62260 , train loss:  0.047623960791718184 , test loss:  0.08508945353183081\n",
      "Epoch:  62270 , step:  62270 , train loss:  0.047620978926861394 , test loss:  0.08508866500683614\n",
      "Epoch:  62280 , step:  62280 , train loss:  0.04761799775650377 , test loss:  0.0850878769313752\n",
      "Epoch:  62290 , step:  62290 , train loss:  0.04761501728037331 , test loss:  0.08508708930523794\n",
      "Epoch:  62300 , step:  62300 , train loss:  0.04761203749819817 , test loss:  0.08508630212821423\n",
      "Epoch:  62310 , step:  62310 , train loss:  0.04760905840970673 , test loss:  0.08508551540009421\n",
      "Epoch:  62320 , step:  62320 , train loss:  0.04760608001462739 , test loss:  0.08508472912066817\n",
      "Epoch:  62330 , step:  62330 , train loss:  0.047603102312688786 , test loss:  0.08508394328972638\n",
      "Epoch:  62340 , step:  62340 , train loss:  0.047600125303619636 , test loss:  0.08508315790705924\n",
      "Epoch:  62350 , step:  62350 , train loss:  0.04759714898714896 , test loss:  0.08508237297245752\n",
      "Epoch:  62360 , step:  62360 , train loss:  0.04759417336300574 , test loss:  0.08508158848571194\n",
      "Epoch:  62370 , step:  62370 , train loss:  0.047591198430919225 , test loss:  0.08508080444661324\n",
      "Epoch:  62380 , step:  62380 , train loss:  0.047588224190618755 , test loss:  0.08508002085495263\n",
      "Epoch:  62390 , step:  62390 , train loss:  0.04758525064183388 , test loss:  0.08507923771052113\n",
      "Epoch:  62400 , step:  62400 , train loss:  0.04758227778429421 , test loss:  0.08507845501310979\n",
      "Epoch:  62410 , step:  62410 , train loss:  0.047579305617729634 , test loss:  0.08507767276251026\n",
      "Epoch:  62420 , step:  62420 , train loss:  0.04757633414187004 , test loss:  0.08507689095851395\n",
      "Epoch:  62430 , step:  62430 , train loss:  0.047573363356445575 , test loss:  0.08507610960091234\n",
      "Epoch:  62440 , step:  62440 , train loss:  0.04757039326118647 , test loss:  0.08507532868949742\n",
      "Epoch:  62450 , step:  62450 , train loss:  0.04756742385582318 , test loss:  0.0850745482240609\n",
      "Epoch:  62460 , step:  62460 , train loss:  0.047564455140086205 , test loss:  0.08507376820439487\n",
      "Epoch:  62470 , step:  62470 , train loss:  0.047561487113706294 , test loss:  0.08507298863029149\n",
      "Epoch:  62480 , step:  62480 , train loss:  0.047558519776414274 , test loss:  0.08507220950154286\n",
      "Epoch:  62490 , step:  62490 , train loss:  0.04755555312794117 , test loss:  0.08507143081794148\n",
      "Epoch:  62500 , step:  62500 , train loss:  0.04755258716801809 , test loss:  0.08507065257927982\n",
      "Epoch:  62510 , step:  62510 , train loss:  0.0475496218963764 , test loss:  0.08506987478535058\n",
      "Epoch:  62520 , step:  62520 , train loss:  0.04754665731274746 , test loss:  0.08506909743594646\n",
      "Epoch:  62530 , step:  62530 , train loss:  0.04754369341686294 , test loss:  0.08506832053086037\n",
      "Epoch:  62540 , step:  62540 , train loss:  0.047540730208454535 , test loss:  0.08506754406988544\n",
      "Epoch:  62550 , step:  62550 , train loss:  0.04753776768725415 , test loss:  0.08506676805281449\n",
      "Epoch:  62560 , step:  62560 , train loss:  0.04753480585299383 , test loss:  0.08506599247944115\n",
      "Epoch:  62570 , step:  62570 , train loss:  0.047531844705405744 , test loss:  0.08506521734955858\n",
      "Epoch:  62580 , step:  62580 , train loss:  0.04752888424422223 , test loss:  0.08506444266296039\n",
      "Epoch:  62590 , step:  62590 , train loss:  0.04752592446917578 , test loss:  0.08506366841944024\n",
      "Epoch:  62600 , step:  62600 , train loss:  0.047522965379998984 , test loss:  0.08506289461879178\n",
      "Epoch:  62610 , step:  62610 , train loss:  0.04752000697642464 , test loss:  0.08506212126080905\n",
      "Epoch:  62620 , step:  62620 , train loss:  0.04751704925818568 , test loss:  0.08506134834528588\n",
      "Epoch:  62630 , step:  62630 , train loss:  0.04751409222501516 , test loss:  0.08506057587201665\n",
      "Epoch:  62640 , step:  62640 , train loss:  0.04751113587664625 , test loss:  0.0850598038407953\n",
      "Epoch:  62650 , step:  62650 , train loss:  0.047508180212812404 , test loss:  0.08505903225141652\n",
      "Epoch:  62660 , step:  62660 , train loss:  0.047505225233247045 , test loss:  0.08505826110367468\n",
      "Epoch:  62670 , step:  62670 , train loss:  0.04750227093768388 , test loss:  0.08505749039736445\n",
      "Epoch:  62680 , step:  62680 , train loss:  0.04749931732585663 , test loss:  0.08505672013228067\n",
      "Epoch:  62690 , step:  62690 , train loss:  0.04749636439749934 , test loss:  0.08505595030821805\n",
      "Epoch:  62700 , step:  62700 , train loss:  0.047493412152346044 , test loss:  0.08505518092497169\n",
      "Epoch:  62710 , step:  62710 , train loss:  0.04749046059013101 , test loss:  0.08505441198233682\n",
      "Epoch:  62720 , step:  62720 , train loss:  0.04748750971058859 , test loss:  0.08505364348010856\n",
      "Epoch:  62730 , step:  62730 , train loss:  0.047484559513453324 , test loss:  0.08505287541808233\n",
      "Epoch:  62740 , step:  62740 , train loss:  0.0474816099984599 , test loss:  0.08505210779605364\n",
      "Epoch:  62750 , step:  62750 , train loss:  0.04747866116534311 , test loss:  0.08505134061381808\n",
      "Epoch:  62760 , step:  62760 , train loss:  0.04747571301383796 , test loss:  0.08505057387117157\n",
      "Epoch:  62770 , step:  62770 , train loss:  0.047472765543679525 , test loss:  0.08504980756790968\n",
      "Epoch:  62780 , step:  62780 , train loss:  0.04746981875460311 , test loss:  0.08504904170382878\n",
      "Epoch:  62790 , step:  62790 , train loss:  0.04746687264634406 , test loss:  0.08504827627872467\n",
      "Epoch:  62800 , step:  62800 , train loss:  0.04746392721863796 , test loss:  0.08504751129239373\n",
      "Epoch:  62810 , step:  62810 , train loss:  0.04746098247122047 , test loss:  0.08504674674463247\n",
      "Epoch:  62820 , step:  62820 , train loss:  0.047458038403827466 , test loss:  0.085045982635237\n",
      "Epoch:  62830 , step:  62830 , train loss:  0.0474550950161949 , test loss:  0.0850452189640042\n",
      "Epoch:  62840 , step:  62840 , train loss:  0.047452152308058895 , test loss:  0.08504445573073105\n",
      "Epoch:  62850 , step:  62850 , train loss:  0.04744921027915575 , test loss:  0.08504369293521398\n",
      "Epoch:  62860 , step:  62860 , train loss:  0.04744626892922188 , test loss:  0.08504293057725015\n",
      "Epoch:  62870 , step:  62870 , train loss:  0.047443328257993814 , test loss:  0.08504216865663654\n",
      "Epoch:  62880 , step:  62880 , train loss:  0.04744038826520828 , test loss:  0.08504140717317063\n",
      "Epoch:  62890 , step:  62890 , train loss:  0.04743744895060214 , test loss:  0.08504064612664965\n",
      "Epoch:  62900 , step:  62900 , train loss:  0.04743451031391234 , test loss:  0.0850398855168708\n",
      "Epoch:  62910 , step:  62910 , train loss:  0.04743157235487603 , test loss:  0.085039125343632\n",
      "Epoch:  62920 , step:  62920 , train loss:  0.04742863507323053 , test loss:  0.085038365606731\n",
      "Epoch:  62930 , step:  62930 , train loss:  0.04742569846871319 , test loss:  0.08503760630596548\n",
      "Epoch:  62940 , step:  62940 , train loss:  0.047422762541061696 , test loss:  0.08503684744113342\n",
      "Epoch:  62950 , step:  62950 , train loss:  0.04741982729001363 , test loss:  0.08503608901203302\n",
      "Epoch:  62960 , step:  62960 , train loss:  0.047416892715306906 , test loss:  0.08503533101846243\n",
      "Epoch:  62970 , step:  62970 , train loss:  0.047413958816679536 , test loss:  0.08503457346021973\n",
      "Epoch:  62980 , step:  62980 , train loss:  0.047411025593869636 , test loss:  0.08503381633710366\n",
      "Epoch:  62990 , step:  62990 , train loss:  0.04740809304661548 , test loss:  0.08503305964891263\n",
      "Epoch:  63000 , step:  63000 , train loss:  0.04740516117465553 , test loss:  0.08503230339544555\n",
      "Epoch:  63010 , step:  63010 , train loss:  0.047402229977728326 , test loss:  0.08503154757650108\n",
      "Epoch:  63020 , step:  63020 , train loss:  0.047399299455572606 , test loss:  0.08503079219187806\n",
      "Epoch:  63030 , step:  63030 , train loss:  0.0473963696079272 , test loss:  0.08503003724137564\n",
      "Epoch:  63040 , step:  63040 , train loss:  0.04739344043453115 , test loss:  0.08502928272479307\n",
      "Epoch:  63050 , step:  63050 , train loss:  0.047390511935123524 , test loss:  0.08502852864192947\n",
      "Epoch:  63060 , step:  63060 , train loss:  0.047387584109443666 , test loss:  0.08502777499258435\n",
      "Epoch:  63070 , step:  63070 , train loss:  0.04738465695723099 , test loss:  0.08502702177655724\n",
      "Epoch:  63080 , step:  63080 , train loss:  0.04738173047822506 , test loss:  0.08502626899364789\n",
      "Epoch:  63090 , step:  63090 , train loss:  0.047378804672165556 , test loss:  0.0850255166436559\n",
      "Epoch:  63100 , step:  63100 , train loss:  0.04737587953879238 , test loss:  0.0850247647263813\n",
      "Epoch:  63110 , step:  63110 , train loss:  0.04737295507784548 , test loss:  0.08502401324162398\n",
      "Epoch:  63120 , step:  63120 , train loss:  0.04737003128906502 , test loss:  0.08502326218918414\n",
      "Epoch:  63130 , step:  63130 , train loss:  0.047367108172191294 , test loss:  0.08502251156886212\n",
      "Epoch:  63140 , step:  63140 , train loss:  0.047364185726964665 , test loss:  0.0850217613804582\n",
      "Epoch:  63150 , step:  63150 , train loss:  0.047361263953125746 , test loss:  0.08502101162377296\n",
      "Epoch:  63160 , step:  63160 , train loss:  0.04735834285041522 , test loss:  0.08502026229860683\n",
      "Epoch:  63170 , step:  63170 , train loss:  0.047355422418573934 , test loss:  0.08501951340476066\n",
      "Epoch:  63180 , step:  63180 , train loss:  0.04735250265734284 , test loss:  0.08501876494203547\n",
      "Epoch:  63190 , step:  63190 , train loss:  0.04734958356646311 , test loss:  0.08501801691023203\n",
      "Epoch:  63200 , step:  63200 , train loss:  0.047346665145675995 , test loss:  0.08501726930915143\n",
      "Epoch:  63210 , step:  63210 , train loss:  0.04734374739472289 , test loss:  0.0850165221385951\n",
      "Epoch:  63220 , step:  63220 , train loss:  0.047340830313345364 , test loss:  0.08501577539836429\n",
      "Epoch:  63230 , step:  63230 , train loss:  0.047337913901285084 , test loss:  0.0850150290882604\n",
      "Epoch:  63240 , step:  63240 , train loss:  0.0473349981582839 , test loss:  0.0850142832080848\n",
      "Epoch:  63250 , step:  63250 , train loss:  0.04733208308408376 , test loss:  0.08501353775763944\n",
      "Epoch:  63260 , step:  63260 , train loss:  0.047329168678426806 , test loss:  0.08501279273672607\n",
      "Epoch:  63270 , step:  63270 , train loss:  0.04732625494105526 , test loss:  0.08501204814514655\n",
      "Epoch:  63280 , step:  63280 , train loss:  0.04732334187171153 , test loss:  0.08501130398270315\n",
      "Epoch:  63290 , step:  63290 , train loss:  0.047320429470138134 , test loss:  0.08501056024919776\n",
      "Epoch:  63300 , step:  63300 , train loss:  0.04731751773607776 , test loss:  0.08500981694443283\n",
      "Epoch:  63310 , step:  63310 , train loss:  0.04731460666927321 , test loss:  0.08500907406821072\n",
      "Epoch:  63320 , step:  63320 , train loss:  0.04731169626946743 , test loss:  0.08500833162033392\n",
      "Epoch:  63330 , step:  63330 , train loss:  0.04730878653640351 , test loss:  0.08500758960060521\n",
      "Epoch:  63340 , step:  63340 , train loss:  0.04730587746982469 , test loss:  0.08500684800882713\n",
      "Epoch:  63350 , step:  63350 , train loss:  0.04730296906947435 , test loss:  0.08500610684480274\n",
      "Epoch:  63360 , step:  63360 , train loss:  0.04730006133509597 , test loss:  0.08500536610833492\n",
      "Epoch:  63370 , step:  63370 , train loss:  0.04729715426643323 , test loss:  0.08500462579922693\n",
      "Epoch:  63380 , step:  63380 , train loss:  0.04729424786322991 , test loss:  0.08500388591728186\n",
      "Epoch:  63390 , step:  63390 , train loss:  0.0472913421252299 , test loss:  0.08500314646230289\n",
      "Epoch:  63400 , step:  63400 , train loss:  0.04728843705217732 , test loss:  0.08500240743409389\n",
      "Epoch:  63410 , step:  63410 , train loss:  0.047285532643816346 , test loss:  0.08500166883245819\n",
      "Epoch:  63420 , step:  63420 , train loss:  0.04728262889989133 , test loss:  0.08500093065719967\n",
      "Epoch:  63430 , step:  63430 , train loss:  0.04727972582014675 , test loss:  0.08500019290812183\n",
      "Epoch:  63440 , step:  63440 , train loss:  0.04727682340432722 , test loss:  0.0849994555850289\n",
      "Epoch:  63450 , step:  63450 , train loss:  0.04727392165217755 , test loss:  0.08499871868772481\n",
      "Epoch:  63460 , step:  63460 , train loss:  0.04727102056344258 , test loss:  0.08499798221601375\n",
      "Epoch:  63470 , step:  63470 , train loss:  0.04726812013786735 , test loss:  0.0849972461697001\n",
      "Epoch:  63480 , step:  63480 , train loss:  0.04726522037519707 , test loss:  0.08499651054858813\n",
      "Epoch:  63490 , step:  63490 , train loss:  0.04726232127517704 , test loss:  0.08499577535248237\n",
      "Epoch:  63500 , step:  63500 , train loss:  0.047259422837552696 , test loss:  0.0849950405811876\n",
      "Epoch:  63510 , step:  63510 , train loss:  0.047256525062069656 , test loss:  0.08499430623450846\n",
      "Epoch:  63520 , step:  63520 , train loss:  0.047253627948473605 , test loss:  0.08499357231224978\n",
      "Epoch:  63530 , step:  63530 , train loss:  0.04725073149651043 , test loss:  0.08499283881421668\n",
      "Epoch:  63540 , step:  63540 , train loss:  0.047247835705926174 , test loss:  0.08499210574021407\n",
      "Epoch:  63550 , step:  63550 , train loss:  0.04724494057646691 , test loss:  0.08499137309004728\n",
      "Epoch:  63560 , step:  63560 , train loss:  0.04724204610787894 , test loss:  0.08499064086352175\n",
      "Epoch:  63570 , step:  63570 , train loss:  0.04723915229990871 , test loss:  0.08498990906044285\n",
      "Epoch:  63580 , step:  63580 , train loss:  0.047236259152302736 , test loss:  0.08498917768061602\n",
      "Epoch:  63590 , step:  63590 , train loss:  0.047233366664807715 , test loss:  0.08498844672384731\n",
      "Epoch:  63600 , step:  63600 , train loss:  0.04723047483717049 , test loss:  0.08498771618994212\n",
      "Epoch:  63610 , step:  63610 , train loss:  0.047227583669138026 , test loss:  0.08498698607870649\n",
      "Epoch:  63620 , step:  63620 , train loss:  0.04722469316045739 , test loss:  0.08498625638994661\n",
      "Epoch:  63630 , step:  63630 , train loss:  0.04722180331087583 , test loss:  0.08498552712346841\n",
      "Epoch:  63640 , step:  63640 , train loss:  0.04721891412014077 , test loss:  0.08498479827907822\n",
      "Epoch:  63650 , step:  63650 , train loss:  0.04721602558799966 , test loss:  0.08498406985658255\n",
      "Epoch:  63660 , step:  63660 , train loss:  0.04721313771420015 , test loss:  0.08498334185578774\n",
      "Epoch:  63670 , step:  63670 , train loss:  0.047210250498490085 , test loss:  0.08498261427650051\n",
      "Epoch:  63680 , step:  63680 , train loss:  0.04720736394061733 , test loss:  0.08498188711852749\n",
      "Epoch:  63690 , step:  63690 , train loss:  0.047204478040329956 , test loss:  0.0849811603816756\n",
      "Epoch:  63700 , step:  63700 , train loss:  0.047201592797376145 , test loss:  0.0849804340657517\n",
      "Epoch:  63710 , step:  63710 , train loss:  0.04719870821150426 , test loss:  0.08497970817056297\n",
      "Epoch:  63720 , step:  63720 , train loss:  0.047195824282462745 , test loss:  0.0849789826959167\n",
      "Epoch:  63730 , step:  63730 , train loss:  0.04719294101000016 , test loss:  0.08497825764161977\n",
      "Epoch:  63740 , step:  63740 , train loss:  0.04719005839386533 , test loss:  0.08497753300747996\n",
      "Epoch:  63750 , step:  63750 , train loss:  0.04718717643380704 , test loss:  0.08497680879330467\n",
      "Epoch:  63760 , step:  63760 , train loss:  0.047184295129574345 , test loss:  0.0849760849989016\n",
      "Epoch:  63770 , step:  63770 , train loss:  0.04718141448091639 , test loss:  0.08497536162407847\n",
      "Epoch:  63780 , step:  63780 , train loss:  0.04717853448758242 , test loss:  0.08497463866864315\n",
      "Epoch:  63790 , step:  63790 , train loss:  0.04717565514932189 , test loss:  0.0849739161324038\n",
      "Epoch:  63800 , step:  63800 , train loss:  0.04717277646588433 , test loss:  0.08497319401516827\n",
      "Epoch:  63810 , step:  63810 , train loss:  0.04716989843701943 , test loss:  0.08497247231674483\n",
      "Epoch:  63820 , step:  63820 , train loss:  0.04716702106247697 , test loss:  0.08497175103694193\n",
      "Epoch:  63830 , step:  63830 , train loss:  0.04716414434200698 , test loss:  0.08497103017556804\n",
      "Epoch:  63840 , step:  63840 , train loss:  0.047161268275359484 , test loss:  0.08497030973243151\n",
      "Epoch:  63850 , step:  63850 , train loss:  0.04715839286228474 , test loss:  0.08496958970734111\n",
      "Epoch:  63860 , step:  63860 , train loss:  0.0471555181025331 , test loss:  0.08496887010010569\n",
      "Epoch:  63870 , step:  63870 , train loss:  0.047152643995855045 , test loss:  0.08496815091053413\n",
      "Epoch:  63880 , step:  63880 , train loss:  0.04714977054200121 , test loss:  0.08496743213843527\n",
      "Epoch:  63890 , step:  63890 , train loss:  0.047146897740722345 , test loss:  0.08496671378361857\n",
      "Epoch:  63900 , step:  63900 , train loss:  0.04714402559176937 , test loss:  0.08496599584589289\n",
      "Epoch:  63910 , step:  63910 , train loss:  0.04714115409489329 , test loss:  0.08496527832506783\n",
      "Epoch:  63920 , step:  63920 , train loss:  0.04713828324984529 , test loss:  0.08496456122095265\n",
      "Epoch:  63930 , step:  63930 , train loss:  0.047135413056376675 , test loss:  0.08496384453335722\n",
      "Epoch:  63940 , step:  63940 , train loss:  0.04713254351423885 , test loss:  0.08496312826209114\n",
      "Epoch:  63950 , step:  63950 , train loss:  0.04712967462318338 , test loss:  0.08496241240696407\n",
      "Epoch:  63960 , step:  63960 , train loss:  0.04712680638296199 , test loss:  0.08496169696778608\n",
      "Epoch:  63970 , step:  63970 , train loss:  0.04712393879332649 , test loss:  0.08496098194436708\n",
      "Epoch:  63980 , step:  63980 , train loss:  0.04712107185402888 , test loss:  0.08496026733651735\n",
      "Epoch:  63990 , step:  63990 , train loss:  0.047118205564821235 , test loss:  0.08495955314404705\n",
      "Epoch:  64000 , step:  64000 , train loss:  0.04711533992545578 , test loss:  0.08495883936676656\n",
      "Epoch:  64010 , step:  64010 , train loss:  0.04711247493568488 , test loss:  0.08495812600448621\n",
      "Epoch:  64020 , step:  64020 , train loss:  0.04710961059526108 , test loss:  0.08495741305701697\n",
      "Epoch:  64030 , step:  64030 , train loss:  0.04710674690393696 , test loss:  0.08495670052416927\n",
      "Epoch:  64040 , step:  64040 , train loss:  0.04710388386146531 , test loss:  0.08495598840575394\n",
      "Epoch:  64050 , step:  64050 , train loss:  0.04710102146759901 , test loss:  0.08495527670158187\n",
      "Epoch:  64060 , step:  64060 , train loss:  0.04709815972209113 , test loss:  0.08495456541146437\n",
      "Epoch:  64070 , step:  64070 , train loss:  0.047095298624694824 , test loss:  0.08495385453521236\n",
      "Epoch:  64080 , step:  64080 , train loss:  0.047092438175163336 , test loss:  0.084953144072637\n",
      "Epoch:  64090 , step:  64090 , train loss:  0.04708957837325017 , test loss:  0.08495243402354988\n",
      "Epoch:  64100 , step:  64100 , train loss:  0.047086719218708835 , test loss:  0.08495172438776252\n",
      "Epoch:  64110 , step:  64110 , train loss:  0.04708386071129306 , test loss:  0.0849510151650864\n",
      "Epoch:  64120 , step:  64120 , train loss:  0.04708100285075665 , test loss:  0.0849503063553332\n",
      "Epoch:  64130 , step:  64130 , train loss:  0.04707814563685355 , test loss:  0.08494959795831498\n",
      "Epoch:  64140 , step:  64140 , train loss:  0.0470752890693379 , test loss:  0.08494888997384333\n",
      "Epoch:  64150 , step:  64150 , train loss:  0.047072433147963874 , test loss:  0.08494818240173044\n",
      "Epoch:  64160 , step:  64160 , train loss:  0.047069577872485856 , test loss:  0.08494747524178865\n",
      "Epoch:  64170 , step:  64170 , train loss:  0.047066723242658294 , test loss:  0.08494676849383007\n",
      "Epoch:  64180 , step:  64180 , train loss:  0.04706386925823583 , test loss:  0.08494606215766684\n",
      "Epoch:  64190 , step:  64190 , train loss:  0.047061015918973235 , test loss:  0.0849453562331118\n",
      "Epoch:  64200 , step:  64200 , train loss:  0.04705816322462536 , test loss:  0.08494465071997755\n",
      "Epoch:  64210 , step:  64210 , train loss:  0.04705531117494724 , test loss:  0.08494394561807665\n",
      "Epoch:  64220 , step:  64220 , train loss:  0.047052459769694 , test loss:  0.08494324092722201\n",
      "Epoch:  64230 , step:  64230 , train loss:  0.047049609008620916 , test loss:  0.08494253664722651\n",
      "Epoch:  64240 , step:  64240 , train loss:  0.047046758891483395 , test loss:  0.08494183277790313\n",
      "Epoch:  64250 , step:  64250 , train loss:  0.04704390941803699 , test loss:  0.08494112931906503\n",
      "Epoch:  64260 , step:  64260 , train loss:  0.04704106058803733 , test loss:  0.08494042627052564\n",
      "Epoch:  64270 , step:  64270 , train loss:  0.04703821240124024 , test loss:  0.08493972363209822\n",
      "Epoch:  64280 , step:  64280 , train loss:  0.04703536485740167 , test loss:  0.08493902140359633\n",
      "Epoch:  64290 , step:  64290 , train loss:  0.047032517956277636 , test loss:  0.08493831958483343\n",
      "Epoch:  64300 , step:  64300 , train loss:  0.04702967169762439 , test loss:  0.08493761817562341\n",
      "Epoch:  64310 , step:  64310 , train loss:  0.04702682608119818 , test loss:  0.08493691717577997\n",
      "Epoch:  64320 , step:  64320 , train loss:  0.04702398110675549 , test loss:  0.0849362165851169\n",
      "Epoch:  64330 , step:  64330 , train loss:  0.0470211367740529 , test loss:  0.08493551640344853\n",
      "Epoch:  64340 , step:  64340 , train loss:  0.04701829308284714 , test loss:  0.08493481663058869\n",
      "Epoch:  64350 , step:  64350 , train loss:  0.04701545003289505 , test loss:  0.08493411726635197\n",
      "Epoch:  64360 , step:  64360 , train loss:  0.047012607623953574 , test loss:  0.08493341831055248\n",
      "Epoch:  64370 , step:  64370 , train loss:  0.04700976585577982 , test loss:  0.08493271976300483\n",
      "Epoch:  64380 , step:  64380 , train loss:  0.04700692472813105 , test loss:  0.08493202162352342\n",
      "Epoch:  64390 , step:  64390 , train loss:  0.04700408424076461 , test loss:  0.08493132389192315\n",
      "Epoch:  64400 , step:  64400 , train loss:  0.04700124439343797 , test loss:  0.08493062656801888\n",
      "Epoch:  64410 , step:  64410 , train loss:  0.046998405185908775 , test loss:  0.08492992965162524\n",
      "Epoch:  64420 , step:  64420 , train loss:  0.04699556661793476 , test loss:  0.08492923314255744\n",
      "Epoch:  64430 , step:  64430 , train loss:  0.04699272868927383 , test loss:  0.08492853704063044\n",
      "Epoch:  64440 , step:  64440 , train loss:  0.046989891399684 , test loss:  0.08492784134565969\n",
      "Epoch:  64450 , step:  64450 , train loss:  0.046987054748923354 , test loss:  0.08492714605746039\n",
      "Epoch:  64460 , step:  64460 , train loss:  0.0469842187367502 , test loss:  0.08492645117584821\n",
      "Epoch:  64470 , step:  64470 , train loss:  0.046981383362922936 , test loss:  0.08492575670063832\n",
      "Epoch:  64480 , step:  64480 , train loss:  0.046978548627200065 , test loss:  0.08492506263164677\n",
      "Epoch:  64490 , step:  64490 , train loss:  0.04697571452934027 , test loss:  0.08492436896868917\n",
      "Epoch:  64500 , step:  64500 , train loss:  0.046972881069102315 , test loss:  0.08492367571158144\n",
      "Epoch:  64510 , step:  64510 , train loss:  0.04697004824624513 , test loss:  0.08492298286013956\n",
      "Epoch:  64520 , step:  64520 , train loss:  0.04696721606052773 , test loss:  0.08492229041417969\n",
      "Epoch:  64530 , step:  64530 , train loss:  0.046964384511709284 , test loss:  0.08492159837351794\n",
      "Epoch:  64540 , step:  64540 , train loss:  0.04696155359954914 , test loss:  0.08492090673797058\n",
      "Epoch:  64550 , step:  64550 , train loss:  0.04695872332380668 , test loss:  0.08492021550735433\n",
      "Epoch:  64560 , step:  64560 , train loss:  0.04695589368424146 , test loss:  0.08491952468148536\n",
      "Epoch:  64570 , step:  64570 , train loss:  0.046953064680613145 , test loss:  0.08491883426018061\n",
      "Epoch:  64580 , step:  64580 , train loss:  0.0469502363126816 , test loss:  0.08491814424325665\n",
      "Epoch:  64590 , step:  64590 , train loss:  0.046947408580206726 , test loss:  0.08491745463053058\n",
      "Epoch:  64600 , step:  64600 , train loss:  0.04694458148294858 , test loss:  0.08491676542181914\n",
      "Epoch:  64610 , step:  64610 , train loss:  0.04694175502066737 , test loss:  0.08491607661693948\n",
      "Epoch:  64620 , step:  64620 , train loss:  0.046938929193123445 , test loss:  0.0849153882157087\n",
      "Epoch:  64630 , step:  64630 , train loss:  0.04693610400007721 , test loss:  0.08491470021794421\n",
      "Epoch:  64640 , step:  64640 , train loss:  0.04693327944128928 , test loss:  0.08491401262346354\n",
      "Epoch:  64650 , step:  64650 , train loss:  0.046930455516520315 , test loss:  0.0849133254320838\n",
      "Epoch:  64660 , step:  64660 , train loss:  0.04692763222553118 , test loss:  0.08491263864362282\n",
      "Epoch:  64670 , step:  64670 , train loss:  0.04692480956808285 , test loss:  0.08491195225789844\n",
      "Epoch:  64680 , step:  64680 , train loss:  0.04692198754393639 , test loss:  0.0849112662747283\n",
      "Epoch:  64690 , step:  64690 , train loss:  0.04691916615285296 , test loss:  0.08491058069393045\n",
      "Epoch:  64700 , step:  64700 , train loss:  0.046916345394594 , test loss:  0.08490989551532284\n",
      "Epoch:  64710 , step:  64710 , train loss:  0.046913525268920904 , test loss:  0.08490921073872364\n",
      "Epoch:  64720 , step:  64720 , train loss:  0.04691070577559533 , test loss:  0.08490852636395115\n",
      "Epoch:  64730 , step:  64730 , train loss:  0.04690788691437893 , test loss:  0.08490784239082369\n",
      "Epoch:  64740 , step:  64740 , train loss:  0.0469050686850336 , test loss:  0.0849071588191597\n",
      "Epoch:  64750 , step:  64750 , train loss:  0.046902251087321274 , test loss:  0.08490647564877787\n",
      "Epoch:  64760 , step:  64760 , train loss:  0.04689943412100409 , test loss:  0.08490579287949673\n",
      "Epoch:  64770 , step:  64770 , train loss:  0.04689661778584425 , test loss:  0.08490511051113517\n",
      "Epoch:  64780 , step:  64780 , train loss:  0.04689380208160413 , test loss:  0.08490442854351214\n",
      "Epoch:  64790 , step:  64790 , train loss:  0.046890987008046205 , test loss:  0.08490374697644655\n",
      "Epoch:  64800 , step:  64800 , train loss:  0.046888172564933064 , test loss:  0.08490306580975748\n",
      "Epoch:  64810 , step:  64810 , train loss:  0.04688535875202747 , test loss:  0.08490238504326407\n",
      "Epoch:  64820 , step:  64820 , train loss:  0.046882545569092236 , test loss:  0.08490170467678579\n",
      "Epoch:  64830 , step:  64830 , train loss:  0.04687973301589037 , test loss:  0.08490102471014212\n",
      "Epoch:  64840 , step:  64840 , train loss:  0.046876921092185005 , test loss:  0.08490034514315227\n",
      "Epoch:  64850 , step:  64850 , train loss:  0.04687410979773936 , test loss:  0.0848996659756362\n",
      "Epoch:  64860 , step:  64860 , train loss:  0.04687129913231675 , test loss:  0.08489898720741348\n",
      "Epoch:  64870 , step:  64870 , train loss:  0.04686848909568072 , test loss:  0.08489830883830402\n",
      "Epoch:  64880 , step:  64880 , train loss:  0.04686567968759487 , test loss:  0.08489763086812771\n",
      "Epoch:  64890 , step:  64890 , train loss:  0.04686287090782292 , test loss:  0.08489695329670474\n",
      "Epoch:  64900 , step:  64900 , train loss:  0.046860062756128744 , test loss:  0.08489627612385515\n",
      "Epoch:  64910 , step:  64910 , train loss:  0.04685725523227634 , test loss:  0.08489559934939922\n",
      "Epoch:  64920 , step:  64920 , train loss:  0.04685444833602981 , test loss:  0.08489492297315734\n",
      "Epoch:  64930 , step:  64930 , train loss:  0.046851642067153386 , test loss:  0.08489424699495007\n",
      "Epoch:  64940 , step:  64940 , train loss:  0.046848836425411426 , test loss:  0.08489357141459775\n",
      "Epoch:  64950 , step:  64950 , train loss:  0.04684603141056844 , test loss:  0.08489289623192127\n",
      "Epoch:  64960 , step:  64960 , train loss:  0.04684322702238902 , test loss:  0.0848922214467413\n",
      "Epoch:  64970 , step:  64970 , train loss:  0.04684042326063793 , test loss:  0.08489154705887893\n",
      "Epoch:  64980 , step:  64980 , train loss:  0.04683762012508 , test loss:  0.08489087306815489\n",
      "Epoch:  64990 , step:  64990 , train loss:  0.04683481761548024 , test loss:  0.08489019947439051\n",
      "Epoch:  65000 , step:  65000 , train loss:  0.04683201573160375 , test loss:  0.08488952627740681\n",
      "Epoch:  65010 , step:  65010 , train loss:  0.046829214473215766 , test loss:  0.08488885347702542\n",
      "Epoch:  65020 , step:  65020 , train loss:  0.046826413840081654 , test loss:  0.08488818107306743\n",
      "Epoch:  65030 , step:  65030 , train loss:  0.046823613831966884 , test loss:  0.08488750906535455\n",
      "Epoch:  65040 , step:  65040 , train loss:  0.04682081444863708 , test loss:  0.0848868374537082\n",
      "Epoch:  65050 , step:  65050 , train loss:  0.04681801568985797 , test loss:  0.08488616623795041\n",
      "Epoch:  65060 , step:  65060 , train loss:  0.04681521755539538 , test loss:  0.0848854954179027\n",
      "Epoch:  65070 , step:  65070 , train loss:  0.04681242004501533 , test loss:  0.08488482499338722\n",
      "Epoch:  65080 , step:  65080 , train loss:  0.04680962315848391 , test loss:  0.08488415496422595\n",
      "Epoch:  65090 , step:  65090 , train loss:  0.04680682689556735 , test loss:  0.08488348533024097\n",
      "Epoch:  65100 , step:  65100 , train loss:  0.04680403125603198 , test loss:  0.08488281609125471\n",
      "Epoch:  65110 , step:  65110 , train loss:  0.0468012362396443 , test loss:  0.08488214724708933\n",
      "Epoch:  65120 , step:  65120 , train loss:  0.04679844184617091 , test loss:  0.08488147879756738\n",
      "Epoch:  65130 , step:  65130 , train loss:  0.046795648075378476 , test loss:  0.0848808107425114\n",
      "Epoch:  65140 , step:  65140 , train loss:  0.04679285492703391 , test loss:  0.08488014308174406\n",
      "Epoch:  65150 , step:  65150 , train loss:  0.04679006240090414 , test loss:  0.08487947581508817\n",
      "Epoch:  65160 , step:  65160 , train loss:  0.04678727049675629 , test loss:  0.08487880894236652\n",
      "Epoch:  65170 , step:  65170 , train loss:  0.04678447921435753 , test loss:  0.08487814246340211\n",
      "Epoch:  65180 , step:  65180 , train loss:  0.04678168855347522 , test loss:  0.08487747637801805\n",
      "Epoch:  65190 , step:  65190 , train loss:  0.04677889851387681 , test loss:  0.08487681068603757\n",
      "Epoch:  65200 , step:  65200 , train loss:  0.0467761090953299 , test loss:  0.08487614538728379\n",
      "Epoch:  65210 , step:  65210 , train loss:  0.04677332029760218 , test loss:  0.08487548048158018\n",
      "Epoch:  65220 , step:  65220 , train loss:  0.04677053212046146 , test loss:  0.08487481596875024\n",
      "Epoch:  65230 , step:  65230 , train loss:  0.04676774456367572 , test loss:  0.08487415184861757\n",
      "Epoch:  65240 , step:  65240 , train loss:  0.046764957627013014 , test loss:  0.08487348812100594\n",
      "Epoch:  65250 , step:  65250 , train loss:  0.04676217131024155 , test loss:  0.08487282478573897\n",
      "Epoch:  65260 , step:  65260 , train loss:  0.04675938561312962 , test loss:  0.08487216184264072\n",
      "Epoch:  65270 , step:  65270 , train loss:  0.046756600535445704 , test loss:  0.08487149929153508\n",
      "Epoch:  65280 , step:  65280 , train loss:  0.04675381607695831 , test loss:  0.08487083713224616\n",
      "Epoch:  65290 , step:  65290 , train loss:  0.04675103223743615 , test loss:  0.08487017536459816\n",
      "Epoch:  65300 , step:  65300 , train loss:  0.04674824901664805 , test loss:  0.08486951398841548\n",
      "Epoch:  65310 , step:  65310 , train loss:  0.0467454664143629 , test loss:  0.08486885300352257\n",
      "Epoch:  65320 , step:  65320 , train loss:  0.04674268443034973 , test loss:  0.08486819240974383\n",
      "Epoch:  65330 , step:  65330 , train loss:  0.046739903064377766 , test loss:  0.08486753220690377\n",
      "Epoch:  65340 , step:  65340 , train loss:  0.046737122316216254 , test loss:  0.08486687239482732\n",
      "Epoch:  65350 , step:  65350 , train loss:  0.046734342185634645 , test loss:  0.08486621297333917\n",
      "Epoch:  65360 , step:  65360 , train loss:  0.046731562672402444 , test loss:  0.08486555394226436\n",
      "Epoch:  65370 , step:  65370 , train loss:  0.0467287837762893 , test loss:  0.08486489530142774\n",
      "Epoch:  65380 , step:  65380 , train loss:  0.04672600549706503 , test loss:  0.08486423705065466\n",
      "Epoch:  65390 , step:  65390 , train loss:  0.046723227834499476 , test loss:  0.08486357918977004\n",
      "Epoch:  65400 , step:  65400 , train loss:  0.0467204507883627 , test loss:  0.08486292171859952\n",
      "Epoch:  65410 , step:  65410 , train loss:  0.046717674358424825 , test loss:  0.08486226463696829\n",
      "Epoch:  65420 , step:  65420 , train loss:  0.04671489854445612 , test loss:  0.084861607944702\n",
      "Epoch:  65430 , step:  65430 , train loss:  0.046712123346226944 , test loss:  0.08486095164162627\n",
      "Epoch:  65440 , step:  65440 , train loss:  0.04670934876350786 , test loss:  0.08486029572756688\n",
      "Epoch:  65450 , step:  65450 , train loss:  0.046706574796069406 , test loss:  0.08485964020234957\n",
      "Epoch:  65460 , step:  65460 , train loss:  0.04670380144368237 , test loss:  0.08485898506580032\n",
      "Epoch:  65470 , step:  65470 , train loss:  0.04670102870611763 , test loss:  0.08485833031774508\n",
      "Epoch:  65480 , step:  65480 , train loss:  0.04669825658314612 , test loss:  0.08485767595801004\n",
      "Epoch:  65490 , step:  65490 , train loss:  0.046695485074539016 , test loss:  0.08485702198642146\n",
      "Epoch:  65500 , step:  65500 , train loss:  0.04669271418006748 , test loss:  0.08485636840280583\n",
      "Epoch:  65510 , step:  65510 , train loss:  0.046689943899502886 , test loss:  0.08485571520698941\n",
      "Epoch:  65520 , step:  65520 , train loss:  0.04668717423261671 , test loss:  0.08485506239879864\n",
      "Epoch:  65530 , step:  65530 , train loss:  0.046684405179180495 , test loss:  0.0848544099780603\n",
      "Epoch:  65540 , step:  65540 , train loss:  0.04668163673896599 , test loss:  0.08485375794460112\n",
      "Epoch:  65550 , step:  65550 , train loss:  0.04667886891174503 , test loss:  0.08485310629824795\n",
      "Epoch:  65560 , step:  65560 , train loss:  0.046676101697289495 , test loss:  0.08485245503882764\n",
      "Epoch:  65570 , step:  65570 , train loss:  0.04667333509537149 , test loss:  0.08485180416616744\n",
      "Epoch:  65580 , step:  65580 , train loss:  0.04667056910576321 , test loss:  0.08485115368009422\n",
      "Epoch:  65590 , step:  65590 , train loss:  0.04666780372823694 , test loss:  0.08485050358043537\n",
      "Epoch:  65600 , step:  65600 , train loss:  0.046665038962565096 , test loss:  0.08484985386701842\n",
      "Epoch:  65610 , step:  65610 , train loss:  0.04666227480852024 , test loss:  0.08484920453967032\n",
      "Epoch:  65620 , step:  65620 , train loss:  0.046659511265875024 , test loss:  0.08484855559821898\n",
      "Epoch:  65630 , step:  65630 , train loss:  0.04665674833440222 , test loss:  0.08484790704249189\n",
      "Epoch:  65640 , step:  65640 , train loss:  0.04665398601387477 , test loss:  0.08484725887231696\n",
      "Epoch:  65650 , step:  65650 , train loss:  0.04665122430406563 , test loss:  0.08484661108752195\n",
      "Epoch:  65660 , step:  65660 , train loss:  0.046648463204748 , test loss:  0.08484596368793473\n",
      "Epoch:  65670 , step:  65670 , train loss:  0.04664570271569507 , test loss:  0.08484531667338337\n",
      "Epoch:  65680 , step:  65680 , train loss:  0.04664294283668027 , test loss:  0.084844670043696\n",
      "Epoch:  65690 , step:  65690 , train loss:  0.046640183567477074 , test loss:  0.08484402379870093\n",
      "Epoch:  65700 , step:  65700 , train loss:  0.04663742490785909 , test loss:  0.08484337793822642\n",
      "Epoch:  65710 , step:  65710 , train loss:  0.046634666857600086 , test loss:  0.08484273246210088\n",
      "Epoch:  65720 , step:  65720 , train loss:  0.04663190941647385 , test loss:  0.08484208737015296\n",
      "Epoch:  65730 , step:  65730 , train loss:  0.046629152584254374 , test loss:  0.08484144266221125\n",
      "Epoch:  65740 , step:  65740 , train loss:  0.04662639636071576 , test loss:  0.0848407983381043\n",
      "Epoch:  65750 , step:  65750 , train loss:  0.046623640745632224 , test loss:  0.08484015439766132\n",
      "Epoch:  65760 , step:  65760 , train loss:  0.04662088573877807 , test loss:  0.08483951084071092\n",
      "Epoch:  65770 , step:  65770 , train loss:  0.046618131339927704 , test loss:  0.0848388676670823\n",
      "Epoch:  65780 , step:  65780 , train loss:  0.04661537754885573 , test loss:  0.08483822487660445\n",
      "Epoch:  65790 , step:  65790 , train loss:  0.04661262436533684 , test loss:  0.08483758246910683\n",
      "Epoch:  65800 , step:  65800 , train loss:  0.0466098717891458 , test loss:  0.08483694044441846\n",
      "Epoch:  65810 , step:  65810 , train loss:  0.04660711982005752 , test loss:  0.08483629880236895\n",
      "Epoch:  65820 , step:  65820 , train loss:  0.04660436845784703 , test loss:  0.08483565754278782\n",
      "Epoch:  65830 , step:  65830 , train loss:  0.0466016177022895 , test loss:  0.08483501666550468\n",
      "Epoch:  65840 , step:  65840 , train loss:  0.04659886755316017 , test loss:  0.08483437617034909\n",
      "Epoch:  65850 , step:  65850 , train loss:  0.04659611801023443 , test loss:  0.08483373605715101\n",
      "Epoch:  65860 , step:  65860 , train loss:  0.04659336907328778 , test loss:  0.08483309632574049\n",
      "Epoch:  65870 , step:  65870 , train loss:  0.04659062074209585 , test loss:  0.08483245697594739\n",
      "Epoch:  65880 , step:  65880 , train loss:  0.04658787301643434 , test loss:  0.08483181800760178\n",
      "Epoch:  65890 , step:  65890 , train loss:  0.04658512589607915 , test loss:  0.08483117942053393\n",
      "Epoch:  65900 , step:  65900 , train loss:  0.046582379380806234 , test loss:  0.08483054121457426\n",
      "Epoch:  65910 , step:  65910 , train loss:  0.04657963347039163 , test loss:  0.0848299033895529\n",
      "Epoch:  65920 , step:  65920 , train loss:  0.04657688816461159 , test loss:  0.08482926594530069\n",
      "Epoch:  65930 , step:  65930 , train loss:  0.04657414346324244 , test loss:  0.08482862888164788\n",
      "Epoch:  65940 , step:  65940 , train loss:  0.04657139936606058 , test loss:  0.08482799219842543\n",
      "Epoch:  65950 , step:  65950 , train loss:  0.04656865587284259 , test loss:  0.08482735589546407\n",
      "Epoch:  65960 , step:  65960 , train loss:  0.04656591298336512 , test loss:  0.08482671997259464\n",
      "Epoch:  65970 , step:  65970 , train loss:  0.04656317069740496 , test loss:  0.0848260844296483\n",
      "Epoch:  65980 , step:  65980 , train loss:  0.04656042901473904 , test loss:  0.0848254492664558\n",
      "Epoch:  65990 , step:  65990 , train loss:  0.04655768793514432 , test loss:  0.08482481448284857\n",
      "Epoch:  66000 , step:  66000 , train loss:  0.04655494745839801 , test loss:  0.08482418007865787\n",
      "Epoch:  66010 , step:  66010 , train loss:  0.046552207584277305 , test loss:  0.08482354605371498\n",
      "Epoch:  66020 , step:  66020 , train loss:  0.04654946831255956 , test loss:  0.0848229124078516\n",
      "Epoch:  66030 , step:  66030 , train loss:  0.046546729643022325 , test loss:  0.08482227914089915\n",
      "Epoch:  66040 , step:  66040 , train loss:  0.04654399157544313 , test loss:  0.08482164625268933\n",
      "Epoch:  66050 , step:  66050 , train loss:  0.04654125410959974 , test loss:  0.08482101374305383\n",
      "Epoch:  66060 , step:  66060 , train loss:  0.04653851724526995 , test loss:  0.08482038161182455\n",
      "Epoch:  66070 , step:  66070 , train loss:  0.046535780982231705 , test loss:  0.0848197498588334\n",
      "Epoch:  66080 , step:  66080 , train loss:  0.046533045320263096 , test loss:  0.0848191184839126\n",
      "Epoch:  66090 , step:  66090 , train loss:  0.046530310259142296 , test loss:  0.0848184874868943\n",
      "Epoch:  66100 , step:  66100 , train loss:  0.04652757579864759 , test loss:  0.08481785686761054\n",
      "Epoch:  66110 , step:  66110 , train loss:  0.04652484193855737 , test loss:  0.0848172266258938\n",
      "Epoch:  66120 , step:  66120 , train loss:  0.04652210867865015 , test loss:  0.08481659676157652\n",
      "Epoch:  66130 , step:  66130 , train loss:  0.04651937601870462 , test loss:  0.08481596727449124\n",
      "Epoch:  66140 , step:  66140 , train loss:  0.04651664395849949 , test loss:  0.08481533816447062\n",
      "Epoch:  66150 , step:  66150 , train loss:  0.04651391249781366 , test loss:  0.08481470943134733\n",
      "Epoch:  66160 , step:  66160 , train loss:  0.0465111816364261 , test loss:  0.08481408107495428\n",
      "Epoch:  66170 , step:  66170 , train loss:  0.0465084513741159 , test loss:  0.08481345309512421\n",
      "Epoch:  66180 , step:  66180 , train loss:  0.04650572171066228 , test loss:  0.08481282549169036\n",
      "Epoch:  66190 , step:  66190 , train loss:  0.046502992645844565 , test loss:  0.08481219826448584\n",
      "Epoch:  66200 , step:  66200 , train loss:  0.0465002641794422 , test loss:  0.08481157141334365\n",
      "Epoch:  66210 , step:  66210 , train loss:  0.046497536311234745 , test loss:  0.0848109449380973\n",
      "Epoch:  66220 , step:  66220 , train loss:  0.04649480904100188 , test loss:  0.08481031883858017\n",
      "Epoch:  66230 , step:  66230 , train loss:  0.04649208236852337 , test loss:  0.08480969311462552\n",
      "Epoch:  66240 , step:  66240 , train loss:  0.04648935629357914 , test loss:  0.08480906776606728\n",
      "Epoch:  66250 , step:  66250 , train loss:  0.04648663081594919 , test loss:  0.08480844279273887\n",
      "Epoch:  66260 , step:  66260 , train loss:  0.04648390593541365 , test loss:  0.08480781819447425\n",
      "Epoch:  66270 , step:  66270 , train loss:  0.04648118165175278 , test loss:  0.08480719397110723\n",
      "Epoch:  66280 , step:  66280 , train loss:  0.04647845796474692 , test loss:  0.0848065701224718\n",
      "Epoch:  66290 , step:  66290 , train loss:  0.04647573487417656 , test loss:  0.08480594664840213\n",
      "Epoch:  66300 , step:  66300 , train loss:  0.046473012379822265 , test loss:  0.08480532354873219\n",
      "Epoch:  66310 , step:  66310 , train loss:  0.046470290481464734 , test loss:  0.0848047008232963\n",
      "Epoch:  66320 , step:  66320 , train loss:  0.046467569178884796 , test loss:  0.08480407847192878\n",
      "Epoch:  66330 , step:  66330 , train loss:  0.04646484847186337 , test loss:  0.08480345649446416\n",
      "Epoch:  66340 , step:  66340 , train loss:  0.04646212836018149 , test loss:  0.08480283489073709\n",
      "Epoch:  66350 , step:  66350 , train loss:  0.04645940884362033 , test loss:  0.0848022136605819\n",
      "Epoch:  66360 , step:  66360 , train loss:  0.046456689921961135 , test loss:  0.08480159280383356\n",
      "Epoch:  66370 , step:  66370 , train loss:  0.04645397159498528 , test loss:  0.0848009723203269\n",
      "Epoch:  66380 , step:  66380 , train loss:  0.04645125386247431 , test loss:  0.08480035220989675\n",
      "Epoch:  66390 , step:  66390 , train loss:  0.04644853672420977 , test loss:  0.08479973247237824\n",
      "Epoch:  66400 , step:  66400 , train loss:  0.046445820179973424 , test loss:  0.0847991131076063\n",
      "Epoch:  66410 , step:  66410 , train loss:  0.04644310422954709 , test loss:  0.08479849411541633\n",
      "Epoch:  66420 , step:  66420 , train loss:  0.04644038887271269 , test loss:  0.08479787549564355\n",
      "Epoch:  66430 , step:  66430 , train loss:  0.04643767410925234 , test loss:  0.08479725724812326\n",
      "Epoch:  66440 , step:  66440 , train loss:  0.04643495993894817 , test loss:  0.0847966393726912\n",
      "Epoch:  66450 , step:  66450 , train loss:  0.046432246361582476 , test loss:  0.0847960218691826\n",
      "Epoch:  66460 , step:  66460 , train loss:  0.04642953337693767 , test loss:  0.08479540473743347\n",
      "Epoch:  66470 , step:  66470 , train loss:  0.046426820984796245 , test loss:  0.08479478797727931\n",
      "Epoch:  66480 , step:  66480 , train loss:  0.046424109184940804 , test loss:  0.08479417158855622\n",
      "Epoch:  66490 , step:  66490 , train loss:  0.04642139797715413 , test loss:  0.08479355557110005\n",
      "Epoch:  66500 , step:  66500 , train loss:  0.04641868736121905 , test loss:  0.08479293992474686\n",
      "Epoch:  66510 , step:  66510 , train loss:  0.04641597733691851 , test loss:  0.08479232464933263\n",
      "Epoch:  66520 , step:  66520 , train loss:  0.046413267904035584 , test loss:  0.08479170974469401\n",
      "Epoch:  66530 , step:  66530 , train loss:  0.046410559062353474 , test loss:  0.08479109521066679\n",
      "Epoch:  66540 , step:  66540 , train loss:  0.04640785081165549 , test loss:  0.08479048104708772\n",
      "Epoch:  66550 , step:  66550 , train loss:  0.046405143151725024 , test loss:  0.08478986725379353\n",
      "Epoch:  66560 , step:  66560 , train loss:  0.046402436082345566 , test loss:  0.08478925383062043\n",
      "Epoch:  66570 , step:  66570 , train loss:  0.0463997296033008 , test loss:  0.0847886407774053\n",
      "Epoch:  66580 , step:  66580 , train loss:  0.046397023714374444 , test loss:  0.084788028093985\n",
      "Epoch:  66590 , step:  66590 , train loss:  0.04639431841535034 , test loss:  0.08478741578019626\n",
      "Epoch:  66600 , step:  66600 , train loss:  0.04639161370601248 , test loss:  0.08478680383587615\n",
      "Epoch:  66610 , step:  66610 , train loss:  0.04638890958614495 , test loss:  0.08478619226086188\n",
      "Epoch:  66620 , step:  66620 , train loss:  0.04638620605553192 , test loss:  0.0847855810549904\n",
      "Epoch:  66630 , step:  66630 , train loss:  0.04638350311395771 , test loss:  0.08478497021809904\n",
      "Epoch:  66640 , step:  66640 , train loss:  0.04638080076120672 , test loss:  0.08478435975002517\n",
      "Epoch:  66650 , step:  66650 , train loss:  0.046378098997063466 , test loss:  0.08478374965060623\n",
      "Epoch:  66660 , step:  66660 , train loss:  0.046375397821312614 , test loss:  0.0847831399196798\n",
      "Epoch:  66670 , step:  66670 , train loss:  0.046372697233738894 , test loss:  0.08478253055708354\n",
      "Epoch:  66680 , step:  66680 , train loss:  0.04636999723412715 , test loss:  0.08478192156265515\n",
      "Epoch:  66690 , step:  66690 , train loss:  0.04636729782226239 , test loss:  0.08478131293623245\n",
      "Epoch:  66700 , step:  66700 , train loss:  0.04636459899792964 , test loss:  0.08478070467765327\n",
      "Epoch:  66710 , step:  66710 , train loss:  0.04636190076091414 , test loss:  0.08478009678675576\n",
      "Epoch:  66720 , step:  66720 , train loss:  0.046359203111001175 , test loss:  0.08477948926337782\n",
      "Epoch:  66730 , step:  66730 , train loss:  0.04635650604797617 , test loss:  0.08477888210735786\n",
      "Epoch:  66740 , step:  66740 , train loss:  0.04635380957162462 , test loss:  0.0847782753185341\n",
      "Epoch:  66750 , step:  66750 , train loss:  0.04635111368173219 , test loss:  0.08477766889674479\n",
      "Epoch:  66760 , step:  66760 , train loss:  0.04634841837808459 , test loss:  0.08477706284182854\n",
      "Epoch:  66770 , step:  66770 , train loss:  0.04634572366046769 , test loss:  0.08477645715362388\n",
      "Epoch:  66780 , step:  66780 , train loss:  0.04634302952866745 , test loss:  0.0847758518319693\n",
      "Epoch:  66790 , step:  66790 , train loss:  0.04634033598246998 , test loss:  0.0847752468767037\n",
      "Epoch:  66800 , step:  66800 , train loss:  0.04633764302166142 , test loss:  0.08477464228766592\n",
      "Epoch:  66810 , step:  66810 , train loss:  0.04633495064602809 , test loss:  0.08477403806469479\n",
      "Epoch:  66820 , step:  66820 , train loss:  0.04633225885535637 , test loss:  0.08477343420762948\n",
      "Epoch:  66830 , step:  66830 , train loss:  0.04632956764943283 , test loss:  0.08477283071630894\n",
      "Epoch:  66840 , step:  66840 , train loss:  0.046326877028044035 , test loss:  0.0847722275905723\n",
      "Epoch:  66850 , step:  66850 , train loss:  0.04632418699097673 , test loss:  0.08477162483025909\n",
      "Epoch:  66860 , step:  66860 , train loss:  0.04632149753801779 , test loss:  0.08477102243520862\n",
      "Epoch:  66870 , step:  66870 , train loss:  0.04631880866895415 , test loss:  0.08477042040526016\n",
      "Epoch:  66880 , step:  66880 , train loss:  0.04631612038357289 , test loss:  0.08476981874025356\n",
      "Epoch:  66890 , step:  66890 , train loss:  0.04631343268166115 , test loss:  0.08476921744002827\n",
      "Epoch:  66900 , step:  66900 , train loss:  0.04631074556300627 , test loss:  0.08476861650442427\n",
      "Epoch:  66910 , step:  66910 , train loss:  0.04630805902739557 , test loss:  0.08476801593328108\n",
      "Epoch:  66920 , step:  66920 , train loss:  0.04630537307461659 , test loss:  0.0847674157264389\n",
      "Epoch:  66930 , step:  66930 , train loss:  0.046302687704456946 , test loss:  0.08476681588373754\n",
      "Epoch:  66940 , step:  66940 , train loss:  0.046300002916704344 , test loss:  0.08476621640501728\n",
      "Epoch:  66950 , step:  66950 , train loss:  0.04629731871114662 , test loss:  0.08476561729011818\n",
      "Epoch:  66960 , step:  66960 , train loss:  0.046294635087571714 , test loss:  0.08476501853888066\n",
      "Epoch:  66970 , step:  66970 , train loss:  0.04629195204576767 , test loss:  0.08476442015114505\n",
      "Epoch:  66980 , step:  66980 , train loss:  0.04628926958552265 , test loss:  0.08476382212675178\n",
      "Epoch:  66990 , step:  66990 , train loss:  0.046286587706624915 , test loss:  0.08476322446554149\n",
      "Epoch:  67000 , step:  67000 , train loss:  0.04628390640886282 , test loss:  0.08476262716735485\n",
      "Epoch:  67010 , step:  67010 , train loss:  0.04628122569202488 , test loss:  0.08476203023203249\n",
      "Epoch:  67020 , step:  67020 , train loss:  0.046278545555899664 , test loss:  0.08476143365941519\n",
      "Epoch:  67030 , step:  67030 , train loss:  0.046275866000275875 , test loss:  0.084760837449344\n",
      "Epoch:  67040 , step:  67040 , train loss:  0.04627318702494231 , test loss:  0.08476024160165999\n",
      "Epoch:  67050 , step:  67050 , train loss:  0.0462705086296879 , test loss:  0.08475964611620414\n",
      "Epoch:  67060 , step:  67060 , train loss:  0.046267830814301664 , test loss:  0.08475905099281775\n",
      "Epoch:  67070 , step:  67070 , train loss:  0.046265153578572735 , test loss:  0.08475845623134196\n",
      "Epoch:  67080 , step:  67080 , train loss:  0.04626247692229036 , test loss:  0.08475786183161829\n",
      "Epoch:  67090 , step:  67090 , train loss:  0.04625980084524386 , test loss:  0.08475726779348816\n",
      "Epoch:  67100 , step:  67100 , train loss:  0.04625712534722273 , test loss:  0.0847566741167931\n",
      "Epoch:  67110 , step:  67110 , train loss:  0.04625445042801651 , test loss:  0.08475608080137481\n",
      "Epoch:  67120 , step:  67120 , train loss:  0.04625177608741486 , test loss:  0.084755487847075\n",
      "Epoch:  67130 , step:  67130 , train loss:  0.04624910232520761 , test loss:  0.08475489525373547\n",
      "Epoch:  67140 , step:  67140 , train loss:  0.04624642914118461 , test loss:  0.08475430302119814\n",
      "Epoch:  67150 , step:  67150 , train loss:  0.04624375653513586 , test loss:  0.08475371114930483\n",
      "Epoch:  67160 , step:  67160 , train loss:  0.04624108450685145 , test loss:  0.08475311963789796\n",
      "Epoch:  67170 , step:  67170 , train loss:  0.04623841305612161 , test loss:  0.08475252848681954\n",
      "Epoch:  67180 , step:  67180 , train loss:  0.04623574218273665 , test loss:  0.08475193769591188\n",
      "Epoch:  67190 , step:  67190 , train loss:  0.046233071886487 , test loss:  0.08475134726501729\n",
      "Epoch:  67200 , step:  67200 , train loss:  0.04623040216716319 , test loss:  0.08475075719397827\n",
      "Epoch:  67210 , step:  67210 , train loss:  0.04622773302455587 , test loss:  0.08475016748263736\n",
      "Epoch:  67220 , step:  67220 , train loss:  0.046225064458455756 , test loss:  0.08474957813083701\n",
      "Epoch:  67230 , step:  67230 , train loss:  0.04622239646865373 , test loss:  0.08474898913842027\n",
      "Epoch:  67240 , step:  67240 , train loss:  0.04621972905494075 , test loss:  0.08474840050522969\n",
      "Epoch:  67250 , step:  67250 , train loss:  0.04621706221710787 , test loss:  0.08474781223110815\n",
      "Epoch:  67260 , step:  67260 , train loss:  0.04621439595494627 , test loss:  0.08474722431589886\n",
      "Epoch:  67270 , step:  67270 , train loss:  0.04621173026824725 , test loss:  0.0847466367594447\n",
      "Epoch:  67280 , step:  67280 , train loss:  0.04620906515680216 , test loss:  0.08474604956158889\n",
      "Epoch:  67290 , step:  67290 , train loss:  0.04620640062040253 , test loss:  0.08474546272217452\n",
      "Epoch:  67300 , step:  67300 , train loss:  0.04620373665883995 , test loss:  0.08474487624104532\n",
      "Epoch:  67310 , step:  67310 , train loss:  0.04620107327190612 , test loss:  0.08474429011804437\n",
      "Epoch:  67320 , step:  67320 , train loss:  0.04619841045939286 , test loss:  0.08474370435301529\n",
      "Epoch:  67330 , step:  67330 , train loss:  0.04619574822109209 , test loss:  0.0847431189458016\n",
      "Epoch:  67340 , step:  67340 , train loss:  0.04619308655679583 , test loss:  0.08474253389624711\n",
      "Epoch:  67350 , step:  67350 , train loss:  0.04619042546629623 , test loss:  0.08474194920419556\n",
      "Epoch:  67360 , step:  67360 , train loss:  0.046187764949385535 , test loss:  0.08474136486949081\n",
      "Epoch:  67370 , step:  67370 , train loss:  0.046185105005856056 , test loss:  0.08474078089197679\n",
      "Epoch:  67380 , step:  67380 , train loss:  0.04618244563550025 , test loss:  0.08474019727149747\n",
      "Epoch:  67390 , step:  67390 , train loss:  0.046179786838110706 , test loss:  0.0847396140078972\n",
      "Epoch:  67400 , step:  67400 , train loss:  0.04617712861348006 , test loss:  0.08473903110101999\n",
      "Epoch:  67410 , step:  67410 , train loss:  0.0461744709614011 , test loss:  0.0847384485507103\n",
      "Epoch:  67420 , step:  67420 , train loss:  0.04617181388166667 , test loss:  0.08473786635681232\n",
      "Epoch:  67430 , step:  67430 , train loss:  0.04616915737406978 , test loss:  0.08473728451917083\n",
      "Epoch:  67440 , step:  67440 , train loss:  0.04616650143840351 , test loss:  0.0847367030376302\n",
      "Epoch:  67450 , step:  67450 , train loss:  0.04616384607446105 , test loss:  0.08473612191203504\n",
      "Epoch:  67460 , step:  67460 , train loss:  0.04616119128203567 , test loss:  0.0847355411422302\n",
      "Epoch:  67470 , step:  67470 , train loss:  0.04615853706092081 , test loss:  0.08473496072806053\n",
      "Epoch:  67480 , step:  67480 , train loss:  0.046155883410909974 , test loss:  0.08473438066937086\n",
      "Epoch:  67490 , step:  67490 , train loss:  0.04615323033179674 , test loss:  0.08473380096600625\n",
      "Epoch:  67500 , step:  67500 , train loss:  0.04615057782337485 , test loss:  0.08473322161781174\n",
      "Epoch:  67510 , step:  67510 , train loss:  0.04614792588543816 , test loss:  0.08473264262463258\n",
      "Epoch:  67520 , step:  67520 , train loss:  0.046145274517780534 , test loss:  0.08473206398631404\n",
      "Epoch:  67530 , step:  67530 , train loss:  0.04614262372019604 , test loss:  0.08473148570270134\n",
      "Epoch:  67540 , step:  67540 , train loss:  0.04613997349247883 , test loss:  0.0847309077736402\n",
      "Epoch:  67550 , step:  67550 , train loss:  0.0461373238344231 , test loss:  0.0847303301989759\n",
      "Epoch:  67560 , step:  67560 , train loss:  0.04613467474582323 , test loss:  0.08472975297855409\n",
      "Epoch:  67570 , step:  67570 , train loss:  0.04613202622647368 , test loss:  0.0847291761122206\n",
      "Epoch:  67580 , step:  67580 , train loss:  0.04612937827616899 , test loss:  0.08472859959982106\n",
      "Epoch:  67590 , step:  67590 , train loss:  0.04612673089470381 , test loss:  0.08472802344120145\n",
      "Epoch:  67600 , step:  67600 , train loss:  0.04612408408187294 , test loss:  0.08472744763620774\n",
      "Epoch:  67610 , step:  67610 , train loss:  0.04612143783747122 , test loss:  0.0847268721846859\n",
      "Epoch:  67620 , step:  67620 , train loss:  0.04611879216129365 , test loss:  0.08472629708648209\n",
      "Epoch:  67630 , step:  67630 , train loss:  0.046116147053135284 , test loss:  0.08472572234144256\n",
      "Epoch:  67640 , step:  67640 , train loss:  0.046113502512791324 , test loss:  0.08472514794941373\n",
      "Epoch:  67650 , step:  67650 , train loss:  0.04611085854005704 , test loss:  0.08472457391024192\n",
      "Epoch:  67660 , step:  67660 , train loss:  0.046108215134727834 , test loss:  0.08472400022377342\n",
      "Epoch:  67670 , step:  67670 , train loss:  0.04610557229659919 , test loss:  0.08472342688985504\n",
      "Epoch:  67680 , step:  67680 , train loss:  0.04610293002546672 , test loss:  0.08472285390833316\n",
      "Epoch:  67690 , step:  67690 , train loss:  0.04610028832112612 , test loss:  0.08472228127905487\n",
      "Epoch:  67700 , step:  67700 , train loss:  0.046097647183373204 , test loss:  0.08472170900186675\n",
      "Epoch:  67710 , step:  67710 , train loss:  0.046095006612003886 , test loss:  0.08472113707661583\n",
      "Epoch:  67720 , step:  67720 , train loss:  0.04609236660681415 , test loss:  0.08472056550314912\n",
      "Epoch:  67730 , step:  67730 , train loss:  0.046089727167600146 , test loss:  0.0847199942813136\n",
      "Epoch:  67740 , step:  67740 , train loss:  0.04608708829415809 , test loss:  0.08471942341095662\n",
      "Epoch:  67750 , step:  67750 , train loss:  0.04608444998628428 , test loss:  0.08471885289192506\n",
      "Epoch:  67760 , step:  67760 , train loss:  0.04608181224377519 , test loss:  0.08471828272406677\n",
      "Epoch:  67770 , step:  67770 , train loss:  0.046079175066427315 , test loss:  0.0847177129072287\n",
      "Epoch:  67780 , step:  67780 , train loss:  0.046076538454037315 , test loss:  0.08471714344125875\n",
      "Epoch:  67790 , step:  67790 , train loss:  0.04607390240640189 , test loss:  0.08471657432600417\n",
      "Epoch:  67800 , step:  67800 , train loss:  0.046071266923317915 , test loss:  0.08471600556131294\n",
      "Epoch:  67810 , step:  67810 , train loss:  0.04606863200458234 , test loss:  0.08471543714703275\n",
      "Epoch:  67820 , step:  67820 , train loss:  0.046065997649992176 , test loss:  0.08471486908301132\n",
      "Epoch:  67830 , step:  67830 , train loss:  0.046063363859344585 , test loss:  0.08471430136909683\n",
      "Epoch:  67840 , step:  67840 , train loss:  0.04606073063243685 , test loss:  0.08471373400513701\n",
      "Epoch:  67850 , step:  67850 , train loss:  0.046058097969066285 , test loss:  0.08471316699098018\n",
      "Epoch:  67860 , step:  67860 , train loss:  0.046055465869030386 , test loss:  0.08471260032647449\n",
      "Epoch:  67870 , step:  67870 , train loss:  0.04605283433212669 , test loss:  0.08471203401146811\n",
      "Epoch:  67880 , step:  67880 , train loss:  0.04605020335815284 , test loss:  0.08471146804580956\n",
      "Epoch:  67890 , step:  67890 , train loss:  0.04604757294690664 , test loss:  0.08471090242934723\n",
      "Epoch:  67900 , step:  67900 , train loss:  0.046044943098185964 , test loss:  0.08471033716192979\n",
      "Epoch:  67910 , step:  67910 , train loss:  0.046042313811788756 , test loss:  0.08470977224340555\n",
      "Epoch:  67920 , step:  67920 , train loss:  0.04603968508751308 , test loss:  0.08470920767362344\n",
      "Epoch:  67930 , step:  67930 , train loss:  0.04603705692515712 , test loss:  0.08470864345243223\n",
      "Epoch:  67940 , step:  67940 , train loss:  0.04603442932451918 , test loss:  0.08470807957968066\n",
      "Epoch:  67950 , step:  67950 , train loss:  0.04603180228539762 , test loss:  0.0847075160552178\n",
      "Epoch:  67960 , step:  67960 , train loss:  0.04602917580759092 , test loss:  0.08470695287889281\n",
      "Epoch:  67970 , step:  67970 , train loss:  0.04602654989089767 , test loss:  0.08470639005055462\n",
      "Epoch:  67980 , step:  67980 , train loss:  0.046023924535116535 , test loss:  0.08470582757005236\n",
      "Epoch:  67990 , step:  67990 , train loss:  0.04602129974004632 , test loss:  0.08470526543723565\n",
      "Epoch:  68000 , step:  68000 , train loss:  0.04601867550548591 , test loss:  0.0847047036519537\n",
      "Epoch:  68010 , step:  68010 , train loss:  0.046016051831234306 , test loss:  0.08470414221405596\n",
      "Epoch:  68020 , step:  68020 , train loss:  0.046013428717090576 , test loss:  0.08470358112339205\n",
      "Epoch:  68030 , step:  68030 , train loss:  0.04601080616285395 , test loss:  0.08470302037981137\n",
      "Epoch:  68040 , step:  68040 , train loss:  0.04600818416832371 , test loss:  0.08470245998316397\n",
      "Epoch:  68050 , step:  68050 , train loss:  0.04600556273329922 , test loss:  0.08470189993329935\n",
      "Epoch:  68060 , step:  68060 , train loss:  0.04600294185758001 , test loss:  0.08470134023006753\n",
      "Epoch:  68070 , step:  68070 , train loss:  0.046000321540965686 , test loss:  0.08470078087331855\n",
      "Epoch:  68080 , step:  68080 , train loss:  0.04599770178325593 , test loss:  0.08470022186290226\n",
      "Epoch:  68090 , step:  68090 , train loss:  0.04599508258425053 , test loss:  0.0846996631986691\n",
      "Epoch:  68100 , step:  68100 , train loss:  0.04599246394374944 , test loss:  0.08469910488046897\n",
      "Epoch:  68110 , step:  68110 , train loss:  0.045989845861552615 , test loss:  0.08469854690815237\n",
      "Epoch:  68120 , step:  68120 , train loss:  0.045987228337460176 , test loss:  0.08469798928156974\n",
      "Epoch:  68130 , step:  68130 , train loss:  0.04598461137127233 , test loss:  0.0846974320005712\n",
      "Epoch:  68140 , step:  68140 , train loss:  0.04598199496278939 , test loss:  0.08469687506500773\n",
      "Epoch:  68150 , step:  68150 , train loss:  0.04597937911181174 , test loss:  0.08469631847472969\n",
      "Epoch:  68160 , step:  68160 , train loss:  0.04597676381813994 , test loss:  0.08469576222958804\n",
      "Epoch:  68170 , step:  68170 , train loss:  0.04597414908157453 , test loss:  0.08469520632943327\n",
      "Epoch:  68180 , step:  68180 , train loss:  0.04597153490191627 , test loss:  0.08469465077411648\n",
      "Epoch:  68190 , step:  68190 , train loss:  0.045968921278965943 , test loss:  0.08469409556348859\n",
      "Epoch:  68200 , step:  68200 , train loss:  0.045966308212524495 , test loss:  0.08469354069740065\n",
      "Epoch:  68210 , step:  68210 , train loss:  0.045963695702392886 , test loss:  0.08469298617570388\n",
      "Epoch:  68220 , step:  68220 , train loss:  0.04596108374837226 , test loss:  0.08469243199824938\n",
      "Epoch:  68230 , step:  68230 , train loss:  0.04595847235026382 , test loss:  0.08469187816488848\n",
      "Epoch:  68240 , step:  68240 , train loss:  0.045955861507868886 , test loss:  0.08469132467547255\n",
      "Epoch:  68250 , step:  68250 , train loss:  0.04595325122098886 , test loss:  0.08469077152985309\n",
      "Epoch:  68260 , step:  68260 , train loss:  0.04595064148942525 , test loss:  0.08469021872788166\n",
      "Epoch:  68270 , step:  68270 , train loss:  0.04594803231297969 , test loss:  0.08468966626940977\n",
      "Epoch:  68280 , step:  68280 , train loss:  0.04594542369145385 , test loss:  0.08468911415428931\n",
      "Epoch:  68290 , step:  68290 , train loss:  0.04594281562464958 , test loss:  0.08468856238237202\n",
      "Epoch:  68300 , step:  68300 , train loss:  0.04594020811236877 , test loss:  0.08468801095350977\n",
      "Epoch:  68310 , step:  68310 , train loss:  0.04593760115441345 , test loss:  0.08468745986755451\n",
      "Epoch:  68320 , step:  68320 , train loss:  0.04593499475058571 , test loss:  0.08468690912435832\n",
      "Epoch:  68330 , step:  68330 , train loss:  0.04593238890068777 , test loss:  0.08468635872377331\n",
      "Epoch:  68340 , step:  68340 , train loss:  0.04592978360452196 , test loss:  0.08468580866565172\n",
      "Epoch:  68350 , step:  68350 , train loss:  0.04592717886189065 , test loss:  0.08468525894984558\n",
      "Epoch:  68360 , step:  68360 , train loss:  0.045924574672596356 , test loss:  0.08468470957620755\n",
      "Epoch:  68370 , step:  68370 , train loss:  0.04592197103644172 , test loss:  0.08468416054459015\n",
      "Epoch:  68380 , step:  68380 , train loss:  0.04591936795322941 , test loss:  0.08468361185484573\n",
      "Epoch:  68390 , step:  68390 , train loss:  0.04591676542276225 , test loss:  0.08468306350682706\n",
      "Epoch:  68400 , step:  68400 , train loss:  0.04591416344484315 , test loss:  0.0846825155003867\n",
      "Epoch:  68410 , step:  68410 , train loss:  0.04591156201927512 , test loss:  0.08468196783537746\n",
      "Epoch:  68420 , step:  68420 , train loss:  0.04590896114586123 , test loss:  0.08468142051165232\n",
      "Epoch:  68430 , step:  68430 , train loss:  0.045906360824404704 , test loss:  0.08468087352906395\n",
      "Epoch:  68440 , step:  68440 , train loss:  0.045903761054708864 , test loss:  0.0846803268874657\n",
      "Epoch:  68450 , step:  68450 , train loss:  0.045901161836577095 , test loss:  0.08467978058671058\n",
      "Epoch:  68460 , step:  68460 , train loss:  0.04589856316981286 , test loss:  0.0846792346266517\n",
      "Epoch:  68470 , step:  68470 , train loss:  0.045895965054219806 , test loss:  0.08467868900714233\n",
      "Epoch:  68480 , step:  68480 , train loss:  0.04589336748960162 , test loss:  0.08467814372803596\n",
      "Epoch:  68490 , step:  68490 , train loss:  0.04589077047576207 , test loss:  0.08467759878918585\n",
      "Epoch:  68500 , step:  68500 , train loss:  0.04588817401250508 , test loss:  0.08467705419044572\n",
      "Epoch:  68510 , step:  68510 , train loss:  0.04588557809963464 , test loss:  0.08467650993166899\n",
      "Epoch:  68520 , step:  68520 , train loss:  0.045882982736954785 , test loss:  0.08467596601270948\n",
      "Epoch:  68530 , step:  68530 , train loss:  0.0458803879242698 , test loss:  0.08467542243342087\n",
      "Epoch:  68540 , step:  68540 , train loss:  0.045877793661383906 , test loss:  0.08467487919365696\n",
      "Epoch:  68550 , step:  68550 , train loss:  0.04587519994810149 , test loss:  0.08467433629327185\n",
      "Epoch:  68560 , step:  68560 , train loss:  0.04587260678422706 , test loss:  0.08467379373211936\n",
      "Epoch:  68570 , step:  68570 , train loss:  0.04587001416956519 , test loss:  0.08467325151005364\n",
      "Epoch:  68580 , step:  68580 , train loss:  0.04586742210392054 , test loss:  0.08467270962692892\n",
      "Epoch:  68590 , step:  68590 , train loss:  0.04586483058709791 , test loss:  0.08467216808259935\n",
      "Epoch:  68600 , step:  68600 , train loss:  0.04586223961890215 , test loss:  0.08467162687691926\n",
      "Epoch:  68610 , step:  68610 , train loss:  0.04585964919913826 , test loss:  0.08467108600974305\n",
      "Epoch:  68620 , step:  68620 , train loss:  0.045857059327611284 , test loss:  0.08467054548092541\n",
      "Epoch:  68630 , step:  68630 , train loss:  0.04585447000412642 , test loss:  0.08467000529032073\n",
      "Epoch:  68640 , step:  68640 , train loss:  0.04585188122848892 , test loss:  0.08466946543778353\n",
      "Epoch:  68650 , step:  68650 , train loss:  0.04584929300050416 , test loss:  0.08466892592316885\n",
      "Epoch:  68660 , step:  68660 , train loss:  0.045846705319977574 , test loss:  0.08466838674633143\n",
      "Epoch:  68670 , step:  68670 , train loss:  0.04584411818671473 , test loss:  0.08466784790712598\n",
      "Epoch:  68680 , step:  68680 , train loss:  0.045841531600521276 , test loss:  0.08466730940540755\n",
      "Epoch:  68690 , step:  68690 , train loss:  0.045838945561202986 , test loss:  0.08466677124103122\n",
      "Epoch:  68700 , step:  68700 , train loss:  0.0458363600685657 , test loss:  0.08466623341385224\n",
      "Epoch:  68710 , step:  68710 , train loss:  0.04583377512241537 , test loss:  0.08466569592372569\n",
      "Epoch:  68720 , step:  68720 , train loss:  0.04583119072255803 , test loss:  0.08466515877050682\n",
      "Epoch:  68730 , step:  68730 , train loss:  0.04582860686879982 , test loss:  0.08466462195405099\n",
      "Epoch:  68740 , step:  68740 , train loss:  0.045826023560947005 , test loss:  0.08466408547421381\n",
      "Epoch:  68750 , step:  68750 , train loss:  0.04582344079880587 , test loss:  0.08466354933085071\n",
      "Epoch:  68760 , step:  68760 , train loss:  0.04582085858218289 , test loss:  0.08466301352381736\n",
      "Epoch:  68770 , step:  68770 , train loss:  0.04581827691088458 , test loss:  0.08466247805296949\n",
      "Epoch:  68780 , step:  68780 , train loss:  0.045815695784717586 , test loss:  0.08466194291816269\n",
      "Epoch:  68790 , step:  68790 , train loss:  0.045813115203488596 , test loss:  0.08466140811925313\n",
      "Epoch:  68800 , step:  68800 , train loss:  0.045810535167004444 , test loss:  0.0846608736560963\n",
      "Epoch:  68810 , step:  68810 , train loss:  0.045807955675072023 , test loss:  0.08466033952854854\n",
      "Epoch:  68820 , step:  68820 , train loss:  0.045805376727498404 , test loss:  0.0846598057364659\n",
      "Epoch:  68830 , step:  68830 , train loss:  0.04580279832409064 , test loss:  0.08465927227970448\n",
      "Epoch:  68840 , step:  68840 , train loss:  0.04580022046465595 , test loss:  0.08465873915812047\n",
      "Epoch:  68850 , step:  68850 , train loss:  0.04579764314900165 , test loss:  0.08465820637157036\n",
      "Epoch:  68860 , step:  68860 , train loss:  0.045795066376935124 , test loss:  0.0846576739199106\n",
      "Epoch:  68870 , step:  68870 , train loss:  0.04579249014826386 , test loss:  0.08465714180299738\n",
      "Epoch:  68880 , step:  68880 , train loss:  0.04578991446279545 , test loss:  0.08465661002068757\n",
      "Epoch:  68890 , step:  68890 , train loss:  0.0457873393203376 , test loss:  0.08465607857283772\n",
      "Epoch:  68900 , step:  68900 , train loss:  0.045784764720698055 , test loss:  0.08465554745930451\n",
      "Epoch:  68910 , step:  68910 , train loss:  0.045782190663684746 , test loss:  0.0846550166799447\n",
      "Epoch:  68920 , step:  68920 , train loss:  0.045779617149105604 , test loss:  0.08465448623461533\n",
      "Epoch:  68930 , step:  68930 , train loss:  0.045777044176768686 , test loss:  0.08465395612317322\n",
      "Epoch:  68940 , step:  68940 , train loss:  0.0457744717464822 , test loss:  0.08465342634547561\n",
      "Epoch:  68950 , step:  68950 , train loss:  0.04577189985805438 , test loss:  0.08465289690137931\n",
      "Epoch:  68960 , step:  68960 , train loss:  0.045769328511293575 , test loss:  0.08465236779074185\n",
      "Epoch:  68970 , step:  68970 , train loss:  0.045766757706008264 , test loss:  0.08465183901342026\n",
      "Epoch:  68980 , step:  68980 , train loss:  0.04576418744200698 , test loss:  0.08465131056927198\n",
      "Epoch:  68990 , step:  68990 , train loss:  0.04576161771909834 , test loss:  0.08465078245815451\n",
      "Epoch:  69000 , step:  69000 , train loss:  0.04575904853709113 , test loss:  0.08465025467992524\n",
      "Epoch:  69010 , step:  69010 , train loss:  0.04575647989579415 , test loss:  0.08464972723444199\n",
      "Epoch:  69020 , step:  69020 , train loss:  0.04575391179501632 , test loss:  0.08464920012156223\n",
      "Epoch:  69030 , step:  69030 , train loss:  0.04575134423456672 , test loss:  0.08464867334114375\n",
      "Epoch:  69040 , step:  69040 , train loss:  0.04574877721425442 , test loss:  0.08464814689304434\n",
      "Epoch:  69050 , step:  69050 , train loss:  0.04574621073388862 , test loss:  0.08464762077712201\n",
      "Epoch:  69060 , step:  69060 , train loss:  0.04574364479327868 , test loss:  0.0846470949932346\n",
      "Epoch:  69070 , step:  69070 , train loss:  0.04574107939223395 , test loss:  0.0846465695412403\n",
      "Epoch:  69080 , step:  69080 , train loss:  0.045738514530563976 , test loss:  0.08464604442099721\n",
      "Epoch:  69090 , step:  69090 , train loss:  0.04573595020807832 , test loss:  0.08464551963236358\n",
      "Epoch:  69100 , step:  69100 , train loss:  0.04573338642458667 , test loss:  0.08464499517519772\n",
      "Epoch:  69110 , step:  69110 , train loss:  0.04573082317989883 , test loss:  0.08464447104935785\n",
      "Epoch:  69120 , step:  69120 , train loss:  0.045728260473824646 , test loss:  0.08464394725470266\n",
      "Epoch:  69130 , step:  69130 , train loss:  0.04572569830617414 , test loss:  0.08464342379109036\n",
      "Epoch:  69140 , step:  69140 , train loss:  0.04572313667675733 , test loss:  0.08464290065838001\n",
      "Epoch:  69150 , step:  69150 , train loss:  0.04572057558538442 , test loss:  0.08464237785642997\n",
      "Epoch:  69160 , step:  69160 , train loss:  0.045718015031865594 , test loss:  0.084641855385099\n",
      "Epoch:  69170 , step:  69170 , train loss:  0.045715455016011305 , test loss:  0.08464133324424607\n",
      "Epoch:  69180 , step:  69180 , train loss:  0.04571289553763191 , test loss:  0.08464081143373009\n",
      "Epoch:  69190 , step:  69190 , train loss:  0.04571033659653799 , test loss:  0.08464028995341011\n",
      "Epoch:  69200 , step:  69200 , train loss:  0.04570777819254015 , test loss:  0.08463976880314512\n",
      "Epoch:  69210 , step:  69210 , train loss:  0.04570522032544914 , test loss:  0.08463924798279407\n",
      "Epoch:  69220 , step:  69220 , train loss:  0.04570266299507578 , test loss:  0.08463872749221656\n",
      "Epoch:  69230 , step:  69230 , train loss:  0.04570010620123096 , test loss:  0.08463820733127153\n",
      "Epoch:  69240 , step:  69240 , train loss:  0.045697549943725685 , test loss:  0.0846376874998187\n",
      "Epoch:  69250 , step:  69250 , train loss:  0.04569499422237111 , test loss:  0.08463716799771737\n",
      "Epoch:  69260 , step:  69260 , train loss:  0.04569243903697838 , test loss:  0.08463664882482717\n",
      "Epoch:  69270 , step:  69270 , train loss:  0.04568988438735881 , test loss:  0.08463612998100747\n",
      "Epoch:  69280 , step:  69280 , train loss:  0.045687330273323785 , test loss:  0.08463561146611824\n",
      "Epoch:  69290 , step:  69290 , train loss:  0.045684776694684744 , test loss:  0.08463509328001913\n",
      "Epoch:  69300 , step:  69300 , train loss:  0.04568222365125331 , test loss:  0.08463457542257011\n",
      "Epoch:  69310 , step:  69310 , train loss:  0.045679671142841105 , test loss:  0.08463405789363108\n",
      "Epoch:  69320 , step:  69320 , train loss:  0.04567711916925989 , test loss:  0.08463354069306177\n",
      "Epoch:  69330 , step:  69330 , train loss:  0.045674567730321546 , test loss:  0.08463302382072248\n",
      "Epoch:  69340 , step:  69340 , train loss:  0.045672016825838 , test loss:  0.08463250727647333\n",
      "Epoch:  69350 , step:  69350 , train loss:  0.045669466455621276 , test loss:  0.08463199106017462\n",
      "Epoch:  69360 , step:  69360 , train loss:  0.045666916619483536 , test loss:  0.08463147517168654\n",
      "Epoch:  69370 , step:  69370 , train loss:  0.04566436731723696 , test loss:  0.08463095961086953\n",
      "Epoch:  69380 , step:  69380 , train loss:  0.045661818548693896 , test loss:  0.08463044437758412\n",
      "Epoch:  69390 , step:  69390 , train loss:  0.04565927031366676 , test loss:  0.08462992947169072\n",
      "Epoch:  69400 , step:  69400 , train loss:  0.04565672261196802 , test loss:  0.08462941489304995\n",
      "Epoch:  69410 , step:  69410 , train loss:  0.04565417544341032 , test loss:  0.08462890064152258\n",
      "Epoch:  69420 , step:  69420 , train loss:  0.045651628807806294 , test loss:  0.08462838671696947\n",
      "Epoch:  69430 , step:  69430 , train loss:  0.04564908270496873 , test loss:  0.08462787311925125\n",
      "Epoch:  69440 , step:  69440 , train loss:  0.045646537134710556 , test loss:  0.08462735984822897\n",
      "Epoch:  69450 , step:  69450 , train loss:  0.04564399209684468 , test loss:  0.0846268469037635\n",
      "Epoch:  69460 , step:  69460 , train loss:  0.04564144759118419 , test loss:  0.08462633428571605\n",
      "Epoch:  69470 , step:  69470 , train loss:  0.045638903617542234 , test loss:  0.08462582199394762\n",
      "Epoch:  69480 , step:  69480 , train loss:  0.04563636017573204 , test loss:  0.08462531002831958\n",
      "Epoch:  69490 , step:  69490 , train loss:  0.04563381726556695 , test loss:  0.08462479838869316\n",
      "Epoch:  69500 , step:  69500 , train loss:  0.045631274886860425 , test loss:  0.08462428707492976\n",
      "Epoch:  69510 , step:  69510 , train loss:  0.04562873303942594 , test loss:  0.08462377608689096\n",
      "Epoch:  69520 , step:  69520 , train loss:  0.04562619172307713 , test loss:  0.08462326542443813\n",
      "Epoch:  69530 , step:  69530 , train loss:  0.0456236509376277 , test loss:  0.08462275508743274\n",
      "Epoch:  69540 , step:  69540 , train loss:  0.04562111068289143 , test loss:  0.08462224507573672\n",
      "Epoch:  69550 , step:  69550 , train loss:  0.04561857095868224 , test loss:  0.08462173538921178\n",
      "Epoch:  69560 , step:  69560 , train loss:  0.04561603176481408 , test loss:  0.08462122602771964\n",
      "Epoch:  69570 , step:  69570 , train loss:  0.04561349310110105 , test loss:  0.08462071699112216\n",
      "Epoch:  69580 , step:  69580 , train loss:  0.04561095496735729 , test loss:  0.08462020827928157\n",
      "Epoch:  69590 , step:  69590 , train loss:  0.04560841736339708 , test loss:  0.08461969989205968\n",
      "Epoch:  69600 , step:  69600 , train loss:  0.04560588028903476 , test loss:  0.08461919182931875\n",
      "Epoch:  69610 , step:  69610 , train loss:  0.045603343744084764 , test loss:  0.08461868409092103\n",
      "Epoch:  69620 , step:  69620 , train loss:  0.04560080772836162 , test loss:  0.08461817667672861\n",
      "Epoch:  69630 , step:  69630 , train loss:  0.04559827224167998 , test loss:  0.08461766958660404\n",
      "Epoch:  69640 , step:  69640 , train loss:  0.04559573728385453 , test loss:  0.0846171628204096\n",
      "Epoch:  69650 , step:  69650 , train loss:  0.045593202854700074 , test loss:  0.084616656378008\n",
      "Epoch:  69660 , step:  69660 , train loss:  0.045590668954031535 , test loss:  0.0846161502592617\n",
      "Epoch:  69670 , step:  69670 , train loss:  0.045588135581663906 , test loss:  0.08461564446403327\n",
      "Epoch:  69680 , step:  69680 , train loss:  0.04558560273741223 , test loss:  0.08461513899218547\n",
      "Epoch:  69690 , step:  69690 , train loss:  0.0455830704210917 , test loss:  0.08461463384358126\n",
      "Epoch:  69700 , step:  69700 , train loss:  0.04558053863251757 , test loss:  0.08461412901808348\n",
      "Epoch:  69710 , step:  69710 , train loss:  0.04557800737150523 , test loss:  0.08461362451555499\n",
      "Epoch:  69720 , step:  69720 , train loss:  0.04557547663787011 , test loss:  0.08461312033585891\n",
      "Epoch:  69730 , step:  69730 , train loss:  0.045572946431427695 , test loss:  0.08461261647885826\n",
      "Epoch:  69740 , step:  69740 , train loss:  0.04557041675199366 , test loss:  0.08461211294441628\n",
      "Epoch:  69750 , step:  69750 , train loss:  0.045567887599383704 , test loss:  0.08461160973239637\n",
      "Epoch:  69760 , step:  69760 , train loss:  0.04556535897341367 , test loss:  0.08461110684266158\n",
      "Epoch:  69770 , step:  69770 , train loss:  0.04556283087389943 , test loss:  0.08461060427507534\n",
      "Epoch:  69780 , step:  69780 , train loss:  0.045560303300656954 , test loss:  0.08461010202950138\n",
      "Epoch:  69790 , step:  69790 , train loss:  0.04555777625350239 , test loss:  0.08460960010580307\n",
      "Epoch:  69800 , step:  69800 , train loss:  0.04555524973225183 , test loss:  0.08460909850384421\n",
      "Epoch:  69810 , step:  69810 , train loss:  0.045552723736721576 , test loss:  0.0846085972234882\n",
      "Epoch:  69820 , step:  69820 , train loss:  0.045550198266727994 , test loss:  0.08460809626459896\n",
      "Epoch:  69830 , step:  69830 , train loss:  0.04554767332208751 , test loss:  0.08460759562704044\n",
      "Epoch:  69840 , step:  69840 , train loss:  0.045545148902616635 , test loss:  0.08460709531067657\n",
      "Epoch:  69850 , step:  69850 , train loss:  0.04554262500813204 , test loss:  0.08460659531537121\n",
      "Epoch:  69860 , step:  69860 , train loss:  0.04554010163845039 , test loss:  0.08460609564098857\n",
      "Epoch:  69870 , step:  69870 , train loss:  0.045537578793388524 , test loss:  0.08460559628739273\n",
      "Epoch:  69880 , step:  69880 , train loss:  0.04553505647276332 , test loss:  0.0846050972544477\n",
      "Epoch:  69890 , step:  69890 , train loss:  0.04553253467639179 , test loss:  0.08460459854201817\n",
      "Epoch:  69900 , step:  69900 , train loss:  0.045530013404090956 , test loss:  0.08460410014996839\n",
      "Epoch:  69910 , step:  69910 , train loss:  0.04552749265567803 , test loss:  0.08460360207816268\n",
      "Epoch:  69920 , step:  69920 , train loss:  0.04552497243097022 , test loss:  0.08460310432646559\n",
      "Epoch:  69930 , step:  69930 , train loss:  0.04552245272978492 , test loss:  0.08460260689474176\n",
      "Epoch:  69940 , step:  69940 , train loss:  0.04551993355193954 , test loss:  0.08460210978285576\n",
      "Epoch:  69950 , step:  69950 , train loss:  0.04551741489725162 , test loss:  0.0846016129906725\n",
      "Epoch:  69960 , step:  69960 , train loss:  0.04551489676553873 , test loss:  0.08460111651805664\n",
      "Epoch:  69970 , step:  69970 , train loss:  0.04551237915661862 , test loss:  0.08460062036487319\n",
      "Epoch:  69980 , step:  69980 , train loss:  0.04550986207030905 , test loss:  0.0846001245309871\n",
      "Epoch:  69990 , step:  69990 , train loss:  0.045507345506427904 , test loss:  0.0845996290162631\n",
      "Epoch:  70000 , step:  70000 , train loss:  0.04550482946479319 , test loss:  0.08459913382056662\n",
      "Epoch:  70010 , step:  70010 , train loss:  0.04550231394522293 , test loss:  0.08459863894376293\n",
      "Epoch:  70020 , step:  70020 , train loss:  0.04549979894753528 , test loss:  0.084598144385717\n",
      "Epoch:  70030 , step:  70030 , train loss:  0.0454972844715485 , test loss:  0.08459765014629421\n",
      "Epoch:  70040 , step:  70040 , train loss:  0.045494770517080896 , test loss:  0.08459715622536011\n",
      "Epoch:  70050 , step:  70050 , train loss:  0.0454922570839509 , test loss:  0.0845966626227801\n",
      "Epoch:  70060 , step:  70060 , train loss:  0.04548974417197701 , test loss:  0.08459616933841971\n",
      "Epoch:  70070 , step:  70070 , train loss:  0.045487231780977806 , test loss:  0.08459567637214437\n",
      "Epoch:  70080 , step:  70080 , train loss:  0.045484719910772006 , test loss:  0.08459518372382001\n",
      "Epoch:  70090 , step:  70090 , train loss:  0.04548220856117839 , test loss:  0.08459469139331244\n",
      "Epoch:  70100 , step:  70100 , train loss:  0.04547969773201575 , test loss:  0.08459419938048729\n",
      "Epoch:  70110 , step:  70110 , train loss:  0.04547718742310313 , test loss:  0.08459370768521061\n",
      "Epoch:  70120 , step:  70120 , train loss:  0.045474677634259504 , test loss:  0.08459321630734845\n",
      "Epoch:  70130 , step:  70130 , train loss:  0.04547216836530404 , test loss:  0.0845927252467665\n",
      "Epoch:  70140 , step:  70140 , train loss:  0.04546965961605591 , test loss:  0.08459223450333139\n",
      "Epoch:  70150 , step:  70150 , train loss:  0.045467151386334496 , test loss:  0.08459174407690895\n",
      "Epoch:  70160 , step:  70160 , train loss:  0.04546464367595909 , test loss:  0.0845912539673656\n",
      "Epoch:  70170 , step:  70170 , train loss:  0.045462136484749255 , test loss:  0.08459076417456766\n",
      "Epoch:  70180 , step:  70180 , train loss:  0.04545962981252455 , test loss:  0.08459027469838168\n",
      "Epoch:  70190 , step:  70190 , train loss:  0.04545712365910461 , test loss:  0.08458978553867377\n",
      "Epoch:  70200 , step:  70200 , train loss:  0.04545461802430921 , test loss:  0.08458929669531086\n",
      "Epoch:  70210 , step:  70210 , train loss:  0.04545211290795816 , test loss:  0.08458880816815961\n",
      "Epoch:  70220 , step:  70220 , train loss:  0.045449608309871395 , test loss:  0.0845883199570864\n",
      "Epoch:  70230 , step:  70230 , train loss:  0.04544710422986893 , test loss:  0.08458783206195832\n",
      "Epoch:  70240 , step:  70240 , train loss:  0.045444600667770875 , test loss:  0.08458734448264209\n",
      "Epoch:  70250 , step:  70250 , train loss:  0.045442097623397404 , test loss:  0.0845868572190045\n",
      "Epoch:  70260 , step:  70260 , train loss:  0.045439595096568794 , test loss:  0.08458637027091273\n",
      "Epoch:  70270 , step:  70270 , train loss:  0.04543709308710544 , test loss:  0.08458588363823386\n",
      "Epoch:  70280 , step:  70280 , train loss:  0.04543459159482775 , test loss:  0.08458539732083498\n",
      "Epoch:  70290 , step:  70290 , train loss:  0.04543209061955629 , test loss:  0.08458491131858333\n",
      "Epoch:  70300 , step:  70300 , train loss:  0.04542959016111169 , test loss:  0.0845844256313461\n",
      "Epoch:  70310 , step:  70310 , train loss:  0.04542709021931465 , test loss:  0.08458394025899069\n",
      "Epoch:  70320 , step:  70320 , train loss:  0.04542459079398598 , test loss:  0.08458345520138458\n",
      "Epoch:  70330 , step:  70330 , train loss:  0.04542209188494659 , test loss:  0.08458297045839513\n",
      "Epoch:  70340 , step:  70340 , train loss:  0.04541959349201743 , test loss:  0.08458248602989016\n",
      "Epoch:  70350 , step:  70350 , train loss:  0.04541709561501958 , test loss:  0.08458200191573705\n",
      "Epoch:  70360 , step:  70360 , train loss:  0.0454145982537742 , test loss:  0.08458151811580364\n",
      "Epoch:  70370 , step:  70370 , train loss:  0.04541210140810252 , test loss:  0.0845810346299578\n",
      "Epoch:  70380 , step:  70380 , train loss:  0.045409605077825876 , test loss:  0.08458055145806721\n",
      "Epoch:  70390 , step:  70390 , train loss:  0.04540710926276567 , test loss:  0.0845800685999999\n",
      "Epoch:  70400 , step:  70400 , train loss:  0.04540461396274342 , test loss:  0.08457958605562384\n",
      "Epoch:  70410 , step:  70410 , train loss:  0.04540211917758071 , test loss:  0.08457910382480707\n",
      "Epoch:  70420 , step:  70420 , train loss:  0.04539962490709924 , test loss:  0.08457862190741783\n",
      "Epoch:  70430 , step:  70430 , train loss:  0.04539713115112072 , test loss:  0.08457814030332415\n",
      "Epoch:  70440 , step:  70440 , train loss:  0.04539463790946704 , test loss:  0.08457765901239458\n",
      "Epoch:  70450 , step:  70450 , train loss:  0.04539214518196013 , test loss:  0.08457717803449713\n",
      "Epoch:  70460 , step:  70460 , train loss:  0.04538965296842202 , test loss:  0.08457669736950078\n",
      "Epoch:  70470 , step:  70470 , train loss:  0.04538716126867482 , test loss:  0.08457621701727344\n",
      "Epoch:  70480 , step:  70480 , train loss:  0.04538467008254073 , test loss:  0.08457573697768397\n",
      "Epoch:  70490 , step:  70490 , train loss:  0.04538217940984202 , test loss:  0.08457525725060103\n",
      "Epoch:  70500 , step:  70500 , train loss:  0.04537968925040108 , test loss:  0.08457477783589315\n",
      "Epoch:  70510 , step:  70510 , train loss:  0.04537719960404035 , test loss:  0.08457429873342934\n",
      "Epoch:  70520 , step:  70520 , train loss:  0.045374710470582405 , test loss:  0.08457381994307825\n",
      "Epoch:  70530 , step:  70530 , train loss:  0.045372221849849854 , test loss:  0.08457334146470893\n",
      "Epoch:  70540 , step:  70540 , train loss:  0.0453697337416654 , test loss:  0.08457286329819039\n",
      "Epoch:  70550 , step:  70550 , train loss:  0.04536724614585189 , test loss:  0.08457238544339153\n",
      "Epoch:  70560 , step:  70560 , train loss:  0.04536475906223217 , test loss:  0.0845719079001816\n",
      "Epoch:  70570 , step:  70570 , train loss:  0.04536227249062925 , test loss:  0.08457143066842983\n",
      "Epoch:  70580 , step:  70580 , train loss:  0.04535978643086618 , test loss:  0.08457095374800541\n",
      "Epoch:  70590 , step:  70590 , train loss:  0.04535730088276612 , test loss:  0.08457047713877788\n",
      "Epoch:  70600 , step:  70600 , train loss:  0.04535481584615228 , test loss:  0.08457000084061646\n",
      "Epoch:  70610 , step:  70610 , train loss:  0.045352331320848004 , test loss:  0.08456952485339056\n",
      "Epoch:  70620 , step:  70620 , train loss:  0.045349847306676676 , test loss:  0.08456904917696995\n",
      "Epoch:  70630 , step:  70630 , train loss:  0.045347363803461826 , test loss:  0.08456857381122422\n",
      "Epoch:  70640 , step:  70640 , train loss:  0.045344880811027 , test loss:  0.08456809875602304\n",
      "Epoch:  70650 , step:  70650 , train loss:  0.04534239832919587 , test loss:  0.08456762401123619\n",
      "Epoch:  70660 , step:  70660 , train loss:  0.0453399163577922 , test loss:  0.08456714957673331\n",
      "Epoch:  70670 , step:  70670 , train loss:  0.04533743489663983 , test loss:  0.08456667545238464\n",
      "Epoch:  70680 , step:  70680 , train loss:  0.04533495394556266 , test loss:  0.08456620163806013\n",
      "Epoch:  70690 , step:  70690 , train loss:  0.0453324735043847 , test loss:  0.08456572813362952\n",
      "Epoch:  70700 , step:  70700 , train loss:  0.04532999357293007 , test loss:  0.08456525493896325\n",
      "Epoch:  70710 , step:  70710 , train loss:  0.045327514151022916 , test loss:  0.08456478205393132\n",
      "Epoch:  70720 , step:  70720 , train loss:  0.045325035238487514 , test loss:  0.08456430947840403\n",
      "Epoch:  70730 , step:  70730 , train loss:  0.04532255683514823 , test loss:  0.08456383721225169\n",
      "Epoch:  70740 , step:  70740 , train loss:  0.04532007894082948 , test loss:  0.08456336525534479\n",
      "Epoch:  70750 , step:  70750 , train loss:  0.04531760155535581 , test loss:  0.08456289360755373\n",
      "Epoch:  70760 , step:  70760 , train loss:  0.04531512467855178 , test loss:  0.08456242226874909\n",
      "Epoch:  70770 , step:  70770 , train loss:  0.04531264831024212 , test loss:  0.08456195123880139\n",
      "Epoch:  70780 , step:  70780 , train loss:  0.045310172450251616 , test loss:  0.08456148051758149\n",
      "Epoch:  70790 , step:  70790 , train loss:  0.04530769709840508 , test loss:  0.08456101010495987\n",
      "Epoch:  70800 , step:  70800 , train loss:  0.045305222254527495 , test loss:  0.08456054000080761\n",
      "Epoch:  70810 , step:  70810 , train loss:  0.04530274791844388 , test loss:  0.08456007020499537\n",
      "Epoch:  70820 , step:  70820 , train loss:  0.04530027408997936 , test loss:  0.08455960071739424\n",
      "Epoch:  70830 , step:  70830 , train loss:  0.04529780076895913 , test loss:  0.08455913153787514\n",
      "Epoch:  70840 , step:  70840 , train loss:  0.04529532795520848 , test loss:  0.08455866266630932\n",
      "Epoch:  70850 , step:  70850 , train loss:  0.04529285564855275 , test loss:  0.08455819410256778\n",
      "Epoch:  70860 , step:  70860 , train loss:  0.04529038384881744 , test loss:  0.08455772584652192\n",
      "Epoch:  70870 , step:  70870 , train loss:  0.04528791255582807 , test loss:  0.08455725789804298\n",
      "Epoch:  70880 , step:  70880 , train loss:  0.04528544176941026 , test loss:  0.08455679025700223\n",
      "Epoch:  70890 , step:  70890 , train loss:  0.04528297148938974 , test loss:  0.08455632292327123\n",
      "Epoch:  70900 , step:  70900 , train loss:  0.04528050171559227 , test loss:  0.08455585589672139\n",
      "Epoch:  70910 , step:  70910 , train loss:  0.04527803244784375 , test loss:  0.08455538917722442\n",
      "Epoch:  70920 , step:  70920 , train loss:  0.04527556368597016 , test loss:  0.08455492276465186\n",
      "Epoch:  70930 , step:  70930 , train loss:  0.0452730954297975 , test loss:  0.08455445665887534\n",
      "Epoch:  70940 , step:  70940 , train loss:  0.04527062767915193 , test loss:  0.0845539908597669\n",
      "Epoch:  70950 , step:  70950 , train loss:  0.045268160433859665 , test loss:  0.08455352536719817\n",
      "Epoch:  70960 , step:  70960 , train loss:  0.04526569369374701 , test loss:  0.0845530601810412\n",
      "Epoch:  70970 , step:  70970 , train loss:  0.04526322745864035 , test loss:  0.08455259530116793\n",
      "Epoch:  70980 , step:  70980 , train loss:  0.04526076172836612 , test loss:  0.08455213072745037\n",
      "Epoch:  70990 , step:  70990 , train loss:  0.04525829650275088 , test loss:  0.08455166645976073\n",
      "Epoch:  71000 , step:  71000 , train loss:  0.04525583178162132 , test loss:  0.08455120249797113\n",
      "Epoch:  71010 , step:  71010 , train loss:  0.04525336756480411 , test loss:  0.08455073884195387\n",
      "Epoch:  71020 , step:  71020 , train loss:  0.045250903852126045 , test loss:  0.0845502754915814\n",
      "Epoch:  71030 , step:  71030 , train loss:  0.04524844064341403 , test loss:  0.08454981244672596\n",
      "Epoch:  71040 , step:  71040 , train loss:  0.04524597793849505 , test loss:  0.08454934970726005\n",
      "Epoch:  71050 , step:  71050 , train loss:  0.045243515737196156 , test loss:  0.08454888727305629\n",
      "Epoch:  71060 , step:  71060 , train loss:  0.045241054039344454 , test loss:  0.08454842514398707\n",
      "Epoch:  71070 , step:  71070 , train loss:  0.045238592844767195 , test loss:  0.08454796331992534\n",
      "Epoch:  71080 , step:  71080 , train loss:  0.04523613215329166 , test loss:  0.08454750180074364\n",
      "Epoch:  71090 , step:  71090 , train loss:  0.045233671964745294 , test loss:  0.0845470405863149\n",
      "Epoch:  71100 , step:  71100 , train loss:  0.04523121227895551 , test loss:  0.08454657967651204\n",
      "Epoch:  71110 , step:  71110 , train loss:  0.04522875309574989 , test loss:  0.08454611907120796\n",
      "Epoch:  71120 , step:  71120 , train loss:  0.04522629441495606 , test loss:  0.08454565877027549\n",
      "Epoch:  71130 , step:  71130 , train loss:  0.04522383623640175 , test loss:  0.08454519877358789\n",
      "Epoch:  71140 , step:  71140 , train loss:  0.04522137855991478 , test loss:  0.08454473908101846\n",
      "Epoch:  71150 , step:  71150 , train loss:  0.04521892138532302 , test loss:  0.08454427969244012\n",
      "Epoch:  71160 , step:  71160 , train loss:  0.045216464712454434 , test loss:  0.08454382060772633\n",
      "Epoch:  71170 , step:  71170 , train loss:  0.04521400854113711 , test loss:  0.08454336182675039\n",
      "Epoch:  71180 , step:  71180 , train loss:  0.04521155287119915 , test loss:  0.08454290334938566\n",
      "Epoch:  71190 , step:  71190 , train loss:  0.04520909770246879 , test loss:  0.0845424451755058\n",
      "Epoch:  71200 , step:  71200 , train loss:  0.045206643034774355 , test loss:  0.08454198730498433\n",
      "Epoch:  71210 , step:  71210 , train loss:  0.04520418886794421 , test loss:  0.08454152973769478\n",
      "Epoch:  71220 , step:  71220 , train loss:  0.04520173520180681 , test loss:  0.08454107247351082\n",
      "Epoch:  71230 , step:  71230 , train loss:  0.04519928203619074 , test loss:  0.08454061551230634\n",
      "Epoch:  71240 , step:  71240 , train loss:  0.04519682937092462 , test loss:  0.08454015885395506\n",
      "Epoch:  71250 , step:  71250 , train loss:  0.04519437720583717 , test loss:  0.08453970249833097\n",
      "Epoch:  71260 , step:  71260 , train loss:  0.04519192554075717 , test loss:  0.08453924644530794\n",
      "Epoch:  71270 , step:  71270 , train loss:  0.04518947437551355 , test loss:  0.08453879069476003\n",
      "Epoch:  71280 , step:  71280 , train loss:  0.04518702370993522 , test loss:  0.08453833524656133\n",
      "Epoch:  71290 , step:  71290 , train loss:  0.045184573543851275 , test loss:  0.084537880100586\n",
      "Epoch:  71300 , step:  71300 , train loss:  0.04518212387709082 , test loss:  0.08453742525670828\n",
      "Epoch:  71310 , step:  71310 , train loss:  0.045179674709483086 , test loss:  0.08453697071480254\n",
      "Epoch:  71320 , step:  71320 , train loss:  0.04517722604085733 , test loss:  0.08453651647474318\n",
      "Epoch:  71330 , step:  71330 , train loss:  0.04517477787104298 , test loss:  0.08453606253640443\n",
      "Epoch:  71340 , step:  71340 , train loss:  0.04517233019986945 , test loss:  0.08453560889966082\n",
      "Epoch:  71350 , step:  71350 , train loss:  0.045169883027166305 , test loss:  0.0845351555643871\n",
      "Epoch:  71360 , step:  71360 , train loss:  0.04516743635276319 , test loss:  0.0845347025304578\n",
      "Epoch:  71370 , step:  71370 , train loss:  0.045164990176489776 , test loss:  0.08453424979774754\n",
      "Epoch:  71380 , step:  71380 , train loss:  0.045162544498175854 , test loss:  0.08453379736613117\n",
      "Epoch:  71390 , step:  71390 , train loss:  0.045160099317651296 , test loss:  0.08453334523548357\n",
      "Epoch:  71400 , step:  71400 , train loss:  0.04515765463474608 , test loss:  0.08453289340567958\n",
      "Epoch:  71410 , step:  71410 , train loss:  0.045155210449290197 , test loss:  0.08453244187659416\n",
      "Epoch:  71420 , step:  71420 , train loss:  0.04515276676111382 , test loss:  0.08453199064810238\n",
      "Epoch:  71430 , step:  71430 , train loss:  0.04515032357004709 , test loss:  0.0845315397200794\n",
      "Epoch:  71440 , step:  71440 , train loss:  0.04514788087592029 , test loss:  0.08453108909240026\n",
      "Epoch:  71450 , step:  71450 , train loss:  0.045145438678563805 , test loss:  0.08453063876494031\n",
      "Epoch:  71460 , step:  71460 , train loss:  0.04514299697780809 , test loss:  0.08453018873757477\n",
      "Epoch:  71470 , step:  71470 , train loss:  0.045140555773483644 , test loss:  0.08452973901017895\n",
      "Epoch:  71480 , step:  71480 , train loss:  0.04513811506542107 , test loss:  0.08452928958262837\n",
      "Epoch:  71490 , step:  71490 , train loss:  0.045135674853451054 , test loss:  0.08452884045479866\n",
      "Epoch:  71500 , step:  71500 , train loss:  0.04513323513740439 , test loss:  0.08452839162656516\n",
      "Epoch:  71510 , step:  71510 , train loss:  0.045130795917111874 , test loss:  0.08452794309780348\n",
      "Epoch:  71520 , step:  71520 , train loss:  0.04512835719240449 , test loss:  0.08452749486838959\n",
      "Epoch:  71530 , step:  71530 , train loss:  0.045125918963113236 , test loss:  0.08452704693819899\n",
      "Epoch:  71540 , step:  71540 , train loss:  0.0451234812290692 , test loss:  0.08452659930710765\n",
      "Epoch:  71550 , step:  71550 , train loss:  0.04512104399010354 , test loss:  0.08452615197499123\n",
      "Epoch:  71560 , step:  71560 , train loss:  0.045118607246047535 , test loss:  0.0845257049417262\n",
      "Epoch:  71570 , step:  71570 , train loss:  0.04511617099673253 , test loss:  0.08452525820718816\n",
      "Epoch:  71580 , step:  71580 , train loss:  0.04511373524198989 , test loss:  0.08452481177125347\n",
      "Epoch:  71590 , step:  71590 , train loss:  0.04511129998165117 , test loss:  0.08452436563379791\n",
      "Epoch:  71600 , step:  71600 , train loss:  0.04510886521554793 , test loss:  0.08452391979469806\n",
      "Epoch:  71610 , step:  71610 , train loss:  0.04510643094351182 , test loss:  0.08452347425383014\n",
      "Epoch:  71620 , step:  71620 , train loss:  0.045103997165374575 , test loss:  0.0845230290110704\n",
      "Epoch:  71630 , step:  71630 , train loss:  0.04510156388096805 , test loss:  0.08452258406629533\n",
      "Epoch:  71640 , step:  71640 , train loss:  0.04509913109012412 , test loss:  0.08452213941938133\n",
      "Epoch:  71650 , step:  71650 , train loss:  0.045096698792674764 , test loss:  0.08452169507020516\n",
      "Epoch:  71660 , step:  71660 , train loss:  0.04509426698845207 , test loss:  0.08452125101864336\n",
      "Epoch:  71670 , step:  71670 , train loss:  0.04509183567728817 , test loss:  0.08452080726457253\n",
      "Epoch:  71680 , step:  71680 , train loss:  0.04508940485901525 , test loss:  0.08452036380786948\n",
      "Epoch:  71690 , step:  71690 , train loss:  0.04508697453346569 , test loss:  0.08451992064841096\n",
      "Epoch:  71700 , step:  71700 , train loss:  0.04508454470047181 , test loss:  0.08451947778607394\n",
      "Epoch:  71710 , step:  71710 , train loss:  0.0450821153598661 , test loss:  0.08451903522073542\n",
      "Epoch:  71720 , step:  71720 , train loss:  0.045079686511481115 , test loss:  0.08451859295227226\n",
      "Epoch:  71730 , step:  71730 , train loss:  0.045077258155149486 , test loss:  0.08451815098056169\n",
      "Epoch:  71740 , step:  71740 , train loss:  0.04507483029070389 , test loss:  0.08451770930548073\n",
      "Epoch:  71750 , step:  71750 , train loss:  0.04507240291797712 , test loss:  0.08451726792690661\n",
      "Epoch:  71760 , step:  71760 , train loss:  0.04506997603680208 , test loss:  0.0845168268447167\n",
      "Epoch:  71770 , step:  71770 , train loss:  0.04506754964701165 , test loss:  0.08451638605878821\n",
      "Epoch:  71780 , step:  71780 , train loss:  0.045065123748438914 , test loss:  0.0845159455689987\n",
      "Epoch:  71790 , step:  71790 , train loss:  0.04506269834091695 , test loss:  0.08451550537522542\n",
      "Epoch:  71800 , step:  71800 , train loss:  0.045060273424278964 , test loss:  0.08451506547734604\n",
      "Epoch:  71810 , step:  71810 , train loss:  0.045057848998358185 , test loss:  0.08451462587523818\n",
      "Epoch:  71820 , step:  71820 , train loss:  0.045055425062988014 , test loss:  0.08451418656877942\n",
      "Epoch:  71830 , step:  71830 , train loss:  0.04505300161800184 , test loss:  0.08451374755784767\n",
      "Epoch:  71840 , step:  71840 , train loss:  0.04505057866323314 , test loss:  0.08451330884232049\n",
      "Epoch:  71850 , step:  71850 , train loss:  0.04504815619851558 , test loss:  0.08451287042207599\n",
      "Epoch:  71860 , step:  71860 , train loss:  0.04504573422368275 , test loss:  0.08451243229699187\n",
      "Epoch:  71870 , step:  71870 , train loss:  0.04504331273856844 , test loss:  0.0845119944669463\n",
      "Epoch:  71880 , step:  71880 , train loss:  0.04504089174300645 , test loss:  0.08451155693181707\n",
      "Epoch:  71890 , step:  71890 , train loss:  0.045038471236830684 , test loss:  0.08451111969148249\n",
      "Epoch:  71900 , step:  71900 , train loss:  0.04503605121987513 , test loss:  0.08451068274582081\n",
      "Epoch:  71910 , step:  71910 , train loss:  0.04503363169197385 , test loss:  0.08451024609471011\n",
      "Epoch:  71920 , step:  71920 , train loss:  0.045031212652960985 , test loss:  0.08450980973802873\n",
      "Epoch:  71930 , step:  71930 , train loss:  0.04502879410267076 , test loss:  0.0845093736756552\n",
      "Epoch:  71940 , step:  71940 , train loss:  0.04502637604093745 , test loss:  0.08450893790746772\n",
      "Epoch:  71950 , step:  71950 , train loss:  0.04502395846759546 , test loss:  0.08450850243334505\n",
      "Epoch:  71960 , step:  71960 , train loss:  0.04502154138247925 , test loss:  0.08450806725316541\n",
      "Epoch:  71970 , step:  71970 , train loss:  0.045019124785423324 , test loss:  0.08450763236680775\n",
      "Epoch:  71980 , step:  71980 , train loss:  0.04501670867626234 , test loss:  0.08450719777415075\n",
      "Epoch:  71990 , step:  71990 , train loss:  0.04501429305483095 , test loss:  0.08450676347507305\n",
      "Epoch:  72000 , step:  72000 , train loss:  0.04501187792096398 , test loss:  0.08450632946945356\n",
      "Epoch:  72010 , step:  72010 , train loss:  0.04500946327449625 , test loss:  0.08450589575717125\n",
      "Epoch:  72020 , step:  72020 , train loss:  0.045007049115262675 , test loss:  0.0845054623381047\n",
      "Epoch:  72030 , step:  72030 , train loss:  0.045004635443098295 , test loss:  0.08450502921213318\n",
      "Epoch:  72040 , step:  72040 , train loss:  0.04500222225783819 , test loss:  0.0845045963791359\n",
      "Epoch:  72050 , step:  72050 , train loss:  0.04499980955931751 , test loss:  0.0845041638389918\n",
      "Epoch:  72060 , step:  72060 , train loss:  0.04499739734737153 , test loss:  0.08450373159158023\n",
      "Epoch:  72070 , step:  72070 , train loss:  0.044994985621835556 , test loss:  0.08450329963678035\n",
      "Epoch:  72080 , step:  72080 , train loss:  0.04499257438254498 , test loss:  0.08450286797447168\n",
      "Epoch:  72090 , step:  72090 , train loss:  0.04499016362933532 , test loss:  0.08450243660453331\n",
      "Epoch:  72100 , step:  72100 , train loss:  0.0449877533620421 , test loss:  0.08450200552684478\n",
      "Epoch:  72110 , step:  72110 , train loss:  0.04498534358050099 , test loss:  0.08450157474128582\n",
      "Epoch:  72120 , step:  72120 , train loss:  0.044982934284547686 , test loss:  0.084501144247736\n",
      "Epoch:  72130 , step:  72130 , train loss:  0.04498052547401797 , test loss:  0.08450071404607479\n",
      "Epoch:  72140 , step:  72140 , train loss:  0.04497811714874774 , test loss:  0.08450028413618212\n",
      "Epoch:  72150 , step:  72150 , train loss:  0.044975709308572955 , test loss:  0.0844998545179376\n",
      "Epoch:  72160 , step:  72160 , train loss:  0.04497330195332962 , test loss:  0.08449942519122124\n",
      "Epoch:  72170 , step:  72170 , train loss:  0.04497089508285383 , test loss:  0.08449899615591257\n",
      "Epoch:  72180 , step:  72180 , train loss:  0.04496848869698181 , test loss:  0.08449856741189217\n",
      "Epoch:  72190 , step:  72190 , train loss:  0.044966082795549804 , test loss:  0.08449813895903956\n",
      "Epoch:  72200 , step:  72200 , train loss:  0.04496367737839415 , test loss:  0.08449771079723495\n",
      "Epoch:  72210 , step:  72210 , train loss:  0.04496127244535129 , test loss:  0.08449728292635879\n",
      "Epoch:  72220 , step:  72220 , train loss:  0.04495886799625769 , test loss:  0.08449685534629106\n",
      "Epoch:  72230 , step:  72230 , train loss:  0.04495646403094993 , test loss:  0.08449642805691206\n",
      "Epoch:  72240 , step:  72240 , train loss:  0.04495406054926468 , test loss:  0.084496001058102\n",
      "Epoch:  72250 , step:  72250 , train loss:  0.04495165755103865 , test loss:  0.08449557434974156\n",
      "Epoch:  72260 , step:  72260 , train loss:  0.044949255036108665 , test loss:  0.08449514793171121\n",
      "Epoch:  72270 , step:  72270 , train loss:  0.04494685300431161 , test loss:  0.08449472180389128\n",
      "Epoch:  72280 , step:  72280 , train loss:  0.04494445145548446 , test loss:  0.08449429596616265\n",
      "Epoch:  72290 , step:  72290 , train loss:  0.04494205038946421 , test loss:  0.08449387041840573\n",
      "Epoch:  72300 , step:  72300 , train loss:  0.044939649806088014 , test loss:  0.08449344516050128\n",
      "Epoch:  72310 , step:  72310 , train loss:  0.04493724970519307 , test loss:  0.08449302019233025\n",
      "Epoch:  72320 , step:  72320 , train loss:  0.044934850086616636 , test loss:  0.08449259551377332\n",
      "Epoch:  72330 , step:  72330 , train loss:  0.044932450950196066 , test loss:  0.08449217112471152\n",
      "Epoch:  72340 , step:  72340 , train loss:  0.0449300522957688 , test loss:  0.08449174702502588\n",
      "Epoch:  72350 , step:  72350 , train loss:  0.04492765412317231 , test loss:  0.08449132321459722\n",
      "Epoch:  72360 , step:  72360 , train loss:  0.0449252564322442 , test loss:  0.08449089969330689\n",
      "Epoch:  72370 , step:  72370 , train loss:  0.04492285922282215 , test loss:  0.084490476461036\n",
      "Epoch:  72380 , step:  72380 , train loss:  0.04492046249474386 , test loss:  0.08449005351766577\n",
      "Epoch:  72390 , step:  72390 , train loss:  0.04491806624784717 , test loss:  0.08448963086307729\n",
      "Epoch:  72400 , step:  72400 , train loss:  0.04491567048196997 , test loss:  0.08448920849715226\n",
      "Epoch:  72410 , step:  72410 , train loss:  0.04491327519695018 , test loss:  0.08448878641977188\n",
      "Epoch:  72420 , step:  72420 , train loss:  0.044910880392625904 , test loss:  0.08448836463081759\n",
      "Epoch:  72430 , step:  72430 , train loss:  0.04490848606883524 , test loss:  0.08448794313017123\n",
      "Epoch:  72440 , step:  72440 , train loss:  0.04490609222541636 , test loss:  0.08448752191771404\n",
      "Epoch:  72450 , step:  72450 , train loss:  0.04490369886220759 , test loss:  0.08448710099332779\n",
      "Epoch:  72460 , step:  72460 , train loss:  0.04490130597904724 , test loss:  0.08448668035689445\n",
      "Epoch:  72470 , step:  72470 , train loss:  0.04489891357577376 , test loss:  0.0844862600082956\n",
      "Epoch:  72480 , step:  72480 , train loss:  0.044896521652225616 , test loss:  0.08448583994741304\n",
      "Epoch:  72490 , step:  72490 , train loss:  0.04489413020824145 , test loss:  0.08448542017412873\n",
      "Epoch:  72500 , step:  72500 , train loss:  0.04489173924365988 , test loss:  0.08448500068832478\n",
      "Epoch:  72510 , step:  72510 , train loss:  0.04488934875831963 , test loss:  0.08448458148988312\n",
      "Epoch:  72520 , step:  72520 , train loss:  0.04488695875205956 , test loss:  0.08448416257868589\n",
      "Epoch:  72530 , step:  72530 , train loss:  0.044884569224718514 , test loss:  0.0844837439546152\n",
      "Epoch:  72540 , step:  72540 , train loss:  0.04488218017613543 , test loss:  0.08448332561755334\n",
      "Epoch:  72550 , step:  72550 , train loss:  0.04487979160614943 , test loss:  0.08448290756738251\n",
      "Epoch:  72560 , step:  72560 , train loss:  0.044877403514599554 , test loss:  0.08448248980398519\n",
      "Epoch:  72570 , step:  72570 , train loss:  0.044875015901325026 , test loss:  0.08448207232724371\n",
      "Epoch:  72580 , step:  72580 , train loss:  0.04487262876616511 , test loss:  0.08448165513704059\n",
      "Epoch:  72590 , step:  72590 , train loss:  0.04487024210895915 , test loss:  0.08448123823325829\n",
      "Epoch:  72600 , step:  72600 , train loss:  0.04486785592954656 , test loss:  0.08448082161577941\n",
      "Epoch:  72610 , step:  72610 , train loss:  0.04486547022776686 , test loss:  0.08448040528448668\n",
      "Epoch:  72620 , step:  72620 , train loss:  0.044863085003459584 , test loss:  0.08447998923926282\n",
      "Epoch:  72630 , step:  72630 , train loss:  0.044860700256464395 , test loss:  0.08447957347999054\n",
      "Epoch:  72640 , step:  72640 , train loss:  0.04485831598662103 , test loss:  0.08447915800655274\n",
      "Epoch:  72650 , step:  72650 , train loss:  0.04485593219376928 , test loss:  0.08447874281883223\n",
      "Epoch:  72660 , step:  72660 , train loss:  0.044853548877749005 , test loss:  0.08447832791671202\n",
      "Epoch:  72670 , step:  72670 , train loss:  0.04485116603840019 , test loss:  0.08447791330007533\n",
      "Epoch:  72680 , step:  72680 , train loss:  0.044848783675562824 , test loss:  0.08447749896880496\n",
      "Epoch:  72690 , step:  72690 , train loss:  0.044846401789077034 , test loss:  0.0844770849227842\n",
      "Epoch:  72700 , step:  72700 , train loss:  0.04484402037878299 , test loss:  0.08447667116189625\n",
      "Epoch:  72710 , step:  72710 , train loss:  0.044841639444520937 , test loss:  0.08447625768602426\n",
      "Epoch:  72720 , step:  72720 , train loss:  0.044839258986131225 , test loss:  0.08447584449505174\n",
      "Epoch:  72730 , step:  72730 , train loss:  0.044836879003454226 , test loss:  0.08447543158886212\n",
      "Epoch:  72740 , step:  72740 , train loss:  0.04483449949633045 , test loss:  0.08447501896733878\n",
      "Epoch:  72750 , step:  72750 , train loss:  0.04483212046460045 , test loss:  0.08447460663036513\n",
      "Epoch:  72760 , step:  72760 , train loss:  0.04482974190810482 , test loss:  0.0844741945778249\n",
      "Epoch:  72770 , step:  72770 , train loss:  0.04482736382668433 , test loss:  0.08447378280960166\n",
      "Epoch:  72780 , step:  72780 , train loss:  0.04482498622017969 , test loss:  0.08447337132557897\n",
      "Epoch:  72790 , step:  72790 , train loss:  0.044822609088431796 , test loss:  0.0844729601256407\n",
      "Epoch:  72800 , step:  72800 , train loss:  0.04482023243128158 , test loss:  0.08447254920967089\n",
      "Epoch:  72810 , step:  72810 , train loss:  0.04481785624857003 , test loss:  0.08447213857755302\n",
      "Epoch:  72820 , step:  72820 , train loss:  0.04481548054013826 , test loss:  0.08447172822917139\n",
      "Epoch:  72830 , step:  72830 , train loss:  0.044813105305827376 , test loss:  0.08447131816440992\n",
      "Epoch:  72840 , step:  72840 , train loss:  0.04481073054547867 , test loss:  0.08447090838315235\n",
      "Epoch:  72850 , step:  72850 , train loss:  0.04480835625893339 , test loss:  0.0844704988852833\n",
      "Epoch:  72860 , step:  72860 , train loss:  0.044805982446032934 , test loss:  0.08447008967068656\n",
      "Epoch:  72870 , step:  72870 , train loss:  0.04480360910661878 , test loss:  0.08446968073924668\n",
      "Epoch:  72880 , step:  72880 , train loss:  0.044801236240532456 , test loss:  0.08446927209084781\n",
      "Epoch:  72890 , step:  72890 , train loss:  0.04479886384761552 , test loss:  0.08446886372537428\n",
      "Epoch:  72900 , step:  72900 , train loss:  0.04479649192770971 , test loss:  0.08446845564271042\n",
      "Epoch:  72910 , step:  72910 , train loss:  0.04479412048065672 , test loss:  0.08446804784274103\n",
      "Epoch:  72920 , step:  72920 , train loss:  0.04479174950629845 , test loss:  0.08446764032535053\n",
      "Epoch:  72930 , step:  72930 , train loss:  0.04478937900447676 , test loss:  0.08446723309042335\n",
      "Epoch:  72940 , step:  72940 , train loss:  0.04478700897503363 , test loss:  0.08446682613784443\n",
      "Epoch:  72950 , step:  72950 , train loss:  0.04478463941781114 , test loss:  0.08446641946749829\n",
      "Epoch:  72960 , step:  72960 , train loss:  0.04478227033265138 , test loss:  0.08446601307926968\n",
      "Epoch:  72970 , step:  72970 , train loss:  0.04477990171939657 , test loss:  0.0844656069730435\n",
      "Epoch:  72980 , step:  72980 , train loss:  0.04477753357788899 , test loss:  0.08446520114870498\n",
      "Epoch:  72990 , step:  72990 , train loss:  0.04477516590797096 , test loss:  0.08446479560613869\n",
      "Epoch:  73000 , step:  73000 , train loss:  0.04477279870948493 , test loss:  0.08446439034522976\n",
      "Epoch:  73010 , step:  73010 , train loss:  0.04477043198227341 , test loss:  0.08446398536586341\n",
      "Epoch:  73020 , step:  73020 , train loss:  0.04476806572617894 , test loss:  0.08446358066792474\n",
      "Epoch:  73030 , step:  73030 , train loss:  0.044765699941044165 , test loss:  0.08446317625129889\n",
      "Epoch:  73040 , step:  73040 , train loss:  0.04476333462671184 , test loss:  0.08446277211587104\n",
      "Epoch:  73050 , step:  73050 , train loss:  0.04476096978302474 , test loss:  0.08446236826152674\n",
      "Epoch:  73060 , step:  73060 , train loss:  0.044758605409825736 , test loss:  0.08446196468815127\n",
      "Epoch:  73070 , step:  73070 , train loss:  0.04475624150695777 , test loss:  0.08446156139563005\n",
      "Epoch:  73080 , step:  73080 , train loss:  0.04475387807426383 , test loss:  0.0844611583838485\n",
      "Epoch:  73090 , step:  73090 , train loss:  0.04475151511158704 , test loss:  0.0844607556526925\n",
      "Epoch:  73100 , step:  73100 , train loss:  0.04474915261877058 , test loss:  0.08446035320204737\n",
      "Epoch:  73110 , step:  73110 , train loss:  0.04474679059565764 , test loss:  0.08445995103179899\n",
      "Epoch:  73120 , step:  73120 , train loss:  0.04474442904209156 , test loss:  0.08445954914183305\n",
      "Epoch:  73130 , step:  73130 , train loss:  0.044742067957915715 , test loss:  0.08445914753203515\n",
      "Epoch:  73140 , step:  73140 , train loss:  0.04473970734297357 , test loss:  0.08445874620229143\n",
      "Epoch:  73150 , step:  73150 , train loss:  0.044737347197108646 , test loss:  0.08445834515248765\n",
      "Epoch:  73160 , step:  73160 , train loss:  0.04473498752016453 , test loss:  0.08445794438251\n",
      "Epoch:  73170 , step:  73170 , train loss:  0.044732628311984955 , test loss:  0.08445754389224426\n",
      "Epoch:  73180 , step:  73180 , train loss:  0.044730269572413626 , test loss:  0.08445714368157672\n",
      "Epoch:  73190 , step:  73190 , train loss:  0.04472791130129438 , test loss:  0.08445674375039353\n",
      "Epoch:  73200 , step:  73200 , train loss:  0.044725553498471134 , test loss:  0.08445634409858084\n",
      "Epoch:  73210 , step:  73210 , train loss:  0.04472319616378782 , test loss:  0.08445594472602498\n",
      "Epoch:  73220 , step:  73220 , train loss:  0.04472083929708853 , test loss:  0.0844555456326122\n",
      "Epoch:  73230 , step:  73230 , train loss:  0.04471848289821737 , test loss:  0.08445514681822898\n",
      "Epoch:  73240 , step:  73240 , train loss:  0.04471612696701849 , test loss:  0.0844547482827618\n",
      "Epoch:  73250 , step:  73250 , train loss:  0.044713771503336185 , test loss:  0.0844543500260971\n",
      "Epoch:  73260 , step:  73260 , train loss:  0.0447114165070148 , test loss:  0.08445395204812155\n",
      "Epoch:  73270 , step:  73270 , train loss:  0.04470906197789875 , test loss:  0.08445355434872165\n",
      "Epoch:  73280 , step:  73280 , train loss:  0.044706707915832485 , test loss:  0.08445315692778418\n",
      "Epoch:  73290 , step:  73290 , train loss:  0.04470435432066061 , test loss:  0.08445275978519592\n",
      "Epoch:  73300 , step:  73300 , train loss:  0.0447020011922277 , test loss:  0.08445236292084377\n",
      "Epoch:  73310 , step:  73310 , train loss:  0.0446996485303785 , test loss:  0.08445196633461441\n",
      "Epoch:  73320 , step:  73320 , train loss:  0.04469729633495776 , test loss:  0.08445157002639476\n",
      "Epoch:  73330 , step:  73330 , train loss:  0.044694944605810326 , test loss:  0.08445117399607208\n",
      "Epoch:  73340 , step:  73340 , train loss:  0.044692593342781146 , test loss:  0.0844507782435331\n",
      "Epoch:  73350 , step:  73350 , train loss:  0.04469024254571518 , test loss:  0.08445038276866512\n",
      "Epoch:  73360 , step:  73360 , train loss:  0.044687892214457535 , test loss:  0.08444998757135523\n",
      "Epoch:  73370 , step:  73370 , train loss:  0.044685542348853295 , test loss:  0.08444959265149066\n",
      "Epoch:  73380 , step:  73380 , train loss:  0.044683192948747716 , test loss:  0.08444919800895861\n",
      "Epoch:  73390 , step:  73390 , train loss:  0.044680844013986046 , test loss:  0.0844488036436466\n",
      "Epoch:  73400 , step:  73400 , train loss:  0.044678495544413666 , test loss:  0.08444840955544197\n",
      "Epoch:  73410 , step:  73410 , train loss:  0.04467614753987598 , test loss:  0.08444801574423204\n",
      "Epoch:  73420 , step:  73420 , train loss:  0.04467380000021851 , test loss:  0.08444762220990451\n",
      "Epoch:  73430 , step:  73430 , train loss:  0.04467145292528682 , test loss:  0.08444722895234685\n",
      "Epoch:  73440 , step:  73440 , train loss:  0.04466910631492655 , test loss:  0.08444683597144657\n",
      "Epoch:  73450 , step:  73450 , train loss:  0.044666760168983445 , test loss:  0.0844464432670917\n",
      "Epoch:  73460 , step:  73460 , train loss:  0.04466441448730325 , test loss:  0.08444605083916976\n",
      "Epoch:  73470 , step:  73470 , train loss:  0.044662069269731854 , test loss:  0.08444565868756868\n",
      "Epoch:  73480 , step:  73480 , train loss:  0.0446597245161152 , test loss:  0.08444526681217608\n",
      "Epoch:  73490 , step:  73490 , train loss:  0.04465738022629925 , test loss:  0.08444487521288008\n",
      "Epoch:  73500 , step:  73500 , train loss:  0.04465503640013012 , test loss:  0.0844444838895686\n",
      "Epoch:  73510 , step:  73510 , train loss:  0.04465269303745395 , test loss:  0.08444409284212974\n",
      "Epoch:  73520 , step:  73520 , train loss:  0.04465035013811695 , test loss:  0.08444370207045152\n",
      "Epoch:  73530 , step:  73530 , train loss:  0.04464800770196543 , test loss:  0.08444331157442221\n",
      "Epoch:  73540 , step:  73540 , train loss:  0.04464566572884572 , test loss:  0.08444292135392992\n",
      "Epoch:  73550 , step:  73550 , train loss:  0.044643324218604334 , test loss:  0.08444253140886288\n",
      "Epoch:  73560 , step:  73560 , train loss:  0.044640983171087674 , test loss:  0.08444214173910952\n",
      "Epoch:  73570 , step:  73570 , train loss:  0.04463864258614239 , test loss:  0.08444175234455824\n",
      "Epoch:  73580 , step:  73580 , train loss:  0.044636302463615135 , test loss:  0.08444136322509738\n",
      "Epoch:  73590 , step:  73590 , train loss:  0.044633962803352606 , test loss:  0.08444097438061564\n",
      "Epoch:  73600 , step:  73600 , train loss:  0.044631623605201606 , test loss:  0.08444058581100126\n",
      "Epoch:  73610 , step:  73610 , train loss:  0.044629284869008995 , test loss:  0.08444019751614325\n",
      "Epoch:  73620 , step:  73620 , train loss:  0.044626946594621714 , test loss:  0.08443980949593\n",
      "Epoch:  73630 , step:  73630 , train loss:  0.04462460878188678 , test loss:  0.08443942175025036\n",
      "Epoch:  73640 , step:  73640 , train loss:  0.044622271430651284 , test loss:  0.08443903427899306\n",
      "Epoch:  73650 , step:  73650 , train loss:  0.04461993454076235 , test loss:  0.08443864708204697\n",
      "Epoch:  73660 , step:  73660 , train loss:  0.04461759811206721 , test loss:  0.08443826015930114\n",
      "Epoch:  73670 , step:  73670 , train loss:  0.04461526214441317 , test loss:  0.08443787351064422\n",
      "Epoch:  73680 , step:  73680 , train loss:  0.044612926637647585 , test loss:  0.08443748713596544\n",
      "Epoch:  73690 , step:  73690 , train loss:  0.04461059159161789 , test loss:  0.08443710103515367\n",
      "Epoch:  73700 , step:  73700 , train loss:  0.0446082570061716 , test loss:  0.08443671520809844\n",
      "Epoch:  73710 , step:  73710 , train loss:  0.0446059228811563 , test loss:  0.08443632965468857\n",
      "Epoch:  73720 , step:  73720 , train loss:  0.044603589216419624 , test loss:  0.08443594437481372\n",
      "Epoch:  73730 , step:  73730 , train loss:  0.044601256011809295 , test loss:  0.08443555936836285\n",
      "Epoch:  73740 , step:  73740 , train loss:  0.04459892326717311 , test loss:  0.08443517463522528\n",
      "Epoch:  73750 , step:  73750 , train loss:  0.04459659098235893 , test loss:  0.08443479017529062\n",
      "Epoch:  73760 , step:  73760 , train loss:  0.04459425915721471 , test loss:  0.08443440598844842\n",
      "Epoch:  73770 , step:  73770 , train loss:  0.044591927791588416 , test loss:  0.08443402207458799\n",
      "Epoch:  73780 , step:  73780 , train loss:  0.044589596885328146 , test loss:  0.08443363843359912\n",
      "Epoch:  73790 , step:  73790 , train loss:  0.04458726643828204 , test loss:  0.08443325506537125\n",
      "Epoch:  73800 , step:  73800 , train loss:  0.04458493645029831 , test loss:  0.08443287196979424\n",
      "Epoch:  73810 , step:  73810 , train loss:  0.044582606921225267 , test loss:  0.08443248914675798\n",
      "Epoch:  73820 , step:  73820 , train loss:  0.04458027785091124 , test loss:  0.08443210659615194\n",
      "Epoch:  73830 , step:  73830 , train loss:  0.044577949239204676 , test loss:  0.08443172431786634\n",
      "Epoch:  73840 , step:  73840 , train loss:  0.044575621085954066 , test loss:  0.08443134231179086\n",
      "Epoch:  73850 , step:  73850 , train loss:  0.04457329339100798 , test loss:  0.08443096057781567\n",
      "Epoch:  73860 , step:  73860 , train loss:  0.04457096615421506 , test loss:  0.08443057911583067\n",
      "Epoch:  73870 , step:  73870 , train loss:  0.04456863937542402 , test loss:  0.08443019792572619\n",
      "Epoch:  73880 , step:  73880 , train loss:  0.04456631305448363 , test loss:  0.08442981700739209\n",
      "Epoch:  73890 , step:  73890 , train loss:  0.04456398719124275 , test loss:  0.08442943636071877\n",
      "Epoch:  73900 , step:  73900 , train loss:  0.044561661785550294 , test loss:  0.08442905598559645\n",
      "Epoch:  73910 , step:  73910 , train loss:  0.04455933683725527 , test loss:  0.08442867588191572\n",
      "Epoch:  73920 , step:  73920 , train loss:  0.04455701234620672 , test loss:  0.08442829604956653\n",
      "Epoch:  73930 , step:  73930 , train loss:  0.04455468831225377 , test loss:  0.08442791648843963\n",
      "Epoch:  73940 , step:  73940 , train loss:  0.04455236473524565 , test loss:  0.08442753719842548\n",
      "Epoch:  73950 , step:  73950 , train loss:  0.04455004161503162 , test loss:  0.08442715817941447\n",
      "Epoch:  73960 , step:  73960 , train loss:  0.044547718951461014 , test loss:  0.08442677943129737\n",
      "Epoch:  73970 , step:  73970 , train loss:  0.04454539674438325 , test loss:  0.08442640095396486\n",
      "Epoch:  73980 , step:  73980 , train loss:  0.04454307499364782 , test loss:  0.08442602274730748\n",
      "Epoch:  73990 , step:  73990 , train loss:  0.04454075369910426 , test loss:  0.08442564481121624\n",
      "Epoch:  74000 , step:  74000 , train loss:  0.04453843286060219 , test loss:  0.08442526714558207\n",
      "Epoch:  74010 , step:  74010 , train loss:  0.044536112477991316 , test loss:  0.08442488975029555\n",
      "Epoch:  74020 , step:  74020 , train loss:  0.044533792551121394 , test loss:  0.08442451262524782\n",
      "Epoch:  74030 , step:  74030 , train loss:  0.044531473079842246 , test loss:  0.08442413577032976\n",
      "Epoch:  74040 , step:  74040 , train loss:  0.04452915406400379 , test loss:  0.08442375918543261\n",
      "Epoch:  74050 , step:  74050 , train loss:  0.044526835503456 , test loss:  0.08442338287044729\n",
      "Epoch:  74060 , step:  74060 , train loss:  0.04452451739804888 , test loss:  0.08442300682526512\n",
      "Epoch:  74070 , step:  74070 , train loss:  0.044522199747632545 , test loss:  0.08442263104977715\n",
      "Epoch:  74080 , step:  74080 , train loss:  0.04451988255205722 , test loss:  0.08442225554387503\n",
      "Epoch:  74090 , step:  74090 , train loss:  0.04451756581117313 , test loss:  0.0844218803074498\n",
      "Epoch:  74100 , step:  74100 , train loss:  0.044515249524830554 , test loss:  0.0844215053403931\n",
      "Epoch:  74110 , step:  74110 , train loss:  0.04451293369287994 , test loss:  0.08442113064259617\n",
      "Epoch:  74120 , step:  74120 , train loss:  0.044510618315171696 , test loss:  0.08442075621395047\n",
      "Epoch:  74130 , step:  74130 , train loss:  0.04450830339155638 , test loss:  0.08442038205434774\n",
      "Epoch:  74140 , step:  74140 , train loss:  0.044505988921884586 , test loss:  0.08442000816367962\n",
      "Epoch:  74150 , step:  74150 , train loss:  0.04450367490600695 , test loss:  0.08441963454183764\n",
      "Epoch:  74160 , step:  74160 , train loss:  0.04450136134377423 , test loss:  0.08441926118871368\n",
      "Epoch:  74170 , step:  74170 , train loss:  0.04449904823503724 , test loss:  0.08441888810419941\n",
      "Epoch:  74180 , step:  74180 , train loss:  0.04449673557964681 , test loss:  0.08441851528818674\n",
      "Epoch:  74190 , step:  74190 , train loss:  0.04449442337745391 , test loss:  0.08441814274056768\n",
      "Epoch:  74200 , step:  74200 , train loss:  0.04449211162830955 , test loss:  0.08441777046123397\n",
      "Epoch:  74210 , step:  74210 , train loss:  0.044489800332064795 , test loss:  0.08441739845007766\n",
      "Epoch:  74220 , step:  74220 , train loss:  0.04448748948857083 , test loss:  0.08441702670699094\n",
      "Epoch:  74230 , step:  74230 , train loss:  0.04448517909767881 , test loss:  0.08441665523186602\n",
      "Epoch:  74240 , step:  74240 , train loss:  0.04448286915924008 , test loss:  0.08441628402459479\n",
      "Epoch:  74250 , step:  74250 , train loss:  0.04448055967310596 , test loss:  0.08441591308506961\n",
      "Epoch:  74260 , step:  74260 , train loss:  0.04447825063912788 , test loss:  0.0844155424131829\n",
      "Epoch:  74270 , step:  74270 , train loss:  0.04447594205715732 , test loss:  0.0844151720088267\n",
      "Epoch:  74280 , step:  74280 , train loss:  0.04447363392704588 , test loss:  0.08441480187189389\n",
      "Epoch:  74290 , step:  74290 , train loss:  0.044471326248645145 , test loss:  0.08441443200227647\n",
      "Epoch:  74300 , step:  74300 , train loss:  0.044469019021806847 , test loss:  0.08441406239986714\n",
      "Epoch:  74310 , step:  74310 , train loss:  0.044466712246382725 , test loss:  0.08441369306455854\n",
      "Epoch:  74320 , step:  74320 , train loss:  0.04446440592222462 , test loss:  0.08441332399624306\n",
      "Epoch:  74330 , step:  74330 , train loss:  0.04446210004918445 , test loss:  0.08441295519481354\n",
      "Epoch:  74340 , step:  74340 , train loss:  0.0444597946271142 , test loss:  0.0844125866601627\n",
      "Epoch:  74350 , step:  74350 , train loss:  0.04445748965586586 , test loss:  0.08441221839218338\n",
      "Epoch:  74360 , step:  74360 , train loss:  0.04445518513529156 , test loss:  0.0844118503907683\n",
      "Epoch:  74370 , step:  74370 , train loss:  0.04445288106524351 , test loss:  0.08441148265581049\n",
      "Epoch:  74380 , step:  74380 , train loss:  0.044450577445573916 , test loss:  0.08441111518720279\n",
      "Epoch:  74390 , step:  74390 , train loss:  0.0444482742761351 , test loss:  0.08441074798483823\n",
      "Epoch:  74400 , step:  74400 , train loss:  0.04444597155677945 , test loss:  0.08441038104860993\n",
      "Epoch:  74410 , step:  74410 , train loss:  0.04444366928735942 , test loss:  0.08441001437841084\n",
      "Epoch:  74420 , step:  74420 , train loss:  0.04444136746772752 , test loss:  0.08440964797413421\n",
      "Epoch:  74430 , step:  74430 , train loss:  0.044439066097736325 , test loss:  0.08440928183567348\n",
      "Epoch:  74440 , step:  74440 , train loss:  0.0444367651772385 , test loss:  0.08440891596292158\n",
      "Epoch:  74450 , step:  74450 , train loss:  0.04443446470608678 , test loss:  0.08440855035577216\n",
      "Epoch:  74460 , step:  74460 , train loss:  0.04443216468413393 , test loss:  0.08440818501411845\n",
      "Epoch:  74470 , step:  74470 , train loss:  0.04442986511123282 , test loss:  0.08440781993785387\n",
      "Epoch:  74480 , step:  74480 , train loss:  0.044427565987236385 , test loss:  0.08440745512687196\n",
      "Epoch:  74490 , step:  74490 , train loss:  0.04442526731199763 , test loss:  0.08440709058106624\n",
      "Epoch:  74500 , step:  74500 , train loss:  0.04442296908536956 , test loss:  0.08440672630033036\n",
      "Epoch:  74510 , step:  74510 , train loss:  0.044420671307205346 , test loss:  0.08440636228455813\n",
      "Epoch:  74520 , step:  74520 , train loss:  0.04441837397735821 , test loss:  0.084405998533643\n",
      "Epoch:  74530 , step:  74530 , train loss:  0.044416077095681364 , test loss:  0.08440563504747883\n",
      "Epoch:  74540 , step:  74540 , train loss:  0.044413780662028175 , test loss:  0.08440527182595935\n",
      "Epoch:  74550 , step:  74550 , train loss:  0.04441148467625199 , test loss:  0.08440490886897851\n",
      "Epoch:  74560 , step:  74560 , train loss:  0.044409189138206344 , test loss:  0.08440454617643053\n",
      "Epoch:  74570 , step:  74570 , train loss:  0.04440689404774475 , test loss:  0.08440418374820909\n",
      "Epoch:  74580 , step:  74580 , train loss:  0.044404599404720795 , test loss:  0.0844038215842082\n",
      "Epoch:  74590 , step:  74590 , train loss:  0.04440230520898815 , test loss:  0.08440345968432215\n",
      "Epoch:  74600 , step:  74600 , train loss:  0.04440001146040056 , test loss:  0.0844030980484449\n",
      "Epoch:  74610 , step:  74610 , train loss:  0.04439771815881184 , test loss:  0.08440273667647079\n",
      "Epoch:  74620 , step:  74620 , train loss:  0.044395425304075836 , test loss:  0.08440237556829405\n",
      "Epoch:  74630 , step:  74630 , train loss:  0.04439313289604651 , test loss:  0.08440201472380894\n",
      "Epoch:  74640 , step:  74640 , train loss:  0.04439084093457785 , test loss:  0.0844016541429099\n",
      "Epoch:  74650 , step:  74650 , train loss:  0.04438854941952393 , test loss:  0.08440129382549125\n",
      "Epoch:  74660 , step:  74660 , train loss:  0.04438625835073891 , test loss:  0.08440093377144757\n",
      "Epoch:  74670 , step:  74670 , train loss:  0.044383967728077005 , test loss:  0.08440057398067338\n",
      "Epoch:  74680 , step:  74680 , train loss:  0.04438167755139245 , test loss:  0.08440021445306312\n",
      "Epoch:  74690 , step:  74690 , train loss:  0.044379387820539594 , test loss:  0.08439985518851158\n",
      "Epoch:  74700 , step:  74700 , train loss:  0.04437709853537287 , test loss:  0.08439949618691349\n",
      "Epoch:  74710 , step:  74710 , train loss:  0.04437480969574675 , test loss:  0.08439913744816335\n",
      "Epoch:  74720 , step:  74720 , train loss:  0.04437252130151574 , test loss:  0.08439877897215618\n",
      "Epoch:  74730 , step:  74730 , train loss:  0.04437023335253451 , test loss:  0.08439842075878673\n",
      "Epoch:  74740 , step:  74740 , train loss:  0.044367945848657685 , test loss:  0.08439806280795002\n",
      "Epoch:  74750 , step:  74750 , train loss:  0.04436565878974005 , test loss:  0.08439770511954094\n",
      "Epoch:  74760 , step:  74760 , train loss:  0.044363372175636365 , test loss:  0.08439734769345437\n",
      "Epoch:  74770 , step:  74770 , train loss:  0.04436108600620156 , test loss:  0.08439699052958557\n",
      "Epoch:  74780 , step:  74780 , train loss:  0.04435880028129053 , test loss:  0.08439663362782948\n",
      "Epoch:  74790 , step:  74790 , train loss:  0.04435651500075831 , test loss:  0.08439627698808147\n",
      "Epoch:  74800 , step:  74800 , train loss:  0.04435423016445998 , test loss:  0.08439592061023662\n",
      "Epoch:  74810 , step:  74810 , train loss:  0.044351945772250656 , test loss:  0.0843955644941903\n",
      "Epoch:  74820 , step:  74820 , train loss:  0.04434966182398556 , test loss:  0.08439520863983781\n",
      "Epoch:  74830 , step:  74830 , train loss:  0.044347378319520014 , test loss:  0.08439485304707452\n",
      "Epoch:  74840 , step:  74840 , train loss:  0.04434509525870928 , test loss:  0.08439449771579578\n",
      "Epoch:  74850 , step:  74850 , train loss:  0.044342812641408824 , test loss:  0.08439414264589733\n",
      "Epoch:  74860 , step:  74860 , train loss:  0.04434053046747411 , test loss:  0.0843937878372745\n",
      "Epoch:  74870 , step:  74870 , train loss:  0.044338248736760656 , test loss:  0.0843934332898229\n",
      "Epoch:  74880 , step:  74880 , train loss:  0.044335967449124124 , test loss:  0.08439307900343826\n",
      "Epoch:  74890 , step:  74890 , train loss:  0.0443336866044201 , test loss:  0.08439272497801634\n",
      "Epoch:  74900 , step:  74900 , train loss:  0.04433140620250439 , test loss:  0.08439237121345278\n",
      "Epoch:  74910 , step:  74910 , train loss:  0.0443291262432328 , test loss:  0.0843920177096434\n",
      "Epoch:  74920 , step:  74920 , train loss:  0.04432684672646118 , test loss:  0.08439166446648405\n",
      "Epoch:  74930 , step:  74930 , train loss:  0.04432456765204546 , test loss:  0.08439131148387069\n",
      "Epoch:  74940 , step:  74940 , train loss:  0.04432228901984169 , test loss:  0.08439095876169932\n",
      "Epoch:  74950 , step:  74950 , train loss:  0.04432001082970587 , test loss:  0.08439060629986586\n",
      "Epoch:  74960 , step:  74960 , train loss:  0.044317733081494205 , test loss:  0.08439025409826649\n",
      "Epoch:  74970 , step:  74970 , train loss:  0.04431545577506286 , test loss:  0.08438990215679744\n",
      "Epoch:  74980 , step:  74980 , train loss:  0.0443131789102681 , test loss:  0.08438955047535467\n",
      "Epoch:  74990 , step:  74990 , train loss:  0.04431090248696629 , test loss:  0.08438919905383446\n",
      "Epoch:  75000 , step:  75000 , train loss:  0.04430862650501379 , test loss:  0.0843888478921332\n",
      "Epoch:  75010 , step:  75010 , train loss:  0.044306350964267105 , test loss:  0.0843884969901471\n",
      "Epoch:  75020 , step:  75020 , train loss:  0.04430407586458274 , test loss:  0.08438814634777272\n",
      "Epoch:  75030 , step:  75030 , train loss:  0.0443018012058173 , test loss:  0.08438779596490649\n",
      "Epoch:  75040 , step:  75040 , train loss:  0.04429952698782745 , test loss:  0.08438744584144453\n",
      "Epoch:  75050 , step:  75050 , train loss:  0.04429725321046991 , test loss:  0.08438709597728389\n",
      "Epoch:  75060 , step:  75060 , train loss:  0.044294979873601446 , test loss:  0.08438674637232099\n",
      "Epoch:  75070 , step:  75070 , train loss:  0.04429270697707899 , test loss:  0.08438639702645234\n",
      "Epoch:  75080 , step:  75080 , train loss:  0.044290434520759425 , test loss:  0.08438604793957484\n",
      "Epoch:  75090 , step:  75090 , train loss:  0.04428816250449971 , test loss:  0.08438569911158515\n",
      "Epoch:  75100 , step:  75100 , train loss:  0.04428589092815695 , test loss:  0.08438535054238021\n",
      "Epoch:  75110 , step:  75110 , train loss:  0.04428361979158826 , test loss:  0.0843850022318569\n",
      "Epoch:  75120 , step:  75120 , train loss:  0.044281349094650806 , test loss:  0.08438465417991191\n",
      "Epoch:  75130 , step:  75130 , train loss:  0.044279078837201856 , test loss:  0.08438430638644223\n",
      "Epoch:  75140 , step:  75140 , train loss:  0.04427680901909871 , test loss:  0.08438395885134502\n",
      "Epoch:  75150 , step:  75150 , train loss:  0.04427453964019878 , test loss:  0.08438361157451722\n",
      "Epoch:  75160 , step:  75160 , train loss:  0.04427227070035947 , test loss:  0.08438326455585622\n",
      "Epoch:  75170 , step:  75170 , train loss:  0.04427000219943833 , test loss:  0.08438291779525904\n",
      "Epoch:  75180 , step:  75180 , train loss:  0.044267734137292895 , test loss:  0.08438257129262298\n",
      "Epoch:  75190 , step:  75190 , train loss:  0.04426546651378087 , test loss:  0.08438222504784518\n",
      "Epoch:  75200 , step:  75200 , train loss:  0.044263199328759904 , test loss:  0.08438187906082308\n",
      "Epoch:  75210 , step:  75210 , train loss:  0.04426093258208781 , test loss:  0.08438153333145387\n",
      "Epoch:  75220 , step:  75220 , train loss:  0.044258666273622405 , test loss:  0.0843811878596353\n",
      "Epoch:  75230 , step:  75230 , train loss:  0.04425640040322159 , test loss:  0.08438084264526463\n",
      "Epoch:  75240 , step:  75240 , train loss:  0.04425413497074333 , test loss:  0.08438049768823948\n",
      "Epoch:  75250 , step:  75250 , train loss:  0.044251869976045674 , test loss:  0.08438015298845754\n",
      "Epoch:  75260 , step:  75260 , train loss:  0.04424960541898671 , test loss:  0.08437980854581638\n",
      "Epoch:  75270 , step:  75270 , train loss:  0.044247341299424627 , test loss:  0.08437946436021358\n",
      "Epoch:  75280 , step:  75280 , train loss:  0.0442450776172176 , test loss:  0.08437912043154713\n",
      "Epoch:  75290 , step:  75290 , train loss:  0.04424281437222395 , test loss:  0.08437877675971472\n",
      "Epoch:  75300 , step:  75300 , train loss:  0.04424055156430201 , test loss:  0.08437843334461415\n",
      "Epoch:  75310 , step:  75310 , train loss:  0.04423828919331023 , test loss:  0.0843780901861434\n",
      "Epoch:  75320 , step:  75320 , train loss:  0.04423602725910709 , test loss:  0.08437774728420022\n",
      "Epoch:  75330 , step:  75330 , train loss:  0.04423376576155112 , test loss:  0.08437740463868298\n",
      "Epoch:  75340 , step:  75340 , train loss:  0.044231504700500975 , test loss:  0.0843770622494895\n",
      "Epoch:  75350 , step:  75350 , train loss:  0.04422924407581527 , test loss:  0.08437672011651794\n",
      "Epoch:  75360 , step:  75360 , train loss:  0.044226983887352786 , test loss:  0.08437637823966652\n",
      "Epoch:  75370 , step:  75370 , train loss:  0.04422472413497233 , test loss:  0.08437603661883335\n",
      "Epoch:  75380 , step:  75380 , train loss:  0.044222464818532786 , test loss:  0.08437569525391686\n",
      "Epoch:  75390 , step:  75390 , train loss:  0.04422020593789305 , test loss:  0.08437535414481533\n",
      "Epoch:  75400 , step:  75400 , train loss:  0.044217947492912184 , test loss:  0.08437501329142695\n",
      "Epoch:  75410 , step:  75410 , train loss:  0.04421568948344919 , test loss:  0.08437467269365022\n",
      "Epoch:  75420 , step:  75420 , train loss:  0.04421343190936323 , test loss:  0.08437433235138378\n",
      "Epoch:  75430 , step:  75430 , train loss:  0.04421117477051348 , test loss:  0.08437399226452606\n",
      "Epoch:  75440 , step:  75440 , train loss:  0.044208918066759216 , test loss:  0.08437365243297552\n",
      "Epoch:  75450 , step:  75450 , train loss:  0.044206661797959734 , test loss:  0.08437331285663079\n",
      "Epoch:  75460 , step:  75460 , train loss:  0.04420440596397444 , test loss:  0.08437297353539083\n",
      "Epoch:  75470 , step:  75470 , train loss:  0.044202150564662784 , test loss:  0.08437263446915394\n",
      "Epoch:  75480 , step:  75480 , train loss:  0.04419989559988424 , test loss:  0.08437229565781929\n",
      "Epoch:  75490 , step:  75490 , train loss:  0.04419764106949846 , test loss:  0.08437195710128545\n",
      "Epoch:  75500 , step:  75500 , train loss:  0.044195386973365 , test loss:  0.08437161879945157\n",
      "Epoch:  75510 , step:  75510 , train loss:  0.04419313331134363 , test loss:  0.08437128075221634\n",
      "Epoch:  75520 , step:  75520 , train loss:  0.044190880083294075 , test loss:  0.08437094295947882\n",
      "Epoch:  75530 , step:  75530 , train loss:  0.0441886272890762 , test loss:  0.08437060542113806\n",
      "Epoch:  75540 , step:  75540 , train loss:  0.04418637492854987 , test loss:  0.0843702681370932\n",
      "Epoch:  75550 , step:  75550 , train loss:  0.04418412300157506 , test loss:  0.08436993110724327\n",
      "Epoch:  75560 , step:  75560 , train loss:  0.04418187150801181 , test loss:  0.0843695943314875\n",
      "Epoch:  75570 , step:  75570 , train loss:  0.04417962044772019 , test loss:  0.08436925780972522\n",
      "Epoch:  75580 , step:  75580 , train loss:  0.044177369820560336 , test loss:  0.08436892154185563\n",
      "Epoch:  75590 , step:  75590 , train loss:  0.0441751196263925 , test loss:  0.08436858552777812\n",
      "Epoch:  75600 , step:  75600 , train loss:  0.044172869865076916 , test loss:  0.08436824976739198\n",
      "Epoch:  75610 , step:  75610 , train loss:  0.044170620536473956 , test loss:  0.08436791426059682\n",
      "Epoch:  75620 , step:  75620 , train loss:  0.044168371640444015 , test loss:  0.0843675790072919\n",
      "Epoch:  75630 , step:  75630 , train loss:  0.044166123176847545 , test loss:  0.08436724400737694\n",
      "Epoch:  75640 , step:  75640 , train loss:  0.04416387514554509 , test loss:  0.08436690926075152\n",
      "Epoch:  75650 , step:  75650 , train loss:  0.04416162754639726 , test loss:  0.08436657476731524\n",
      "Epoch:  75660 , step:  75660 , train loss:  0.044159380379264694 , test loss:  0.08436624052696792\n",
      "Epoch:  75670 , step:  75670 , train loss:  0.04415713364400811 , test loss:  0.08436590653960918\n",
      "Epoch:  75680 , step:  75680 , train loss:  0.04415488734048832 , test loss:  0.0843655728051388\n",
      "Epoch:  75690 , step:  75690 , train loss:  0.04415264146856614 , test loss:  0.08436523932345674\n",
      "Epoch:  75700 , step:  75700 , train loss:  0.044150396028102494 , test loss:  0.08436490609446276\n",
      "Epoch:  75710 , step:  75710 , train loss:  0.04414815101895835 , test loss:  0.08436457311805685\n",
      "Epoch:  75720 , step:  75720 , train loss:  0.04414590644099476 , test loss:  0.08436424039413912\n",
      "Epoch:  75730 , step:  75730 , train loss:  0.0441436622940728 , test loss:  0.08436390792260957\n",
      "Epoch:  75740 , step:  75740 , train loss:  0.04414141857805365 , test loss:  0.08436357570336823\n",
      "Epoch:  75750 , step:  75750 , train loss:  0.04413917529279853 , test loss:  0.08436324373631542\n",
      "Epoch:  75760 , step:  75760 , train loss:  0.044136932438168734 , test loss:  0.08436291202135124\n",
      "Epoch:  75770 , step:  75770 , train loss:  0.0441346900140256 , test loss:  0.08436258055837575\n",
      "Epoch:  75780 , step:  75780 , train loss:  0.044132448020230565 , test loss:  0.08436224934728952\n",
      "Epoch:  75790 , step:  75790 , train loss:  0.04413020645664508 , test loss:  0.0843619183879927\n",
      "Epoch:  75800 , step:  75800 , train loss:  0.044127965323130694 , test loss:  0.08436158768038579\n",
      "Epoch:  75810 , step:  75810 , train loss:  0.044125724619549014 , test loss:  0.08436125722436928\n",
      "Epoch:  75820 , step:  75820 , train loss:  0.04412348434576171 , test loss:  0.0843609270198437\n",
      "Epoch:  75830 , step:  75830 , train loss:  0.04412124450163049 , test loss:  0.08436059706670947\n",
      "Epoch:  75840 , step:  75840 , train loss:  0.044119005087017195 , test loss:  0.08436026736486733\n",
      "Epoch:  75850 , step:  75850 , train loss:  0.044116766101783604 , test loss:  0.0843599379142179\n",
      "Epoch:  75860 , step:  75860 , train loss:  0.04411452754579171 , test loss:  0.08435960871466176\n",
      "Epoch:  75870 , step:  75870 , train loss:  0.044112289418903425 , test loss:  0.08435927976609975\n",
      "Epoch:  75880 , step:  75880 , train loss:  0.044110051720980786 , test loss:  0.08435895106843282\n",
      "Epoch:  75890 , step:  75890 , train loss:  0.04410781445188599 , test loss:  0.08435862262156163\n",
      "Epoch:  75900 , step:  75900 , train loss:  0.044105577611481095 , test loss:  0.08435829442538706\n",
      "Epoch:  75910 , step:  75910 , train loss:  0.04410334119962842 , test loss:  0.0843579664798101\n",
      "Epoch:  75920 , step:  75920 , train loss:  0.04410110521619016 , test loss:  0.08435763878473193\n",
      "Epoch:  75930 , step:  75930 , train loss:  0.04409886966102874 , test loss:  0.08435731134005334\n",
      "Epoch:  75940 , step:  75940 , train loss:  0.04409663453400656 , test loss:  0.08435698414567554\n",
      "Epoch:  75950 , step:  75950 , train loss:  0.044094399834986066 , test loss:  0.0843566572014996\n",
      "Epoch:  75960 , step:  75960 , train loss:  0.044092165563829826 , test loss:  0.08435633050742695\n",
      "Epoch:  75970 , step:  75970 , train loss:  0.044089931720400426 , test loss:  0.08435600406335865\n",
      "Epoch:  75980 , step:  75980 , train loss:  0.04408769830456056 , test loss:  0.0843556778691961\n",
      "Epoch:  75990 , step:  75990 , train loss:  0.04408546531617292 , test loss:  0.08435535192484049\n",
      "Epoch:  76000 , step:  76000 , train loss:  0.044083232755100316 , test loss:  0.08435502623019346\n",
      "Epoch:  76010 , step:  76010 , train loss:  0.04408100062120556 , test loss:  0.08435470078515625\n",
      "Epoch:  76020 , step:  76020 , train loss:  0.04407876891435161 , test loss:  0.08435437558963038\n",
      "Epoch:  76030 , step:  76030 , train loss:  0.04407653763440143 , test loss:  0.08435405064351753\n",
      "Epoch:  76040 , step:  76040 , train loss:  0.04407430678121805 , test loss:  0.08435372594671918\n",
      "Epoch:  76050 , step:  76050 , train loss:  0.044072076354664536 , test loss:  0.08435340149913693\n",
      "Epoch:  76060 , step:  76060 , train loss:  0.04406984635460408 , test loss:  0.08435307730067265\n",
      "Epoch:  76070 , step:  76070 , train loss:  0.04406761678089991 , test loss:  0.08435275335122812\n",
      "Epoch:  76080 , step:  76080 , train loss:  0.044065387633415276 , test loss:  0.08435242965070486\n",
      "Epoch:  76090 , step:  76090 , train loss:  0.044063158912013574 , test loss:  0.0843521061990047\n",
      "Epoch:  76100 , step:  76100 , train loss:  0.04406093061655814 , test loss:  0.08435178299602998\n",
      "Epoch:  76110 , step:  76110 , train loss:  0.04405870274691253 , test loss:  0.08435146004168223\n",
      "Epoch:  76120 , step:  76120 , train loss:  0.0440564753029402 , test loss:  0.08435113733586355\n",
      "Epoch:  76130 , step:  76130 , train loss:  0.04405424828450473 , test loss:  0.08435081487847595\n",
      "Epoch:  76140 , step:  76140 , train loss:  0.04405202169146983 , test loss:  0.08435049266942153\n",
      "Epoch:  76150 , step:  76150 , train loss:  0.04404979552369919 , test loss:  0.08435017070860254\n",
      "Epoch:  76160 , step:  76160 , train loss:  0.04404756978105659 , test loss:  0.084349848995921\n",
      "Epoch:  76170 , step:  76170 , train loss:  0.044045344463405856 , test loss:  0.08434952753127929\n",
      "Epoch:  76180 , step:  76180 , train loss:  0.04404311957061089 , test loss:  0.08434920631457972\n",
      "Epoch:  76190 , step:  76190 , train loss:  0.04404089510253568 , test loss:  0.08434888534572443\n",
      "Epoch:  76200 , step:  76200 , train loss:  0.0440386710590442 , test loss:  0.08434856462461597\n",
      "Epoch:  76210 , step:  76210 , train loss:  0.04403644744000054 , test loss:  0.08434824415115676\n",
      "Epoch:  76220 , step:  76220 , train loss:  0.044034224245268865 , test loss:  0.08434792392524915\n",
      "Epoch:  76230 , step:  76230 , train loss:  0.044032001474713374 , test loss:  0.08434760394679579\n",
      "Epoch:  76240 , step:  76240 , train loss:  0.044029779128198325 , test loss:  0.08434728421569923\n",
      "Epoch:  76250 , step:  76250 , train loss:  0.04402755720558805 , test loss:  0.08434696473186191\n",
      "Epoch:  76260 , step:  76260 , train loss:  0.04402533570674694 , test loss:  0.0843466454951868\n",
      "Epoch:  76270 , step:  76270 , train loss:  0.04402311463153945 , test loss:  0.08434632650557652\n",
      "Epoch:  76280 , step:  76280 , train loss:  0.04402089397983007 , test loss:  0.0843460077629338\n",
      "Epoch:  76290 , step:  76290 , train loss:  0.04401867375148337 , test loss:  0.08434568926716161\n",
      "Epoch:  76300 , step:  76300 , train loss:  0.04401645394636399 , test loss:  0.08434537101816274\n",
      "Epoch:  76310 , step:  76310 , train loss:  0.044014234564336646 , test loss:  0.08434505301583982\n",
      "Epoch:  76320 , step:  76320 , train loss:  0.04401201560526606 , test loss:  0.0843447352600962\n",
      "Epoch:  76330 , step:  76330 , train loss:  0.04400979706901705 , test loss:  0.08434441775083482\n",
      "Epoch:  76340 , step:  76340 , train loss:  0.04400757895545451 , test loss:  0.08434410048795866\n",
      "Epoch:  76350 , step:  76350 , train loss:  0.04400536126444337 , test loss:  0.0843437834713708\n",
      "Epoch:  76360 , step:  76360 , train loss:  0.044003143995848634 , test loss:  0.08434346670097458\n",
      "Epoch:  76370 , step:  76370 , train loss:  0.04400092714953532 , test loss:  0.084343150176673\n",
      "Epoch:  76380 , step:  76380 , train loss:  0.0439987107253686 , test loss:  0.08434283389836951\n",
      "Epoch:  76390 , step:  76390 , train loss:  0.043996494723213625 , test loss:  0.08434251786596722\n",
      "Epoch:  76400 , step:  76400 , train loss:  0.04399427914293562 , test loss:  0.08434220207936971\n",
      "Epoch:  76410 , step:  76410 , train loss:  0.04399206398439994 , test loss:  0.08434188653848004\n",
      "Epoch:  76420 , step:  76420 , train loss:  0.043989849247471895 , test loss:  0.08434157124320206\n",
      "Epoch:  76430 , step:  76430 , train loss:  0.04398763493201689 , test loss:  0.08434125619343903\n",
      "Epoch:  76440 , step:  76440 , train loss:  0.04398542103790047 , test loss:  0.08434094138909444\n",
      "Epoch:  76450 , step:  76450 , train loss:  0.043983207564988136 , test loss:  0.0843406268300722\n",
      "Epoch:  76460 , step:  76460 , train loss:  0.04398099451314548 , test loss:  0.08434031251627566\n",
      "Epoch:  76470 , step:  76470 , train loss:  0.04397878188223821 , test loss:  0.08433999844760856\n",
      "Epoch:  76480 , step:  76480 , train loss:  0.04397656967213202 , test loss:  0.08433968462397476\n",
      "Epoch:  76490 , step:  76490 , train loss:  0.04397435788269269 , test loss:  0.084339371045278\n",
      "Epoch:  76500 , step:  76500 , train loss:  0.043972146513786066 , test loss:  0.08433905771142197\n",
      "Epoch:  76510 , step:  76510 , train loss:  0.04396993556527806 , test loss:  0.08433874462231072\n",
      "Epoch:  76520 , step:  76520 , train loss:  0.04396772503703465 , test loss:  0.0843384317778482\n",
      "Epoch:  76530 , step:  76530 , train loss:  0.04396551492892182 , test loss:  0.0843381191779383\n",
      "Epoch:  76540 , step:  76540 , train loss:  0.043963305240805696 , test loss:  0.08433780682248498\n",
      "Epoch:  76550 , step:  76550 , train loss:  0.043961095972552384 , test loss:  0.0843374947113926\n",
      "Epoch:  76560 , step:  76560 , train loss:  0.0439588871240281 , test loss:  0.08433718284456491\n",
      "Epoch:  76570 , step:  76570 , train loss:  0.043956678695099144 , test loss:  0.08433687122190636\n",
      "Epoch:  76580 , step:  76580 , train loss:  0.043954470685631804 , test loss:  0.08433655984332103\n",
      "Epoch:  76590 , step:  76590 , train loss:  0.04395226309549249 , test loss:  0.08433624870871338\n",
      "Epoch:  76600 , step:  76600 , train loss:  0.04395005592454763 , test loss:  0.0843359378179874\n",
      "Epoch:  76610 , step:  76610 , train loss:  0.04394784917266371 , test loss:  0.0843356271710478\n",
      "Epoch:  76620 , step:  76620 , train loss:  0.04394564283970735 , test loss:  0.08433531676779851\n",
      "Epoch:  76630 , step:  76630 , train loss:  0.04394343692554509 , test loss:  0.0843350066081445\n",
      "Epoch:  76640 , step:  76640 , train loss:  0.043941231430043685 , test loss:  0.08433469669199013\n",
      "Epoch:  76650 , step:  76650 , train loss:  0.04393902635306988 , test loss:  0.08433438701923994\n",
      "Epoch:  76660 , step:  76660 , train loss:  0.043936821694490424 , test loss:  0.08433407758979829\n",
      "Epoch:  76670 , step:  76670 , train loss:  0.04393461745417221 , test loss:  0.08433376840357004\n",
      "Epoch:  76680 , step:  76680 , train loss:  0.04393241363198216 , test loss:  0.08433345946045989\n",
      "Epoch:  76690 , step:  76690 , train loss:  0.043930210227787266 , test loss:  0.08433315076037257\n",
      "Epoch:  76700 , step:  76700 , train loss:  0.04392800724145457 , test loss:  0.08433284230321288\n",
      "Epoch:  76710 , step:  76710 , train loss:  0.04392580467285114 , test loss:  0.0843325340888857\n",
      "Epoch:  76720 , step:  76720 , train loss:  0.043923602521844164 , test loss:  0.08433222611729566\n",
      "Epoch:  76730 , step:  76730 , train loss:  0.043921400788300854 , test loss:  0.0843319183883481\n",
      "Epoch:  76740 , step:  76740 , train loss:  0.0439191994720885 , test loss:  0.08433161090194763\n",
      "Epoch:  76750 , step:  76750 , train loss:  0.043916998573074434 , test loss:  0.08433130365799946\n",
      "Epoch:  76760 , step:  76760 , train loss:  0.04391479809112604 , test loss:  0.08433099665640865\n",
      "Epoch:  76770 , step:  76770 , train loss:  0.04391259802611083 , test loss:  0.08433068989708034\n",
      "Epoch:  76780 , step:  76780 , train loss:  0.04391039837789624 , test loss:  0.08433038337991956\n",
      "Epoch:  76790 , step:  76790 , train loss:  0.04390819914634992 , test loss:  0.08433007710483179\n",
      "Epoch:  76800 , step:  76800 , train loss:  0.043906000331339436 , test loss:  0.08432977107172202\n",
      "Epoch:  76810 , step:  76810 , train loss:  0.043903801932732536 , test loss:  0.08432946528049559\n",
      "Epoch:  76820 , step:  76820 , train loss:  0.04390160395039697 , test loss:  0.08432915973105798\n",
      "Epoch:  76830 , step:  76830 , train loss:  0.043899406384200486 , test loss:  0.08432885442331463\n",
      "Epoch:  76840 , step:  76840 , train loss:  0.04389720923401107 , test loss:  0.0843285493571709\n",
      "Epoch:  76850 , step:  76850 , train loss:  0.04389501249969654 , test loss:  0.08432824453253225\n",
      "Epoch:  76860 , step:  76860 , train loss:  0.04389281618112497 , test loss:  0.08432793994930425\n",
      "Epoch:  76870 , step:  76870 , train loss:  0.043890620278164365 , test loss:  0.08432763560739255\n",
      "Epoch:  76880 , step:  76880 , train loss:  0.04388842479068284 , test loss:  0.08432733150670274\n",
      "Epoch:  76890 , step:  76890 , train loss:  0.04388622971854858 , test loss:  0.08432702764714056\n",
      "Epoch:  76900 , step:  76900 , train loss:  0.04388403506162977 , test loss:  0.08432672402861151\n",
      "Epoch:  76910 , step:  76910 , train loss:  0.04388184081979474 , test loss:  0.08432642065102186\n",
      "Epoch:  76920 , step:  76920 , train loss:  0.04387964699291182 , test loss:  0.08432611751427696\n",
      "Epoch:  76930 , step:  76930 , train loss:  0.043877453580849386 , test loss:  0.08432581461828285\n",
      "Epoch:  76940 , step:  76940 , train loss:  0.043875260583475933 , test loss:  0.08432551196294567\n",
      "Epoch:  76950 , step:  76950 , train loss:  0.04387306800065993 , test loss:  0.08432520954817077\n",
      "Epoch:  76960 , step:  76960 , train loss:  0.04387087583227003 , test loss:  0.08432490737386476\n",
      "Epoch:  76970 , step:  76970 , train loss:  0.04386868407817481 , test loss:  0.08432460543993343\n",
      "Epoch:  76980 , step:  76980 , train loss:  0.043866492738242994 , test loss:  0.08432430374628302\n",
      "Epoch:  76990 , step:  76990 , train loss:  0.04386430181234331 , test loss:  0.08432400229281965\n",
      "Epoch:  77000 , step:  77000 , train loss:  0.04386211130034462 , test loss:  0.08432370107944939\n",
      "Epoch:  77010 , step:  77010 , train loss:  0.04385992120211573 , test loss:  0.08432340010607867\n",
      "Epoch:  77020 , step:  77020 , train loss:  0.04385773151752563 , test loss:  0.08432309937261374\n",
      "Epoch:  77030 , step:  77030 , train loss:  0.043855542246443266 , test loss:  0.08432279887896081\n",
      "Epoch:  77040 , step:  77040 , train loss:  0.043853353388737706 , test loss:  0.08432249862502635\n",
      "Epoch:  77050 , step:  77050 , train loss:  0.04385116494427808 , test loss:  0.08432219861071681\n",
      "Epoch:  77060 , step:  77060 , train loss:  0.04384897691293346 , test loss:  0.08432189883593863\n",
      "Epoch:  77070 , step:  77070 , train loss:  0.04384678929457316 , test loss:  0.08432159930059833\n",
      "Epoch:  77080 , step:  77080 , train loss:  0.04384460208906643 , test loss:  0.08432130000460254\n",
      "Epoch:  77090 , step:  77090 , train loss:  0.043842415296282584 , test loss:  0.08432100094785779\n",
      "Epoch:  77100 , step:  77100 , train loss:  0.04384022891609105 , test loss:  0.08432070213027087\n",
      "Epoch:  77110 , step:  77110 , train loss:  0.043838042948361265 , test loss:  0.08432040355174829\n",
      "Epoch:  77120 , step:  77120 , train loss:  0.043835857392962745 , test loss:  0.08432010521219696\n",
      "Epoch:  77130 , step:  77130 , train loss:  0.04383367224976505 , test loss:  0.08431980711152345\n",
      "Epoch:  77140 , step:  77140 , train loss:  0.04383148751863785 , test loss:  0.08431950924963513\n",
      "Epoch:  77150 , step:  77150 , train loss:  0.04382930319945079 , test loss:  0.0843192116264384\n",
      "Epoch:  77160 , step:  77160 , train loss:  0.04382711929207361 , test loss:  0.08431891424184035\n",
      "Epoch:  77170 , step:  77170 , train loss:  0.043824935796376155 , test loss:  0.08431861709574802\n",
      "Epoch:  77180 , step:  77180 , train loss:  0.04382275271222824 , test loss:  0.08431832018806838\n",
      "Epoch:  77190 , step:  77190 , train loss:  0.04382057003949981 , test loss:  0.08431802351870851\n",
      "Epoch:  77200 , step:  77200 , train loss:  0.04381838777806084 , test loss:  0.0843177270875755\n",
      "Epoch:  77210 , step:  77210 , train loss:  0.04381620592778135 , test loss:  0.08431743089457665\n",
      "Epoch:  77220 , step:  77220 , train loss:  0.04381402448853145 , test loss:  0.08431713493961918\n",
      "Epoch:  77230 , step:  77230 , train loss:  0.04381184346018128 , test loss:  0.08431683922261006\n",
      "Epoch:  77240 , step:  77240 , train loss:  0.043809662842601044 , test loss:  0.0843165437434569\n",
      "Epoch:  77250 , step:  77250 , train loss:  0.04380748263566099 , test loss:  0.0843162485020669\n",
      "Epoch:  77260 , step:  77260 , train loss:  0.04380530283923147 , test loss:  0.08431595349834754\n",
      "Epoch:  77270 , step:  77270 , train loss:  0.04380312345318286 , test loss:  0.08431565873220612\n",
      "Epoch:  77280 , step:  77280 , train loss:  0.0438009444773856 , test loss:  0.08431536420355035\n",
      "Epoch:  77290 , step:  77290 , train loss:  0.04379876591171017 , test loss:  0.0843150699122876\n",
      "Epoch:  77300 , step:  77300 , train loss:  0.043796587756027135 , test loss:  0.08431477585832553\n",
      "Epoch:  77310 , step:  77310 , train loss:  0.043794410010207105 , test loss:  0.08431448204157171\n",
      "Epoch:  77320 , step:  77320 , train loss:  0.04379223267412071 , test loss:  0.08431418846193398\n",
      "Epoch:  77330 , step:  77330 , train loss:  0.04379005574763876 , test loss:  0.08431389511931994\n",
      "Epoch:  77340 , step:  77340 , train loss:  0.043787879230631974 , test loss:  0.08431360201363725\n",
      "Epoch:  77350 , step:  77350 , train loss:  0.0437857031229712 , test loss:  0.0843133091447939\n",
      "Epoch:  77360 , step:  77360 , train loss:  0.04378352742452733 , test loss:  0.08431301651269757\n",
      "Epoch:  77370 , step:  77370 , train loss:  0.04378135213517134 , test loss:  0.08431272411725622\n",
      "Epoch:  77380 , step:  77380 , train loss:  0.043779177254774246 , test loss:  0.08431243195837773\n",
      "Epoch:  77390 , step:  77390 , train loss:  0.043777002783207106 , test loss:  0.08431214003597046\n",
      "Epoch:  77400 , step:  77400 , train loss:  0.04377482872034103 , test loss:  0.08431184834994199\n",
      "Epoch:  77410 , step:  77410 , train loss:  0.04377265506604723 , test loss:  0.08431155690020045\n",
      "Epoch:  77420 , step:  77420 , train loss:  0.04377048182019692 , test loss:  0.0843112656866544\n",
      "Epoch:  77430 , step:  77430 , train loss:  0.04376830898266141 , test loss:  0.08431097470921146\n",
      "Epoch:  77440 , step:  77440 , train loss:  0.043766136553312046 , test loss:  0.08431068396778015\n",
      "Epoch:  77450 , step:  77450 , train loss:  0.043763964532020284 , test loss:  0.08431039346226879\n",
      "Epoch:  77460 , step:  77460 , train loss:  0.04376179291865755 , test loss:  0.08431010319258549\n",
      "Epoch:  77470 , step:  77470 , train loss:  0.04375962171309536 , test loss:  0.08430981315863884\n",
      "Epoch:  77480 , step:  77480 , train loss:  0.04375745091520533 , test loss:  0.08430952336033691\n",
      "Epoch:  77490 , step:  77490 , train loss:  0.043755280524859094 , test loss:  0.08430923379758856\n",
      "Epoch:  77500 , step:  77500 , train loss:  0.043753110541928326 , test loss:  0.08430894447030181\n",
      "Epoch:  77510 , step:  77510 , train loss:  0.04375094096628482 , test loss:  0.08430865537838549\n",
      "Epoch:  77520 , step:  77520 , train loss:  0.043748771797800345 , test loss:  0.08430836652174803\n",
      "Epoch:  77530 , step:  77530 , train loss:  0.043746603036346796 , test loss:  0.08430807790029828\n",
      "Epoch:  77540 , step:  77540 , train loss:  0.04374443468179606 , test loss:  0.08430778951394473\n",
      "Epoch:  77550 , step:  77550 , train loss:  0.04374226673402015 , test loss:  0.08430750136259604\n",
      "Epoch:  77560 , step:  77560 , train loss:  0.04374009919289112 , test loss:  0.08430721344616117\n",
      "Epoch:  77570 , step:  77570 , train loss:  0.043737932058281026 , test loss:  0.08430692576454879\n",
      "Epoch:  77580 , step:  77580 , train loss:  0.04373576533006204 , test loss:  0.08430663831766778\n",
      "Epoch:  77590 , step:  77590 , train loss:  0.043733599008106354 , test loss:  0.08430635110542699\n",
      "Epoch:  77600 , step:  77600 , train loss:  0.043731433092286255 , test loss:  0.08430606412773539\n",
      "Epoch:  77610 , step:  77610 , train loss:  0.043729267582474037 , test loss:  0.08430577738450198\n",
      "Epoch:  77620 , step:  77620 , train loss:  0.043727102478542085 , test loss:  0.08430549087563587\n",
      "Epoch:  77630 , step:  77630 , train loss:  0.04372493778036283 , test loss:  0.08430520460104597\n",
      "Epoch:  77640 , step:  77640 , train loss:  0.04372277348780879 , test loss:  0.08430491856064137\n",
      "Epoch:  77650 , step:  77650 , train loss:  0.043720609600752476 , test loss:  0.08430463275433155\n",
      "Epoch:  77660 , step:  77660 , train loss:  0.0437184461190665 , test loss:  0.08430434718202548\n",
      "Epoch:  77670 , step:  77670 , train loss:  0.043716283042623524 , test loss:  0.08430406184363239\n",
      "Epoch:  77680 , step:  77680 , train loss:  0.04371412037129629 , test loss:  0.08430377673906166\n",
      "Epoch:  77690 , step:  77690 , train loss:  0.04371195810495752 , test loss:  0.08430349186822256\n",
      "Epoch:  77700 , step:  77700 , train loss:  0.04370979624348006 , test loss:  0.0843032072310244\n",
      "Epoch:  77710 , step:  77710 , train loss:  0.0437076347867368 , test loss:  0.08430292282737673\n",
      "Epoch:  77720 , step:  77720 , train loss:  0.04370547373460068 , test loss:  0.08430263865718891\n",
      "Epoch:  77730 , step:  77730 , train loss:  0.043703313086944674 , test loss:  0.08430235472037075\n",
      "Epoch:  77740 , step:  77740 , train loss:  0.04370115284364188 , test loss:  0.08430207101683161\n",
      "Epoch:  77750 , step:  77750 , train loss:  0.043698993004565374 , test loss:  0.08430178754648092\n",
      "Epoch:  77760 , step:  77760 , train loss:  0.04369683356958833 , test loss:  0.08430150430922867\n",
      "Epoch:  77770 , step:  77770 , train loss:  0.043694674538583944 , test loss:  0.08430122130498441\n",
      "Epoch:  77780 , step:  77780 , train loss:  0.04369251591142552 , test loss:  0.08430093853365789\n",
      "Epoch:  77790 , step:  77790 , train loss:  0.04369035768798638 , test loss:  0.08430065599515886\n",
      "Epoch:  77800 , step:  77800 , train loss:  0.04368819986813994 , test loss:  0.0843003736893972\n",
      "Epoch:  77810 , step:  77810 , train loss:  0.043686042451759596 , test loss:  0.08430009161628266\n",
      "Epoch:  77820 , step:  77820 , train loss:  0.04368388543871889 , test loss:  0.08429980977572535\n",
      "Epoch:  77830 , step:  77830 , train loss:  0.04368172882889133 , test loss:  0.08429952816763513\n",
      "Epoch:  77840 , step:  77840 , train loss:  0.04367957262215058 , test loss:  0.08429924679192201\n",
      "Epoch:  77850 , step:  77850 , train loss:  0.04367741681837026 , test loss:  0.08429896564849595\n",
      "Epoch:  77860 , step:  77860 , train loss:  0.04367526141742412 , test loss:  0.0842986847372672\n",
      "Epoch:  77870 , step:  77870 , train loss:  0.04367310641918593 , test loss:  0.08429840405814591\n",
      "Epoch:  77880 , step:  77880 , train loss:  0.04367095182352954 , test loss:  0.08429812361104211\n",
      "Epoch:  77890 , step:  77890 , train loss:  0.043668797630328826 , test loss:  0.08429784339586625\n",
      "Epoch:  77900 , step:  77900 , train loss:  0.04366664383945774 , test loss:  0.0842975634125282\n",
      "Epoch:  77910 , step:  77910 , train loss:  0.04366449045079026 , test loss:  0.08429728366093865\n",
      "Epoch:  77920 , step:  77920 , train loss:  0.043662337464200465 , test loss:  0.08429700414100799\n",
      "Epoch:  77930 , step:  77930 , train loss:  0.0436601848795625 , test loss:  0.0842967248526464\n",
      "Epoch:  77940 , step:  77940 , train loss:  0.043658032696750455 , test loss:  0.08429644579576437\n",
      "Epoch:  77950 , step:  77950 , train loss:  0.04365588091563859 , test loss:  0.08429616697027235\n",
      "Epoch:  77960 , step:  77960 , train loss:  0.0436537295361012 , test loss:  0.08429588837608101\n",
      "Epoch:  77970 , step:  77970 , train loss:  0.04365157855801262 , test loss:  0.08429561001310083\n",
      "Epoch:  77980 , step:  77980 , train loss:  0.043649427981247185 , test loss:  0.08429533188124243\n",
      "Epoch:  77990 , step:  77990 , train loss:  0.04364727780567939 , test loss:  0.08429505398041671\n",
      "Epoch:  78000 , step:  78000 , train loss:  0.04364512803118373 , test loss:  0.08429477631053407\n",
      "Epoch:  78010 , step:  78010 , train loss:  0.04364297865763475 , test loss:  0.08429449887150542\n",
      "Epoch:  78020 , step:  78020 , train loss:  0.043640829684907075 , test loss:  0.08429422166324158\n",
      "Epoch:  78030 , step:  78030 , train loss:  0.04363868111287534 , test loss:  0.08429394468565314\n",
      "Epoch:  78040 , step:  78040 , train loss:  0.043636532941414286 , test loss:  0.08429366793865126\n",
      "Epoch:  78050 , step:  78050 , train loss:  0.0436343851703987 , test loss:  0.08429339142214683\n",
      "Epoch:  78060 , step:  78060 , train loss:  0.043632237799703384 , test loss:  0.08429311513605063\n",
      "Epoch:  78070 , step:  78070 , train loss:  0.04363009082920324 , test loss:  0.08429283908027387\n",
      "Epoch:  78080 , step:  78080 , train loss:  0.043627944258773206 , test loss:  0.08429256325472759\n",
      "Epoch:  78090 , step:  78090 , train loss:  0.04362579808828826 , test loss:  0.08429228765932269\n",
      "Epoch:  78100 , step:  78100 , train loss:  0.04362365231762349 , test loss:  0.08429201229397072\n",
      "Epoch:  78110 , step:  78110 , train loss:  0.043621506946653975 , test loss:  0.08429173715858256\n",
      "Epoch:  78120 , step:  78120 , train loss:  0.043619361975254886 , test loss:  0.08429146225306958\n",
      "Epoch:  78130 , step:  78130 , train loss:  0.043617217403301434 , test loss:  0.08429118757734295\n",
      "Epoch:  78140 , step:  78140 , train loss:  0.04361507323066887 , test loss:  0.08429091313131391\n",
      "Epoch:  78150 , step:  78150 , train loss:  0.04361292945723257 , test loss:  0.084290638914894\n",
      "Epoch:  78160 , step:  78160 , train loss:  0.04361078608286786 , test loss:  0.0842903649279946\n",
      "Epoch:  78170 , step:  78170 , train loss:  0.0436086431074502 , test loss:  0.08429009117052698\n",
      "Epoch:  78180 , step:  78180 , train loss:  0.04360650053085508 , test loss:  0.084289817642403\n",
      "Epoch:  78190 , step:  78190 , train loss:  0.043604358352958034 , test loss:  0.08428954434353388\n",
      "Epoch:  78200 , step:  78200 , train loss:  0.043602216573634694 , test loss:  0.08428927127383125\n",
      "Epoch:  78210 , step:  78210 , train loss:  0.04360007519276063 , test loss:  0.08428899843320688\n",
      "Epoch:  78220 , step:  78220 , train loss:  0.04359793421021164 , test loss:  0.08428872582157229\n",
      "Epoch:  78230 , step:  78230 , train loss:  0.04359579362586343 , test loss:  0.0842884534388392\n",
      "Epoch:  78240 , step:  78240 , train loss:  0.043593653439591866 , test loss:  0.08428818128491945\n",
      "Epoch:  78250 , step:  78250 , train loss:  0.04359151365127277 , test loss:  0.08428790935972467\n",
      "Epoch:  78260 , step:  78260 , train loss:  0.04358937426078208 , test loss:  0.0842876376631669\n",
      "Epoch:  78270 , step:  78270 , train loss:  0.0435872352679958 , test loss:  0.08428736619515777\n",
      "Epoch:  78280 , step:  78280 , train loss:  0.04358509667278995 , test loss:  0.08428709495560965\n",
      "Epoch:  78290 , step:  78290 , train loss:  0.043582958475040616 , test loss:  0.0842868239444341\n",
      "Epoch:  78300 , step:  78300 , train loss:  0.04358082067462393 , test loss:  0.08428655316154306\n",
      "Epoch:  78310 , step:  78310 , train loss:  0.043578683271416126 , test loss:  0.08428628260684895\n",
      "Epoch:  78320 , step:  78320 , train loss:  0.04357654626529342 , test loss:  0.08428601228026374\n",
      "Epoch:  78330 , step:  78330 , train loss:  0.043574409656132115 , test loss:  0.08428574218169947\n",
      "Epoch:  78340 , step:  78340 , train loss:  0.04357227344380862 , test loss:  0.08428547231106821\n",
      "Epoch:  78350 , step:  78350 , train loss:  0.04357013762819928 , test loss:  0.08428520266828254\n",
      "Epoch:  78360 , step:  78360 , train loss:  0.04356800220918063 , test loss:  0.08428493325325444\n",
      "Epoch:  78370 , step:  78370 , train loss:  0.04356586718662913 , test loss:  0.08428466406589624\n",
      "Epoch:  78380 , step:  78380 , train loss:  0.04356373256042142 , test loss:  0.08428439510612033\n",
      "Epoch:  78390 , step:  78390 , train loss:  0.0435615983304341 , test loss:  0.0842841263738392\n",
      "Epoch:  78400 , step:  78400 , train loss:  0.043559464496543826 , test loss:  0.08428385786896532\n",
      "Epoch:  78410 , step:  78410 , train loss:  0.043557331058627385 , test loss:  0.08428358959141105\n",
      "Epoch:  78420 , step:  78420 , train loss:  0.04355519801656155 , test loss:  0.08428332154108904\n",
      "Epoch:  78430 , step:  78430 , train loss:  0.04355306537022318 , test loss:  0.08428305371791159\n",
      "Epoch:  78440 , step:  78440 , train loss:  0.043550933119489155 , test loss:  0.0842827861217915\n",
      "Epoch:  78450 , step:  78450 , train loss:  0.04354880126423643 , test loss:  0.08428251875264126\n",
      "Epoch:  78460 , step:  78460 , train loss:  0.04354666980434201 , test loss:  0.08428225161037384\n",
      "Epoch:  78470 , step:  78470 , train loss:  0.043544538739683 , test loss:  0.08428198469490185\n",
      "Epoch:  78480 , step:  78480 , train loss:  0.04354240807013649 , test loss:  0.08428171800613805\n",
      "Epoch:  78490 , step:  78490 , train loss:  0.04354027779557964 , test loss:  0.08428145154399523\n",
      "Epoch:  78500 , step:  78500 , train loss:  0.04353814791588967 , test loss:  0.08428118530838631\n",
      "Epoch:  78510 , step:  78510 , train loss:  0.04353601843094386 , test loss:  0.08428091929922413\n",
      "Epoch:  78520 , step:  78520 , train loss:  0.04353388934061955 , test loss:  0.08428065351642174\n",
      "Epoch:  78530 , step:  78530 , train loss:  0.043531760644794144 , test loss:  0.08428038795989208\n",
      "Epoch:  78540 , step:  78540 , train loss:  0.04352963234334504 , test loss:  0.08428012262954815\n",
      "Epoch:  78550 , step:  78550 , train loss:  0.04352750443614976 , test loss:  0.08427985752530286\n",
      "Epoch:  78560 , step:  78560 , train loss:  0.04352537692308583 , test loss:  0.08427959264706972\n",
      "Epoch:  78570 , step:  78570 , train loss:  0.04352324980403087 , test loss:  0.08427932799476161\n",
      "Epoch:  78580 , step:  78580 , train loss:  0.04352112307886249 , test loss:  0.08427906356829187\n",
      "Epoch:  78590 , step:  78590 , train loss:  0.04351899674745846 , test loss:  0.08427879936757367\n",
      "Epoch:  78600 , step:  78600 , train loss:  0.043516870809696455 , test loss:  0.08427853539252023\n",
      "Epoch:  78610 , step:  78610 , train loss:  0.04351474526545439 , test loss:  0.08427827164304506\n",
      "Epoch:  78620 , step:  78620 , train loss:  0.04351262011461005 , test loss:  0.08427800811906128\n",
      "Epoch:  78630 , step:  78630 , train loss:  0.043510495357041365 , test loss:  0.08427774482048264\n",
      "Epoch:  78640 , step:  78640 , train loss:  0.04350837099262636 , test loss:  0.08427748174722227\n",
      "Epoch:  78650 , step:  78650 , train loss:  0.043506247021243 , test loss:  0.0842772188991939\n",
      "Epoch:  78660 , step:  78660 , train loss:  0.043504123442769396 , test loss:  0.08427695627631099\n",
      "Epoch:  78670 , step:  78670 , train loss:  0.04350200025708367 , test loss:  0.08427669387848702\n",
      "Epoch:  78680 , step:  78680 , train loss:  0.04349987746406402 , test loss:  0.08427643170563578\n",
      "Epoch:  78690 , step:  78690 , train loss:  0.043497755063588675 , test loss:  0.08427616975767083\n",
      "Epoch:  78700 , step:  78700 , train loss:  0.04349563305553592 , test loss:  0.08427590803450583\n",
      "Epoch:  78710 , step:  78710 , train loss:  0.043493511439784124 , test loss:  0.08427564653605461\n",
      "Epoch:  78720 , step:  78720 , train loss:  0.043491390216211664 , test loss:  0.08427538526223095\n",
      "Epoch:  78730 , step:  78730 , train loss:  0.04348926938469699 , test loss:  0.08427512421294873\n",
      "Epoch:  78740 , step:  78740 , train loss:  0.04348714894511861 , test loss:  0.08427486338812178\n",
      "Epoch:  78750 , step:  78750 , train loss:  0.04348502889735509 , test loss:  0.08427460278766401\n",
      "Epoch:  78760 , step:  78760 , train loss:  0.04348290924128501 , test loss:  0.08427434241148947\n",
      "Epoch:  78770 , step:  78770 , train loss:  0.04348078997678709 , test loss:  0.08427408225951186\n",
      "Epoch:  78780 , step:  78780 , train loss:  0.04347867110373997 , test loss:  0.08427382233164536\n",
      "Epoch:  78790 , step:  78790 , train loss:  0.043476552622022484 , test loss:  0.08427356262780435\n",
      "Epoch:  78800 , step:  78800 , train loss:  0.04347443453151342 , test loss:  0.08427330314790275\n",
      "Epoch:  78810 , step:  78810 , train loss:  0.04347231683209168 , test loss:  0.08427304389185458\n",
      "Epoch:  78820 , step:  78820 , train loss:  0.043470199523636156 , test loss:  0.08427278485957411\n",
      "Epoch:  78830 , step:  78830 , train loss:  0.04346808260602584 , test loss:  0.08427252605097585\n",
      "Epoch:  78840 , step:  78840 , train loss:  0.04346596607913976 , test loss:  0.08427226746597377\n",
      "Epoch:  78850 , step:  78850 , train loss:  0.04346384994285704 , test loss:  0.08427200910448227\n",
      "Epoch:  78860 , step:  78860 , train loss:  0.04346173419705675 , test loss:  0.08427175096641584\n",
      "Epoch:  78870 , step:  78870 , train loss:  0.04345961884161813 , test loss:  0.0842714930516888\n",
      "Epoch:  78880 , step:  78880 , train loss:  0.04345750387642041 , test loss:  0.08427123536021568\n",
      "Epoch:  78890 , step:  78890 , train loss:  0.04345538930134289 , test loss:  0.0842709778919109\n",
      "Epoch:  78900 , step:  78900 , train loss:  0.043453275116264886 , test loss:  0.08427072064668906\n",
      "Epoch:  78910 , step:  78910 , train loss:  0.04345116132106587 , test loss:  0.08427046362446469\n",
      "Epoch:  78920 , step:  78920 , train loss:  0.04344904791562522 , test loss:  0.08427020682515256\n",
      "Epoch:  78930 , step:  78930 , train loss:  0.04344693489982248 , test loss:  0.08426995024866717\n",
      "Epoch:  78940 , step:  78940 , train loss:  0.04344482227353721 , test loss:  0.08426969389492331\n",
      "Epoch:  78950 , step:  78950 , train loss:  0.043442710036649 , test loss:  0.0842694377638356\n",
      "Epoch:  78960 , step:  78960 , train loss:  0.04344059818903753 , test loss:  0.08426918185531898\n",
      "Epoch:  78970 , step:  78970 , train loss:  0.04343848673058249 , test loss:  0.08426892616928819\n",
      "Epoch:  78980 , step:  78980 , train loss:  0.04343637566116365 , test loss:  0.08426867070565806\n",
      "Epoch:  78990 , step:  78990 , train loss:  0.043434264980660886 , test loss:  0.08426841546434363\n",
      "Epoch:  79000 , step:  79000 , train loss:  0.043432154688954006 , test loss:  0.08426816044525991\n",
      "Epoch:  79010 , step:  79010 , train loss:  0.04343004478592297 , test loss:  0.08426790564832155\n",
      "Epoch:  79020 , step:  79020 , train loss:  0.04342793527144772 , test loss:  0.0842676510734439\n",
      "Epoch:  79030 , step:  79030 , train loss:  0.0434258261454083 , test loss:  0.08426739672054204\n",
      "Epoch:  79040 , step:  79040 , train loss:  0.04342371740768479 , test loss:  0.0842671425895309\n",
      "Epoch:  79050 , step:  79050 , train loss:  0.04342160905815734 , test loss:  0.08426688868032574\n",
      "Epoch:  79060 , step:  79060 , train loss:  0.04341950109670611 , test loss:  0.08426663499284179\n",
      "Epoch:  79070 , step:  79070 , train loss:  0.04341739352321136 , test loss:  0.08426638152699426\n",
      "Epoch:  79080 , step:  79080 , train loss:  0.04341528633755336 , test loss:  0.08426612828269835\n",
      "Epoch:  79090 , step:  79090 , train loss:  0.043413179539612455 , test loss:  0.08426587525986955\n",
      "Epoch:  79100 , step:  79100 , train loss:  0.04341107312926904 , test loss:  0.08426562245842305\n",
      "Epoch:  79110 , step:  79110 , train loss:  0.04340896710640353 , test loss:  0.0842653698782742\n",
      "Epoch:  79120 , step:  79120 , train loss:  0.043406861470896486 , test loss:  0.08426511751933889\n",
      "Epoch:  79130 , step:  79130 , train loss:  0.04340475622262838 , test loss:  0.08426486538153216\n",
      "Epoch:  79140 , step:  79140 , train loss:  0.043402651361479895 , test loss:  0.08426461346476959\n",
      "Epoch:  79150 , step:  79150 , train loss:  0.04340054688733161 , test loss:  0.08426436176896691\n",
      "Epoch:  79160 , step:  79160 , train loss:  0.04339844280006425 , test loss:  0.08426411029403967\n",
      "Epoch:  79170 , step:  79170 , train loss:  0.04339633909955859 , test loss:  0.08426385903990323\n",
      "Epoch:  79180 , step:  79180 , train loss:  0.043394235785695436 , test loss:  0.0842636080064739\n",
      "Epoch:  79190 , step:  79190 , train loss:  0.04339213285835559 , test loss:  0.08426335719366694\n",
      "Epoch:  79200 , step:  79200 , train loss:  0.043390030317420045 , test loss:  0.08426310660139819\n",
      "Epoch:  79210 , step:  79210 , train loss:  0.043387928162769714 , test loss:  0.0842628562295835\n",
      "Epoch:  79220 , step:  79220 , train loss:  0.043385826394285606 , test loss:  0.08426260607813874\n",
      "Epoch:  79230 , step:  79230 , train loss:  0.043383725011848825 , test loss:  0.08426235614697984\n",
      "Epoch:  79240 , step:  79240 , train loss:  0.043381624015340445 , test loss:  0.08426210643602275\n",
      "Epoch:  79250 , step:  79250 , train loss:  0.043379523404641636 , test loss:  0.08426185694518326\n",
      "Epoch:  79260 , step:  79260 , train loss:  0.04337742317963366 , test loss:  0.08426160767437756\n",
      "Epoch:  79270 , step:  79270 , train loss:  0.04337532334019774 , test loss:  0.08426135862352153\n",
      "Epoch:  79280 , step:  79280 , train loss:  0.04337322388621524 , test loss:  0.08426110979253142\n",
      "Epoch:  79290 , step:  79290 , train loss:  0.04337112481756749 , test loss:  0.08426086118132328\n",
      "Epoch:  79300 , step:  79300 , train loss:  0.04336902613413595 , test loss:  0.08426061278981353\n",
      "Epoch:  79310 , step:  79310 , train loss:  0.04336692783580208 , test loss:  0.08426036461791808\n",
      "Epoch:  79320 , step:  79320 , train loss:  0.04336482992244742 , test loss:  0.08426011666555328\n",
      "Epoch:  79330 , step:  79330 , train loss:  0.04336273239395352 , test loss:  0.08425986893263555\n",
      "Epoch:  79340 , step:  79340 , train loss:  0.04336063525020205 , test loss:  0.08425962141908103\n",
      "Epoch:  79350 , step:  79350 , train loss:  0.04335853849107465 , test loss:  0.08425937412480619\n",
      "Epoch:  79360 , step:  79360 , train loss:  0.043356442116453106 , test loss:  0.08425912704972746\n",
      "Epoch:  79370 , step:  79370 , train loss:  0.043354346126219134 , test loss:  0.08425888019376125\n",
      "Epoch:  79380 , step:  79380 , train loss:  0.043352250520254626 , test loss:  0.08425863355682413\n",
      "Epoch:  79390 , step:  79390 , train loss:  0.04335015529844144 , test loss:  0.08425838713883256\n",
      "Epoch:  79400 , step:  79400 , train loss:  0.043348060460661524 , test loss:  0.08425814093970335\n",
      "Epoch:  79410 , step:  79410 , train loss:  0.043345966006796854 , test loss:  0.08425789495935276\n",
      "Epoch:  79420 , step:  79420 , train loss:  0.04334387193672949 , test loss:  0.0842576491976977\n",
      "Epoch:  79430 , step:  79430 , train loss:  0.04334177825034151 , test loss:  0.08425740365465482\n",
      "Epoch:  79440 , step:  79440 , train loss:  0.04333968494751505 , test loss:  0.08425715833014075\n",
      "Epoch:  79450 , step:  79450 , train loss:  0.043337592028132275 , test loss:  0.08425691322407242\n",
      "Epoch:  79460 , step:  79460 , train loss:  0.0433354994920755 , test loss:  0.08425666833636669\n",
      "Epoch:  79470 , step:  79470 , train loss:  0.04333340733922698 , test loss:  0.08425642366694022\n",
      "Epoch:  79480 , step:  79480 , train loss:  0.04333131556946905 , test loss:  0.08425617921570996\n",
      "Epoch:  79490 , step:  79490 , train loss:  0.043329224182684103 , test loss:  0.08425593498259282\n",
      "Epoch:  79500 , step:  79500 , train loss:  0.04332713317875459 , test loss:  0.08425569096750588\n",
      "Epoch:  79510 , step:  79510 , train loss:  0.04332504255756304 , test loss:  0.08425544717036627\n",
      "Epoch:  79520 , step:  79520 , train loss:  0.04332295231899195 , test loss:  0.08425520359109086\n",
      "Epoch:  79530 , step:  79530 , train loss:  0.04332086246292395 , test loss:  0.08425496022959676\n",
      "Epoch:  79540 , step:  79540 , train loss:  0.04331877298924168 , test loss:  0.08425471708580112\n",
      "Epoch:  79550 , step:  79550 , train loss:  0.043316683897827855 , test loss:  0.08425447415962128\n",
      "Epoch:  79560 , step:  79560 , train loss:  0.04331459518856519 , test loss:  0.08425423145097416\n",
      "Epoch:  79570 , step:  79570 , train loss:  0.04331250686133653 , test loss:  0.08425398895977727\n",
      "Epoch:  79580 , step:  79580 , train loss:  0.04331041891602469 , test loss:  0.08425374668594798\n",
      "Epoch:  79590 , step:  79590 , train loss:  0.04330833135251258 , test loss:  0.08425350462940333\n",
      "Epoch:  79600 , step:  79600 , train loss:  0.04330624417068317 , test loss:  0.08425326279006086\n",
      "Epoch:  79610 , step:  79610 , train loss:  0.04330415737041947 , test loss:  0.08425302116783778\n",
      "Epoch:  79620 , step:  79620 , train loss:  0.04330207095160449 , test loss:  0.08425277976265202\n",
      "Epoch:  79630 , step:  79630 , train loss:  0.04329998491412137 , test loss:  0.08425253857442067\n",
      "Epoch:  79640 , step:  79640 , train loss:  0.04329789925785324 , test loss:  0.08425229760306148\n",
      "Epoch:  79650 , step:  79650 , train loss:  0.04329581398268335 , test loss:  0.08425205684849192\n",
      "Epoch:  79660 , step:  79660 , train loss:  0.04329372908849491 , test loss:  0.08425181631062954\n",
      "Epoch:  79670 , step:  79670 , train loss:  0.04329164457517121 , test loss:  0.08425157598939213\n",
      "Epoch:  79680 , step:  79680 , train loss:  0.04328956044259569 , test loss:  0.08425133588469738\n",
      "Epoch:  79690 , step:  79690 , train loss:  0.043287476690651645 , test loss:  0.08425109599646304\n",
      "Epoch:  79700 , step:  79700 , train loss:  0.04328539331922261 , test loss:  0.08425085632460662\n",
      "Epoch:  79710 , step:  79710 , train loss:  0.04328331032819207 , test loss:  0.08425061686904624\n",
      "Epoch:  79720 , step:  79720 , train loss:  0.04328122771744358 , test loss:  0.08425037762969953\n",
      "Epoch:  79730 , step:  79730 , train loss:  0.04327914548686076 , test loss:  0.08425013860648455\n",
      "Epoch:  79740 , step:  79740 , train loss:  0.04327706363632722 , test loss:  0.08424989979931928\n",
      "Epoch:  79750 , step:  79750 , train loss:  0.04327498216572672 , test loss:  0.08424966120812145\n",
      "Epoch:  79760 , step:  79760 , train loss:  0.04327290107494299 , test loss:  0.08424942283280944\n",
      "Epoch:  79770 , step:  79770 , train loss:  0.04327082036385985 , test loss:  0.08424918467330089\n",
      "Epoch:  79780 , step:  79780 , train loss:  0.04326874003236113 , test loss:  0.08424894672951398\n",
      "Epoch:  79790 , step:  79790 , train loss:  0.043266660080330775 , test loss:  0.0842487090013671\n",
      "Epoch:  79800 , step:  79800 , train loss:  0.04326458050765272 , test loss:  0.08424847148877827\n",
      "Epoch:  79810 , step:  79810 , train loss:  0.043262501314210985 , test loss:  0.08424823419166558\n",
      "Epoch:  79820 , step:  79820 , train loss:  0.0432604224998896 , test loss:  0.08424799710994747\n",
      "Epoch:  79830 , step:  79830 , train loss:  0.0432583440645727 , test loss:  0.08424776024354207\n",
      "Epoch:  79840 , step:  79840 , train loss:  0.04325626600814444 , test loss:  0.08424752359236776\n",
      "Epoch:  79850 , step:  79850 , train loss:  0.04325418833048899 , test loss:  0.08424728715634291\n",
      "Epoch:  79860 , step:  79860 , train loss:  0.04325211103149065 , test loss:  0.08424705093538581\n",
      "Epoch:  79870 , step:  79870 , train loss:  0.04325003411103372 , test loss:  0.08424681492941508\n",
      "Epoch:  79880 , step:  79880 , train loss:  0.04324795756900251 , test loss:  0.0842465791383491\n",
      "Epoch:  79890 , step:  79890 , train loss:  0.04324588140528147 , test loss:  0.08424634356210638\n",
      "Epoch:  79900 , step:  79900 , train loss:  0.04324380561975507 , test loss:  0.0842461082006056\n",
      "Epoch:  79910 , step:  79910 , train loss:  0.04324173021230776 , test loss:  0.08424587305376513\n",
      "Epoch:  79920 , step:  79920 , train loss:  0.04323965518282412 , test loss:  0.08424563812150393\n",
      "Epoch:  79930 , step:  79930 , train loss:  0.04323758053118877 , test loss:  0.08424540340374037\n",
      "Epoch:  79940 , step:  79940 , train loss:  0.043235506257286324 , test loss:  0.08424516890039319\n",
      "Epoch:  79950 , step:  79950 , train loss:  0.04323343236100152 , test loss:  0.08424493461138127\n",
      "Epoch:  79960 , step:  79960 , train loss:  0.0432313588422191 , test loss:  0.08424470053662345\n",
      "Epoch:  79970 , step:  79970 , train loss:  0.04322928570082388 , test loss:  0.0842444666760384\n",
      "Epoch:  79980 , step:  79980 , train loss:  0.043227212936700676 , test loss:  0.08424423302954506\n",
      "Epoch:  79990 , step:  79990 , train loss:  0.04322514054973441 , test loss:  0.08424399959706226\n",
      "Epoch:  80000 , step:  80000 , train loss:  0.043223068539810054 , test loss:  0.08424376637850915\n",
      "Epoch:  80010 , step:  80010 , train loss:  0.04322099690681256 , test loss:  0.08424353337380452\n",
      "Epoch:  80020 , step:  80020 , train loss:  0.043218925650626995 , test loss:  0.08424330058286769\n",
      "Epoch:  80030 , step:  80030 , train loss:  0.043216854771138485 , test loss:  0.08424306800561736\n",
      "Epoch:  80040 , step:  80040 , train loss:  0.04321478426823215 , test loss:  0.08424283564197278\n",
      "Epoch:  80050 , step:  80050 , train loss:  0.04321271414179316 , test loss:  0.08424260349185296\n",
      "Epoch:  80060 , step:  80060 , train loss:  0.04321064439170683 , test loss:  0.08424237155517737\n",
      "Epoch:  80070 , step:  80070 , train loss:  0.043208575017858415 , test loss:  0.08424213983186499\n",
      "Epoch:  80080 , step:  80080 , train loss:  0.043206506020133234 , test loss:  0.0842419083218351\n",
      "Epoch:  80090 , step:  80090 , train loss:  0.043204437398416715 , test loss:  0.0842416770250071\n",
      "Epoch:  80100 , step:  80100 , train loss:  0.04320236915259428 , test loss:  0.08424144594130022\n",
      "Epoch:  80110 , step:  80110 , train loss:  0.043200301282551465 , test loss:  0.08424121507063399\n",
      "Epoch:  80120 , step:  80120 , train loss:  0.04319823378817376 , test loss:  0.08424098441292757\n",
      "Epoch:  80130 , step:  80130 , train loss:  0.04319616666934678 , test loss:  0.08424075396810057\n",
      "Epoch:  80140 , step:  80140 , train loss:  0.043194099925956145 , test loss:  0.08424052373607248\n",
      "Epoch:  80150 , step:  80150 , train loss:  0.04319203355788756 , test loss:  0.08424029371676284\n",
      "Epoch:  80160 , step:  80160 , train loss:  0.04318996756502676 , test loss:  0.0842400639100911\n",
      "Epoch:  80170 , step:  80170 , train loss:  0.04318790194725953 , test loss:  0.08423983431597694\n",
      "Epoch:  80180 , step:  80180 , train loss:  0.04318583670447168 , test loss:  0.08423960493434003\n",
      "Epoch:  80190 , step:  80190 , train loss:  0.04318377183654912 , test loss:  0.0842393757651\n",
      "Epoch:  80200 , step:  80200 , train loss:  0.0431817073433778 , test loss:  0.0842391468081765\n",
      "Epoch:  80210 , step:  80210 , train loss:  0.04317964322484365 , test loss:  0.08423891806348945\n",
      "Epoch:  80220 , step:  80220 , train loss:  0.043177579480832735 , test loss:  0.08423868953095857\n",
      "Epoch:  80230 , step:  80230 , train loss:  0.04317551611123114 , test loss:  0.0842384612105037\n",
      "Epoch:  80240 , step:  80240 , train loss:  0.043173453115924944 , test loss:  0.08423823310204463\n",
      "Epoch:  80250 , step:  80250 , train loss:  0.04317139049480038 , test loss:  0.08423800520550141\n",
      "Epoch:  80260 , step:  80260 , train loss:  0.04316932824774362 , test loss:  0.08423777752079373\n",
      "Epoch:  80270 , step:  80270 , train loss:  0.043167266374640995 , test loss:  0.08423755004784185\n",
      "Epoch:  80280 , step:  80280 , train loss:  0.043165204875378796 , test loss:  0.08423732278656583\n",
      "Epoch:  80290 , step:  80290 , train loss:  0.043163143749843376 , test loss:  0.08423709573688541\n",
      "Epoch:  80300 , step:  80300 , train loss:  0.043161082997921194 , test loss:  0.08423686889872085\n",
      "Epoch:  80310 , step:  80310 , train loss:  0.04315902261949868 , test loss:  0.08423664227199244\n",
      "Epoch:  80320 , step:  80320 , train loss:  0.043156962614462384 , test loss:  0.08423641585662023\n",
      "Epoch:  80330 , step:  80330 , train loss:  0.04315490298269885 , test loss:  0.08423618965252438\n",
      "Epoch:  80340 , step:  80340 , train loss:  0.04315284372409469 , test loss:  0.08423596365962528\n",
      "Epoch:  80350 , step:  80350 , train loss:  0.04315078483853658 , test loss:  0.08423573787784304\n",
      "Epoch:  80360 , step:  80360 , train loss:  0.043148726325911206 , test loss:  0.08423551230709811\n",
      "Epoch:  80370 , step:  80370 , train loss:  0.04314666818610534 , test loss:  0.0842352869473107\n",
      "Epoch:  80380 , step:  80380 , train loss:  0.04314461041900579 , test loss:  0.08423506179840144\n",
      "Epoch:  80390 , step:  80390 , train loss:  0.04314255302449943 , test loss:  0.08423483686029054\n",
      "Epoch:  80400 , step:  80400 , train loss:  0.04314049600247313 , test loss:  0.08423461213289873\n",
      "Epoch:  80410 , step:  80410 , train loss:  0.04313843935281385 , test loss:  0.08423438761614643\n",
      "Epoch:  80420 , step:  80420 , train loss:  0.043136383075408596 , test loss:  0.08423416330995405\n",
      "Epoch:  80430 , step:  80430 , train loss:  0.04313432717014445 , test loss:  0.08423393921424238\n",
      "Epoch:  80440 , step:  80440 , train loss:  0.043132271636908436 , test loss:  0.08423371532893195\n",
      "Epoch:  80450 , step:  80450 , train loss:  0.04313021647558774 , test loss:  0.08423349165394348\n",
      "Epoch:  80460 , step:  80460 , train loss:  0.043128161686069554 , test loss:  0.08423326818919757\n",
      "Epoch:  80470 , step:  80470 , train loss:  0.04312610726824113 , test loss:  0.08423304493461503\n",
      "Epoch:  80480 , step:  80480 , train loss:  0.043124053221989735 , test loss:  0.08423282189011673\n",
      "Epoch:  80490 , step:  80490 , train loss:  0.043121999547202715 , test loss:  0.0842325990556233\n",
      "Epoch:  80500 , step:  80500 , train loss:  0.04311994624376746 , test loss:  0.08423237643105576\n",
      "Epoch:  80510 , step:  80510 , train loss:  0.04311789331157139 , test loss:  0.08423215401633488\n",
      "Epoch:  80520 , step:  80520 , train loss:  0.04311584075050199 , test loss:  0.08423193181138187\n",
      "Epoch:  80530 , step:  80530 , train loss:  0.04311378856044681 , test loss:  0.08423170981611736\n",
      "Epoch:  80540 , step:  80540 , train loss:  0.043111736741293406 , test loss:  0.08423148803046222\n",
      "Epoch:  80550 , step:  80550 , train loss:  0.04310968529292939 , test loss:  0.08423126645433804\n",
      "Epoch:  80560 , step:  80560 , train loss:  0.04310763421524248 , test loss:  0.08423104508766548\n",
      "Epoch:  80570 , step:  80570 , train loss:  0.04310558350812034 , test loss:  0.08423082393036585\n",
      "Epoch:  80580 , step:  80580 , train loss:  0.043103533171450784 , test loss:  0.08423060298236038\n",
      "Epoch:  80590 , step:  80590 , train loss:  0.04310148320512161 , test loss:  0.08423038224357002\n",
      "Epoch:  80600 , step:  80600 , train loss:  0.04309943360902069 , test loss:  0.08423016171391606\n",
      "Epoch:  80610 , step:  80610 , train loss:  0.0430973843830359 , test loss:  0.0842299413933198\n",
      "Epoch:  80620 , step:  80620 , train loss:  0.04309533552705527 , test loss:  0.08422972128170259\n",
      "Epoch:  80630 , step:  80630 , train loss:  0.04309328704096674 , test loss:  0.08422950137898584\n",
      "Epoch:  80640 , step:  80640 , train loss:  0.04309123892465838 , test loss:  0.0842292816850906\n",
      "Epoch:  80650 , step:  80650 , train loss:  0.04308919117801831 , test loss:  0.08422906219993886\n",
      "Epoch:  80660 , step:  80660 , train loss:  0.04308714380093469 , test loss:  0.08422884292345152\n",
      "Epoch:  80670 , step:  80670 , train loss:  0.04308509679329568 , test loss:  0.08422862385555041\n",
      "Epoch:  80680 , step:  80680 , train loss:  0.043083050154989574 , test loss:  0.08422840499615673\n",
      "Epoch:  80690 , step:  80690 , train loss:  0.04308100388590462 , test loss:  0.08422818634519251\n",
      "Epoch:  80700 , step:  80700 , train loss:  0.04307895798592919 , test loss:  0.08422796790257892\n",
      "Epoch:  80710 , step:  80710 , train loss:  0.04307691245495164 , test loss:  0.08422774966823794\n",
      "Epoch:  80720 , step:  80720 , train loss:  0.04307486729286044 , test loss:  0.08422753164209114\n",
      "Epoch:  80730 , step:  80730 , train loss:  0.043072822499544035 , test loss:  0.08422731382406\n",
      "Epoch:  80740 , step:  80740 , train loss:  0.043070778074891 , test loss:  0.0842270962140666\n",
      "Epoch:  80750 , step:  80750 , train loss:  0.04306873401878986 , test loss:  0.0842268788120325\n",
      "Epoch:  80760 , step:  80760 , train loss:  0.04306669033112931 , test loss:  0.08422666161787963\n",
      "Epoch:  80770 , step:  80770 , train loss:  0.04306464701179797 , test loss:  0.08422644463152984\n",
      "Epoch:  80780 , step:  80780 , train loss:  0.04306260406068455 , test loss:  0.08422622785290505\n",
      "Epoch:  80790 , step:  80790 , train loss:  0.04306056147767784 , test loss:  0.08422601128192723\n",
      "Epoch:  80800 , step:  80800 , train loss:  0.04305851926266669 , test loss:  0.08422579491851818\n",
      "Epoch:  80810 , step:  80810 , train loss:  0.04305647741553988 , test loss:  0.08422557876259994\n",
      "Epoch:  80820 , step:  80820 , train loss:  0.043054435936186373 , test loss:  0.08422536281409489\n",
      "Epoch:  80830 , step:  80830 , train loss:  0.04305239482449515 , test loss:  0.0842251470729249\n",
      "Epoch:  80840 , step:  80840 , train loss:  0.04305035408035514 , test loss:  0.08422493153901194\n",
      "Epoch:  80850 , step:  80850 , train loss:  0.043048313703655444 , test loss:  0.08422471621227831\n",
      "Epoch:  80860 , step:  80860 , train loss:  0.043046273694285146 , test loss:  0.08422450109264627\n",
      "Epoch:  80870 , step:  80870 , train loss:  0.043044234052133395 , test loss:  0.08422428618003788\n",
      "Epoch:  80880 , step:  80880 , train loss:  0.043042194777089385 , test loss:  0.08422407147437559\n",
      "Epoch:  80890 , step:  80890 , train loss:  0.04304015586904235 , test loss:  0.08422385697558163\n",
      "Epoch:  80900 , step:  80900 , train loss:  0.04303811732788156 , test loss:  0.08422364268357822\n",
      "Epoch:  80910 , step:  80910 , train loss:  0.043036079153496366 , test loss:  0.08422342859828791\n",
      "Epoch:  80920 , step:  80920 , train loss:  0.043034041345776125 , test loss:  0.08422321471963301\n",
      "Epoch:  80930 , step:  80930 , train loss:  0.043032003904610296 , test loss:  0.08422300104753613\n",
      "Epoch:  80940 , step:  80940 , train loss:  0.04302996682988835 , test loss:  0.08422278758191953\n",
      "Epoch:  80950 , step:  80950 , train loss:  0.04302793012149977 , test loss:  0.08422257432270595\n",
      "Epoch:  80960 , step:  80960 , train loss:  0.043025893779334155 , test loss:  0.08422236126981791\n",
      "Epoch:  80970 , step:  80970 , train loss:  0.04302385780328111 , test loss:  0.08422214842317792\n",
      "Epoch:  80980 , step:  80980 , train loss:  0.04302182219323031 , test loss:  0.0842219357827087\n",
      "Epoch:  80990 , step:  80990 , train loss:  0.043019786949071444 , test loss:  0.08422172334833286\n",
      "Epoch:  81000 , step:  81000 , train loss:  0.043017752070694255 , test loss:  0.08422151111997307\n",
      "Epoch:  81010 , step:  81010 , train loss:  0.043015717557988574 , test loss:  0.0842212990975521\n",
      "Epoch:  81020 , step:  81020 , train loss:  0.04301368341084422 , test loss:  0.08422108728099283\n",
      "Epoch:  81030 , step:  81030 , train loss:  0.0430116496291511 , test loss:  0.08422087567021794\n",
      "Epoch:  81040 , step:  81040 , train loss:  0.043009616212799175 , test loss:  0.08422066426515046\n",
      "Epoch:  81050 , step:  81050 , train loss:  0.043007583161678395 , test loss:  0.08422045306571314\n",
      "Epoch:  81060 , step:  81060 , train loss:  0.04300555047567881 , test loss:  0.08422024207182904\n",
      "Epoch:  81070 , step:  81070 , train loss:  0.04300351815469049 , test loss:  0.08422003128342098\n",
      "Epoch:  81080 , step:  81080 , train loss:  0.043001486198603577 , test loss:  0.084219820700412\n",
      "Epoch:  81090 , step:  81090 , train loss:  0.04299945460730826 , test loss:  0.08421961032272522\n",
      "Epoch:  81100 , step:  81100 , train loss:  0.042997423380694726 , test loss:  0.08421940015028372\n",
      "Epoch:  81110 , step:  81110 , train loss:  0.042995392518653267 , test loss:  0.08421919018301038\n",
      "Epoch:  81120 , step:  81120 , train loss:  0.042993362021074163 , test loss:  0.08421898042082858\n",
      "Epoch:  81130 , step:  81130 , train loss:  0.04299133188784782 , test loss:  0.08421877086366139\n",
      "Epoch:  81140 , step:  81140 , train loss:  0.04298930211886461 , test loss:  0.08421856151143203\n",
      "Epoch:  81150 , step:  81150 , train loss:  0.04298727271401499 , test loss:  0.08421835236406379\n",
      "Epoch:  81160 , step:  81160 , train loss:  0.04298524367318946 , test loss:  0.08421814342147997\n",
      "Epoch:  81170 , step:  81170 , train loss:  0.04298321499627858 , test loss:  0.08421793468360386\n",
      "Epoch:  81180 , step:  81180 , train loss:  0.0429811866831729 , test loss:  0.08421772615035891\n",
      "Epoch:  81190 , step:  81190 , train loss:  0.04297915873376308 , test loss:  0.08421751782166847\n",
      "Epoch:  81200 , step:  81200 , train loss:  0.042977131147939836 , test loss:  0.08421730969745593\n",
      "Epoch:  81210 , step:  81210 , train loss:  0.04297510392559386 , test loss:  0.08421710177764473\n",
      "Epoch:  81220 , step:  81220 , train loss:  0.04297307706661594 , test loss:  0.0842168940621586\n",
      "Epoch:  81230 , step:  81230 , train loss:  0.04297105057089691 , test loss:  0.08421668655092086\n",
      "Epoch:  81240 , step:  81240 , train loss:  0.04296902443832759 , test loss:  0.08421647924385522\n",
      "Epoch:  81250 , step:  81250 , train loss:  0.04296699866879895 , test loss:  0.08421627214088512\n",
      "Epoch:  81260 , step:  81260 , train loss:  0.04296497326220193 , test loss:  0.08421606524193431\n",
      "Epoch:  81270 , step:  81270 , train loss:  0.04296294821842753 , test loss:  0.08421585854692655\n",
      "Epoch:  81280 , step:  81280 , train loss:  0.0429609235373668 , test loss:  0.0842156520557856\n",
      "Epoch:  81290 , step:  81290 , train loss:  0.04295889921891085 , test loss:  0.08421544576843498\n",
      "Epoch:  81300 , step:  81300 , train loss:  0.04295687526295083 , test loss:  0.08421523968479873\n",
      "Epoch:  81310 , step:  81310 , train loss:  0.04295485166937792 , test loss:  0.08421503380480058\n",
      "Epoch:  81320 , step:  81320 , train loss:  0.042952828438083325 , test loss:  0.08421482812836427\n",
      "Epoch:  81330 , step:  81330 , train loss:  0.042950805568958424 , test loss:  0.0842146226554138\n",
      "Epoch:  81340 , step:  81340 , train loss:  0.042948783061894424 , test loss:  0.08421441738587322\n",
      "Epoch:  81350 , step:  81350 , train loss:  0.04294676091678276 , test loss:  0.0842142123196664\n",
      "Epoch:  81360 , step:  81360 , train loss:  0.04294473913351487 , test loss:  0.0842140074567174\n",
      "Epoch:  81370 , step:  81370 , train loss:  0.04294271771198217 , test loss:  0.08421380279695029\n",
      "Epoch:  81380 , step:  81380 , train loss:  0.04294069665207623 , test loss:  0.08421359834028896\n",
      "Epoch:  81390 , step:  81390 , train loss:  0.042938675953688554 , test loss:  0.0842133940866577\n",
      "Epoch:  81400 , step:  81400 , train loss:  0.04293665561671075 , test loss:  0.08421319003598075\n",
      "Epoch:  81410 , step:  81410 , train loss:  0.042934635641034516 , test loss:  0.08421298618818215\n",
      "Epoch:  81420 , step:  81420 , train loss:  0.0429326160265515 , test loss:  0.0842127825431861\n",
      "Epoch:  81430 , step:  81430 , train loss:  0.04293059677315344 , test loss:  0.08421257910091699\n",
      "Epoch:  81440 , step:  81440 , train loss:  0.04292857788073216 , test loss:  0.08421237586129902\n",
      "Epoch:  81450 , step:  81450 , train loss:  0.04292655934917944 , test loss:  0.08421217282425647\n",
      "Epoch:  81460 , step:  81460 , train loss:  0.0429245411783872 , test loss:  0.08421196998971386\n",
      "Epoch:  81470 , step:  81470 , train loss:  0.042922523368247346 , test loss:  0.08421176735759545\n",
      "Epoch:  81480 , step:  81480 , train loss:  0.04292050591865185 , test loss:  0.08421156492782572\n",
      "Epoch:  81490 , step:  81490 , train loss:  0.04291848882949271 , test loss:  0.08421136270032917\n",
      "Epoch:  81500 , step:  81500 , train loss:  0.042916472100662004 , test loss:  0.08421116067503029\n",
      "Epoch:  81510 , step:  81510 , train loss:  0.04291445573205185 , test loss:  0.08421095885185369\n",
      "Epoch:  81520 , step:  81520 , train loss:  0.04291243972355433 , test loss:  0.08421075723072391\n",
      "Epoch:  81530 , step:  81530 , train loss:  0.042910424075061716 , test loss:  0.08421055581156552\n",
      "Epoch:  81540 , step:  81540 , train loss:  0.04290840878646621 , test loss:  0.08421035459430322\n",
      "Epoch:  81550 , step:  81550 , train loss:  0.042906393857660104 , test loss:  0.08421015357886168\n",
      "Epoch:  81560 , step:  81560 , train loss:  0.04290437928853575 , test loss:  0.08420995276516545\n",
      "Epoch:  81570 , step:  81570 , train loss:  0.04290236507898547 , test loss:  0.0842097521531395\n",
      "Epoch:  81580 , step:  81580 , train loss:  0.04290035122890173 , test loss:  0.08420955174270872\n",
      "Epoch:  81590 , step:  81590 , train loss:  0.042898337738177014 , test loss:  0.08420935153379769\n",
      "Epoch:  81600 , step:  81600 , train loss:  0.04289632460670379 , test loss:  0.08420915152633136\n",
      "Epoch:  81610 , step:  81610 , train loss:  0.042894311834374636 , test loss:  0.0842089517202347\n",
      "Epoch:  81620 , step:  81620 , train loss:  0.04289229942108217 , test loss:  0.08420875211543244\n",
      "Epoch:  81630 , step:  81630 , train loss:  0.042890287366718995 , test loss:  0.08420855271184978\n",
      "Epoch:  81640 , step:  81640 , train loss:  0.042888275671177825 , test loss:  0.08420835350941153\n",
      "Epoch:  81650 , step:  81650 , train loss:  0.042886264334351436 , test loss:  0.084208154508043\n",
      "Epoch:  81660 , step:  81660 , train loss:  0.04288425335613256 , test loss:  0.08420795570766898\n",
      "Epoch:  81670 , step:  81670 , train loss:  0.04288224273641405 , test loss:  0.08420775710821471\n",
      "Epoch:  81680 , step:  81680 , train loss:  0.04288023247508877 , test loss:  0.08420755870960538\n",
      "Epoch:  81690 , step:  81690 , train loss:  0.04287822257204964 , test loss:  0.08420736051176622\n",
      "Epoch:  81700 , step:  81700 , train loss:  0.042876213027189636 , test loss:  0.08420716251462218\n",
      "Epoch:  81710 , step:  81710 , train loss:  0.04287420384040175 , test loss:  0.08420696471809869\n",
      "Epoch:  81720 , step:  81720 , train loss:  0.042872195011579045 , test loss:  0.08420676712212095\n",
      "Epoch:  81730 , step:  81730 , train loss:  0.04287018654061461 , test loss:  0.08420656972661443\n",
      "Epoch:  81740 , step:  81740 , train loss:  0.04286817842740161 , test loss:  0.08420637253150429\n",
      "Epoch:  81750 , step:  81750 , train loss:  0.04286617067183321 , test loss:  0.08420617553671607\n",
      "Epoch:  81760 , step:  81760 , train loss:  0.04286416327380264 , test loss:  0.08420597874217506\n",
      "Epoch:  81770 , step:  81770 , train loss:  0.04286215623320319 , test loss:  0.08420578214780687\n",
      "Epoch:  81780 , step:  81780 , train loss:  0.04286014954992818 , test loss:  0.08420558575353691\n",
      "Epoch:  81790 , step:  81790 , train loss:  0.042858143223871 , test loss:  0.08420538955929072\n",
      "Epoch:  81800 , step:  81800 , train loss:  0.042856137254925 , test loss:  0.08420519356499383\n",
      "Epoch:  81810 , step:  81810 , train loss:  0.04285413164298369 , test loss:  0.08420499777057183\n",
      "Epoch:  81820 , step:  81820 , train loss:  0.04285212638794056 , test loss:  0.08420480217595035\n",
      "Epoch:  81830 , step:  81830 , train loss:  0.04285012148968914 , test loss:  0.0842046067810552\n",
      "Epoch:  81840 , step:  81840 , train loss:  0.042848116948123044 , test loss:  0.08420441158581207\n",
      "Epoch:  81850 , step:  81850 , train loss:  0.042846112763135887 , test loss:  0.0842042165901464\n",
      "Epoch:  81860 , step:  81860 , train loss:  0.04284410893462136 , test loss:  0.08420402179398419\n",
      "Epoch:  81870 , step:  81870 , train loss:  0.04284210546247319 , test loss:  0.08420382719725134\n",
      "Epoch:  81880 , step:  81880 , train loss:  0.04284010234658512 , test loss:  0.08420363279987356\n",
      "Epoch:  81890 , step:  81890 , train loss:  0.04283809958685101 , test loss:  0.08420343860177677\n",
      "Epoch:  81900 , step:  81900 , train loss:  0.04283609718316469 , test loss:  0.08420324460288696\n",
      "Epoch:  81910 , step:  81910 , train loss:  0.04283409513542005 , test loss:  0.0842030508031298\n",
      "Epoch:  81920 , step:  81920 , train loss:  0.04283209344351106 , test loss:  0.08420285720243137\n",
      "Epoch:  81930 , step:  81930 , train loss:  0.04283009210733171 , test loss:  0.08420266380071785\n",
      "Epoch:  81940 , step:  81940 , train loss:  0.04282809112677601 , test loss:  0.08420247059791516\n",
      "Epoch:  81950 , step:  81950 , train loss:  0.042826090501738086 , test loss:  0.08420227759394948\n",
      "Epoch:  81960 , step:  81960 , train loss:  0.04282409023211202 , test loss:  0.0842020847887468\n",
      "Epoch:  81970 , step:  81970 , train loss:  0.04282209031779198 , test loss:  0.0842018921822333\n",
      "Epoch:  81980 , step:  81980 , train loss:  0.042820090758672234 , test loss:  0.08420169977433528\n",
      "Epoch:  81990 , step:  81990 , train loss:  0.04281809155464698 , test loss:  0.08420150756497892\n",
      "Epoch:  82000 , step:  82000 , train loss:  0.04281609270561054 , test loss:  0.08420131555409048\n",
      "Epoch:  82010 , step:  82010 , train loss:  0.04281409421145726 , test loss:  0.08420112374159625\n",
      "Epoch:  82020 , step:  82020 , train loss:  0.042812096072081536 , test loss:  0.08420093212742234\n",
      "Epoch:  82030 , step:  82030 , train loss:  0.0428100982873778 , test loss:  0.08420074071149529\n",
      "Epoch:  82040 , step:  82040 , train loss:  0.042808100857240516 , test loss:  0.08420054949374163\n",
      "Epoch:  82050 , step:  82050 , train loss:  0.04280610378156423 , test loss:  0.08420035847408762\n",
      "Epoch:  82060 , step:  82060 , train loss:  0.04280410706024348 , test loss:  0.08420016765245969\n",
      "Epoch:  82070 , step:  82070 , train loss:  0.04280211069317291 , test loss:  0.0841999770287844\n",
      "Epoch:  82080 , step:  82080 , train loss:  0.04280011468024717 , test loss:  0.08419978660298826\n",
      "Epoch:  82090 , step:  82090 , train loss:  0.04279811902136093 , test loss:  0.084199596374998\n",
      "Epoch:  82100 , step:  82100 , train loss:  0.042796123716408944 , test loss:  0.08419940634473988\n",
      "Epoch:  82110 , step:  82110 , train loss:  0.04279412876528603 , test loss:  0.08419921651214086\n",
      "Epoch:  82120 , step:  82120 , train loss:  0.04279213416788699 , test loss:  0.08419902687712731\n",
      "Epoch:  82130 , step:  82130 , train loss:  0.04279013992410669 , test loss:  0.0841988374396262\n",
      "Epoch:  82140 , step:  82140 , train loss:  0.04278814603384007 , test loss:  0.08419864819956412\n",
      "Epoch:  82150 , step:  82150 , train loss:  0.04278615249698209 , test loss:  0.08419845915686802\n",
      "Epoch:  82160 , step:  82160 , train loss:  0.042784159313427744 , test loss:  0.08419827031146447\n",
      "Epoch:  82170 , step:  82170 , train loss:  0.0427821664830721 , test loss:  0.0841980816632806\n",
      "Epoch:  82180 , step:  82180 , train loss:  0.04278017400581023 , test loss:  0.08419789321224287\n",
      "Epoch:  82190 , step:  82190 , train loss:  0.04277818188153731 , test loss:  0.08419770495827839\n",
      "Epoch:  82200 , step:  82200 , train loss:  0.04277619011014847 , test loss:  0.0841975169013142\n",
      "Epoch:  82210 , step:  82210 , train loss:  0.04277419869153899 , test loss:  0.08419732904127718\n",
      "Epoch:  82220 , step:  82220 , train loss:  0.0427722076256041 , test loss:  0.08419714137809432\n",
      "Epoch:  82230 , step:  82230 , train loss:  0.042770216912239124 , test loss:  0.08419695391169277\n",
      "Epoch:  82240 , step:  82240 , train loss:  0.04276822655133944 , test loss:  0.08419676664199949\n",
      "Epoch:  82250 , step:  82250 , train loss:  0.0427662365428004 , test loss:  0.08419657956894161\n",
      "Epoch:  82260 , step:  82260 , train loss:  0.04276424688651749 , test loss:  0.0841963926924463\n",
      "Epoch:  82270 , step:  82270 , train loss:  0.04276225758238619 , test loss:  0.08419620601244071\n",
      "Epoch:  82280 , step:  82280 , train loss:  0.042760268630302044 , test loss:  0.08419601952885228\n",
      "Epoch:  82290 , step:  82290 , train loss:  0.0427582800301606 , test loss:  0.08419583324160787\n",
      "Epoch:  82300 , step:  82300 , train loss:  0.04275629178185747 , test loss:  0.084195647150635\n",
      "Epoch:  82310 , step:  82310 , train loss:  0.042754303885288336 , test loss:  0.08419546125586094\n",
      "Epoch:  82320 , step:  82320 , train loss:  0.04275231634034892 , test loss:  0.08419527555721297\n",
      "Epoch:  82330 , step:  82330 , train loss:  0.04275032914693495 , test loss:  0.08419509005461845\n",
      "Epoch:  82340 , step:  82340 , train loss:  0.04274834230494219 , test loss:  0.08419490474800502\n",
      "Epoch:  82350 , step:  82350 , train loss:  0.04274635581426653 , test loss:  0.08419471963729995\n",
      "Epoch:  82360 , step:  82360 , train loss:  0.04274436967480383 , test loss:  0.08419453472243067\n",
      "Epoch:  82370 , step:  82370 , train loss:  0.04274238388645 , test loss:  0.08419435000332492\n",
      "Epoch:  82380 , step:  82380 , train loss:  0.042740398449101005 , test loss:  0.08419416547991013\n",
      "Epoch:  82390 , step:  82390 , train loss:  0.0427384133626529 , test loss:  0.08419398115211392\n",
      "Epoch:  82400 , step:  82400 , train loss:  0.04273642862700167 , test loss:  0.08419379701986368\n",
      "Epoch:  82410 , step:  82410 , train loss:  0.04273444424204347 , test loss:  0.08419361308308737\n",
      "Epoch:  82420 , step:  82420 , train loss:  0.04273246020767438 , test loss:  0.08419342934171244\n",
      "Epoch:  82430 , step:  82430 , train loss:  0.04273047652379064 , test loss:  0.0841932457956667\n",
      "Epoch:  82440 , step:  82440 , train loss:  0.04272849319028842 , test loss:  0.08419306244487802\n",
      "Epoch:  82450 , step:  82450 , train loss:  0.042726510207064056 , test loss:  0.08419287928927392\n",
      "Epoch:  82460 , step:  82460 , train loss:  0.0427245275740138 , test loss:  0.08419269632878246\n",
      "Epoch:  82470 , step:  82470 , train loss:  0.04272254529103404 , test loss:  0.08419251356333134\n",
      "Epoch:  82480 , step:  82480 , train loss:  0.042720563358021164 , test loss:  0.08419233099284874\n",
      "Epoch:  82490 , step:  82490 , train loss:  0.042718581774871624 , test loss:  0.08419214861726233\n",
      "Epoch:  82500 , step:  82500 , train loss:  0.04271660054148189 , test loss:  0.08419196643649995\n",
      "Epoch:  82510 , step:  82510 , train loss:  0.04271461965774852 , test loss:  0.08419178445048978\n",
      "Epoch:  82520 , step:  82520 , train loss:  0.04271263912356805 , test loss:  0.0841916026591597\n",
      "Epoch:  82530 , step:  82530 , train loss:  0.042710658938837076 , test loss:  0.08419142106243803\n",
      "Epoch:  82540 , step:  82540 , train loss:  0.04270867910345231 , test loss:  0.08419123966025253\n",
      "Epoch:  82550 , step:  82550 , train loss:  0.04270669961731041 , test loss:  0.08419105845253153\n",
      "Epoch:  82560 , step:  82560 , train loss:  0.04270472048030815 , test loss:  0.08419087743920314\n",
      "Epoch:  82570 , step:  82570 , train loss:  0.0427027416923423 , test loss:  0.08419069662019545\n",
      "Epoch:  82580 , step:  82580 , train loss:  0.04270076325330969 , test loss:  0.08419051599543684\n",
      "Epoch:  82590 , step:  82590 , train loss:  0.04269878516310718 , test loss:  0.08419033556485553\n",
      "Epoch:  82600 , step:  82600 , train loss:  0.0426968074216317 , test loss:  0.08419015532837969\n",
      "Epoch:  82610 , step:  82610 , train loss:  0.0426948300287802 , test loss:  0.08418997528593776\n",
      "Epoch:  82620 , step:  82620 , train loss:  0.042692852984449685 , test loss:  0.08418979543745797\n",
      "Epoch:  82630 , step:  82630 , train loss:  0.04269087628853718 , test loss:  0.0841896157828689\n",
      "Epoch:  82640 , step:  82640 , train loss:  0.0426888999409398 , test loss:  0.08418943632209878\n",
      "Epoch:  82650 , step:  82650 , train loss:  0.04268692394155467 , test loss:  0.0841892570550761\n",
      "Epoch:  82660 , step:  82660 , train loss:  0.042684948290278926 , test loss:  0.08418907798172935\n",
      "Epoch:  82670 , step:  82670 , train loss:  0.04268297298700982 , test loss:  0.08418889910198717\n",
      "Epoch:  82680 , step:  82680 , train loss:  0.04268099803164458 , test loss:  0.084188720415778\n",
      "Epoch:  82690 , step:  82690 , train loss:  0.04267902342408053 , test loss:  0.08418854192303038\n",
      "Epoch:  82700 , step:  82700 , train loss:  0.04267704916421499 , test loss:  0.08418836362367307\n",
      "Epoch:  82710 , step:  82710 , train loss:  0.04267507525194536 , test loss:  0.08418818551763466\n",
      "Epoch:  82720 , step:  82720 , train loss:  0.04267310168716907 , test loss:  0.08418800760484382\n",
      "Epoch:  82730 , step:  82730 , train loss:  0.04267112846978356 , test loss:  0.08418782988522923\n",
      "Epoch:  82740 , step:  82740 , train loss:  0.04266915559968636 , test loss:  0.08418765235871961\n",
      "Epoch:  82750 , step:  82750 , train loss:  0.042667183076775056 , test loss:  0.08418747502524383\n",
      "Epoch:  82760 , step:  82760 , train loss:  0.0426652109009472 , test loss:  0.08418729788473078\n",
      "Epoch:  82770 , step:  82770 , train loss:  0.04266323907210043 , test loss:  0.08418712093710926\n",
      "Epoch:  82780 , step:  82780 , train loss:  0.04266126759013247 , test loss:  0.08418694418230811\n",
      "Epoch:  82790 , step:  82790 , train loss:  0.04265929645494101 , test loss:  0.08418676762025616\n",
      "Epoch:  82800 , step:  82800 , train loss:  0.04265732566642381 , test loss:  0.08418659125088243\n",
      "Epoch:  82810 , step:  82810 , train loss:  0.0426553552244787 , test loss:  0.08418641507411603\n",
      "Epoch:  82820 , step:  82820 , train loss:  0.042653385129003546 , test loss:  0.08418623908988576\n",
      "Epoch:  82830 , step:  82830 , train loss:  0.0426514153798962 , test loss:  0.08418606329812076\n",
      "Epoch:  82840 , step:  82840 , train loss:  0.04264944597705463 , test loss:  0.08418588769875021\n",
      "Epoch:  82850 , step:  82850 , train loss:  0.04264747692037679 , test loss:  0.08418571229170316\n",
      "Epoch:  82860 , step:  82860 , train loss:  0.042645508209760735 , test loss:  0.08418553707690866\n",
      "Epoch:  82870 , step:  82870 , train loss:  0.04264353984510448 , test loss:  0.08418536205429589\n",
      "Epoch:  82880 , step:  82880 , train loss:  0.04264157182630616 , test loss:  0.08418518722379426\n",
      "Epoch:  82890 , step:  82890 , train loss:  0.04263960415326394 , test loss:  0.0841850125853329\n",
      "Epoch:  82900 , step:  82900 , train loss:  0.04263763682587598 , test loss:  0.08418483813884098\n",
      "Epoch:  82910 , step:  82910 , train loss:  0.042635669844040526 , test loss:  0.0841846638842478\n",
      "Epoch:  82920 , step:  82920 , train loss:  0.042633703207655825 , test loss:  0.08418448982148286\n",
      "Epoch:  82930 , step:  82930 , train loss:  0.04263173691662021 , test loss:  0.08418431595047542\n",
      "Epoch:  82940 , step:  82940 , train loss:  0.04262977097083204 , test loss:  0.084184142271155\n",
      "Epoch:  82950 , step:  82950 , train loss:  0.04262780537018973 , test loss:  0.0841839687834508\n",
      "Epoch:  82960 , step:  82960 , train loss:  0.042625840114591716 , test loss:  0.08418379548729285\n",
      "Epoch:  82970 , step:  82970 , train loss:  0.04262387520393645 , test loss:  0.08418362238261003\n",
      "Epoch:  82980 , step:  82980 , train loss:  0.04262191063812249 , test loss:  0.0841834494693322\n",
      "Epoch:  82990 , step:  82990 , train loss:  0.042619946417048415 , test loss:  0.08418327674738889\n",
      "Epoch:  83000 , step:  83000 , train loss:  0.04261798254061281 , test loss:  0.08418310421670953\n",
      "Epoch:  83010 , step:  83010 , train loss:  0.04261601900871432 , test loss:  0.08418293187722378\n",
      "Epoch:  83020 , step:  83020 , train loss:  0.042614055821251655 , test loss:  0.08418275972886155\n",
      "Epoch:  83030 , step:  83030 , train loss:  0.04261209297812353 , test loss:  0.0841825877715524\n",
      "Epoch:  83040 , step:  83040 , train loss:  0.04261013047922875 , test loss:  0.08418241600522619\n",
      "Epoch:  83050 , step:  83050 , train loss:  0.042608168324466136 , test loss:  0.08418224442981252\n",
      "Epoch:  83060 , step:  83060 , train loss:  0.04260620651373451 , test loss:  0.08418207304524124\n",
      "Epoch:  83070 , step:  83070 , train loss:  0.04260424504693283 , test loss:  0.08418190185144202\n",
      "Epoch:  83080 , step:  83080 , train loss:  0.04260228392395998 , test loss:  0.08418173084834492\n",
      "Epoch:  83090 , step:  83090 , train loss:  0.042600323144715 , test loss:  0.0841815600358797\n",
      "Epoch:  83100 , step:  83100 , train loss:  0.04259836270909687 , test loss:  0.08418138941397653\n",
      "Epoch:  83110 , step:  83110 , train loss:  0.04259640261700472 , test loss:  0.0841812189825651\n",
      "Epoch:  83120 , step:  83120 , train loss:  0.042594442868337606 , test loss:  0.08418104874157557\n",
      "Epoch:  83130 , step:  83130 , train loss:  0.0425924834629947 , test loss:  0.08418087869093788\n",
      "Epoch:  83140 , step:  83140 , train loss:  0.042590524400875206 , test loss:  0.08418070883058196\n",
      "Epoch:  83150 , step:  83150 , train loss:  0.042588565681878354 , test loss:  0.0841805391604383\n",
      "Epoch:  83160 , step:  83160 , train loss:  0.04258660730590341 , test loss:  0.08418036968043656\n",
      "Epoch:  83170 , step:  83170 , train loss:  0.042584649272849696 , test loss:  0.08418020039050722\n",
      "Epoch:  83180 , step:  83180 , train loss:  0.042582691582616586 , test loss:  0.08418003129058031\n",
      "Epoch:  83190 , step:  83190 , train loss:  0.042580734235103494 , test loss:  0.08417986238058597\n",
      "Epoch:  83200 , step:  83200 , train loss:  0.04257877723020983 , test loss:  0.0841796936604547\n",
      "Epoch:  83210 , step:  83210 , train loss:  0.0425768205678351 , test loss:  0.0841795251301165\n",
      "Epoch:  83220 , step:  83220 , train loss:  0.04257486424787883 , test loss:  0.08417935678950188\n",
      "Epoch:  83230 , step:  83230 , train loss:  0.04257290827024058 , test loss:  0.0841791886385411\n",
      "Epoch:  83240 , step:  83240 , train loss:  0.04257095263481996 , test loss:  0.08417902067716475\n",
      "Epoch:  83250 , step:  83250 , train loss:  0.042568997341516646 , test loss:  0.08417885290530278\n",
      "Epoch:  83260 , step:  83260 , train loss:  0.04256704239023031 , test loss:  0.08417868532288593\n",
      "Epoch:  83270 , step:  83270 , train loss:  0.04256508778086068 , test loss:  0.08417851792984472\n",
      "Epoch:  83280 , step:  83280 , train loss:  0.042563133513307524 , test loss:  0.08417835072610956\n",
      "Epoch:  83290 , step:  83290 , train loss:  0.042561179587470695 , test loss:  0.08417818371161098\n",
      "Epoch:  83300 , step:  83300 , train loss:  0.04255922600325001 , test loss:  0.08417801688627953\n",
      "Epoch:  83310 , step:  83310 , train loss:  0.042557272760545384 , test loss:  0.0841778502500458\n",
      "Epoch:  83320 , step:  83320 , train loss:  0.04255531985925678 , test loss:  0.08417768380284059\n",
      "Epoch:  83330 , step:  83330 , train loss:  0.04255336729928414 , test loss:  0.08417751754459452\n",
      "Epoch:  83340 , step:  83340 , train loss:  0.04255141508052754 , test loss:  0.08417735147523812\n",
      "Epoch:  83350 , step:  83350 , train loss:  0.04254946320288699 , test loss:  0.08417718559470219\n",
      "Epoch:  83360 , step:  83360 , train loss:  0.0425475116662626 , test loss:  0.08417701990291765\n",
      "Epoch:  83370 , step:  83370 , train loss:  0.042545560470554536 , test loss:  0.08417685439981501\n",
      "Epoch:  83380 , step:  83380 , train loss:  0.04254360961566298 , test loss:  0.08417668908532529\n",
      "Epoch:  83390 , step:  83390 , train loss:  0.04254165910148817 , test loss:  0.08417652395937923\n",
      "Epoch:  83400 , step:  83400 , train loss:  0.04253970892793037 , test loss:  0.08417635902190784\n",
      "Epoch:  83410 , step:  83410 , train loss:  0.04253775909488986 , test loss:  0.08417619427284193\n",
      "Epoch:  83420 , step:  83420 , train loss:  0.042535809602267036 , test loss:  0.08417602971211255\n",
      "Epoch:  83430 , step:  83430 , train loss:  0.042533860449962275 , test loss:  0.08417586533965064\n",
      "Epoch:  83440 , step:  83440 , train loss:  0.042531911637876 , test loss:  0.08417570115538713\n",
      "Epoch:  83450 , step:  83450 , train loss:  0.04252996316590869 , test loss:  0.08417553715925324\n",
      "Epoch:  83460 , step:  83460 , train loss:  0.04252801503396087 , test loss:  0.08417537335117994\n",
      "Epoch:  83470 , step:  83470 , train loss:  0.0425260672419331 , test loss:  0.08417520973109828\n",
      "Epoch:  83480 , step:  83480 , train loss:  0.04252411978972595 , test loss:  0.08417504629893963\n",
      "Epoch:  83490 , step:  83490 , train loss:  0.04252217267724007 , test loss:  0.08417488305463493\n",
      "Epoch:  83500 , step:  83500 , train loss:  0.04252022590437616 , test loss:  0.08417471999811545\n",
      "Epoch:  83510 , step:  83510 , train loss:  0.04251827947103492 , test loss:  0.08417455712931246\n",
      "Epoch:  83520 , step:  83520 , train loss:  0.0425163333771171 , test loss:  0.08417439444815708\n",
      "Epoch:  83530 , step:  83530 , train loss:  0.042514387622523506 , test loss:  0.08417423195458085\n",
      "Epoch:  83540 , step:  83540 , train loss:  0.04251244220715503 , test loss:  0.08417406964851483\n",
      "Epoch:  83550 , step:  83550 , train loss:  0.04251049713091247 , test loss:  0.08417390752989067\n",
      "Epoch:  83560 , step:  83560 , train loss:  0.0425085523936968 , test loss:  0.08417374559863966\n",
      "Epoch:  83570 , step:  83570 , train loss:  0.042506607995409 , test loss:  0.08417358385469316\n",
      "Epoch:  83580 , step:  83580 , train loss:  0.042504663935950045 , test loss:  0.08417342229798258\n",
      "Epoch:  83590 , step:  83590 , train loss:  0.04250272021522097 , test loss:  0.0841732609284396\n",
      "Epoch:  83600 , step:  83600 , train loss:  0.04250077683312288 , test loss:  0.08417309974599545\n",
      "Epoch:  83610 , step:  83610 , train loss:  0.04249883378955691 , test loss:  0.0841729387505819\n",
      "Epoch:  83620 , step:  83620 , train loss:  0.042496891084424206 , test loss:  0.08417277794213057\n",
      "Epoch:  83630 , step:  83630 , train loss:  0.04249494871762595 , test loss:  0.08417261732057295\n",
      "Epoch:  83640 , step:  83640 , train loss:  0.04249300668906347 , test loss:  0.08417245688584064\n",
      "Epoch:  83650 , step:  83650 , train loss:  0.04249106499863801 , test loss:  0.08417229663786545\n",
      "Epoch:  83660 , step:  83660 , train loss:  0.0424891236462509 , test loss:  0.08417213657657918\n",
      "Epoch:  83670 , step:  83670 , train loss:  0.04248718263180349 , test loss:  0.08417197670191336\n",
      "Epoch:  83680 , step:  83680 , train loss:  0.042485241955197234 , test loss:  0.08417181701379983\n",
      "Epoch:  83690 , step:  83690 , train loss:  0.042483301616333534 , test loss:  0.08417165751217032\n",
      "Epoch:  83700 , step:  83700 , train loss:  0.04248136161511391 , test loss:  0.08417149819695673\n",
      "Epoch:  83710 , step:  83710 , train loss:  0.042479421951439895 , test loss:  0.08417133906809086\n",
      "Epoch:  83720 , step:  83720 , train loss:  0.04247748262521306 , test loss:  0.08417118012550479\n",
      "Epoch:  83730 , step:  83730 , train loss:  0.04247554363633502 , test loss:  0.08417102136913021\n",
      "Epoch:  83740 , step:  83740 , train loss:  0.042473604984707404 , test loss:  0.08417086279889929\n",
      "Epoch:  83750 , step:  83750 , train loss:  0.04247166667023188 , test loss:  0.08417070441474381\n",
      "Epoch:  83760 , step:  83760 , train loss:  0.04246972869281028 , test loss:  0.08417054621659598\n",
      "Epoch:  83770 , step:  83770 , train loss:  0.04246779105234433 , test loss:  0.08417038820438784\n",
      "Epoch:  83780 , step:  83780 , train loss:  0.042465853748735786 , test loss:  0.08417023037805131\n",
      "Epoch:  83790 , step:  83790 , train loss:  0.04246391678188656 , test loss:  0.08417007273751864\n",
      "Epoch:  83800 , step:  83800 , train loss:  0.042461980151698546 , test loss:  0.0841699152827219\n",
      "Epoch:  83810 , step:  83810 , train loss:  0.042460043858073664 , test loss:  0.08416975801359322\n",
      "Epoch:  83820 , step:  83820 , train loss:  0.04245810790091389 , test loss:  0.08416960093006504\n",
      "Epoch:  83830 , step:  83830 , train loss:  0.04245617228012123 , test loss:  0.0841694440320693\n",
      "Epoch:  83840 , step:  83840 , train loss:  0.04245423699559776 , test loss:  0.08416928731953831\n",
      "Epoch:  83850 , step:  83850 , train loss:  0.04245230204724556 , test loss:  0.08416913079240451\n",
      "Epoch:  83860 , step:  83860 , train loss:  0.04245036743496675 , test loss:  0.08416897445060036\n",
      "Epoch:  83870 , step:  83870 , train loss:  0.042448433158663534 , test loss:  0.08416881829405774\n",
      "Epoch:  83880 , step:  83880 , train loss:  0.042446499218238114 , test loss:  0.08416866232270949\n",
      "Epoch:  83890 , step:  83890 , train loss:  0.04244456561359275 , test loss:  0.08416850653648786\n",
      "Epoch:  83900 , step:  83900 , train loss:  0.04244263234462974 , test loss:  0.08416835093532539\n",
      "Epoch:  83910 , step:  83910 , train loss:  0.04244069941125137 , test loss:  0.08416819551915446\n",
      "Epoch:  83920 , step:  83920 , train loss:  0.04243876681336011 , test loss:  0.08416804028790767\n",
      "Epoch:  83930 , step:  83930 , train loss:  0.042436834550858296 , test loss:  0.08416788524151739\n",
      "Epoch:  83940 , step:  83940 , train loss:  0.04243490262364839 , test loss:  0.08416773037991641\n",
      "Epoch:  83950 , step:  83950 , train loss:  0.04243297103163292 , test loss:  0.08416757570303726\n",
      "Epoch:  83960 , step:  83960 , train loss:  0.042431039774714394 , test loss:  0.08416742121081247\n",
      "Epoch:  83970 , step:  83970 , train loss:  0.04242910885279541 , test loss:  0.08416726690317483\n",
      "Epoch:  83980 , step:  83980 , train loss:  0.04242717826577855 , test loss:  0.08416711278005706\n",
      "Epoch:  83990 , step:  83990 , train loss:  0.04242524801356651 , test loss:  0.08416695884139183\n",
      "Epoch:  84000 , step:  84000 , train loss:  0.04242331809606193 , test loss:  0.08416680508711202\n",
      "Epoch:  84010 , step:  84010 , train loss:  0.04242138851316759 , test loss:  0.08416665151715011\n",
      "Epoch:  84020 , step:  84020 , train loss:  0.042419459264786234 , test loss:  0.08416649813143935\n",
      "Epoch:  84030 , step:  84030 , train loss:  0.0424175303508207 , test loss:  0.08416634492991233\n",
      "Epoch:  84040 , step:  84040 , train loss:  0.04241560177117381 , test loss:  0.08416619191250181\n",
      "Epoch:  84050 , step:  84050 , train loss:  0.042413673525748485 , test loss:  0.08416603907914105\n",
      "Epoch:  84060 , step:  84060 , train loss:  0.042411745614447625 , test loss:  0.08416588642976286\n",
      "Epoch:  84070 , step:  84070 , train loss:  0.04240981803717424 , test loss:  0.08416573396430005\n",
      "Epoch:  84080 , step:  84080 , train loss:  0.0424078907938313 , test loss:  0.08416558168268586\n",
      "Epoch:  84090 , step:  84090 , train loss:  0.042405963884321905 , test loss:  0.08416542958485301\n",
      "Epoch:  84100 , step:  84100 , train loss:  0.042404037308549095 , test loss:  0.0841652776707349\n",
      "Epoch:  84110 , step:  84110 , train loss:  0.04240211106641602 , test loss:  0.08416512594026442\n",
      "Epoch:  84120 , step:  84120 , train loss:  0.04240018515782585 , test loss:  0.0841649743933749\n",
      "Epoch:  84130 , step:  84130 , train loss:  0.0423982595826818 , test loss:  0.08416482302999945\n",
      "Epoch:  84140 , step:  84140 , train loss:  0.04239633434088711 , test loss:  0.08416467185007097\n",
      "Epoch:  84150 , step:  84150 , train loss:  0.042394409432345063 , test loss:  0.08416452085352298\n",
      "Epoch:  84160 , step:  84160 , train loss:  0.042392484856959005 , test loss:  0.08416437004028852\n",
      "Epoch:  84170 , step:  84170 , train loss:  0.042390560614632276 , test loss:  0.08416421941030108\n",
      "Epoch:  84180 , step:  84180 , train loss:  0.04238863670526831 , test loss:  0.08416406896349395\n",
      "Epoch:  84190 , step:  84190 , train loss:  0.04238671312877053 , test loss:  0.08416391869980039\n",
      "Epoch:  84200 , step:  84200 , train loss:  0.042384789885042434 , test loss:  0.08416376861915353\n",
      "Epoch:  84210 , step:  84210 , train loss:  0.04238286697398752 , test loss:  0.08416361872148732\n",
      "Epoch:  84220 , step:  84220 , train loss:  0.04238094439550939 , test loss:  0.08416346900673481\n",
      "Epoch:  84230 , step:  84230 , train loss:  0.04237902214951163 , test loss:  0.08416331947482958\n",
      "Epoch:  84240 , step:  84240 , train loss:  0.04237710023589785 , test loss:  0.0841631701257051\n",
      "Epoch:  84250 , step:  84250 , train loss:  0.04237517865457179 , test loss:  0.08416302095929484\n",
      "Epoch:  84260 , step:  84260 , train loss:  0.04237325740543714 , test loss:  0.08416287197553253\n",
      "Epoch:  84270 , step:  84270 , train loss:  0.04237133648839765 , test loss:  0.08416272317435154\n",
      "Epoch:  84280 , step:  84280 , train loss:  0.04236941590335714 , test loss:  0.08416257455568563\n",
      "Epoch:  84290 , step:  84290 , train loss:  0.04236749565021943 , test loss:  0.08416242611946831\n",
      "Epoch:  84300 , step:  84300 , train loss:  0.042365575728888404 , test loss:  0.08416227786563323\n",
      "Epoch:  84310 , step:  84310 , train loss:  0.04236365613926797 , test loss:  0.08416212979411432\n",
      "Epoch:  84320 , step:  84320 , train loss:  0.04236173688126211 , test loss:  0.08416198190484522\n",
      "Epoch:  84330 , step:  84330 , train loss:  0.04235981795477479 , test loss:  0.08416183419775959\n",
      "Epoch:  84340 , step:  84340 , train loss:  0.04235789935971005 , test loss:  0.08416168667279128\n",
      "Epoch:  84350 , step:  84350 , train loss:  0.04235598109597198 , test loss:  0.08416153932987411\n",
      "Epoch:  84360 , step:  84360 , train loss:  0.04235406316346465 , test loss:  0.08416139216894203\n",
      "Epoch:  84370 , step:  84370 , train loss:  0.04235214556209226 , test loss:  0.08416124518992885\n",
      "Epoch:  84380 , step:  84380 , train loss:  0.04235022829175897 , test loss:  0.08416109839276846\n",
      "Epoch:  84390 , step:  84390 , train loss:  0.04234831135236901 , test loss:  0.08416095177739479\n",
      "Epoch:  84400 , step:  84400 , train loss:  0.042346394743826676 , test loss:  0.08416080534374172\n",
      "Epoch:  84410 , step:  84410 , train loss:  0.042344478466036214 , test loss:  0.08416065909174361\n",
      "Epoch:  84420 , step:  84420 , train loss:  0.04234256251890203 , test loss:  0.08416051302133411\n",
      "Epoch:  84430 , step:  84430 , train loss:  0.04234064690232847 , test loss:  0.0841603671324476\n",
      "Epoch:  84440 , step:  84440 , train loss:  0.04233873161621998 , test loss:  0.0841602214250179\n",
      "Epoch:  84450 , step:  84450 , train loss:  0.042336816660481016 , test loss:  0.08416007589897939\n",
      "Epoch:  84460 , step:  84460 , train loss:  0.04233490203501607 , test loss:  0.08415993055426595\n",
      "Epoch:  84470 , step:  84470 , train loss:  0.04233298773972968 , test loss:  0.0841597853908119\n",
      "Epoch:  84480 , step:  84480 , train loss:  0.04233107377452645 , test loss:  0.08415964040855149\n",
      "Epoch:  84490 , step:  84490 , train loss:  0.042329160139310956 , test loss:  0.084159495607419\n",
      "Epoch:  84500 , step:  84500 , train loss:  0.0423272468339879 , test loss:  0.08415935098734859\n",
      "Epoch:  84510 , step:  84510 , train loss:  0.042325333858461926 , test loss:  0.08415920654827462\n",
      "Epoch:  84520 , step:  84520 , train loss:  0.04232342121263783 , test loss:  0.08415906229013134\n",
      "Epoch:  84530 , step:  84530 , train loss:  0.0423215088964203 , test loss:  0.08415891821285326\n",
      "Epoch:  84540 , step:  84540 , train loss:  0.042319596909714215 , test loss:  0.08415877431637472\n",
      "Epoch:  84550 , step:  84550 , train loss:  0.0423176852524244 , test loss:  0.08415863060063009\n",
      "Epoch:  84560 , step:  84560 , train loss:  0.04231577392445574 , test loss:  0.08415848706555372\n",
      "Epoch:  84570 , step:  84570 , train loss:  0.04231386292571317 , test loss:  0.08415834371108034\n",
      "Epoch:  84580 , step:  84580 , train loss:  0.04231195225610167 , test loss:  0.08415820053714437\n",
      "Epoch:  84590 , step:  84590 , train loss:  0.042310041915526196 , test loss:  0.0841580575436803\n",
      "Epoch:  84600 , step:  84600 , train loss:  0.04230813190389183 , test loss:  0.08415791473062283\n",
      "Epoch:  84610 , step:  84610 , train loss:  0.04230622222110364 , test loss:  0.08415777209790636\n",
      "Epoch:  84620 , step:  84620 , train loss:  0.042304312867066755 , test loss:  0.0841576296454657\n",
      "Epoch:  84630 , step:  84630 , train loss:  0.04230240384168633 , test loss:  0.08415748737323543\n",
      "Epoch:  84640 , step:  84640 , train loss:  0.04230049514486753 , test loss:  0.08415734528115029\n",
      "Epoch:  84650 , step:  84650 , train loss:  0.04229858677651562 , test loss:  0.08415720336914477\n",
      "Epoch:  84660 , step:  84660 , train loss:  0.042296678736535864 , test loss:  0.08415706163715385\n",
      "Epoch:  84670 , step:  84670 , train loss:  0.04229477102483359 , test loss:  0.0841569200851123\n",
      "Epoch:  84680 , step:  84680 , train loss:  0.04229286364131412 , test loss:  0.08415677871295502\n",
      "Epoch:  84690 , step:  84690 , train loss:  0.04229095658588285 , test loss:  0.08415663752061661\n",
      "Epoch:  84700 , step:  84700 , train loss:  0.04228904985844523 , test loss:  0.08415649650803195\n",
      "Epoch:  84710 , step:  84710 , train loss:  0.04228714345890668 , test loss:  0.08415635567513617\n",
      "Epoch:  84720 , step:  84720 , train loss:  0.04228523738717274 , test loss:  0.08415621502186406\n",
      "Epoch:  84730 , step:  84730 , train loss:  0.04228333164314893 , test loss:  0.08415607454815058\n",
      "Epoch:  84740 , step:  84740 , train loss:  0.042281426226740866 , test loss:  0.08415593425393077\n",
      "Epoch:  84750 , step:  84750 , train loss:  0.042279521137854124 , test loss:  0.08415579413913944\n",
      "Epoch:  84760 , step:  84760 , train loss:  0.042277616376394385 , test loss:  0.08415565420371189\n",
      "Epoch:  84770 , step:  84770 , train loss:  0.04227571194226732 , test loss:  0.08415551444758301\n",
      "Epoch:  84780 , step:  84780 , train loss:  0.04227380783537866 , test loss:  0.08415537487068797\n",
      "Epoch:  84790 , step:  84790 , train loss:  0.04227190405563421 , test loss:  0.08415523547296205\n",
      "Epoch:  84800 , step:  84800 , train loss:  0.04227000060293974 , test loss:  0.0841550962543401\n",
      "Epoch:  84810 , step:  84810 , train loss:  0.04226809747720113 , test loss:  0.08415495721475758\n",
      "Epoch:  84820 , step:  84820 , train loss:  0.04226619467832426 , test loss:  0.08415481835414966\n",
      "Epoch:  84830 , step:  84830 , train loss:  0.04226429220621503 , test loss:  0.08415467967245138\n",
      "Epoch:  84840 , step:  84840 , train loss:  0.0422623900607794 , test loss:  0.08415454116959828\n",
      "Epoch:  84850 , step:  84850 , train loss:  0.04226048824192338 , test loss:  0.08415440284552554\n",
      "Epoch:  84860 , step:  84860 , train loss:  0.04225858674955302 , test loss:  0.08415426470016849\n",
      "Epoch:  84870 , step:  84870 , train loss:  0.04225668558357438 , test loss:  0.08415412673346258\n",
      "Epoch:  84880 , step:  84880 , train loss:  0.04225478474389358 , test loss:  0.08415398894534308\n",
      "Epoch:  84890 , step:  84890 , train loss:  0.04225288423041675 , test loss:  0.08415385133574559\n",
      "Epoch:  84900 , step:  84900 , train loss:  0.0422509840430501 , test loss:  0.08415371390460545\n",
      "Epoch:  84910 , step:  84910 , train loss:  0.04224908418169986 , test loss:  0.08415357665185806\n",
      "Epoch:  84920 , step:  84920 , train loss:  0.04224718464627229 , test loss:  0.08415343957743911\n",
      "Epoch:  84930 , step:  84930 , train loss:  0.04224528543667368 , test loss:  0.08415330268128406\n",
      "Epoch:  84940 , step:  84940 , train loss:  0.04224338655281039 , test loss:  0.08415316596332846\n",
      "Epoch:  84950 , step:  84950 , train loss:  0.04224148799458878 , test loss:  0.08415302942350801\n",
      "Epoch:  84960 , step:  84960 , train loss:  0.04223958976191526 , test loss:  0.08415289306175827\n",
      "Epoch:  84970 , step:  84970 , train loss:  0.042237691854696315 , test loss:  0.08415275687801478\n",
      "Epoch:  84980 , step:  84980 , train loss:  0.04223579427283842 , test loss:  0.08415262087221355\n",
      "Epoch:  84990 , step:  84990 , train loss:  0.04223389701624808 , test loss:  0.08415248504428985\n",
      "Epoch:  85000 , step:  85000 , train loss:  0.04223200008483192 , test loss:  0.08415234939417962\n",
      "Epoch:  85010 , step:  85010 , train loss:  0.04223010347849647 , test loss:  0.08415221392181878\n",
      "Epoch:  85020 , step:  85020 , train loss:  0.04222820719714844 , test loss:  0.08415207862714312\n",
      "Epoch:  85030 , step:  85030 , train loss:  0.04222631124069447 , test loss:  0.08415194351008828\n",
      "Epoch:  85040 , step:  85040 , train loss:  0.04222441560904128 , test loss:  0.08415180857059022\n",
      "Epoch:  85050 , step:  85050 , train loss:  0.04222252030209565 , test loss:  0.08415167380858479\n",
      "Epoch:  85060 , step:  85060 , train loss:  0.04222062531976435 , test loss:  0.08415153922400806\n",
      "Epoch:  85070 , step:  85070 , train loss:  0.042218730661954224 , test loss:  0.08415140481679567\n",
      "Epoch:  85080 , step:  85080 , train loss:  0.042216836328572115 , test loss:  0.08415127058688404\n",
      "Epoch:  85090 , step:  85090 , train loss:  0.04221494231952499 , test loss:  0.08415113653420887\n",
      "Epoch:  85100 , step:  85100 , train loss:  0.04221304863471974 , test loss:  0.0841510026587062\n",
      "Epoch:  85110 , step:  85110 , train loss:  0.04221115527406331 , test loss:  0.08415086896031203\n",
      "Epoch:  85120 , step:  85120 , train loss:  0.04220926223746281 , test loss:  0.0841507354389627\n",
      "Epoch:  85130 , step:  85130 , train loss:  0.04220736952482525 , test loss:  0.08415060209459416\n",
      "Epoch:  85140 , step:  85140 , train loss:  0.04220547713605772 , test loss:  0.08415046892714255\n",
      "Epoch:  85150 , step:  85150 , train loss:  0.042203585071067334 , test loss:  0.08415033593654406\n",
      "Epoch:  85160 , step:  85160 , train loss:  0.04220169332976129 , test loss:  0.08415020312273502\n",
      "Epoch:  85170 , step:  85170 , train loss:  0.04219980191204681 , test loss:  0.08415007048565169\n",
      "Epoch:  85180 , step:  85180 , train loss:  0.0421979108178311 , test loss:  0.0841499380252301\n",
      "Epoch:  85190 , step:  85190 , train loss:  0.04219602004702142 , test loss:  0.08414980574140653\n",
      "Epoch:  85200 , step:  85200 , train loss:  0.04219412959952516 , test loss:  0.08414967363411754\n",
      "Epoch:  85210 , step:  85210 , train loss:  0.042192239475249624 , test loss:  0.08414954170329939\n",
      "Epoch:  85220 , step:  85220 , train loss:  0.04219034967410221 , test loss:  0.08414940994888842\n",
      "Epoch:  85230 , step:  85230 , train loss:  0.04218846019599036 , test loss:  0.08414927837082103\n",
      "Epoch:  85240 , step:  85240 , train loss:  0.042186571040821526 , test loss:  0.08414914696903372\n",
      "Epoch:  85250 , step:  85250 , train loss:  0.04218468220850322 , test loss:  0.08414901574346294\n",
      "Epoch:  85260 , step:  85260 , train loss:  0.04218279369894298 , test loss:  0.08414888469404516\n",
      "Epoch:  85270 , step:  85270 , train loss:  0.042180905512048374 , test loss:  0.08414875382071697\n",
      "Epoch:  85280 , step:  85280 , train loss:  0.04217901764772706 , test loss:  0.08414862312341483\n",
      "Epoch:  85290 , step:  85290 , train loss:  0.04217713010588666 , test loss:  0.08414849260207535\n",
      "Epoch:  85300 , step:  85300 , train loss:  0.04217524288643486 , test loss:  0.08414836225663513\n",
      "Epoch:  85310 , step:  85310 , train loss:  0.04217335598927937 , test loss:  0.08414823208703079\n",
      "Epoch:  85320 , step:  85320 , train loss:  0.04217146941432802 , test loss:  0.08414810209319898\n",
      "Epoch:  85330 , step:  85330 , train loss:  0.04216958316148854 , test loss:  0.0841479722750765\n",
      "Epoch:  85340 , step:  85340 , train loss:  0.042167697230668794 , test loss:  0.08414784263260003\n",
      "Epoch:  85350 , step:  85350 , train loss:  0.04216581162177667 , test loss:  0.08414771316570623\n",
      "Epoch:  85360 , step:  85360 , train loss:  0.04216392633472008 , test loss:  0.0841475838743319\n",
      "Epoch:  85370 , step:  85370 , train loss:  0.04216204136940695 , test loss:  0.08414745475841402\n",
      "Epoch:  85380 , step:  85380 , train loss:  0.0421601567257453 , test loss:  0.08414732581788918\n",
      "Epoch:  85390 , step:  85390 , train loss:  0.04215827240364313 , test loss:  0.08414719705269444\n",
      "Epoch:  85400 , step:  85400 , train loss:  0.04215638840300849 , test loss:  0.08414706846276654\n",
      "Epoch:  85410 , step:  85410 , train loss:  0.042154504723749514 , test loss:  0.08414694004804259\n",
      "Epoch:  85420 , step:  85420 , train loss:  0.042152621365774294 , test loss:  0.08414681180845937\n",
      "Epoch:  85430 , step:  85430 , train loss:  0.042150738328991064 , test loss:  0.0841466837439537\n",
      "Epoch:  85440 , step:  85440 , train loss:  0.04214885561330795 , test loss:  0.08414655585446305\n",
      "Epoch:  85450 , step:  85450 , train loss:  0.04214697321863327 , test loss:  0.08414642813992415\n",
      "Epoch:  85460 , step:  85460 , train loss:  0.04214509114487524 , test loss:  0.08414630060027399\n",
      "Epoch:  85470 , step:  85470 , train loss:  0.04214320939194224 , test loss:  0.08414617323544994\n",
      "Epoch:  85480 , step:  85480 , train loss:  0.04214132795974259 , test loss:  0.08414604604538883\n",
      "Epoch:  85490 , step:  85490 , train loss:  0.042139446848184686 , test loss:  0.08414591903002792\n",
      "Epoch:  85500 , step:  85500 , train loss:  0.04213756605717697 , test loss:  0.08414579218930456\n",
      "Epoch:  85510 , step:  85510 , train loss:  0.04213568558662791 , test loss:  0.0841456655231557\n",
      "Epoch:  85520 , step:  85520 , train loss:  0.04213380543644597 , test loss:  0.08414553903151863\n",
      "Epoch:  85530 , step:  85530 , train loss:  0.042131925606539736 , test loss:  0.08414541271433067\n",
      "Epoch:  85540 , step:  85540 , train loss:  0.04213004609681776 , test loss:  0.08414528657152916\n",
      "Epoch:  85550 , step:  85550 , train loss:  0.04212816690718868 , test loss:  0.0841451606030513\n",
      "Epoch:  85560 , step:  85560 , train loss:  0.04212628803756111 , test loss:  0.08414503480883441\n",
      "Epoch:  85570 , step:  85570 , train loss:  0.04212440948784375 , test loss:  0.08414490918881597\n",
      "Epoch:  85580 , step:  85580 , train loss:  0.04212253125794533 , test loss:  0.08414478374293337\n",
      "Epoch:  85590 , step:  85590 , train loss:  0.042120653347774616 , test loss:  0.084144658471124\n",
      "Epoch:  85600 , step:  85600 , train loss:  0.04211877575724036 , test loss:  0.08414453337332518\n",
      "Epoch:  85610 , step:  85610 , train loss:  0.04211689848625145 , test loss:  0.08414440844947463\n",
      "Epoch:  85620 , step:  85620 , train loss:  0.04211502153471674 , test loss:  0.08414428369950977\n",
      "Epoch:  85630 , step:  85630 , train loss:  0.042113144902545106 , test loss:  0.08414415912336809\n",
      "Epoch:  85640 , step:  85640 , train loss:  0.04211126858964552 , test loss:  0.08414403472098733\n",
      "Epoch:  85650 , step:  85650 , train loss:  0.04210939259592694 , test loss:  0.08414391049230487\n",
      "Epoch:  85660 , step:  85660 , train loss:  0.042107516921298396 , test loss:  0.08414378643725842\n",
      "Epoch:  85670 , step:  85670 , train loss:  0.04210564156566895 , test loss:  0.0841436625557856\n",
      "Epoch:  85680 , step:  85680 , train loss:  0.04210376652894765 , test loss:  0.08414353884782425\n",
      "Epoch:  85690 , step:  85690 , train loss:  0.04210189181104366 , test loss:  0.08414341531331183\n",
      "Epoch:  85700 , step:  85700 , train loss:  0.04210001741186612 , test loss:  0.08414329195218632\n",
      "Epoch:  85710 , step:  85710 , train loss:  0.04209814333132421 , test loss:  0.08414316876438531\n",
      "Epoch:  85720 , step:  85720 , train loss:  0.04209626956932723 , test loss:  0.0841430457498466\n",
      "Epoch:  85730 , step:  85730 , train loss:  0.042094396125784375 , test loss:  0.08414292290850808\n",
      "Epoch:  85740 , step:  85740 , train loss:  0.04209252300060496 , test loss:  0.08414280024030746\n",
      "Epoch:  85750 , step:  85750 , train loss:  0.04209065019369838 , test loss:  0.08414267774518291\n",
      "Epoch:  85760 , step:  85760 , train loss:  0.042088777704973976 , test loss:  0.08414255542307209\n",
      "Epoch:  85770 , step:  85770 , train loss:  0.04208690553434119 , test loss:  0.08414243327391309\n",
      "Epoch:  85780 , step:  85780 , train loss:  0.042085033681709405 , test loss:  0.08414231129764373\n",
      "Epoch:  85790 , step:  85790 , train loss:  0.042083162146988186 , test loss:  0.08414218949420199\n",
      "Epoch:  85800 , step:  85800 , train loss:  0.04208129093008701 , test loss:  0.08414206786352589\n",
      "Epoch:  85810 , step:  85810 , train loss:  0.04207942003091545 , test loss:  0.08414194640555354\n",
      "Epoch:  85820 , step:  85820 , train loss:  0.0420775494493831 , test loss:  0.08414182512022306\n",
      "Epoch:  85830 , step:  85830 , train loss:  0.0420756791853996 , test loss:  0.08414170400747242\n",
      "Epoch:  85840 , step:  85840 , train loss:  0.04207380923887461 , test loss:  0.08414158306723975\n",
      "Epoch:  85850 , step:  85850 , train loss:  0.04207193960971785 , test loss:  0.08414146229946325\n",
      "Epoch:  85860 , step:  85860 , train loss:  0.042070070297839005 , test loss:  0.08414134170408107\n",
      "Epoch:  85870 , step:  85870 , train loss:  0.04206820130314793 , test loss:  0.08414122128103164\n",
      "Epoch:  85880 , step:  85880 , train loss:  0.04206633262555439 , test loss:  0.084141101030253\n",
      "Epoch:  85890 , step:  85890 , train loss:  0.04206446426496824 , test loss:  0.08414098095168328\n",
      "Epoch:  85900 , step:  85900 , train loss:  0.042062596221299396 , test loss:  0.0841408610452608\n",
      "Epoch:  85910 , step:  85910 , train loss:  0.042060728494457716 , test loss:  0.08414074131092422\n",
      "Epoch:  85920 , step:  85920 , train loss:  0.042058861084353194 , test loss:  0.08414062174861155\n",
      "Epoch:  85930 , step:  85930 , train loss:  0.04205699399089584 , test loss:  0.08414050235826123\n",
      "Epoch:  85940 , step:  85940 , train loss:  0.042055127213995666 , test loss:  0.0841403831398117\n",
      "Epoch:  85950 , step:  85950 , train loss:  0.042053260753562714 , test loss:  0.08414026409320155\n",
      "Epoch:  85960 , step:  85960 , train loss:  0.042051394609507096 , test loss:  0.08414014521836892\n",
      "Epoch:  85970 , step:  85970 , train loss:  0.04204952878173897 , test loss:  0.0841400265152525\n",
      "Epoch:  85980 , step:  85980 , train loss:  0.042047663270168456 , test loss:  0.08413990798379055\n",
      "Epoch:  85990 , step:  85990 , train loss:  0.04204579807470584 , test loss:  0.084139789623922\n",
      "Epoch:  86000 , step:  86000 , train loss:  0.042043933195261306 , test loss:  0.08413967143558516\n",
      "Epoch:  86010 , step:  86010 , train loss:  0.04204206863174515 , test loss:  0.0841395534187187\n",
      "Epoch:  86020 , step:  86020 , train loss:  0.042040204384067664 , test loss:  0.08413943557326113\n",
      "Epoch:  86030 , step:  86030 , train loss:  0.042038340452139264 , test loss:  0.08413931789915136\n",
      "Epoch:  86040 , step:  86040 , train loss:  0.04203647683587026 , test loss:  0.08413920039632784\n",
      "Epoch:  86050 , step:  86050 , train loss:  0.04203461353517114 , test loss:  0.08413908306472924\n",
      "Epoch:  86060 , step:  86060 , train loss:  0.04203275054995231 , test loss:  0.08413896590429447\n",
      "Epoch:  86070 , step:  86070 , train loss:  0.04203088788012428 , test loss:  0.08413884891496212\n",
      "Epoch:  86080 , step:  86080 , train loss:  0.042029025525597584 , test loss:  0.08413873209667108\n",
      "Epoch:  86090 , step:  86090 , train loss:  0.0420271634862828 , test loss:  0.08413861544936015\n",
      "Epoch:  86100 , step:  86100 , train loss:  0.04202530176209054 , test loss:  0.08413849897296816\n",
      "Epoch:  86110 , step:  86110 , train loss:  0.042023440352931374 , test loss:  0.08413838266743409\n",
      "Epoch:  86120 , step:  86120 , train loss:  0.04202157925871602 , test loss:  0.08413826653269656\n",
      "Epoch:  86130 , step:  86130 , train loss:  0.04201971847935519 , test loss:  0.0841381505686946\n",
      "Epoch:  86140 , step:  86140 , train loss:  0.04201785801475965 , test loss:  0.08413803477536734\n",
      "Epoch:  86150 , step:  86150 , train loss:  0.042015997864840116 , test loss:  0.08413791915265352\n",
      "Epoch:  86160 , step:  86160 , train loss:  0.042014138029507424 , test loss:  0.08413780370049219\n",
      "Epoch:  86170 , step:  86170 , train loss:  0.04201227850867247 , test loss:  0.08413768841882242\n",
      "Epoch:  86180 , step:  86180 , train loss:  0.042010419302246076 , test loss:  0.0841375733075834\n",
      "Epoch:  86190 , step:  86190 , train loss:  0.042008560410139194 , test loss:  0.08413745836671391\n",
      "Epoch:  86200 , step:  86200 , train loss:  0.04200670183226278 , test loss:  0.08413734359615349\n",
      "Epoch:  86210 , step:  86210 , train loss:  0.04200484356852784 , test loss:  0.08413722899584088\n",
      "Epoch:  86220 , step:  86220 , train loss:  0.04200298561884535 , test loss:  0.08413711456571553\n",
      "Epoch:  86230 , step:  86230 , train loss:  0.04200112798312642 , test loss:  0.08413700030571629\n",
      "Epoch:  86240 , step:  86240 , train loss:  0.041999270661282145 , test loss:  0.08413688621578283\n",
      "Epoch:  86250 , step:  86250 , train loss:  0.04199741365322365 , test loss:  0.08413677229585399\n",
      "Epoch:  86260 , step:  86260 , train loss:  0.04199555695886208 , test loss:  0.0841366585458692\n",
      "Epoch:  86270 , step:  86270 , train loss:  0.041993700578108674 , test loss:  0.08413654496576786\n",
      "Epoch:  86280 , step:  86280 , train loss:  0.04199184451087464 , test loss:  0.08413643155548918\n",
      "Epoch:  86290 , step:  86290 , train loss:  0.041989988757071287 , test loss:  0.08413631831497248\n",
      "Epoch:  86300 , step:  86300 , train loss:  0.04198813331660991 , test loss:  0.08413620524415723\n",
      "Epoch:  86310 , step:  86310 , train loss:  0.04198627818940184 , test loss:  0.08413609234298286\n",
      "Epoch:  86320 , step:  86320 , train loss:  0.04198442337535848 , test loss:  0.08413597961138883\n",
      "Epoch:  86330 , step:  86330 , train loss:  0.04198256887439123 , test loss:  0.08413586704931444\n",
      "Epoch:  86340 , step:  86340 , train loss:  0.04198071468641154 , test loss:  0.08413575465669913\n",
      "Epoch:  86350 , step:  86350 , train loss:  0.041978860811330905 , test loss:  0.08413564243348264\n",
      "Epoch:  86360 , step:  86360 , train loss:  0.04197700724906087 , test loss:  0.08413553037960453\n",
      "Epoch:  86370 , step:  86370 , train loss:  0.04197515399951294 , test loss:  0.08413541849500421\n",
      "Epoch:  86380 , step:  86380 , train loss:  0.041973301062598715 , test loss:  0.08413530677962143\n",
      "Epoch:  86390 , step:  86390 , train loss:  0.041971448438229836 , test loss:  0.08413519523339562\n",
      "Epoch:  86400 , step:  86400 , train loss:  0.04196959612631799 , test loss:  0.08413508385626654\n",
      "Epoch:  86410 , step:  86410 , train loss:  0.04196774412677484 , test loss:  0.0841349726481738\n",
      "Epoch:  86420 , step:  86420 , train loss:  0.04196589243951212 , test loss:  0.08413486160905721\n",
      "Epoch:  86430 , step:  86430 , train loss:  0.04196404106444162 , test loss:  0.08413475073885637\n",
      "Epoch:  86440 , step:  86440 , train loss:  0.04196219000147513 , test loss:  0.0841346400375114\n",
      "Epoch:  86450 , step:  86450 , train loss:  0.04196033925052448 , test loss:  0.08413452950496166\n",
      "Epoch:  86460 , step:  86460 , train loss:  0.041958488811501535 , test loss:  0.08413441914114717\n",
      "Epoch:  86470 , step:  86470 , train loss:  0.041956638684318204 , test loss:  0.0841343089460076\n",
      "Epoch:  86480 , step:  86480 , train loss:  0.04195478886888646 , test loss:  0.084134198919483\n",
      "Epoch:  86490 , step:  86490 , train loss:  0.04195293936511824 , test loss:  0.08413408906151311\n",
      "Epoch:  86500 , step:  86500 , train loss:  0.04195109017292558 , test loss:  0.08413397937203788\n",
      "Epoch:  86510 , step:  86510 , train loss:  0.04194924129222052 , test loss:  0.08413386985099738\n",
      "Epoch:  86520 , step:  86520 , train loss:  0.04194739272291514 , test loss:  0.08413376049833146\n",
      "Epoch:  86530 , step:  86530 , train loss:  0.04194554446492155 , test loss:  0.08413365131398033\n",
      "Epoch:  86540 , step:  86540 , train loss:  0.041943696518151934 , test loss:  0.08413354229788375\n",
      "Epoch:  86550 , step:  86550 , train loss:  0.04194184888251841 , test loss:  0.08413343344998173\n",
      "Epoch:  86560 , step:  86560 , train loss:  0.04194000155793327 , test loss:  0.08413332477021461\n",
      "Epoch:  86570 , step:  86570 , train loss:  0.04193815454430874 , test loss:  0.08413321625852234\n",
      "Epoch:  86580 , step:  86580 , train loss:  0.041936307841557115 , test loss:  0.08413310791484507\n",
      "Epoch:  86590 , step:  86590 , train loss:  0.04193446144959069 , test loss:  0.08413299973912297\n",
      "Epoch:  86600 , step:  86600 , train loss:  0.04193261536832186 , test loss:  0.08413289173129627\n",
      "Epoch:  86610 , step:  86610 , train loss:  0.04193076959766302 , test loss:  0.08413278389130498\n",
      "Epoch:  86620 , step:  86620 , train loss:  0.0419289241375266 , test loss:  0.08413267621908968\n",
      "Epoch:  86630 , step:  86630 , train loss:  0.04192707898782505 , test loss:  0.08413256871459045\n",
      "Epoch:  86640 , step:  86640 , train loss:  0.04192523414847086 , test loss:  0.0841324613777474\n",
      "Epoch:  86650 , step:  86650 , train loss:  0.041923389619376576 , test loss:  0.08413235420850111\n",
      "Epoch:  86660 , step:  86660 , train loss:  0.04192154540045478 , test loss:  0.08413224720679191\n",
      "Epoch:  86670 , step:  86670 , train loss:  0.04191970149161805 , test loss:  0.08413214037256005\n",
      "Epoch:  86680 , step:  86680 , train loss:  0.041917857892779034 , test loss:  0.08413203370574586\n",
      "Epoch:  86690 , step:  86690 , train loss:  0.04191601460385042 , test loss:  0.08413192720628991\n",
      "Epoch:  86700 , step:  86700 , train loss:  0.04191417162474488 , test loss:  0.08413182087413264\n",
      "Epoch:  86710 , step:  86710 , train loss:  0.041912328955375174 , test loss:  0.08413171470921454\n",
      "Epoch:  86720 , step:  86720 , train loss:  0.041910486595654106 , test loss:  0.08413160871147606\n",
      "Epoch:  86730 , step:  86730 , train loss:  0.041908644545494436 , test loss:  0.08413150288085762\n",
      "Epoch:  86740 , step:  86740 , train loss:  0.04190680280480903 , test loss:  0.08413139721730008\n",
      "Epoch:  86750 , step:  86750 , train loss:  0.04190496137351076 , test loss:  0.08413129172074367\n",
      "Epoch:  86760 , step:  86760 , train loss:  0.041903120251512556 , test loss:  0.08413118639112928\n",
      "Epoch:  86770 , step:  86770 , train loss:  0.041901279438727346 , test loss:  0.08413108122839734\n",
      "Epoch:  86780 , step:  86780 , train loss:  0.041899438935068124 , test loss:  0.08413097623248864\n",
      "Epoch:  86790 , step:  86790 , train loss:  0.041897598740447926 , test loss:  0.08413087140334385\n",
      "Epoch:  86800 , step:  86800 , train loss:  0.04189575885477974 , test loss:  0.08413076674090356\n",
      "Epoch:  86810 , step:  86810 , train loss:  0.04189391927797672 , test loss:  0.08413066224510866\n",
      "Epoch:  86820 , step:  86820 , train loss:  0.04189208000995197 , test loss:  0.08413055791589988\n",
      "Epoch:  86830 , step:  86830 , train loss:  0.04189024105061861 , test loss:  0.08413045375321795\n",
      "Epoch:  86840 , step:  86840 , train loss:  0.04188840239988985 , test loss:  0.0841303497570037\n",
      "Epoch:  86850 , step:  86850 , train loss:  0.04188656405767894 , test loss:  0.08413024592719802\n",
      "Epoch:  86860 , step:  86860 , train loss:  0.04188472602389909 , test loss:  0.08413014226374181\n",
      "Epoch:  86870 , step:  86870 , train loss:  0.0418828882984636 , test loss:  0.0841300387665759\n",
      "Epoch:  86880 , step:  86880 , train loss:  0.04188105088128583 , test loss:  0.0841299354356411\n",
      "Epoch:  86890 , step:  86890 , train loss:  0.041879213772279114 , test loss:  0.08412983227087846\n",
      "Epoch:  86900 , step:  86900 , train loss:  0.04187737697135686 , test loss:  0.08412972927222911\n",
      "Epoch:  86910 , step:  86910 , train loss:  0.04187554047843246 , test loss:  0.08412962643963384\n",
      "Epoch:  86920 , step:  86920 , train loss:  0.041873704293419425 , test loss:  0.08412952377303391\n",
      "Epoch:  86930 , step:  86930 , train loss:  0.041871868416231216 , test loss:  0.08412942127236997\n",
      "Epoch:  86940 , step:  86940 , train loss:  0.041870032846781376 , test loss:  0.08412931893758338\n",
      "Epoch:  86950 , step:  86950 , train loss:  0.04186819758498348 , test loss:  0.08412921676861525\n",
      "Epoch:  86960 , step:  86960 , train loss:  0.0418663626307511 , test loss:  0.08412911476540677\n",
      "Epoch:  86970 , step:  86970 , train loss:  0.04186452798399791 , test loss:  0.08412901292789879\n",
      "Epoch:  86980 , step:  86980 , train loss:  0.04186269364463754 , test loss:  0.08412891125603289\n",
      "Epoch:  86990 , step:  86990 , train loss:  0.0418608596125837 , test loss:  0.08412880974974997\n",
      "Epoch:  87000 , step:  87000 , train loss:  0.04185902588775013 , test loss:  0.08412870840899128\n",
      "Epoch:  87010 , step:  87010 , train loss:  0.04185719247005061 , test loss:  0.0841286072336983\n",
      "Epoch:  87020 , step:  87020 , train loss:  0.04185535935939892 , test loss:  0.08412850622381214\n",
      "Epoch:  87030 , step:  87030 , train loss:  0.04185352655570892 , test loss:  0.08412840537927418\n",
      "Epoch:  87040 , step:  87040 , train loss:  0.04185169405889445 , test loss:  0.08412830470002576\n",
      "Epoch:  87050 , step:  87050 , train loss:  0.04184986186886946 , test loss:  0.08412820418600822\n",
      "Epoch:  87060 , step:  87060 , train loss:  0.04184802998554786 , test loss:  0.08412810383716288\n",
      "Epoch:  87070 , step:  87070 , train loss:  0.0418461984088436 , test loss:  0.08412800365343125\n",
      "Epoch:  87080 , step:  87080 , train loss:  0.04184436713867074 , test loss:  0.08412790363475475\n",
      "Epoch:  87090 , step:  87090 , train loss:  0.04184253617494328 , test loss:  0.084127803781075\n",
      "Epoch:  87100 , step:  87100 , train loss:  0.04184070551757533 , test loss:  0.08412770409233315\n",
      "Epoch:  87110 , step:  87110 , train loss:  0.041838875166480945 , test loss:  0.0841276045684709\n",
      "Epoch:  87120 , step:  87120 , train loss:  0.041837045121574344 , test loss:  0.08412750520943002\n",
      "Epoch:  87130 , step:  87130 , train loss:  0.04183521538276961 , test loss:  0.08412740601515162\n",
      "Epoch:  87140 , step:  87140 , train loss:  0.041833385949981025 , test loss:  0.0841273069855776\n",
      "Epoch:  87150 , step:  87150 , train loss:  0.04183155682312281 , test loss:  0.08412720812064965\n",
      "Epoch:  87160 , step:  87160 , train loss:  0.041829728002109254 , test loss:  0.08412710942030915\n",
      "Epoch:  87170 , step:  87170 , train loss:  0.04182789948685463 , test loss:  0.08412701088449799\n",
      "Epoch:  87180 , step:  87180 , train loss:  0.04182607127727333 , test loss:  0.08412691251315771\n",
      "Epoch:  87190 , step:  87190 , train loss:  0.0418242433732797 , test loss:  0.08412681430623016\n",
      "Epoch:  87200 , step:  87200 , train loss:  0.04182241577478816 , test loss:  0.08412671626365696\n",
      "Epoch:  87210 , step:  87210 , train loss:  0.04182058848171319 , test loss:  0.08412661838538006\n",
      "Epoch:  87220 , step:  87220 , train loss:  0.04181876149396921 , test loss:  0.08412652067134123\n",
      "Epoch:  87230 , step:  87230 , train loss:  0.041816934811470786 , test loss:  0.08412642312148212\n",
      "Epoch:  87240 , step:  87240 , train loss:  0.04181510843413241 , test loss:  0.08412632573574476\n",
      "Epoch:  87250 , step:  87250 , train loss:  0.04181328236186871 , test loss:  0.084126228514071\n",
      "Epoch:  87260 , step:  87260 , train loss:  0.04181145659459428 , test loss:  0.08412613145640259\n",
      "Epoch:  87270 , step:  87270 , train loss:  0.041809631132223794 , test loss:  0.08412603456268165\n",
      "Epoch:  87280 , step:  87280 , train loss:  0.04180780597467189 , test loss:  0.08412593783284998\n",
      "Epoch:  87290 , step:  87290 , train loss:  0.0418059811218533 , test loss:  0.08412584126684983\n",
      "Epoch:  87300 , step:  87300 , train loss:  0.04180415657368279 , test loss:  0.08412574486462294\n",
      "Epoch:  87310 , step:  87310 , train loss:  0.04180233233007511 , test loss:  0.08412564862611148\n",
      "Epoch:  87320 , step:  87320 , train loss:  0.04180050839094511 , test loss:  0.08412555255125724\n",
      "Epoch:  87330 , step:  87330 , train loss:  0.041798684756207634 , test loss:  0.08412545664000273\n",
      "Epoch:  87340 , step:  87340 , train loss:  0.04179686142577752 , test loss:  0.08412536089228992\n",
      "Epoch:  87350 , step:  87350 , train loss:  0.04179503839956973 , test loss:  0.08412526530806082\n",
      "Epoch:  87360 , step:  87360 , train loss:  0.041793215677499204 , test loss:  0.08412516988725753\n",
      "Epoch:  87370 , step:  87370 , train loss:  0.041791393259480926 , test loss:  0.0841250746298224\n",
      "Epoch:  87380 , step:  87380 , train loss:  0.04178957114542991 , test loss:  0.08412497953569756\n",
      "Epoch:  87390 , step:  87390 , train loss:  0.041787749335261165 , test loss:  0.0841248846048252\n",
      "Epoch:  87400 , step:  87400 , train loss:  0.04178592782888984 , test loss:  0.08412478983714775\n",
      "Epoch:  87410 , step:  87410 , train loss:  0.041784106626231034 , test loss:  0.08412469523260739\n",
      "Epoch:  87420 , step:  87420 , train loss:  0.041782285727199844 , test loss:  0.08412460079114654\n",
      "Epoch:  87430 , step:  87430 , train loss:  0.041780465131711506 , test loss:  0.0841245065127073\n",
      "Epoch:  87440 , step:  87440 , train loss:  0.04177864483968123 , test loss:  0.08412441239723219\n",
      "Epoch:  87450 , step:  87450 , train loss:  0.041776824851024255 , test loss:  0.08412431844466338\n",
      "Epoch:  87460 , step:  87460 , train loss:  0.04177500516565585 , test loss:  0.08412422465494374\n",
      "Epoch:  87470 , step:  87470 , train loss:  0.041773185783491366 , test loss:  0.08412413102801536\n",
      "Epoch:  87480 , step:  87480 , train loss:  0.04177136670444611 , test loss:  0.08412403756382092\n",
      "Epoch:  87490 , step:  87490 , train loss:  0.041769547928435506 , test loss:  0.08412394426230252\n",
      "Epoch:  87500 , step:  87500 , train loss:  0.04176772945537491 , test loss:  0.08412385112340307\n",
      "Epoch:  87510 , step:  87510 , train loss:  0.04176591128517983 , test loss:  0.084123758147065\n",
      "Epoch:  87520 , step:  87520 , train loss:  0.04176409341776575 , test loss:  0.0841236653332308\n",
      "Epoch:  87530 , step:  87530 , train loss:  0.04176227585304812 , test loss:  0.08412357268184326\n",
      "Epoch:  87540 , step:  87540 , train loss:  0.04176045859094255 , test loss:  0.08412348019284474\n",
      "Epoch:  87550 , step:  87550 , train loss:  0.0417586416313646 , test loss:  0.08412338786617785\n",
      "Epoch:  87560 , step:  87560 , train loss:  0.041756824974229896 , test loss:  0.0841232957017854\n",
      "Epoch:  87570 , step:  87570 , train loss:  0.04175500861945406 , test loss:  0.08412320369961025\n",
      "Epoch:  87580 , step:  87580 , train loss:  0.04175319256695279 , test loss:  0.08412311185959491\n",
      "Epoch:  87590 , step:  87590 , train loss:  0.04175137681664178 , test loss:  0.08412302018168206\n",
      "Epoch:  87600 , step:  87600 , train loss:  0.0417495613684368 , test loss:  0.0841229286658146\n",
      "Epoch:  87610 , step:  87610 , train loss:  0.04174774622225362 , test loss:  0.08412283731193516\n",
      "Epoch:  87620 , step:  87620 , train loss:  0.041745931378008035 , test loss:  0.08412274611998675\n",
      "Epoch:  87630 , step:  87630 , train loss:  0.04174411683561592 , test loss:  0.08412265508991211\n",
      "Epoch:  87640 , step:  87640 , train loss:  0.04174230259499314 , test loss:  0.08412256422165419\n",
      "Epoch:  87650 , step:  87650 , train loss:  0.041740488656055605 , test loss:  0.08412247351515575\n",
      "Epoch:  87660 , step:  87660 , train loss:  0.04173867501871925 , test loss:  0.08412238297035977\n",
      "Epoch:  87670 , step:  87670 , train loss:  0.04173686168290007 , test loss:  0.08412229258720928\n",
      "Epoch:  87680 , step:  87680 , train loss:  0.04173504864851406 , test loss:  0.08412220236564706\n",
      "Epoch:  87690 , step:  87690 , train loss:  0.04173323591547728 , test loss:  0.0841221123056163\n",
      "Epoch:  87700 , step:  87700 , train loss:  0.04173142348370578 , test loss:  0.08412202240705983\n",
      "Epoch:  87710 , step:  87710 , train loss:  0.04172961135311566 , test loss:  0.08412193266992078\n",
      "Epoch:  87720 , step:  87720 , train loss:  0.04172779952362312 , test loss:  0.08412184309414224\n",
      "Epoch:  87730 , step:  87730 , train loss:  0.041725987995144286 , test loss:  0.08412175367966736\n",
      "Epoch:  87740 , step:  87740 , train loss:  0.04172417676759539 , test loss:  0.08412166442643915\n",
      "Epoch:  87750 , step:  87750 , train loss:  0.04172236584089262 , test loss:  0.08412157533440066\n",
      "Epoch:  87760 , step:  87760 , train loss:  0.0417205552149523 , test loss:  0.0841214864034951\n",
      "Epoch:  87770 , step:  87770 , train loss:  0.041718744889690726 , test loss:  0.08412139763366591\n",
      "Epoch:  87780 , step:  87780 , train loss:  0.041716934865024216 , test loss:  0.0841213090248561\n",
      "Epoch:  87790 , step:  87790 , train loss:  0.041715125140869176 , test loss:  0.08412122057700883\n",
      "Epoch:  87800 , step:  87800 , train loss:  0.04171331571714198 , test loss:  0.08412113229006747\n",
      "Epoch:  87810 , step:  87810 , train loss:  0.04171150659375906 , test loss:  0.08412104416397546\n",
      "Epoch:  87820 , step:  87820 , train loss:  0.04170969777063688 , test loss:  0.08412095619867581\n",
      "Epoch:  87830 , step:  87830 , train loss:  0.04170788924769198 , test loss:  0.08412086839411223\n",
      "Epoch:  87840 , step:  87840 , train loss:  0.04170608102484086 , test loss:  0.08412078075022772\n",
      "Epoch:  87850 , step:  87850 , train loss:  0.0417042731020001 , test loss:  0.08412069326696589\n",
      "Epoch:  87860 , step:  87860 , train loss:  0.04170246547908629 , test loss:  0.08412060594427008\n",
      "Epoch:  87870 , step:  87870 , train loss:  0.04170065815601608 , test loss:  0.08412051878208368\n",
      "Epoch:  87880 , step:  87880 , train loss:  0.04169885113270607 , test loss:  0.08412043178035025\n",
      "Epoch:  87890 , step:  87890 , train loss:  0.04169704440907306 , test loss:  0.08412034493901328\n",
      "Epoch:  87900 , step:  87900 , train loss:  0.0416952379850337 , test loss:  0.08412025825801618\n",
      "Epoch:  87910 , step:  87910 , train loss:  0.041693431860504775 , test loss:  0.08412017173730249\n",
      "Epoch:  87920 , step:  87920 , train loss:  0.04169162603540308 , test loss:  0.08412008537681588\n",
      "Epoch:  87930 , step:  87930 , train loss:  0.041689820509645446 , test loss:  0.08411999917649995\n",
      "Epoch:  87940 , step:  87940 , train loss:  0.04168801528314873 , test loss:  0.08411991313629825\n",
      "Epoch:  87950 , step:  87950 , train loss:  0.04168621035582979 , test loss:  0.08411982725615431\n",
      "Epoch:  87960 , step:  87960 , train loss:  0.04168440572760562 , test loss:  0.08411974153601207\n",
      "Epoch:  87970 , step:  87970 , train loss:  0.041682601398393124 , test loss:  0.08411965597581486\n",
      "Epoch:  87980 , step:  87980 , train loss:  0.041680797368109306 , test loss:  0.08411957057550676\n",
      "Epoch:  87990 , step:  87990 , train loss:  0.041678993636671165 , test loss:  0.08411948533503112\n",
      "Epoch:  88000 , step:  88000 , train loss:  0.04167719020399577 , test loss:  0.08411940025433202\n",
      "Epoch:  88010 , step:  88010 , train loss:  0.04167538707000023 , test loss:  0.08411931533335308\n",
      "Epoch:  88020 , step:  88020 , train loss:  0.04167358423460166 , test loss:  0.08411923057203813\n",
      "Epoch:  88030 , step:  88030 , train loss:  0.04167178169771714 , test loss:  0.08411914597033128\n",
      "Epoch:  88040 , step:  88040 , train loss:  0.041669979459263914 , test loss:  0.08411906152817586\n",
      "Epoch:  88050 , step:  88050 , train loss:  0.0416681775191592 , test loss:  0.08411897724551601\n",
      "Epoch:  88060 , step:  88060 , train loss:  0.04166637587732023 , test loss:  0.08411889312229581\n",
      "Epoch:  88070 , step:  88070 , train loss:  0.041664574533664296 , test loss:  0.08411880915845898\n",
      "Epoch:  88080 , step:  88080 , train loss:  0.04166277348810871 , test loss:  0.0841187253539494\n",
      "Epoch:  88090 , step:  88090 , train loss:  0.041660972740570755 , test loss:  0.08411864170871129\n",
      "Epoch:  88100 , step:  88100 , train loss:  0.04165917229096791 , test loss:  0.08411855822268864\n",
      "Epoch:  88110 , step:  88110 , train loss:  0.04165737213921749 , test loss:  0.08411847489582532\n",
      "Epoch:  88120 , step:  88120 , train loss:  0.04165557228523701 , test loss:  0.08411839172806548\n",
      "Epoch:  88130 , step:  88130 , train loss:  0.04165377272894388 , test loss:  0.08411830871935318\n",
      "Epoch:  88140 , step:  88140 , train loss:  0.04165197347025564 , test loss:  0.0841182258696324\n",
      "Epoch:  88150 , step:  88150 , train loss:  0.041650174509089824 , test loss:  0.08411814317884729\n",
      "Epoch:  88160 , step:  88160 , train loss:  0.04164837584536399 , test loss:  0.08411806064694229\n",
      "Epoch:  88170 , step:  88170 , train loss:  0.04164657747899574 , test loss:  0.08411797827386124\n",
      "Epoch:  88180 , step:  88180 , train loss:  0.04164477940990272 , test loss:  0.0841178960595485\n",
      "Epoch:  88190 , step:  88190 , train loss:  0.04164298163800256 , test loss:  0.08411781400394822\n",
      "Epoch:  88200 , step:  88200 , train loss:  0.04164118416321301 , test loss:  0.08411773210700477\n",
      "Epoch:  88210 , step:  88210 , train loss:  0.04163938698545177 , test loss:  0.08411765036866234\n",
      "Epoch:  88220 , step:  88220 , train loss:  0.04163759010463659 , test loss:  0.08411756878886531\n",
      "Epoch:  88230 , step:  88230 , train loss:  0.04163579352068528 , test loss:  0.08411748736755786\n",
      "Epoch:  88240 , step:  88240 , train loss:  0.04163399723351564 , test loss:  0.08411740610468449\n",
      "Epoch:  88250 , step:  88250 , train loss:  0.04163220124304559 , test loss:  0.08411732500018942\n",
      "Epoch:  88260 , step:  88260 , train loss:  0.04163040554919295 , test loss:  0.08411724405401697\n",
      "Epoch:  88270 , step:  88270 , train loss:  0.04162861015187568 , test loss:  0.08411716326611182\n",
      "Epoch:  88280 , step:  88280 , train loss:  0.04162681505101168 , test loss:  0.08411708263641832\n",
      "Epoch:  88290 , step:  88290 , train loss:  0.04162502024651902 , test loss:  0.08411700216488067\n",
      "Epoch:  88300 , step:  88300 , train loss:  0.041623225738315645 , test loss:  0.08411692185144382\n",
      "Epoch:  88310 , step:  88310 , train loss:  0.04162143152631963 , test loss:  0.08411684169605196\n",
      "Epoch:  88320 , step:  88320 , train loss:  0.04161963761044905 , test loss:  0.08411676169864989\n",
      "Epoch:  88330 , step:  88330 , train loss:  0.041617843990622044 , test loss:  0.08411668185918188\n",
      "Epoch:  88340 , step:  88340 , train loss:  0.04161605066675672 , test loss:  0.0841166021775927\n",
      "Epoch:  88350 , step:  88350 , train loss:  0.04161425763877127 , test loss:  0.08411652265382685\n",
      "Epoch:  88360 , step:  88360 , train loss:  0.0416124649065839 , test loss:  0.08411644328782918\n",
      "Epoch:  88370 , step:  88370 , train loss:  0.04161067247011284 , test loss:  0.08411636407954398\n",
      "Epoch:  88380 , step:  88380 , train loss:  0.04160888032927635 , test loss:  0.08411628502891629\n",
      "Epoch:  88390 , step:  88390 , train loss:  0.04160708848399277 , test loss:  0.08411620613589059\n",
      "Epoch:  88400 , step:  88400 , train loss:  0.041605296934180414 , test loss:  0.0841161274004118\n",
      "Epoch:  88410 , step:  88410 , train loss:  0.041603505679757624 , test loss:  0.0841160488224243\n",
      "Epoch:  88420 , step:  88420 , train loss:  0.04160171472064284 , test loss:  0.08411597040187331\n",
      "Epoch:  88430 , step:  88430 , train loss:  0.04159992405675446 , test loss:  0.08411589213870359\n",
      "Epoch:  88440 , step:  88440 , train loss:  0.041598133688010984 , test loss:  0.0841158140328598\n",
      "Epoch:  88450 , step:  88450 , train loss:  0.04159634361433085 , test loss:  0.08411573608428682\n",
      "Epoch:  88460 , step:  88460 , train loss:  0.04159455383563261 , test loss:  0.0841156582929295\n",
      "Epoch:  88470 , step:  88470 , train loss:  0.04159276435183483 , test loss:  0.08411558065873279\n",
      "Epoch:  88480 , step:  88480 , train loss:  0.0415909751628561 , test loss:  0.08411550318164165\n",
      "Epoch:  88490 , step:  88490 , train loss:  0.04158918626861501 , test loss:  0.08411542586160099\n",
      "Epoch:  88500 , step:  88500 , train loss:  0.04158739766903021 , test loss:  0.0841153486985557\n",
      "Epoch:  88510 , step:  88510 , train loss:  0.04158560936402043 , test loss:  0.08411527169245084\n",
      "Epoch:  88520 , step:  88520 , train loss:  0.041583821353504316 , test loss:  0.08411519484323164\n",
      "Epoch:  88530 , step:  88530 , train loss:  0.04158203363740069 , test loss:  0.08411511815084287\n",
      "Epoch:  88540 , step:  88540 , train loss:  0.04158024621562825 , test loss:  0.08411504161522951\n",
      "Epoch:  88550 , step:  88550 , train loss:  0.041578459088105875 , test loss:  0.0841149652363371\n",
      "Epoch:  88560 , step:  88560 , train loss:  0.041576672254752346 , test loss:  0.08411488901411027\n",
      "Epoch:  88570 , step:  88570 , train loss:  0.04157488571548657 , test loss:  0.08411481294849432\n",
      "Epoch:  88580 , step:  88580 , train loss:  0.041573099470227436 , test loss:  0.08411473703943449\n",
      "Epoch:  88590 , step:  88590 , train loss:  0.04157131351889389 , test loss:  0.08411466128687596\n",
      "Epoch:  88600 , step:  88600 , train loss:  0.041569527861404866 , test loss:  0.08411458569076381\n",
      "Epoch:  88610 , step:  88610 , train loss:  0.04156774249767939 , test loss:  0.08411451025104365\n",
      "Epoch:  88620 , step:  88620 , train loss:  0.04156595742763648 , test loss:  0.08411443496766019\n",
      "Epoch:  88630 , step:  88630 , train loss:  0.04156417265119521 , test loss:  0.08411435984055907\n",
      "Epoch:  88640 , step:  88640 , train loss:  0.04156238816827464 , test loss:  0.08411428486968553\n",
      "Epoch:  88650 , step:  88650 , train loss:  0.0415606039787939 , test loss:  0.08411421005498479\n",
      "Epoch:  88660 , step:  88660 , train loss:  0.04155882008267219 , test loss:  0.08411413539640239\n",
      "Epoch:  88670 , step:  88670 , train loss:  0.041557036479828634 , test loss:  0.08411406089388379\n",
      "Epoch:  88680 , step:  88680 , train loss:  0.04155525317018246 , test loss:  0.08411398654737388\n",
      "Epoch:  88690 , step:  88690 , train loss:  0.041553470153652954 , test loss:  0.08411391235681857\n",
      "Epoch:  88700 , step:  88700 , train loss:  0.04155168743015934 , test loss:  0.08411383832216317\n",
      "Epoch:  88710 , step:  88710 , train loss:  0.04154990499962095 , test loss:  0.08411376444335299\n",
      "Epoch:  88720 , step:  88720 , train loss:  0.04154812286195714 , test loss:  0.0841136907203337\n",
      "Epoch:  88730 , step:  88730 , train loss:  0.04154634101708728 , test loss:  0.08411361715305087\n",
      "Epoch:  88740 , step:  88740 , train loss:  0.04154455946493075 , test loss:  0.08411354374144984\n",
      "Epoch:  88750 , step:  88750 , train loss:  0.041542778205407 , test loss:  0.08411347048547634\n",
      "Epoch:  88760 , step:  88760 , train loss:  0.04154099723843547 , test loss:  0.08411339738507594\n",
      "Epoch:  88770 , step:  88770 , train loss:  0.04153921656393571 , test loss:  0.08411332444019427\n",
      "Epoch:  88780 , step:  88780 , train loss:  0.04153743618182721 , test loss:  0.0841132516507769\n",
      "Epoch:  88790 , step:  88790 , train loss:  0.04153565609202953 , test loss:  0.0841131790167694\n",
      "Epoch:  88800 , step:  88800 , train loss:  0.04153387629446226 , test loss:  0.08411310653811763\n",
      "Epoch:  88810 , step:  88810 , train loss:  0.04153209678904502 , test loss:  0.0841130342147673\n",
      "Epoch:  88820 , step:  88820 , train loss:  0.04153031757569749 , test loss:  0.08411296204666399\n",
      "Epoch:  88830 , step:  88830 , train loss:  0.04152853865433929 , test loss:  0.08411289003375354\n",
      "Epoch:  88840 , step:  88840 , train loss:  0.04152676002489022 , test loss:  0.0841128181759817\n",
      "Epoch:  88850 , step:  88850 , train loss:  0.041524981687269966 , test loss:  0.08411274647329435\n",
      "Epoch:  88860 , step:  88860 , train loss:  0.041523203641398304 , test loss:  0.08411267492563727\n",
      "Epoch:  88870 , step:  88870 , train loss:  0.04152142588719507 , test loss:  0.0841126035329562\n",
      "Epoch:  88880 , step:  88880 , train loss:  0.04151964842458007 , test loss:  0.08411253229519713\n",
      "Epoch:  88890 , step:  88890 , train loss:  0.0415178712534732 , test loss:  0.08411246121230598\n",
      "Epoch:  88900 , step:  88900 , train loss:  0.04151609437379436 , test loss:  0.0841123902842286\n",
      "Epoch:  88910 , step:  88910 , train loss:  0.04151431778546342 , test loss:  0.08411231951091086\n",
      "Epoch:  88920 , step:  88920 , train loss:  0.041512541488400445 , test loss:  0.08411224889229894\n",
      "Epoch:  88930 , step:  88930 , train loss:  0.04151076548252536 , test loss:  0.08411217842833865\n",
      "Epoch:  88940 , step:  88940 , train loss:  0.04150898976775819 , test loss:  0.08411210811897608\n",
      "Epoch:  88950 , step:  88950 , train loss:  0.04150721434401901 , test loss:  0.08411203796415723\n",
      "Epoch:  88960 , step:  88960 , train loss:  0.0415054392112279 , test loss:  0.08411196796382811\n",
      "Epoch:  88970 , step:  88970 , train loss:  0.04150366436930495 , test loss:  0.08411189811793492\n",
      "Epoch:  88980 , step:  88980 , train loss:  0.04150188981817034 , test loss:  0.08411182842642372\n",
      "Epoch:  88990 , step:  88990 , train loss:  0.04150011555774426 , test loss:  0.08411175888924065\n",
      "Epoch:  89000 , step:  89000 , train loss:  0.04149834158794686 , test loss:  0.08411168950633179\n",
      "Epoch:  89010 , step:  89010 , train loss:  0.04149656790869842 , test loss:  0.08411162027764336\n",
      "Epoch:  89020 , step:  89020 , train loss:  0.04149479451991922 , test loss:  0.08411155120312166\n",
      "Epoch:  89030 , step:  89030 , train loss:  0.041493021421529525 , test loss:  0.0841114822827126\n",
      "Epoch:  89040 , step:  89040 , train loss:  0.041491248613449674 , test loss:  0.08411141351636277\n",
      "Epoch:  89050 , step:  89050 , train loss:  0.04148947609560006 , test loss:  0.08411134490401823\n",
      "Epoch:  89060 , step:  89060 , train loss:  0.04148770386790105 , test loss:  0.08411127644562545\n",
      "Epoch:  89070 , step:  89070 , train loss:  0.041485931930273084 , test loss:  0.08411120814113064\n",
      "Epoch:  89080 , step:  89080 , train loss:  0.04148416028263657 , test loss:  0.08411113999048003\n",
      "Epoch:  89090 , step:  89090 , train loss:  0.041482388924912066 , test loss:  0.08411107199362014\n",
      "Epoch:  89100 , step:  89100 , train loss:  0.04148061785702003 , test loss:  0.08411100415049735\n",
      "Epoch:  89110 , step:  89110 , train loss:  0.041478847078881 , test loss:  0.08411093646105793\n",
      "Epoch:  89120 , step:  89120 , train loss:  0.04147707659041562 , test loss:  0.08411086892524844\n",
      "Epoch:  89130 , step:  89130 , train loss:  0.04147530639154442 , test loss:  0.0841108015430152\n",
      "Epoch:  89140 , step:  89140 , train loss:  0.04147353648218809 , test loss:  0.08411073431430481\n",
      "Epoch:  89150 , step:  89150 , train loss:  0.04147176686226728 , test loss:  0.08411066723906392\n",
      "Epoch:  89160 , step:  89160 , train loss:  0.04146999753170267 , test loss:  0.08411060031723862\n",
      "Epoch:  89170 , step:  89170 , train loss:  0.041468228490415024 , test loss:  0.08411053354877589\n",
      "Epoch:  89180 , step:  89180 , train loss:  0.041466459738325086 , test loss:  0.08411046693362205\n",
      "Epoch:  89190 , step:  89190 , train loss:  0.041464691275353634 , test loss:  0.08411040047172368\n",
      "Epoch:  89200 , step:  89200 , train loss:  0.0414629231014215 , test loss:  0.0841103341630276\n",
      "Epoch:  89210 , step:  89210 , train loss:  0.041461155216449545 , test loss:  0.08411026800748028\n",
      "Epoch:  89220 , step:  89220 , train loss:  0.041459387620358626 , test loss:  0.08411020200502836\n",
      "Epoch:  89230 , step:  89230 , train loss:  0.041457620313069675 , test loss:  0.08411013615561856\n",
      "Epoch:  89240 , step:  89240 , train loss:  0.04145585329450363 , test loss:  0.08411007045919767\n",
      "Epoch:  89250 , step:  89250 , train loss:  0.04145408656458146 , test loss:  0.08411000491571241\n",
      "Epoch:  89260 , step:  89260 , train loss:  0.041452320123224154 , test loss:  0.08410993952510942\n",
      "Epoch:  89270 , step:  89270 , train loss:  0.041450553970352765 , test loss:  0.08410987428733549\n",
      "Epoch:  89280 , step:  89280 , train loss:  0.04144878810588837 , test loss:  0.08410980920233753\n",
      "Epoch:  89290 , step:  89290 , train loss:  0.04144702252975203 , test loss:  0.08410974427006232\n",
      "Epoch:  89300 , step:  89300 , train loss:  0.041445257241864895 , test loss:  0.08410967949045656\n",
      "Epoch:  89310 , step:  89310 , train loss:  0.041443492242148106 , test loss:  0.08410961486346719\n",
      "Epoch:  89320 , step:  89320 , train loss:  0.04144172753052284 , test loss:  0.08410955038904144\n",
      "Epoch:  89330 , step:  89330 , train loss:  0.04143996310691033 , test loss:  0.08410948606712575\n",
      "Epoch:  89340 , step:  89340 , train loss:  0.04143819897123181 , test loss:  0.0841094218976673\n",
      "Epoch:  89350 , step:  89350 , train loss:  0.04143643512340858 , test loss:  0.08410935788061295\n",
      "Epoch:  89360 , step:  89360 , train loss:  0.04143467156336193 , test loss:  0.08410929401590973\n",
      "Epoch:  89370 , step:  89370 , train loss:  0.04143290829101316 , test loss:  0.08410923030350466\n",
      "Epoch:  89380 , step:  89380 , train loss:  0.0414311453062837 , test loss:  0.08410916674334468\n",
      "Epoch:  89390 , step:  89390 , train loss:  0.04142938260909492 , test loss:  0.08410910333537688\n",
      "Epoch:  89400 , step:  89400 , train loss:  0.04142762019936821 , test loss:  0.0841090400795483\n",
      "Epoch:  89410 , step:  89410 , train loss:  0.04142585807702508 , test loss:  0.0841089769758061\n",
      "Epoch:  89420 , step:  89420 , train loss:  0.04142409624198699 , test loss:  0.08410891402409747\n",
      "Epoch:  89430 , step:  89430 , train loss:  0.04142233469417545 , test loss:  0.0841088512243696\n",
      "Epoch:  89440 , step:  89440 , train loss:  0.041420573433512055 , test loss:  0.08410878857656934\n",
      "Epoch:  89450 , step:  89450 , train loss:  0.04141881245991833 , test loss:  0.08410872608064406\n",
      "Epoch:  89460 , step:  89460 , train loss:  0.04141705177331589 , test loss:  0.08410866373654101\n",
      "Epoch:  89470 , step:  89470 , train loss:  0.0414152913736264 , test loss:  0.08410860154420734\n",
      "Epoch:  89480 , step:  89480 , train loss:  0.04141353126077149 , test loss:  0.08410853950359043\n",
      "Epoch:  89490 , step:  89490 , train loss:  0.0414117714346729 , test loss:  0.08410847761463736\n",
      "Epoch:  89500 , step:  89500 , train loss:  0.04141001189525232 , test loss:  0.08410841587729559\n",
      "Epoch:  89510 , step:  89510 , train loss:  0.041408252642431545 , test loss:  0.08410835429151241\n",
      "Epoch:  89520 , step:  89520 , train loss:  0.041406493676132305 , test loss:  0.08410829285723523\n",
      "Epoch:  89530 , step:  89530 , train loss:  0.04140473499627649 , test loss:  0.08410823157441119\n",
      "Epoch:  89540 , step:  89540 , train loss:  0.04140297660278589 , test loss:  0.0841081704429878\n",
      "Epoch:  89550 , step:  89550 , train loss:  0.04140121849558241 , test loss:  0.0841081094629126\n",
      "Epoch:  89560 , step:  89560 , train loss:  0.04139946067458796 , test loss:  0.08410804863413295\n",
      "Epoch:  89570 , step:  89570 , train loss:  0.041397703139724455 , test loss:  0.08410798795659616\n",
      "Epoch:  89580 , step:  89580 , train loss:  0.041395945890913895 , test loss:  0.08410792743025003\n",
      "Epoch:  89590 , step:  89590 , train loss:  0.041394188928078224 , test loss:  0.08410786705504185\n",
      "Epoch:  89600 , step:  89600 , train loss:  0.04139243225113952 , test loss:  0.084107806830919\n",
      "Epoch:  89610 , step:  89610 , train loss:  0.041390675860019814 , test loss:  0.08410774675782938\n",
      "Epoch:  89620 , step:  89620 , train loss:  0.041388919754641214 , test loss:  0.08410768683572019\n",
      "Epoch:  89630 , step:  89630 , train loss:  0.04138716393492581 , test loss:  0.08410762706453924\n",
      "Epoch:  89640 , step:  89640 , train loss:  0.041385408400795784 , test loss:  0.08410756744423425\n",
      "Epoch:  89650 , step:  89650 , train loss:  0.04138365315217328 , test loss:  0.08410750797475272\n",
      "Epoch:  89660 , step:  89660 , train loss:  0.041381898188980505 , test loss:  0.0841074486560424\n",
      "Epoch:  89670 , step:  89670 , train loss:  0.04138014351113971 , test loss:  0.08410738948805069\n",
      "Epoch:  89680 , step:  89680 , train loss:  0.04137838911857315 , test loss:  0.08410733047072544\n",
      "Epoch:  89690 , step:  89690 , train loss:  0.04137663501120313 , test loss:  0.08410727160401459\n",
      "Epoch:  89700 , step:  89700 , train loss:  0.04137488118895197 , test loss:  0.08410721288786578\n",
      "Epoch:  89710 , step:  89710 , train loss:  0.04137312765174202 , test loss:  0.08410715432222676\n",
      "Epoch:  89720 , step:  89720 , train loss:  0.041371374399495654 , test loss:  0.08410709590704511\n",
      "Epoch:  89730 , step:  89730 , train loss:  0.04136962143213534 , test loss:  0.08410703764226894\n",
      "Epoch:  89740 , step:  89740 , train loss:  0.04136786874958344 , test loss:  0.08410697952784613\n",
      "Epoch:  89750 , step:  89750 , train loss:  0.04136611635176249 , test loss:  0.0841069215637243\n",
      "Epoch:  89760 , step:  89760 , train loss:  0.041364364238594964 , test loss:  0.08410686374985145\n",
      "Epoch:  89770 , step:  89770 , train loss:  0.04136261241000339 , test loss:  0.0841068060861755\n",
      "Epoch:  89780 , step:  89780 , train loss:  0.0413608608659104 , test loss:  0.08410674857264426\n",
      "Epoch:  89790 , step:  89790 , train loss:  0.04135910960623847 , test loss:  0.08410669120920596\n",
      "Epoch:  89800 , step:  89800 , train loss:  0.04135735863091029 , test loss:  0.08410663399580826\n",
      "Epoch:  89810 , step:  89810 , train loss:  0.04135560793984851 , test loss:  0.08410657693239931\n",
      "Epoch:  89820 , step:  89820 , train loss:  0.04135385753297581 , test loss:  0.08410652001892716\n",
      "Epoch:  89830 , step:  89830 , train loss:  0.041352107410214854 , test loss:  0.08410646325533971\n",
      "Epoch:  89840 , step:  89840 , train loss:  0.04135035757148845 , test loss:  0.0841064066415852\n",
      "Epoch:  89850 , step:  89850 , train loss:  0.041348608016719315 , test loss:  0.08410635017761169\n",
      "Epoch:  89860 , step:  89860 , train loss:  0.041346858745830294 , test loss:  0.08410629386336718\n",
      "Epoch:  89870 , step:  89870 , train loss:  0.041345109758744183 , test loss:  0.08410623769879991\n",
      "Epoch:  89880 , step:  89880 , train loss:  0.04134336105538383 , test loss:  0.084106181683858\n",
      "Epoch:  89890 , step:  89890 , train loss:  0.04134161263567215 , test loss:  0.08410612581848957\n",
      "Epoch:  89900 , step:  89900 , train loss:  0.04133986449953206 , test loss:  0.08410607010264279\n",
      "Epoch:  89910 , step:  89910 , train loss:  0.041338116646886455 , test loss:  0.08410601453626597\n",
      "Epoch:  89920 , step:  89920 , train loss:  0.04133636907765837 , test loss:  0.0841059591193073\n",
      "Epoch:  89930 , step:  89930 , train loss:  0.04133462179177078 , test loss:  0.08410590385171518\n",
      "Epoch:  89940 , step:  89940 , train loss:  0.041332874789146705 , test loss:  0.08410584873343765\n",
      "Epoch:  89950 , step:  89950 , train loss:  0.041331128069709275 , test loss:  0.0841057937644231\n",
      "Epoch:  89960 , step:  89960 , train loss:  0.041329381633381505 , test loss:  0.08410573894461992\n",
      "Epoch:  89970 , step:  89970 , train loss:  0.04132763548008656 , test loss:  0.08410568427397655\n",
      "Epoch:  89980 , step:  89980 , train loss:  0.04132588960974757 , test loss:  0.08410562975244124\n",
      "Epoch:  89990 , step:  89990 , train loss:  0.04132414402228775 , test loss:  0.08410557537996244\n",
      "Epoch:  90000 , step:  90000 , train loss:  0.04132239871763027 , test loss:  0.0841055211564885\n",
      "Epoch:  90010 , step:  90010 , train loss:  0.04132065369569837 , test loss:  0.08410546708196792\n",
      "Epoch:  90020 , step:  90020 , train loss:  0.04131890895641535 , test loss:  0.08410541315634924\n",
      "Epoch:  90030 , step:  90030 , train loss:  0.04131716449970448 , test loss:  0.08410535937958075\n",
      "Epoch:  90040 , step:  90040 , train loss:  0.04131542032548911 , test loss:  0.08410530575161099\n",
      "Epoch:  90050 , step:  90050 , train loss:  0.04131367643369257 , test loss:  0.08410525227238856\n",
      "Epoch:  90060 , step:  90060 , train loss:  0.04131193282423825 , test loss:  0.08410519894186212\n",
      "Epoch:  90070 , step:  90070 , train loss:  0.041310189497049586 , test loss:  0.08410514575998002\n",
      "Epoch:  90080 , step:  90080 , train loss:  0.04130844645205001 , test loss:  0.08410509272669099\n",
      "Epoch:  90090 , step:  90090 , train loss:  0.04130670368916299 , test loss:  0.08410503984194352\n",
      "Epoch:  90100 , step:  90100 , train loss:  0.04130496120831203 , test loss:  0.08410498710568666\n",
      "Epoch:  90110 , step:  90110 , train loss:  0.04130321900942066 , test loss:  0.08410493451786846\n",
      "Epoch:  90120 , step:  90120 , train loss:  0.041301477092412435 , test loss:  0.08410488207843801\n",
      "Epoch:  90130 , step:  90130 , train loss:  0.041299735457210975 , test loss:  0.08410482978734397\n",
      "Epoch:  90140 , step:  90140 , train loss:  0.04129799410373986 , test loss:  0.08410477764453494\n",
      "Epoch:  90150 , step:  90150 , train loss:  0.04129625303192275 , test loss:  0.0841047256499597\n",
      "Epoch:  90160 , step:  90160 , train loss:  0.041294512241683325 , test loss:  0.08410467380356705\n",
      "Epoch:  90170 , step:  90170 , train loss:  0.041292771732945294 , test loss:  0.08410462210530582\n",
      "Epoch:  90180 , step:  90180 , train loss:  0.04129103150563238 , test loss:  0.08410457055512462\n",
      "Epoch:  90190 , step:  90190 , train loss:  0.04128929155966836 , test loss:  0.08410451915297248\n",
      "Epoch:  90200 , step:  90200 , train loss:  0.04128755189497702 , test loss:  0.08410446789879851\n",
      "Epoch:  90210 , step:  90210 , train loss:  0.04128581251148218 , test loss:  0.08410441679255107\n",
      "Epoch:  90220 , step:  90220 , train loss:  0.04128407340910768 , test loss:  0.08410436583417931\n",
      "Epoch:  90230 , step:  90230 , train loss:  0.041282334587777414 , test loss:  0.0841043150236322\n",
      "Epoch:  90240 , step:  90240 , train loss:  0.04128059604741531 , test loss:  0.08410426436085865\n",
      "Epoch:  90250 , step:  90250 , train loss:  0.04127885778794525 , test loss:  0.0841042138458075\n",
      "Epoch:  90260 , step:  90260 , train loss:  0.04127711980929127 , test loss:  0.0841041634784279\n",
      "Epoch:  90270 , step:  90270 , train loss:  0.04127538211137731 , test loss:  0.08410411325866883\n",
      "Epoch:  90280 , step:  90280 , train loss:  0.041273644694127413 , test loss:  0.08410406318647924\n",
      "Epoch:  90290 , step:  90290 , train loss:  0.04127190755746562 , test loss:  0.08410401326180823\n",
      "Epoch:  90300 , step:  90300 , train loss:  0.041270170701316035 , test loss:  0.08410396348460489\n",
      "Epoch:  90310 , step:  90310 , train loss:  0.04126843412560277 , test loss:  0.08410391385481827\n",
      "Epoch:  90320 , step:  90320 , train loss:  0.04126669783024993 , test loss:  0.0841038643723976\n",
      "Epoch:  90330 , step:  90330 , train loss:  0.04126496181518174 , test loss:  0.08410381503729197\n",
      "Epoch:  90340 , step:  90340 , train loss:  0.04126322608032231 , test loss:  0.08410376584945041\n",
      "Epoch:  90350 , step:  90350 , train loss:  0.04126149062559595 , test loss:  0.08410371680882232\n",
      "Epoch:  90360 , step:  90360 , train loss:  0.04125975545092689 , test loss:  0.08410366791535671\n",
      "Epoch:  90370 , step:  90370 , train loss:  0.04125802055623941 , test loss:  0.08410361916900291\n",
      "Epoch:  90380 , step:  90380 , train loss:  0.041256285941457804 , test loss:  0.08410357056971017\n",
      "Epoch:  90390 , step:  90390 , train loss:  0.041254551606506415 , test loss:  0.08410352211742776\n",
      "Epoch:  90400 , step:  90400 , train loss:  0.041252817551309655 , test loss:  0.08410347381210488\n",
      "Epoch:  90410 , step:  90410 , train loss:  0.04125108377579187 , test loss:  0.08410342565369096\n",
      "Epoch:  90420 , step:  90420 , train loss:  0.04124935027987751 , test loss:  0.08410337764213534\n",
      "Epoch:  90430 , step:  90430 , train loss:  0.041247617063491046 , test loss:  0.08410332977738738\n",
      "Epoch:  90440 , step:  90440 , train loss:  0.041245884126556935 , test loss:  0.08410328205939617\n",
      "Epoch:  90450 , step:  90450 , train loss:  0.04124415146899968 , test loss:  0.0841032344881115\n",
      "Epoch:  90460 , step:  90460 , train loss:  0.04124241909074389 , test loss:  0.08410318706348252\n",
      "Epoch:  90470 , step:  90470 , train loss:  0.041240686991714036 , test loss:  0.08410313978545894\n",
      "Epoch:  90480 , step:  90480 , train loss:  0.041238955171834825 , test loss:  0.08410309265399012\n",
      "Epoch:  90490 , step:  90490 , train loss:  0.041237223631030794 , test loss:  0.08410304566902548\n",
      "Epoch:  90500 , step:  90500 , train loss:  0.04123549236922666 , test loss:  0.08410299883051453\n",
      "Epoch:  90510 , step:  90510 , train loss:  0.041233761386347045 , test loss:  0.08410295213840673\n",
      "Epoch:  90520 , step:  90520 , train loss:  0.04123203068231674 , test loss:  0.08410290559265182\n",
      "Epoch:  90530 , step:  90530 , train loss:  0.04123030025706041 , test loss:  0.0841028591931992\n",
      "Epoch:  90540 , step:  90540 , train loss:  0.04122857011050288 , test loss:  0.08410281293999868\n",
      "Epoch:  90550 , step:  90550 , train loss:  0.04122684024256894 , test loss:  0.08410276683299964\n",
      "Epoch:  90560 , step:  90560 , train loss:  0.041225110653183385 , test loss:  0.08410272087215172\n",
      "Epoch:  90570 , step:  90570 , train loss:  0.04122338134227111 , test loss:  0.0841026750574047\n",
      "Epoch:  90580 , step:  90580 , train loss:  0.04122165230975697 , test loss:  0.08410262938870827\n",
      "Epoch:  90590 , step:  90590 , train loss:  0.0412199235555659 , test loss:  0.08410258386601212\n",
      "Epoch:  90600 , step:  90600 , train loss:  0.04121819507962281 , test loss:  0.08410253848926581\n",
      "Epoch:  90610 , step:  90610 , train loss:  0.0412164668818527 , test loss:  0.08410249325841924\n",
      "Epoch:  90620 , step:  90620 , train loss:  0.041214738962180565 , test loss:  0.08410244817342212\n",
      "Epoch:  90630 , step:  90630 , train loss:  0.04121301132053143 , test loss:  0.0841024032342243\n",
      "Epoch:  90640 , step:  90640 , train loss:  0.041211283956830325 , test loss:  0.08410235844077557\n",
      "Epoch:  90650 , step:  90650 , train loss:  0.041209556871002355 , test loss:  0.08410231379302564\n",
      "Epoch:  90660 , step:  90660 , train loss:  0.04120783006297263 , test loss:  0.08410226929092447\n",
      "Epoch:  90670 , step:  90670 , train loss:  0.041206103532666294 , test loss:  0.0841022249344219\n",
      "Epoch:  90680 , step:  90680 , train loss:  0.0412043772800085 , test loss:  0.08410218072346792\n",
      "Epoch:  90690 , step:  90690 , train loss:  0.04120265130492446 , test loss:  0.08410213665801232\n",
      "Epoch:  90700 , step:  90700 , train loss:  0.04120092560733936 , test loss:  0.08410209273800513\n",
      "Epoch:  90710 , step:  90710 , train loss:  0.04119920018717853 , test loss:  0.08410204896339626\n",
      "Epoch:  90720 , step:  90720 , train loss:  0.04119747504436715 , test loss:  0.08410200533413557\n",
      "Epoch:  90730 , step:  90730 , train loss:  0.041195750178830634 , test loss:  0.08410196185017331\n",
      "Epoch:  90740 , step:  90740 , train loss:  0.04119402559049426 , test loss:  0.08410191851145925\n",
      "Epoch:  90750 , step:  90750 , train loss:  0.04119230127928339 , test loss:  0.08410187531794346\n",
      "Epoch:  90760 , step:  90760 , train loss:  0.041190577245123434 , test loss:  0.08410183226957617\n",
      "Epoch:  90770 , step:  90770 , train loss:  0.04118885348793981 , test loss:  0.08410178936630729\n",
      "Epoch:  90780 , step:  90780 , train loss:  0.04118713000765797 , test loss:  0.08410174660808703\n",
      "Epoch:  90790 , step:  90790 , train loss:  0.041185406804203406 , test loss:  0.08410170399486562\n",
      "Epoch:  90800 , step:  90800 , train loss:  0.0411836838775016 , test loss:  0.08410166152659301\n",
      "Epoch:  90810 , step:  90810 , train loss:  0.04118196122747811 , test loss:  0.08410161920321942\n",
      "Epoch:  90820 , step:  90820 , train loss:  0.04118023885405848 , test loss:  0.08410157702469509\n",
      "Epoch:  90830 , step:  90830 , train loss:  0.041178516757168306 , test loss:  0.08410153499097023\n",
      "Epoch:  90840 , step:  90840 , train loss:  0.041176794936733235 , test loss:  0.08410149310199502\n",
      "Epoch:  90850 , step:  90850 , train loss:  0.041175073392678875 , test loss:  0.08410145135771965\n",
      "Epoch:  90860 , step:  90860 , train loss:  0.0411733521249309 , test loss:  0.08410140975809434\n",
      "Epoch:  90870 , step:  90870 , train loss:  0.04117163113341504 , test loss:  0.08410136830306982\n",
      "Epoch:  90880 , step:  90880 , train loss:  0.04116991041805703 , test loss:  0.08410132699259594\n",
      "Epoch:  90890 , step:  90890 , train loss:  0.041168189978782604 , test loss:  0.08410128582662321\n",
      "Epoch:  90900 , step:  90900 , train loss:  0.04116646981551757 , test loss:  0.08410124480510196\n",
      "Epoch:  90910 , step:  90910 , train loss:  0.04116474992818772 , test loss:  0.08410120392798284\n",
      "Epoch:  90920 , step:  90920 , train loss:  0.04116303031671891 , test loss:  0.08410116319521581\n",
      "Epoch:  90930 , step:  90930 , train loss:  0.04116131098103706 , test loss:  0.08410112260675155\n",
      "Epoch:  90940 , step:  90940 , train loss:  0.04115959192106798 , test loss:  0.08410108216254027\n",
      "Epoch:  90950 , step:  90950 , train loss:  0.04115787313673767 , test loss:  0.08410104186253281\n",
      "Epoch:  90960 , step:  90960 , train loss:  0.04115615462797206 , test loss:  0.0841010017066794\n",
      "Epoch:  90970 , step:  90970 , train loss:  0.04115443639469711 , test loss:  0.08410096169493053\n",
      "Epoch:  90980 , step:  90980 , train loss:  0.04115271843683887 , test loss:  0.08410092182723673\n",
      "Epoch:  90990 , step:  90990 , train loss:  0.04115100075432334 , test loss:  0.08410088210354873\n",
      "Epoch:  91000 , step:  91000 , train loss:  0.04114928334707663 , test loss:  0.08410084252381683\n",
      "Epoch:  91010 , step:  91010 , train loss:  0.0411475662150248 , test loss:  0.08410080308799192\n",
      "Epoch:  91020 , step:  91020 , train loss:  0.041145849358094004 , test loss:  0.08410076379602441\n",
      "Epoch:  91030 , step:  91030 , train loss:  0.04114413277621037 , test loss:  0.08410072464786485\n",
      "Epoch:  91040 , step:  91040 , train loss:  0.041142416469300065 , test loss:  0.08410068564346401\n",
      "Epoch:  91050 , step:  91050 , train loss:  0.04114070043728932 , test loss:  0.0841006467827725\n",
      "Epoch:  91060 , step:  91060 , train loss:  0.04113898468010438 , test loss:  0.08410060806574103\n",
      "Epoch:  91070 , step:  91070 , train loss:  0.041137269197671474 , test loss:  0.08410056949232042\n",
      "Epoch:  91080 , step:  91080 , train loss:  0.04113555398991691 , test loss:  0.08410053106246142\n",
      "Epoch:  91090 , step:  91090 , train loss:  0.04113383905676699 , test loss:  0.08410049277611475\n",
      "Epoch:  91100 , step:  91100 , train loss:  0.0411321243981481 , test loss:  0.08410045463323118\n",
      "Epoch:  91110 , step:  91110 , train loss:  0.041130410013986575 , test loss:  0.08410041663376128\n",
      "Epoch:  91120 , step:  91120 , train loss:  0.041128695904208805 , test loss:  0.08410037877765625\n",
      "Epoch:  91130 , step:  91130 , train loss:  0.041126982068741276 , test loss:  0.08410034106486668\n",
      "Epoch:  91140 , step:  91140 , train loss:  0.04112526850751037 , test loss:  0.08410030349534345\n",
      "Epoch:  91150 , step:  91150 , train loss:  0.04112355522044262 , test loss:  0.0841002660690376\n",
      "Epoch:  91160 , step:  91160 , train loss:  0.04112184220746454 , test loss:  0.08410022878589966\n",
      "Epoch:  91170 , step:  91170 , train loss:  0.04112012946850265 , test loss:  0.08410019164588099\n",
      "Epoch:  91180 , step:  91180 , train loss:  0.0411184170034835 , test loss:  0.08410015464893233\n",
      "Epoch:  91190 , step:  91190 , train loss:  0.04111670481233372 , test loss:  0.08410011779500465\n",
      "Epoch:  91200 , step:  91200 , train loss:  0.04111499289497991 , test loss:  0.08410008108404891\n",
      "Epoch:  91210 , step:  91210 , train loss:  0.04111328125134873 , test loss:  0.08410004451601627\n",
      "Epoch:  91220 , step:  91220 , train loss:  0.04111156988136687 , test loss:  0.0841000080908575\n",
      "Epoch:  91230 , step:  91230 , train loss:  0.041109858784961026 , test loss:  0.08409997180852377\n",
      "Epoch:  91240 , step:  91240 , train loss:  0.04110814796205789 , test loss:  0.0840999356689662\n",
      "Epoch:  91250 , step:  91250 , train loss:  0.04110643741258428 , test loss:  0.08409989967213591\n",
      "Epoch:  91260 , step:  91260 , train loss:  0.04110472713646694 , test loss:  0.08409986381798396\n",
      "Epoch:  91270 , step:  91270 , train loss:  0.04110301713363271 , test loss:  0.08409982810646129\n",
      "Epoch:  91280 , step:  91280 , train loss:  0.041101307404008455 , test loss:  0.08409979253751933\n",
      "Epoch:  91290 , step:  91290 , train loss:  0.04109959794752098 , test loss:  0.08409975711110912\n",
      "Epoch:  91300 , step:  91300 , train loss:  0.04109788876409724 , test loss:  0.08409972182718187\n",
      "Epoch:  91310 , step:  91310 , train loss:  0.04109617985366415 , test loss:  0.08409968668568873\n",
      "Epoch:  91320 , step:  91320 , train loss:  0.04109447121614863 , test loss:  0.08409965168658111\n",
      "Epoch:  91330 , step:  91330 , train loss:  0.041092762851477706 , test loss:  0.08409961682980996\n",
      "Epoch:  91340 , step:  91340 , train loss:  0.04109105475957833 , test loss:  0.08409958211532673\n",
      "Epoch:  91350 , step:  91350 , train loss:  0.04108934694037759 , test loss:  0.08409954754308284\n",
      "Epoch:  91360 , step:  91360 , train loss:  0.041087639393802514 , test loss:  0.08409951311302949\n",
      "Epoch:  91370 , step:  91370 , train loss:  0.0410859321197802 , test loss:  0.08409947882511798\n",
      "Epoch:  91380 , step:  91380 , train loss:  0.04108422511823778 , test loss:  0.08409944467929964\n",
      "Epoch:  91390 , step:  91390 , train loss:  0.041082518389102374 , test loss:  0.08409941067552597\n",
      "Epoch:  91400 , step:  91400 , train loss:  0.04108081193230118 , test loss:  0.0840993768137483\n",
      "Epoch:  91410 , step:  91410 , train loss:  0.041079105747761385 , test loss:  0.08409934309391798\n",
      "Epoch:  91420 , step:  91420 , train loss:  0.0410773998354102 , test loss:  0.08409930951598651\n",
      "Epoch:  91430 , step:  91430 , train loss:  0.04107569419517489 , test loss:  0.08409927607990537\n",
      "Epoch:  91440 , step:  91440 , train loss:  0.04107398882698275 , test loss:  0.084099242785626\n",
      "Epoch:  91450 , step:  91450 , train loss:  0.041072283730761064 , test loss:  0.0840992096331\n",
      "Epoch:  91460 , step:  91460 , train loss:  0.0410705789064372 , test loss:  0.08409917662227881\n",
      "Epoch:  91470 , step:  91470 , train loss:  0.04106887435393848 , test loss:  0.08409914375311395\n",
      "Epoch:  91480 , step:  91480 , train loss:  0.04106717007319231 , test loss:  0.08409911102555696\n",
      "Epoch:  91490 , step:  91490 , train loss:  0.041065466064126144 , test loss:  0.08409907843955938\n",
      "Epoch:  91500 , step:  91500 , train loss:  0.04106376232666738 , test loss:  0.08409904599507304\n",
      "Epoch:  91510 , step:  91510 , train loss:  0.04106205886074348 , test loss:  0.08409901369204921\n",
      "Epoch:  91520 , step:  91520 , train loss:  0.041060355666281974 , test loss:  0.08409898153043988\n",
      "Epoch:  91530 , step:  91530 , train loss:  0.04105865274321039 , test loss:  0.08409894951019635\n",
      "Epoch:  91540 , step:  91540 , train loss:  0.041056950091456275 , test loss:  0.08409891763127071\n",
      "Epoch:  91550 , step:  91550 , train loss:  0.041055247710947186 , test loss:  0.08409888589361446\n",
      "Epoch:  91560 , step:  91560 , train loss:  0.04105354560161077 , test loss:  0.0840988542971792\n",
      "Epoch:  91570 , step:  91570 , train loss:  0.04105184376337461 , test loss:  0.08409882284191689\n",
      "Epoch:  91580 , step:  91580 , train loss:  0.04105014219616642 , test loss:  0.08409879152777922\n",
      "Epoch:  91590 , step:  91590 , train loss:  0.04104844089991388 , test loss:  0.08409876035471797\n",
      "Epoch:  91600 , step:  91600 , train loss:  0.041046739874544685 , test loss:  0.08409872932268496\n",
      "Epoch:  91610 , step:  91610 , train loss:  0.04104503911998658 , test loss:  0.08409869843163194\n",
      "Epoch:  91620 , step:  91620 , train loss:  0.04104333863616734 , test loss:  0.08409866768151085\n",
      "Epoch:  91630 , step:  91630 , train loss:  0.04104163842301478 , test loss:  0.08409863707227344\n",
      "Epoch:  91640 , step:  91640 , train loss:  0.04103993848045668 , test loss:  0.08409860660387164\n",
      "Epoch:  91650 , step:  91650 , train loss:  0.04103823880842093 , test loss:  0.0840985762762573\n",
      "Epoch:  91660 , step:  91660 , train loss:  0.0410365394068354 , test loss:  0.0840985460893826\n",
      "Epoch:  91670 , step:  91670 , train loss:  0.04103484027562801 , test loss:  0.08409851604319936\n",
      "Epoch:  91680 , step:  91680 , train loss:  0.041033141414726644 , test loss:  0.08409848613765943\n",
      "Epoch:  91690 , step:  91690 , train loss:  0.041031442824059294 , test loss:  0.08409845637271465\n",
      "Epoch:  91700 , step:  91700 , train loss:  0.04102974450355395 , test loss:  0.08409842674831744\n",
      "Epoch:  91710 , step:  91710 , train loss:  0.041028046453138615 , test loss:  0.0840983972644196\n",
      "Epoch:  91720 , step:  91720 , train loss:  0.04102634867274133 , test loss:  0.08409836792097324\n",
      "Epoch:  91730 , step:  91730 , train loss:  0.041024651162290156 , test loss:  0.0840983387179302\n",
      "Epoch:  91740 , step:  91740 , train loss:  0.0410229539217132 , test loss:  0.084098309655243\n",
      "Epoch:  91750 , step:  91750 , train loss:  0.041021256950938584 , test loss:  0.08409828073286353\n",
      "Epoch:  91760 , step:  91760 , train loss:  0.041019560249894435 , test loss:  0.0840982519507438\n",
      "Epoch:  91770 , step:  91770 , train loss:  0.04101786381850895 , test loss:  0.08409822330883605\n",
      "Epoch:  91780 , step:  91780 , train loss:  0.04101616765671031 , test loss:  0.08409819480709248\n",
      "Epoch:  91790 , step:  91790 , train loss:  0.04101447176442677 , test loss:  0.08409816644546507\n",
      "Epoch:  91800 , step:  91800 , train loss:  0.04101277614158655 , test loss:  0.08409813822390634\n",
      "Epoch:  91810 , step:  91810 , train loss:  0.04101108078811796 , test loss:  0.0840981101423685\n",
      "Epoch:  91820 , step:  91820 , train loss:  0.04100938570394929 , test loss:  0.08409808220080349\n",
      "Epoch:  91830 , step:  91830 , train loss:  0.041007690889008894 , test loss:  0.08409805439916372\n",
      "Epoch:  91840 , step:  91840 , train loss:  0.0410059963432251 , test loss:  0.08409802673740158\n",
      "Epoch:  91850 , step:  91850 , train loss:  0.04100430206652636 , test loss:  0.08409799921546954\n",
      "Epoch:  91860 , step:  91860 , train loss:  0.041002608058841004 , test loss:  0.08409797183331956\n",
      "Epoch:  91870 , step:  91870 , train loss:  0.04100091432009755 , test loss:  0.0840979445909041\n",
      "Epoch:  91880 , step:  91880 , train loss:  0.04099922085022443 , test loss:  0.0840979174881756\n",
      "Epoch:  91890 , step:  91890 , train loss:  0.040997527649150156 , test loss:  0.08409789052508644\n",
      "Epoch:  91900 , step:  91900 , train loss:  0.04099583471680322 , test loss:  0.08409786370158887\n",
      "Epoch:  91910 , step:  91910 , train loss:  0.04099414205311221 , test loss:  0.08409783701763544\n",
      "Epoch:  91920 , step:  91920 , train loss:  0.04099244965800566 , test loss:  0.08409781047317866\n",
      "Epoch:  91930 , step:  91930 , train loss:  0.040990757531412216 , test loss:  0.08409778406817099\n",
      "Epoch:  91940 , step:  91940 , train loss:  0.04098906567326046 , test loss:  0.0840977578025649\n",
      "Epoch:  91950 , step:  91950 , train loss:  0.04098737408347909 , test loss:  0.08409773167631293\n",
      "Epoch:  91960 , step:  91960 , train loss:  0.04098568276199678 , test loss:  0.08409770568936752\n",
      "Epoch:  91970 , step:  91970 , train loss:  0.04098399170874224 , test loss:  0.08409767984168123\n",
      "Epoch:  91980 , step:  91980 , train loss:  0.04098230092364418 , test loss:  0.08409765413320669\n",
      "Epoch:  91990 , step:  91990 , train loss:  0.040980610406631396 , test loss:  0.08409762856389641\n",
      "Epoch:  92000 , step:  92000 , train loss:  0.04097892015763264 , test loss:  0.084097603133703\n",
      "Epoch:  92010 , step:  92010 , train loss:  0.04097723017657676 , test loss:  0.08409757784257911\n",
      "Epoch:  92020 , step:  92020 , train loss:  0.04097554046339258 , test loss:  0.08409755269047735\n",
      "Epoch:  92030 , step:  92030 , train loss:  0.040973851018008975 , test loss:  0.08409752767735054\n",
      "Epoch:  92040 , step:  92040 , train loss:  0.04097216184035484 , test loss:  0.08409750280315113\n",
      "Epoch:  92050 , step:  92050 , train loss:  0.040970472930359075 , test loss:  0.08409747806783198\n",
      "Epoch:  92060 , step:  92060 , train loss:  0.04096878428795066 , test loss:  0.08409745347134581\n",
      "Epoch:  92070 , step:  92070 , train loss:  0.04096709591305855 , test loss:  0.08409742901364542\n",
      "Epoch:  92080 , step:  92080 , train loss:  0.04096540780561177 , test loss:  0.08409740469468342\n",
      "Epoch:  92090 , step:  92090 , train loss:  0.04096371996553931 , test loss:  0.08409738051441269\n",
      "Epoch:  92100 , step:  92100 , train loss:  0.04096203239277024 , test loss:  0.084097356472786\n",
      "Epoch:  92110 , step:  92110 , train loss:  0.04096034508723367 , test loss:  0.08409733256975634\n",
      "Epoch:  92120 , step:  92120 , train loss:  0.04095865804885864 , test loss:  0.08409730880527633\n",
      "Epoch:  92130 , step:  92130 , train loss:  0.040956971277574354 , test loss:  0.08409728517929892\n",
      "Epoch:  92140 , step:  92140 , train loss:  0.04095528477330992 , test loss:  0.08409726169177695\n",
      "Epoch:  92150 , step:  92150 , train loss:  0.040953598535994555 , test loss:  0.08409723834266332\n",
      "Epoch:  92160 , step:  92160 , train loss:  0.040951912565557455 , test loss:  0.08409721513191108\n",
      "Epoch:  92170 , step:  92170 , train loss:  0.04095022686192785 , test loss:  0.08409719205947311\n",
      "Epoch:  92180 , step:  92180 , train loss:  0.04094854142503502 , test loss:  0.08409716912530231\n",
      "Epoch:  92190 , step:  92190 , train loss:  0.040946856254808275 , test loss:  0.08409714632935174\n",
      "Epoch:  92200 , step:  92200 , train loss:  0.0409451713511769 , test loss:  0.08409712367157428\n",
      "Epoch:  92210 , step:  92210 , train loss:  0.04094348671407023 , test loss:  0.08409710115192313\n",
      "Epoch:  92220 , step:  92220 , train loss:  0.04094180234341766 , test loss:  0.08409707877035139\n",
      "Epoch:  92230 , step:  92230 , train loss:  0.04094011823914858 , test loss:  0.08409705652681179\n",
      "Epoch:  92240 , step:  92240 , train loss:  0.040938434401192436 , test loss:  0.0840970344212577\n",
      "Epoch:  92250 , step:  92250 , train loss:  0.04093675082947862 , test loss:  0.08409701245364203\n",
      "Epoch:  92260 , step:  92260 , train loss:  0.04093506752393666 , test loss:  0.08409699062391793\n",
      "Epoch:  92270 , step:  92270 , train loss:  0.04093338448449603 , test loss:  0.08409696893203877\n",
      "Epoch:  92280 , step:  92280 , train loss:  0.04093170171108626 , test loss:  0.08409694737795739\n",
      "Epoch:  92290 , step:  92290 , train loss:  0.04093001920363692 , test loss:  0.08409692596162716\n",
      "Epoch:  92300 , step:  92300 , train loss:  0.04092833696207756 , test loss:  0.08409690468300128\n",
      "Epoch:  92310 , step:  92310 , train loss:  0.04092665498633781 , test loss:  0.08409688354203285\n",
      "Epoch:  92320 , step:  92320 , train loss:  0.04092497327634731 , test loss:  0.08409686253867527\n",
      "Epoch:  92330 , step:  92330 , train loss:  0.040923291832035684 , test loss:  0.0840968416728817\n",
      "Epoch:  92340 , step:  92340 , train loss:  0.04092161065333265 , test loss:  0.08409682094460533\n",
      "Epoch:  92350 , step:  92350 , train loss:  0.04091992974016788 , test loss:  0.08409680035379945\n",
      "Epoch:  92360 , step:  92360 , train loss:  0.04091824909247116 , test loss:  0.08409677990041771\n",
      "Epoch:  92370 , step:  92370 , train loss:  0.04091656871017222 , test loss:  0.08409675958441319\n",
      "Epoch:  92380 , step:  92380 , train loss:  0.040914888593200846 , test loss:  0.08409673940573913\n",
      "Epoch:  92390 , step:  92390 , train loss:  0.04091320874148689 , test loss:  0.08409671936434925\n",
      "Epoch:  92400 , step:  92400 , train loss:  0.04091152915496014 , test loss:  0.08409669946019657\n",
      "Epoch:  92410 , step:  92410 , train loss:  0.040909849833550505 , test loss:  0.08409667969323469\n",
      "Epoch:  92420 , step:  92420 , train loss:  0.04090817077718786 , test loss:  0.08409666006341705\n",
      "Epoch:  92430 , step:  92430 , train loss:  0.04090649198580214 , test loss:  0.08409664057069728\n",
      "Epoch:  92440 , step:  92440 , train loss:  0.040904813459323244 , test loss:  0.08409662121502828\n",
      "Epoch:  92450 , step:  92450 , train loss:  0.04090313519768122 , test loss:  0.08409660199636403\n",
      "Epoch:  92460 , step:  92460 , train loss:  0.04090145720080597 , test loss:  0.08409658291465782\n",
      "Epoch:  92470 , step:  92470 , train loss:  0.040899779468627606 , test loss:  0.08409656396986344\n",
      "Epoch:  92480 , step:  92480 , train loss:  0.04089810200107614 , test loss:  0.08409654516193428\n",
      "Epoch:  92490 , step:  92490 , train loss:  0.04089642479808163 , test loss:  0.08409652649082389\n",
      "Epoch:  92500 , step:  92500 , train loss:  0.040894747859574185 , test loss:  0.08409650795648586\n",
      "Epoch:  92510 , step:  92510 , train loss:  0.04089307118548395 , test loss:  0.0840964895588738\n",
      "Epoch:  92520 , step:  92520 , train loss:  0.040891394775741066 , test loss:  0.08409647129794123\n",
      "Epoch:  92530 , step:  92530 , train loss:  0.04088971863027571 , test loss:  0.08409645317364176\n",
      "Epoch:  92540 , step:  92540 , train loss:  0.04088804274901807 , test loss:  0.08409643518592963\n",
      "Epoch:  92550 , step:  92550 , train loss:  0.04088636713189843 , test loss:  0.084096417334758\n",
      "Epoch:  92560 , step:  92560 , train loss:  0.04088469177884699 , test loss:  0.08409639962008061\n",
      "Epoch:  92570 , step:  92570 , train loss:  0.04088301668979407 , test loss:  0.08409638204185114\n",
      "Epoch:  92580 , step:  92580 , train loss:  0.040881341864669955 , test loss:  0.08409636460002355\n",
      "Epoch:  92590 , step:  92590 , train loss:  0.04087966730340498 , test loss:  0.0840963472945515\n",
      "Epoch:  92600 , step:  92600 , train loss:  0.04087799300592952 , test loss:  0.0840963301253887\n",
      "Epoch:  92610 , step:  92610 , train loss:  0.04087631897217395 , test loss:  0.08409631309248909\n",
      "Epoch:  92620 , step:  92620 , train loss:  0.04087464520206868 , test loss:  0.08409629619580637\n",
      "Epoch:  92630 , step:  92630 , train loss:  0.04087297169554414 , test loss:  0.08409627943529438\n",
      "Epoch:  92640 , step:  92640 , train loss:  0.04087129845253082 , test loss:  0.08409626281090699\n",
      "Epoch:  92650 , step:  92650 , train loss:  0.04086962547295917 , test loss:  0.08409624632259817\n",
      "Epoch:  92660 , step:  92660 , train loss:  0.040867952756759744 , test loss:  0.08409622997032178\n",
      "Epoch:  92670 , step:  92670 , train loss:  0.040866280303863044 , test loss:  0.0840962137540315\n",
      "Epoch:  92680 , step:  92680 , train loss:  0.04086460811419966 , test loss:  0.08409619767368164\n",
      "Epoch:  92690 , step:  92690 , train loss:  0.04086293618770019 , test loss:  0.08409618172922587\n",
      "Epoch:  92700 , step:  92700 , train loss:  0.04086126452429522 , test loss:  0.08409616592061848\n",
      "Epoch:  92710 , step:  92710 , train loss:  0.040859593123915405 , test loss:  0.08409615024781311\n",
      "Epoch:  92720 , step:  92720 , train loss:  0.04085792198649145 , test loss:  0.08409613471076396\n",
      "Epoch:  92730 , step:  92730 , train loss:  0.04085625111195398 , test loss:  0.08409611930942497\n",
      "Epoch:  92740 , step:  92740 , train loss:  0.04085458050023377 , test loss:  0.08409610404375041\n",
      "Epoch:  92750 , step:  92750 , train loss:  0.040852910151261565 , test loss:  0.08409608891369397\n",
      "Epoch:  92760 , step:  92760 , train loss:  0.0408512400649681 , test loss:  0.08409607391921002\n",
      "Epoch:  92770 , step:  92770 , train loss:  0.04084957024128418 , test loss:  0.08409605906025258\n",
      "Epoch:  92780 , step:  92780 , train loss:  0.04084790068014067 , test loss:  0.08409604433677591\n",
      "Epoch:  92790 , step:  92790 , train loss:  0.04084623138146835 , test loss:  0.0840960297487339\n",
      "Epoch:  92800 , step:  92800 , train loss:  0.04084456234519814 , test loss:  0.0840960152960809\n",
      "Epoch:  92810 , step:  92810 , train loss:  0.040842893571260946 , test loss:  0.08409600097877101\n",
      "Epoch:  92820 , step:  92820 , train loss:  0.04084122505958766 , test loss:  0.0840959867967584\n",
      "Epoch:  92830 , step:  92830 , train loss:  0.04083955681010926 , test loss:  0.08409597274999746\n",
      "Epoch:  92840 , step:  92840 , train loss:  0.04083788882275669 , test loss:  0.08409595883844231\n",
      "Epoch:  92850 , step:  92850 , train loss:  0.04083622109746098 , test loss:  0.08409594506204712\n",
      "Epoch:  92860 , step:  92860 , train loss:  0.040834553634153135 , test loss:  0.08409593142076639\n",
      "Epoch:  92870 , step:  92870 , train loss:  0.04083288643276421 , test loss:  0.08409591791455422\n",
      "Epoch:  92880 , step:  92880 , train loss:  0.04083121949322532 , test loss:  0.08409590454336502\n",
      "Epoch:  92890 , step:  92890 , train loss:  0.0408295528154675 , test loss:  0.08409589130715306\n",
      "Epoch:  92900 , step:  92900 , train loss:  0.04082788639942195 , test loss:  0.08409587820587276\n",
      "Epoch:  92910 , step:  92910 , train loss:  0.040826220245019744 , test loss:  0.08409586523947846\n",
      "Epoch:  92920 , step:  92920 , train loss:  0.04082455435219212 , test loss:  0.08409585240792468\n",
      "Epoch:  92930 , step:  92930 , train loss:  0.04082288872087031 , test loss:  0.08409583971116569\n",
      "Epoch:  92940 , step:  92940 , train loss:  0.04082122335098546 , test loss:  0.0840958271491559\n",
      "Epoch:  92950 , step:  92950 , train loss:  0.04081955824246888 , test loss:  0.08409581472184977\n",
      "Epoch:  92960 , step:  92960 , train loss:  0.04081789339525183 , test loss:  0.08409580242920194\n",
      "Epoch:  92970 , step:  92970 , train loss:  0.04081622880926563 , test loss:  0.08409579027116655\n",
      "Epoch:  92980 , step:  92980 , train loss:  0.04081456448444161 , test loss:  0.08409577824769837\n",
      "Epoch:  92990 , step:  92990 , train loss:  0.04081290042071111 , test loss:  0.0840957663587518\n",
      "Epoch:  93000 , step:  93000 , train loss:  0.040811236618005536 , test loss:  0.0840957546042816\n",
      "Epoch:  93010 , step:  93010 , train loss:  0.040809573076256274 , test loss:  0.08409574298424215\n",
      "Epoch:  93020 , step:  93020 , train loss:  0.040807909795394795 , test loss:  0.08409573149858811\n",
      "Epoch:  93030 , step:  93030 , train loss:  0.04080624677535248 , test loss:  0.08409572014727397\n",
      "Epoch:  93040 , step:  93040 , train loss:  0.040804584016060914 , test loss:  0.08409570893025461\n",
      "Epoch:  93050 , step:  93050 , train loss:  0.04080292151745152 , test loss:  0.08409569784748416\n",
      "Epoch:  93060 , step:  93060 , train loss:  0.04080125927945586 , test loss:  0.0840956868989178\n",
      "Epoch:  93070 , step:  93070 , train loss:  0.040799597302005525 , test loss:  0.08409567608451002\n",
      "Epoch:  93080 , step:  93080 , train loss:  0.04079793558503203 , test loss:  0.0840956654042154\n",
      "Epoch:  93090 , step:  93090 , train loss:  0.04079627412846705 , test loss:  0.08409565485798871\n",
      "Epoch:  93100 , step:  93100 , train loss:  0.0407946129322422 , test loss:  0.08409564444578475\n",
      "Epoch:  93110 , step:  93110 , train loss:  0.04079295199628913 , test loss:  0.08409563416755826\n",
      "Epoch:  93120 , step:  93120 , train loss:  0.04079129132053954 , test loss:  0.08409562402326388\n",
      "Epoch:  93130 , step:  93130 , train loss:  0.040789630904925114 , test loss:  0.08409561401285658\n",
      "Epoch:  93140 , step:  93140 , train loss:  0.04078797074937759 , test loss:  0.08409560413629096\n",
      "Epoch:  93150 , step:  93150 , train loss:  0.04078631085382877 , test loss:  0.08409559439352206\n",
      "Epoch:  93160 , step:  93160 , train loss:  0.04078465121821039 , test loss:  0.08409558478450456\n",
      "Epoch:  93170 , step:  93170 , train loss:  0.04078299184245434 , test loss:  0.0840955753091935\n",
      "Epoch:  93180 , step:  93180 , train loss:  0.040781332726492314 , test loss:  0.08409556596754357\n",
      "Epoch:  93190 , step:  93190 , train loss:  0.04077967387025631 , test loss:  0.08409555675950983\n",
      "Epoch:  93200 , step:  93200 , train loss:  0.04077801527367816 , test loss:  0.08409554768504718\n",
      "Epoch:  93210 , step:  93210 , train loss:  0.04077635693668977 , test loss:  0.08409553874411048\n",
      "Epoch:  93220 , step:  93220 , train loss:  0.04077469885922308 , test loss:  0.08409552993665466\n",
      "Epoch:  93230 , step:  93230 , train loss:  0.040773041041210045 , test loss:  0.08409552126263482\n",
      "Epoch:  93240 , step:  93240 , train loss:  0.04077138348258268 , test loss:  0.0840955127220057\n",
      "Epoch:  93250 , step:  93250 , train loss:  0.04076972618327295 , test loss:  0.08409550431472261\n",
      "Epoch:  93260 , step:  93260 , train loss:  0.04076806914321294 , test loss:  0.08409549604074035\n",
      "Epoch:  93270 , step:  93270 , train loss:  0.04076641236233469 , test loss:  0.08409548790001418\n",
      "Epoch:  93280 , step:  93280 , train loss:  0.04076475584057029 , test loss:  0.08409547989249912\n",
      "Epoch:  93290 , step:  93290 , train loss:  0.04076309957785182 , test loss:  0.08409547201815004\n",
      "Epoch:  93300 , step:  93300 , train loss:  0.04076144357411148 , test loss:  0.08409546427692234\n",
      "Epoch:  93310 , step:  93310 , train loss:  0.04075978782928138 , test loss:  0.08409545666877102\n",
      "Epoch:  93320 , step:  93320 , train loss:  0.0407581323432937 , test loss:  0.0840954491936511\n",
      "Epoch:  93330 , step:  93330 , train loss:  0.04075647711608069 , test loss:  0.08409544185151792\n",
      "Epoch:  93340 , step:  93340 , train loss:  0.04075482214757455 , test loss:  0.08409543464232654\n",
      "Epoch:  93350 , step:  93350 , train loss:  0.04075316743770757 , test loss:  0.08409542756603204\n",
      "Epoch:  93360 , step:  93360 , train loss:  0.04075151298641204 , test loss:  0.08409542062259012\n",
      "Epoch:  93370 , step:  93370 , train loss:  0.04074985879362024 , test loss:  0.08409541381195557\n",
      "Epoch:  93380 , step:  93380 , train loss:  0.040748204859264524 , test loss:  0.08409540713408366\n",
      "Epoch:  93390 , step:  93390 , train loss:  0.04074655118327723 , test loss:  0.08409540058892977\n",
      "Epoch:  93400 , step:  93400 , train loss:  0.04074489776559079 , test loss:  0.08409539417644932\n",
      "Epoch:  93410 , step:  93410 , train loss:  0.040743244606137576 , test loss:  0.08409538789659762\n",
      "Epoch:  93420 , step:  93420 , train loss:  0.04074159170485004 , test loss:  0.0840953817493297\n",
      "Epoch:  93430 , step:  93430 , train loss:  0.0407399390616606 , test loss:  0.08409537573460109\n",
      "Epoch:  93440 , step:  93440 , train loss:  0.04073828667650184 , test loss:  0.08409536985236726\n",
      "Epoch:  93450 , step:  93450 , train loss:  0.04073663454930616 , test loss:  0.08409536410258345\n",
      "Epoch:  93460 , step:  93460 , train loss:  0.04073498268000616 , test loss:  0.08409535848520491\n",
      "Epoch:  93470 , step:  93470 , train loss:  0.04073333106853436 , test loss:  0.08409535300018735\n",
      "Epoch:  93480 , step:  93480 , train loss:  0.04073167971482337 , test loss:  0.08409534764748598\n",
      "Epoch:  93490 , step:  93490 , train loss:  0.040730028618805786 , test loss:  0.08409534242705641\n",
      "Epoch:  93500 , step:  93500 , train loss:  0.04072837778041424 , test loss:  0.08409533733885406\n",
      "Epoch:  93510 , step:  93510 , train loss:  0.04072672719958141 , test loss:  0.08409533238283438\n",
      "Epoch:  93520 , step:  93520 , train loss:  0.040725076876239955 , test loss:  0.08409532755895298\n",
      "Epoch:  93530 , step:  93530 , train loss:  0.04072342681032259 , test loss:  0.08409532286716528\n",
      "Epoch:  93540 , step:  93540 , train loss:  0.04072177700176205 , test loss:  0.08409531830742706\n",
      "Epoch:  93550 , step:  93550 , train loss:  0.04072012745049106 , test loss:  0.08409531387969361\n",
      "Epoch:  93560 , step:  93560 , train loss:  0.040718478156442466 , test loss:  0.08409530958392056\n",
      "Epoch:  93570 , step:  93570 , train loss:  0.04071682911954903 , test loss:  0.08409530542006376\n",
      "Epoch:  93580 , step:  93580 , train loss:  0.040715180339743584 , test loss:  0.08409530138807852\n",
      "Epoch:  93590 , step:  93590 , train loss:  0.04071353181695899 , test loss:  0.08409529748792062\n",
      "Epoch:  93600 , step:  93600 , train loss:  0.04071188355112815 , test loss:  0.08409529371954562\n",
      "Epoch:  93610 , step:  93610 , train loss:  0.04071023554218393 , test loss:  0.0840952900829094\n",
      "Epoch:  93620 , step:  93620 , train loss:  0.040708587790059265 , test loss:  0.08409528657796739\n",
      "Epoch:  93630 , step:  93630 , train loss:  0.04070694029468711 , test loss:  0.08409528320467552\n",
      "Epoch:  93640 , step:  93640 , train loss:  0.04070529305600048 , test loss:  0.0840952799629895\n",
      "Epoch:  93650 , step:  93650 , train loss:  0.040703646073932336 , test loss:  0.08409527685286494\n",
      "Epoch:  93660 , step:  93660 , train loss:  0.04070199934841572 , test loss:  0.08409527387425764\n",
      "Epoch:  93670 , step:  93670 , train loss:  0.04070035287938366 , test loss:  0.08409527102712355\n",
      "Epoch:  93680 , step:  93680 , train loss:  0.040698706666769294 , test loss:  0.08409526831141827\n",
      "Epoch:  93690 , step:  93690 , train loss:  0.040697060710505634 , test loss:  0.08409526572709779\n",
      "Epoch:  93700 , step:  93700 , train loss:  0.0406954150105259 , test loss:  0.0840952632741178\n",
      "Epoch:  93710 , step:  93710 , train loss:  0.04069376956676318 , test loss:  0.08409526095243428\n",
      "Epoch:  93720 , step:  93720 , train loss:  0.04069212437915067 , test loss:  0.08409525876200305\n",
      "Epoch:  93730 , step:  93730 , train loss:  0.040690479447621546 , test loss:  0.08409525670278001\n",
      "Epoch:  93740 , step:  93740 , train loss:  0.04068883477210907 , test loss:  0.08409525477472109\n",
      "Epoch:  93750 , step:  93750 , train loss:  0.040687190352546455 , test loss:  0.08409525297778224\n",
      "Epoch:  93760 , step:  93760 , train loss:  0.04068554618886697 , test loss:  0.08409525131191939\n",
      "Epoch:  93770 , step:  93770 , train loss:  0.04068390228100396 , test loss:  0.08409524977708834\n",
      "Epoch:  93780 , step:  93780 , train loss:  0.04068225862889068 , test loss:  0.08409524837324536\n",
      "Epoch:  93790 , step:  93790 , train loss:  0.04068061523246053 , test loss:  0.08409524710034631\n",
      "Epoch:  93800 , step:  93800 , train loss:  0.04067897209164686 , test loss:  0.0840952459583471\n",
      "Epoch:  93810 , step:  93810 , train loss:  0.040677329206383034 , test loss:  0.08409524494720406\n",
      "Epoch:  93820 , step:  93820 , train loss:  0.04067568657660251 , test loss:  0.08409524406687299\n",
      "Epoch:  93830 , step:  93830 , train loss:  0.04067404420223873 , test loss:  0.08409524331731005\n",
      "Epoch:  93840 , step:  93840 , train loss:  0.040672402083225125 , test loss:  0.08409524269847134\n",
      "Epoch:  93850 , step:  93850 , train loss:  0.040670760219495256 , test loss:  0.08409524221031307\n",
      "Epoch:  93860 , step:  93860 , train loss:  0.04066911861098257 , test loss:  0.08409524185279121\n",
      "Epoch:  93870 , step:  93870 , train loss:  0.04066747725762062 , test loss:  0.08409524162586204\n",
      "Epoch:  93880 , step:  93880 , train loss:  0.04066583615934299 , test loss:  0.08409524152948149\n",
      "Epoch:  93890 , step:  93890 , train loss:  0.04066419531608324 , test loss:  0.08409524156360591\n",
      "Epoch:  93900 , step:  93900 , train loss:  0.040662554727775024 , test loss:  0.08409524172819162\n",
      "Epoch:  93910 , step:  93910 , train loss:  0.040660914394351946 , test loss:  0.08409524202319478\n",
      "Epoch:  93920 , step:  93920 , train loss:  0.04065927431574768 , test loss:  0.08409524244857151\n",
      "Epoch:  93930 , step:  93930 , train loss:  0.04065763449189592 , test loss:  0.08409524300427806\n",
      "Epoch:  93940 , step:  93940 , train loss:  0.040655994922730365 , test loss:  0.08409524369027097\n",
      "Epoch:  93950 , step:  93950 , train loss:  0.04065435560818474 , test loss:  0.08409524450650617\n",
      "Epoch:  93960 , step:  93960 , train loss:  0.040652716548192816 , test loss:  0.08409524545294007\n",
      "Epoch:  93970 , step:  93970 , train loss:  0.04065107774268837 , test loss:  0.08409524652952914\n",
      "Epoch:  93980 , step:  93980 , train loss:  0.04064943919160519 , test loss:  0.0840952477362297\n",
      "Epoch:  93990 , step:  93990 , train loss:  0.040647800894877156 , test loss:  0.08409524907299805\n",
      "Epoch:  94000 , step:  94000 , train loss:  0.040646162852438085 , test loss:  0.08409525053979053\n",
      "Epoch:  94010 , step:  94010 , train loss:  0.04064452506422186 , test loss:  0.0840952521365637\n",
      "Epoch:  94020 , step:  94020 , train loss:  0.0406428875301624 , test loss:  0.08409525386327386\n",
      "Epoch:  94030 , step:  94030 , train loss:  0.0406412502501936 , test loss:  0.08409525571987728\n",
      "Epoch:  94040 , step:  94040 , train loss:  0.04063961322424946 , test loss:  0.08409525770633068\n",
      "Epoch:  94050 , step:  94050 , train loss:  0.04063797645226392 , test loss:  0.08409525982259049\n",
      "Epoch:  94060 , step:  94060 , train loss:  0.04063633993417098 , test loss:  0.08409526206861312\n",
      "Epoch:  94070 , step:  94070 , train loss:  0.04063470366990466 , test loss:  0.0840952644443552\n",
      "Epoch:  94080 , step:  94080 , train loss:  0.04063306765939904 , test loss:  0.08409526694977308\n",
      "Epoch:  94090 , step:  94090 , train loss:  0.040631431902588167 , test loss:  0.08409526958482334\n",
      "Epoch:  94100 , step:  94100 , train loss:  0.04062979639940612 , test loss:  0.08409527234946276\n",
      "Epoch:  94110 , step:  94110 , train loss:  0.04062816114978706 , test loss:  0.0840952752436476\n",
      "Epoch:  94120 , step:  94120 , train loss:  0.040626526153665105 , test loss:  0.08409527826733464\n",
      "Epoch:  94130 , step:  94130 , train loss:  0.04062489141097441 , test loss:  0.08409528142048055\n",
      "Epoch:  94140 , step:  94140 , train loss:  0.040623256921649184 , test loss:  0.0840952847030418\n",
      "Epoch:  94150 , step:  94150 , train loss:  0.040621622685623665 , test loss:  0.08409528811497509\n",
      "Epoch:  94160 , step:  94160 , train loss:  0.040619988702832045 , test loss:  0.08409529165623705\n",
      "Epoch:  94170 , step:  94170 , train loss:  0.04061835497320865 , test loss:  0.08409529532678459\n",
      "Epoch:  94180 , step:  94180 , train loss:  0.04061672149668769 , test loss:  0.08409529912657417\n",
      "Epoch:  94190 , step:  94190 , train loss:  0.04061508827320358 , test loss:  0.08409530305556256\n",
      "Epoch:  94200 , step:  94200 , train loss:  0.04061345530269054 , test loss:  0.08409530711370662\n",
      "Epoch:  94210 , step:  94210 , train loss:  0.04061182258508297 , test loss:  0.0840953113009629\n",
      "Epoch:  94220 , step:  94220 , train loss:  0.04061019012031528 , test loss:  0.08409531561728843\n",
      "Epoch:  94230 , step:  94230 , train loss:  0.04060855790832188 , test loss:  0.08409532006263987\n",
      "Epoch:  94240 , step:  94240 , train loss:  0.040606925949037184 , test loss:  0.08409532463697368\n",
      "Epoch:  94250 , step:  94250 , train loss:  0.04060529424239562 , test loss:  0.08409532934024733\n",
      "Epoch:  94260 , step:  94260 , train loss:  0.040603662788331704 , test loss:  0.08409533417241724\n",
      "Epoch:  94270 , step:  94270 , train loss:  0.04060203158677995 , test loss:  0.0840953391334404\n",
      "Epoch:  94280 , step:  94280 , train loss:  0.040600400637674845 , test loss:  0.0840953442232738\n",
      "Epoch:  94290 , step:  94290 , train loss:  0.040598769940950956 , test loss:  0.08409534944187413\n",
      "Epoch:  94300 , step:  94300 , train loss:  0.04059713949654284 , test loss:  0.0840953547891984\n",
      "Epoch:  94310 , step:  94310 , train loss:  0.04059550930438514 , test loss:  0.08409536026520362\n",
      "Epoch:  94320 , step:  94320 , train loss:  0.040593879364412465 , test loss:  0.08409536586984666\n",
      "Epoch:  94330 , step:  94330 , train loss:  0.040592249676559417 , test loss:  0.08409537160308447\n",
      "Epoch:  94340 , step:  94340 , train loss:  0.0405906202407607 , test loss:  0.0840953774648739\n",
      "Epoch:  94350 , step:  94350 , train loss:  0.04058899105695101 , test loss:  0.0840953834551724\n",
      "Epoch:  94360 , step:  94360 , train loss:  0.04058736212506505 , test loss:  0.08409538957393646\n",
      "Epoch:  94370 , step:  94370 , train loss:  0.040585733445037554 , test loss:  0.08409539582112341\n",
      "Epoch:  94380 , step:  94380 , train loss:  0.04058410501680331 , test loss:  0.08409540219669034\n",
      "Epoch:  94390 , step:  94390 , train loss:  0.0405824768402971 , test loss:  0.08409540870059408\n",
      "Epoch:  94400 , step:  94400 , train loss:  0.0405808489154537 , test loss:  0.08409541533279202\n",
      "Epoch:  94410 , step:  94410 , train loss:  0.040579221242208016 , test loss:  0.0840954220932411\n",
      "Epoch:  94420 , step:  94420 , train loss:  0.04057759382049486 , test loss:  0.08409542898189853\n",
      "Epoch:  94430 , step:  94430 , train loss:  0.04057596665024908 , test loss:  0.0840954359987213\n",
      "Epoch:  94440 , step:  94440 , train loss:  0.04057433973140567 , test loss:  0.08409544314366667\n",
      "Epoch:  94450 , step:  94450 , train loss:  0.040572713063899486 , test loss:  0.08409545041669182\n",
      "Epoch:  94460 , step:  94460 , train loss:  0.04057108664766552 , test loss:  0.08409545781775397\n",
      "Epoch:  94470 , step:  94470 , train loss:  0.040569460482638736 , test loss:  0.0840954653468104\n",
      "Epoch:  94480 , step:  94480 , train loss:  0.040567834568754145 , test loss:  0.08409547300381814\n",
      "Epoch:  94490 , step:  94490 , train loss:  0.04056620890594674 , test loss:  0.0840954807887346\n",
      "Epoch:  94500 , step:  94500 , train loss:  0.040564583494151624 , test loss:  0.08409548870151701\n",
      "Epoch:  94510 , step:  94510 , train loss:  0.04056295833330379 , test loss:  0.08409549674212267\n",
      "Epoch:  94520 , step:  94520 , train loss:  0.04056133342333843 , test loss:  0.0840955049105089\n",
      "Epoch:  94530 , step:  94530 , train loss:  0.0405597087641906 , test loss:  0.08409551320663297\n",
      "Epoch:  94540 , step:  94540 , train loss:  0.04055808435579546 , test loss:  0.08409552163045218\n",
      "Epoch:  94550 , step:  94550 , train loss:  0.04055646019808815 , test loss:  0.08409553018192388\n",
      "Epoch:  94560 , step:  94560 , train loss:  0.040554836291003885 , test loss:  0.08409553886100553\n",
      "Epoch:  94570 , step:  94570 , train loss:  0.040553212634477885 , test loss:  0.08409554766765459\n",
      "Epoch:  94580 , step:  94580 , train loss:  0.04055158922844536 , test loss:  0.08409555660182823\n",
      "Epoch:  94590 , step:  94590 , train loss:  0.0405499660728416 , test loss:  0.08409556566348406\n",
      "Epoch:  94600 , step:  94600 , train loss:  0.040548343167601884 , test loss:  0.0840955748525795\n",
      "Epoch:  94610 , step:  94610 , train loss:  0.04054672051266148 , test loss:  0.08409558416907194\n",
      "Epoch:  94620 , step:  94620 , train loss:  0.04054509810795577 , test loss:  0.08409559361291881\n",
      "Epoch:  94630 , step:  94630 , train loss:  0.04054347595342008 , test loss:  0.0840956031840778\n",
      "Epoch:  94640 , step:  94640 , train loss:  0.04054185404898976 , test loss:  0.08409561288250623\n",
      "Epoch:  94650 , step:  94650 , train loss:  0.040540232394600285 , test loss:  0.08409562270816166\n",
      "Epoch:  94660 , step:  94660 , train loss:  0.040538610990187 , test loss:  0.08409563266100172\n",
      "Epoch:  94670 , step:  94670 , train loss:  0.040536989835685426 , test loss:  0.08409564274098377\n",
      "Epoch:  94680 , step:  94680 , train loss:  0.04053536893103098 , test loss:  0.08409565294806576\n",
      "Epoch:  94690 , step:  94690 , train loss:  0.040533748276159166 , test loss:  0.08409566328220507\n",
      "Epoch:  94700 , step:  94700 , train loss:  0.04053212787100552 , test loss:  0.08409567374335913\n",
      "Epoch:  94710 , step:  94710 , train loss:  0.04053050771550557 , test loss:  0.08409568433148588\n",
      "Epoch:  94720 , step:  94720 , train loss:  0.04052888780959487 , test loss:  0.08409569504654292\n",
      "Epoch:  94730 , step:  94730 , train loss:  0.04052726815320903 , test loss:  0.08409570588848769\n",
      "Epoch:  94740 , step:  94740 , train loss:  0.04052564874628364 , test loss:  0.08409571685727821\n",
      "Epoch:  94750 , step:  94750 , train loss:  0.04052402958875437 , test loss:  0.08409572795287183\n",
      "Epoch:  94760 , step:  94760 , train loss:  0.04052241068055682 , test loss:  0.08409573917522654\n",
      "Epoch:  94770 , step:  94770 , train loss:  0.040520792021626714 , test loss:  0.08409575052429993\n",
      "Epoch:  94780 , step:  94780 , train loss:  0.040519173611899745 , test loss:  0.08409576200004974\n",
      "Epoch:  94790 , step:  94790 , train loss:  0.040517555451311645 , test loss:  0.08409577360243392\n",
      "Epoch:  94800 , step:  94800 , train loss:  0.04051593753979815 , test loss:  0.08409578533140995\n",
      "Epoch:  94810 , step:  94810 , train loss:  0.04051431987729506 , test loss:  0.084095797186936\n",
      "Epoch:  94820 , step:  94820 , train loss:  0.04051270246373815 , test loss:  0.08409580916896965\n",
      "Epoch:  94830 , step:  94830 , train loss:  0.04051108529906327 , test loss:  0.08409582127746872\n",
      "Epoch:  94840 , step:  94840 , train loss:  0.040509468383206196 , test loss:  0.0840958335123913\n",
      "Epoch:  94850 , step:  94850 , train loss:  0.04050785171610285 , test loss:  0.08409584587369491\n",
      "Epoch:  94860 , step:  94860 , train loss:  0.04050623529768913 , test loss:  0.08409585836133769\n",
      "Epoch:  94870 , step:  94870 , train loss:  0.040504619127900904 , test loss:  0.08409587097527745\n",
      "Epoch:  94880 , step:  94880 , train loss:  0.04050300320667415 , test loss:  0.08409588371547225\n",
      "Epoch:  94890 , step:  94890 , train loss:  0.04050138753394485 , test loss:  0.08409589658187994\n",
      "Epoch:  94900 , step:  94900 , train loss:  0.040499772109648904 , test loss:  0.08409590957445845\n",
      "Epoch:  94910 , step:  94910 , train loss:  0.04049815693372237 , test loss:  0.08409592269316588\n",
      "Epoch:  94920 , step:  94920 , train loss:  0.04049654200610129 , test loss:  0.08409593593796\n",
      "Epoch:  94930 , step:  94930 , train loss:  0.04049492732672167 , test loss:  0.08409594930879885\n",
      "Epoch:  94940 , step:  94940 , train loss:  0.04049331289551962 , test loss:  0.08409596280564081\n",
      "Epoch:  94950 , step:  94950 , train loss:  0.04049169871243123 , test loss:  0.08409597642844345\n",
      "Epoch:  94960 , step:  94960 , train loss:  0.04049008477739264 , test loss:  0.08409599017716517\n",
      "Epoch:  94970 , step:  94970 , train loss:  0.040488471090339956 , test loss:  0.0840960040517639\n",
      "Epoch:  94980 , step:  94980 , train loss:  0.040486857651209386 , test loss:  0.08409601805219784\n",
      "Epoch:  94990 , step:  94990 , train loss:  0.040485244459937085 , test loss:  0.0840960321784249\n",
      "Epoch:  95000 , step:  95000 , train loss:  0.04048363151645928 , test loss:  0.08409604643040346\n",
      "Epoch:  95010 , step:  95010 , train loss:  0.040482018820712215 , test loss:  0.08409606080809146\n",
      "Epoch:  95020 , step:  95020 , train loss:  0.04048040637263215 , test loss:  0.08409607531144704\n",
      "Epoch:  95030 , step:  95030 , train loss:  0.040478794172155344 , test loss:  0.08409608994042857\n",
      "Epoch:  95040 , step:  95040 , train loss:  0.04047718221921813 , test loss:  0.08409610469499415\n",
      "Epoch:  95050 , step:  95050 , train loss:  0.04047557051375683 , test loss:  0.08409611957510207\n",
      "Epoch:  95060 , step:  95060 , train loss:  0.04047395905570778 , test loss:  0.08409613458071045\n",
      "Epoch:  95070 , step:  95070 , train loss:  0.040472347845007396 , test loss:  0.08409614971177766\n",
      "Epoch:  95080 , step:  95080 , train loss:  0.04047073688159203 , test loss:  0.08409616496826174\n",
      "Epoch:  95090 , step:  95090 , train loss:  0.040469126165398106 , test loss:  0.0840961803501212\n",
      "Epoch:  95100 , step:  95100 , train loss:  0.04046751569636212 , test loss:  0.08409619585731434\n",
      "Epoch:  95110 , step:  95110 , train loss:  0.04046590547442048 , test loss:  0.0840962114897995\n",
      "Epoch:  95120 , step:  95120 , train loss:  0.04046429549950969 , test loss:  0.08409622724753472\n",
      "Epoch:  95130 , step:  95130 , train loss:  0.04046268577156628 , test loss:  0.08409624313047845\n",
      "Epoch:  95140 , step:  95140 , train loss:  0.040461076290526796 , test loss:  0.08409625913858935\n",
      "Epoch:  95150 , step:  95150 , train loss:  0.040459467056327726 , test loss:  0.08409627527182563\n",
      "Epoch:  95160 , step:  95160 , train loss:  0.04045785806890571 , test loss:  0.08409629153014558\n",
      "Epoch:  95170 , step:  95170 , train loss:  0.04045624932819736 , test loss:  0.08409630791350765\n",
      "Epoch:  95180 , step:  95180 , train loss:  0.04045464083413925 , test loss:  0.08409632442187033\n",
      "Epoch:  95190 , step:  95190 , train loss:  0.0404530325866681 , test loss:  0.08409634105519205\n",
      "Epoch:  95200 , step:  95200 , train loss:  0.04045142458572052 , test loss:  0.08409635781343139\n",
      "Epoch:  95210 , step:  95210 , train loss:  0.04044981683123326 , test loss:  0.0840963746965468\n",
      "Epoch:  95220 , step:  95220 , train loss:  0.040448209323142977 , test loss:  0.08409639170449666\n",
      "Epoch:  95230 , step:  95230 , train loss:  0.040446602061386484 , test loss:  0.08409640883723961\n",
      "Epoch:  95240 , step:  95240 , train loss:  0.04044499504590045 , test loss:  0.08409642609473408\n",
      "Epoch:  95250 , step:  95250 , train loss:  0.04044338827662174 , test loss:  0.08409644347693852\n",
      "Epoch:  95260 , step:  95260 , train loss:  0.04044178175348713 , test loss:  0.08409646098381178\n",
      "Epoch:  95270 , step:  95270 , train loss:  0.040440175476433464 , test loss:  0.08409647861531243\n",
      "Epoch:  95280 , step:  95280 , train loss:  0.0404385694453976 , test loss:  0.08409649637139886\n",
      "Epoch:  95290 , step:  95290 , train loss:  0.04043696366031642 , test loss:  0.08409651425202982\n",
      "Epoch:  95300 , step:  95300 , train loss:  0.04043535812112677 , test loss:  0.08409653225716392\n",
      "Epoch:  95310 , step:  95310 , train loss:  0.04043375282776566 , test loss:  0.08409655038675977\n",
      "Epoch:  95320 , step:  95320 , train loss:  0.04043214778016995 , test loss:  0.08409656864077607\n",
      "Epoch:  95330 , step:  95330 , train loss:  0.040430542978276675 , test loss:  0.08409658701917148\n",
      "Epoch:  95340 , step:  95340 , train loss:  0.04042893842202281 , test loss:  0.08409660552190483\n",
      "Epoch:  95350 , step:  95350 , train loss:  0.04042733411134533 , test loss:  0.08409662414893497\n",
      "Epoch:  95360 , step:  95360 , train loss:  0.040425730046181316 , test loss:  0.08409664290022018\n",
      "Epoch:  95370 , step:  95370 , train loss:  0.04042412622646779 , test loss:  0.08409666177571945\n",
      "Epoch:  95380 , step:  95380 , train loss:  0.040422522652141885 , test loss:  0.08409668077539155\n",
      "Epoch:  95390 , step:  95390 , train loss:  0.04042091932314067 , test loss:  0.08409669989919533\n",
      "Epoch:  95400 , step:  95400 , train loss:  0.040419316239401246 , test loss:  0.08409671914708955\n",
      "Epoch:  95410 , step:  95410 , train loss:  0.04041771340086081 , test loss:  0.084096738519033\n",
      "Epoch:  95420 , step:  95420 , train loss:  0.040416110807456516 , test loss:  0.08409675801498466\n",
      "Epoch:  95430 , step:  95430 , train loss:  0.04041450845912557 , test loss:  0.08409677763490317\n",
      "Epoch:  95440 , step:  95440 , train loss:  0.040412906355805155 , test loss:  0.08409679737874742\n",
      "Epoch:  95450 , step:  95450 , train loss:  0.04041130449743256 , test loss:  0.08409681724647637\n",
      "Epoch:  95460 , step:  95460 , train loss:  0.040409702883945006 , test loss:  0.08409683723804906\n",
      "Epoch:  95470 , step:  95470 , train loss:  0.040408101515279786 , test loss:  0.08409685735342419\n",
      "Epoch:  95480 , step:  95480 , train loss:  0.040406500391374243 , test loss:  0.08409687759256078\n",
      "Epoch:  95490 , step:  95490 , train loss:  0.040404899512165665 , test loss:  0.08409689795541783\n",
      "Epoch:  95500 , step:  95500 , train loss:  0.040403298877591406 , test loss:  0.08409691844195423\n",
      "Epoch:  95510 , step:  95510 , train loss:  0.04040169848758885 , test loss:  0.08409693905212899\n",
      "Epoch:  95520 , step:  95520 , train loss:  0.04040009834209542 , test loss:  0.08409695978590113\n",
      "Epoch:  95530 , step:  95530 , train loss:  0.040398498441048494 , test loss:  0.08409698064322965\n",
      "Epoch:  95540 , step:  95540 , train loss:  0.04039689878438555 , test loss:  0.08409700162407371\n",
      "Epoch:  95550 , step:  95550 , train loss:  0.04039529937204401 , test loss:  0.08409702272839219\n",
      "Epoch:  95560 , step:  95560 , train loss:  0.040393700203961394 , test loss:  0.08409704395614426\n",
      "Epoch:  95570 , step:  95570 , train loss:  0.04039210128007523 , test loss:  0.08409706530728892\n",
      "Epoch:  95580 , step:  95580 , train loss:  0.040390502600322994 , test loss:  0.08409708678178539\n",
      "Epoch:  95590 , step:  95590 , train loss:  0.04038890416464229 , test loss:  0.08409710837959267\n",
      "Epoch:  95600 , step:  95600 , train loss:  0.04038730597297067 , test loss:  0.08409713010067008\n",
      "Epoch:  95610 , step:  95610 , train loss:  0.04038570802524572 , test loss:  0.08409715194497645\n",
      "Epoch:  95620 , step:  95620 , train loss:  0.04038411032140513 , test loss:  0.08409717391247123\n",
      "Epoch:  95630 , step:  95630 , train loss:  0.040382512861386465 , test loss:  0.08409719600311354\n",
      "Epoch:  95640 , step:  95640 , train loss:  0.04038091564512744 , test loss:  0.08409721821686261\n",
      "Epoch:  95650 , step:  95650 , train loss:  0.040379318672565716 , test loss:  0.08409724055367761\n",
      "Epoch:  95660 , step:  95660 , train loss:  0.04037772194363902 , test loss:  0.0840972630135177\n",
      "Epoch:  95670 , step:  95670 , train loss:  0.04037612545828508 , test loss:  0.08409728559634223\n",
      "Epoch:  95680 , step:  95680 , train loss:  0.04037452921644165 , test loss:  0.08409730830211057\n",
      "Epoch:  95690 , step:  95690 , train loss:  0.04037293321804652 , test loss:  0.0840973311307819\n",
      "Epoch:  95700 , step:  95700 , train loss:  0.04037133746303746 , test loss:  0.08409735408231547\n",
      "Epoch:  95710 , step:  95710 , train loss:  0.04036974195135234 , test loss:  0.08409737715667061\n",
      "Epoch:  95720 , step:  95720 , train loss:  0.04036814668292896 , test loss:  0.08409740035380663\n",
      "Epoch:  95730 , step:  95730 , train loss:  0.040366551657705224 , test loss:  0.08409742367368296\n",
      "Epoch:  95740 , step:  95740 , train loss:  0.040364956875618994 , test loss:  0.08409744711625895\n",
      "Epoch:  95750 , step:  95750 , train loss:  0.0403633623366082 , test loss:  0.08409747068149395\n",
      "Epoch:  95760 , step:  95760 , train loss:  0.04036176804061074 , test loss:  0.08409749436934738\n",
      "Epoch:  95770 , step:  95770 , train loss:  0.04036017398756463 , test loss:  0.08409751817977874\n",
      "Epoch:  95780 , step:  95780 , train loss:  0.040358580177407836 , test loss:  0.08409754211274732\n",
      "Epoch:  95790 , step:  95790 , train loss:  0.04035698661007829 , test loss:  0.08409756616821266\n",
      "Epoch:  95800 , step:  95800 , train loss:  0.0403553932855141 , test loss:  0.08409759034613412\n",
      "Epoch:  95810 , step:  95810 , train loss:  0.04035380020365328 , test loss:  0.08409761464647131\n",
      "Epoch:  95820 , step:  95820 , train loss:  0.04035220736443388 , test loss:  0.08409763906918367\n",
      "Epoch:  95830 , step:  95830 , train loss:  0.04035061476779403 , test loss:  0.08409766361423078\n",
      "Epoch:  95840 , step:  95840 , train loss:  0.04034902241367178 , test loss:  0.08409768828157213\n",
      "Epoch:  95850 , step:  95850 , train loss:  0.04034743030200531 , test loss:  0.08409771307116715\n",
      "Epoch:  95860 , step:  95860 , train loss:  0.04034583843273278 , test loss:  0.0840977379829755\n",
      "Epoch:  95870 , step:  95870 , train loss:  0.04034424680579234 , test loss:  0.08409776301695678\n",
      "Epoch:  95880 , step:  95880 , train loss:  0.04034265542112223 , test loss:  0.08409778817307058\n",
      "Epoch:  95890 , step:  95890 , train loss:  0.040341064278660604 , test loss:  0.08409781345127651\n",
      "Epoch:  95900 , step:  95900 , train loss:  0.04033947337834579 , test loss:  0.084097838851534\n",
      "Epoch:  95910 , step:  95910 , train loss:  0.04033788272011599 , test loss:  0.08409786437380312\n",
      "Epoch:  95920 , step:  95920 , train loss:  0.04033629230390955 , test loss:  0.0840978900180432\n",
      "Epoch:  95930 , step:  95930 , train loss:  0.04033470212966472 , test loss:  0.08409791578421408\n",
      "Epoch:  95940 , step:  95940 , train loss:  0.040333112197319845 , test loss:  0.08409794167227537\n",
      "Epoch:  95950 , step:  95950 , train loss:  0.04033152250681334 , test loss:  0.0840979676821868\n",
      "Epoch:  95960 , step:  95960 , train loss:  0.04032993305808352 , test loss:  0.08409799381390809\n",
      "Epoch:  95970 , step:  95970 , train loss:  0.04032834385106878 , test loss:  0.08409802006739889\n",
      "Epoch:  95980 , step:  95980 , train loss:  0.040326754885707604 , test loss:  0.08409804644261912\n",
      "Epoch:  95990 , step:  95990 , train loss:  0.040325166161938354 , test loss:  0.08409807293952845\n",
      "Epoch:  96000 , step:  96000 , train loss:  0.04032357767969958 , test loss:  0.08409809955808675\n",
      "Epoch:  96010 , step:  96010 , train loss:  0.0403219894389297 , test loss:  0.08409812629825378\n",
      "Epoch:  96020 , step:  96020 , train loss:  0.04032040143956725 , test loss:  0.08409815315998934\n",
      "Epoch:  96030 , step:  96030 , train loss:  0.04031881368155074 , test loss:  0.08409818014325328\n",
      "Epoch:  96040 , step:  96040 , train loss:  0.040317226164818754 , test loss:  0.08409820724800555\n",
      "Epoch:  96050 , step:  96050 , train loss:  0.04031563888930987 , test loss:  0.08409823447420592\n",
      "Epoch:  96060 , step:  96060 , train loss:  0.04031405185496264 , test loss:  0.08409826182181432\n",
      "Epoch:  96070 , step:  96070 , train loss:  0.04031246506171574 , test loss:  0.08409828929079063\n",
      "Epoch:  96080 , step:  96080 , train loss:  0.04031087850950777 , test loss:  0.08409831688109479\n",
      "Epoch:  96090 , step:  96090 , train loss:  0.040309292198277405 , test loss:  0.08409834459268671\n",
      "Epoch:  96100 , step:  96100 , train loss:  0.040307706127963326 , test loss:  0.08409837242552634\n",
      "Epoch:  96110 , step:  96110 , train loss:  0.04030612029850428 , test loss:  0.0840984003795737\n",
      "Epoch:  96120 , step:  96120 , train loss:  0.0403045347098389 , test loss:  0.08409842845478865\n",
      "Epoch:  96130 , step:  96130 , train loss:  0.04030294936190602 , test loss:  0.0840984566511315\n",
      "Epoch:  96140 , step:  96140 , train loss:  0.040301364254644395 , test loss:  0.08409848496856191\n",
      "Epoch:  96150 , step:  96150 , train loss:  0.04029977938799282 , test loss:  0.08409851340704008\n",
      "Epoch:  96160 , step:  96160 , train loss:  0.040298194761890074 , test loss:  0.08409854196652611\n",
      "Epoch:  96170 , step:  96170 , train loss:  0.040296610376275036 , test loss:  0.0840985706469799\n",
      "Epoch:  96180 , step:  96180 , train loss:  0.04029502623108655 , test loss:  0.08409859944836162\n",
      "Epoch:  96190 , step:  96190 , train loss:  0.0402934423262635 , test loss:  0.08409862837063124\n",
      "Epoch:  96200 , step:  96200 , train loss:  0.04029185866174477 , test loss:  0.08409865741374907\n",
      "Epoch:  96210 , step:  96210 , train loss:  0.04029027523746933 , test loss:  0.0840986865776752\n",
      "Epoch:  96220 , step:  96220 , train loss:  0.040288692053376075 , test loss:  0.0840987158623698\n",
      "Epoch:  96230 , step:  96230 , train loss:  0.04028710910940402 , test loss:  0.08409874526779298\n",
      "Epoch:  96240 , step:  96240 , train loss:  0.040285526405492114 , test loss:  0.08409877479390483\n",
      "Epoch:  96250 , step:  96250 , train loss:  0.0402839439415794 , test loss:  0.08409880444066561\n",
      "Epoch:  96260 , step:  96260 , train loss:  0.040282361717604874 , test loss:  0.08409883420803577\n",
      "Epoch:  96270 , step:  96270 , train loss:  0.04028077973350764 , test loss:  0.0840988640959752\n",
      "Epoch:  96280 , step:  96280 , train loss:  0.040279197989226745 , test loss:  0.0840988941044442\n",
      "Epoch:  96290 , step:  96290 , train loss:  0.040277616484701295 , test loss:  0.08409892423340312\n",
      "Epoch:  96300 , step:  96300 , train loss:  0.040276035219870394 , test loss:  0.08409895448281218\n",
      "Epoch:  96310 , step:  96310 , train loss:  0.04027445419467322 , test loss:  0.08409898485263173\n",
      "Epoch:  96320 , step:  96320 , train loss:  0.040272873409048913 , test loss:  0.08409901534282194\n",
      "Epoch:  96330 , step:  96330 , train loss:  0.04027129286293664 , test loss:  0.0840990459533433\n",
      "Epoch:  96340 , step:  96340 , train loss:  0.04026971255627565 , test loss:  0.084099076684156\n",
      "Epoch:  96350 , step:  96350 , train loss:  0.040268132489005154 , test loss:  0.08409910753522054\n",
      "Epoch:  96360 , step:  96360 , train loss:  0.04026655266106439 , test loss:  0.08409913850649722\n",
      "Epoch:  96370 , step:  96370 , train loss:  0.040264973072392636 , test loss:  0.08409916959794646\n",
      "Epoch:  96380 , step:  96380 , train loss:  0.04026339372292918 , test loss:  0.08409920080952864\n",
      "Epoch:  96390 , step:  96390 , train loss:  0.040261814612613384 , test loss:  0.08409923214120395\n",
      "Epoch:  96400 , step:  96400 , train loss:  0.04026023574138452 , test loss:  0.08409926359293317\n",
      "Epoch:  96410 , step:  96410 , train loss:  0.04025865710918197 , test loss:  0.08409929516467655\n",
      "Epoch:  96420 , step:  96420 , train loss:  0.040257078715945124 , test loss:  0.08409932685639471\n",
      "Epoch:  96430 , step:  96430 , train loss:  0.04025550056161336 , test loss:  0.084099358668048\n",
      "Epoch:  96440 , step:  96440 , train loss:  0.040253922646126136 , test loss:  0.08409939059959687\n",
      "Epoch:  96450 , step:  96450 , train loss:  0.04025234496942288 , test loss:  0.08409942265100197\n",
      "Epoch:  96460 , step:  96460 , train loss:  0.040250767531443056 , test loss:  0.0840994548222238\n",
      "Epoch:  96470 , step:  96470 , train loss:  0.040249190332126164 , test loss:  0.08409948711322271\n",
      "Epoch:  96480 , step:  96480 , train loss:  0.040247613371411674 , test loss:  0.08409951952395955\n",
      "Epoch:  96490 , step:  96490 , train loss:  0.04024603664923917 , test loss:  0.08409955205439479\n",
      "Epoch:  96500 , step:  96500 , train loss:  0.040244460165548175 , test loss:  0.08409958470448899\n",
      "Epoch:  96510 , step:  96510 , train loss:  0.04024288392027827 , test loss:  0.0840996174742027\n",
      "Epoch:  96520 , step:  96520 , train loss:  0.04024130791336904 , test loss:  0.0840996503634967\n",
      "Epoch:  96530 , step:  96530 , train loss:  0.04023973214476011 , test loss:  0.08409968337233151\n",
      "Epoch:  96540 , step:  96540 , train loss:  0.04023815661439111 , test loss:  0.08409971650066783\n",
      "Epoch:  96550 , step:  96550 , train loss:  0.04023658132220172 , test loss:  0.08409974974846632\n",
      "Epoch:  96560 , step:  96560 , train loss:  0.040235006268131604 , test loss:  0.08409978311568761\n",
      "Epoch:  96570 , step:  96570 , train loss:  0.040233431452120466 , test loss:  0.08409981660229233\n",
      "Epoch:  96580 , step:  96580 , train loss:  0.04023185687410803 , test loss:  0.08409985020824153\n",
      "Epoch:  96590 , step:  96590 , train loss:  0.040230282534034066 , test loss:  0.08409988393349564\n",
      "Epoch:  96600 , step:  96600 , train loss:  0.0402287084318383 , test loss:  0.08409991777801547\n",
      "Epoch:  96610 , step:  96610 , train loss:  0.04022713456746054 , test loss:  0.0840999517417619\n",
      "Epoch:  96620 , step:  96620 , train loss:  0.04022556094084059 , test loss:  0.08409998582469559\n",
      "Epoch:  96630 , step:  96630 , train loss:  0.040223987551918305 , test loss:  0.08410002002677741\n",
      "Epoch:  96640 , step:  96640 , train loss:  0.04022241440063348 , test loss:  0.08410005434796801\n",
      "Epoch:  96650 , step:  96650 , train loss:  0.040220841486926076 , test loss:  0.0841000887882284\n",
      "Epoch:  96660 , step:  96660 , train loss:  0.040219268810735904 , test loss:  0.08410012334751943\n",
      "Epoch:  96670 , step:  96670 , train loss:  0.04021769637200293 , test loss:  0.08410015802580194\n",
      "Epoch:  96680 , step:  96680 , train loss:  0.04021612417066707 , test loss:  0.08410019282303664\n",
      "Epoch:  96690 , step:  96690 , train loss:  0.04021455220666828 , test loss:  0.08410022773918452\n",
      "Epoch:  96700 , step:  96700 , train loss:  0.04021298047994657 , test loss:  0.08410026277420661\n",
      "Epoch:  96710 , step:  96710 , train loss:  0.040211408990441906 , test loss:  0.0841002979280635\n",
      "Epoch:  96720 , step:  96720 , train loss:  0.04020983773809434 , test loss:  0.0841003332007165\n",
      "Epoch:  96730 , step:  96730 , train loss:  0.0402082667228439 , test loss:  0.0841003685921264\n",
      "Epoch:  96740 , step:  96740 , train loss:  0.040206695944630624 , test loss:  0.08410040410225411\n",
      "Epoch:  96750 , step:  96750 , train loss:  0.040205125403394645 , test loss:  0.08410043973106082\n",
      "Epoch:  96760 , step:  96760 , train loss:  0.040203555099076076 , test loss:  0.0841004754785073\n",
      "Epoch:  96770 , step:  96770 , train loss:  0.04020198503161501 , test loss:  0.08410051134455461\n",
      "Epoch:  96780 , step:  96780 , train loss:  0.040200415200951606 , test loss:  0.08410054732916389\n",
      "Epoch:  96790 , step:  96790 , train loss:  0.04019884560702605 , test loss:  0.08410058343229629\n",
      "Epoch:  96800 , step:  96800 , train loss:  0.040197276249778514 , test loss:  0.08410061965391265\n",
      "Epoch:  96810 , step:  96810 , train loss:  0.040195707129149244 , test loss:  0.08410065599397393\n",
      "Epoch:  96820 , step:  96820 , train loss:  0.04019413824507842 , test loss:  0.08410069245244149\n",
      "Epoch:  96830 , step:  96830 , train loss:  0.04019256959750637 , test loss:  0.08410072902927641\n",
      "Epoch:  96840 , step:  96840 , train loss:  0.04019100118637332 , test loss:  0.0841007657244398\n",
      "Epoch:  96850 , step:  96850 , train loss:  0.04018943301161958 , test loss:  0.08410080253789252\n",
      "Epoch:  96860 , step:  96860 , train loss:  0.040187865073185475 , test loss:  0.08410083946959618\n",
      "Epoch:  96870 , step:  96870 , train loss:  0.04018629737101134 , test loss:  0.08410087651951155\n",
      "Epoch:  96880 , step:  96880 , train loss:  0.04018472990503757 , test loss:  0.0841009136876\n",
      "Epoch:  96890 , step:  96890 , train loss:  0.04018316267520449 , test loss:  0.08410095097382295\n",
      "Epoch:  96900 , step:  96900 , train loss:  0.04018159568145253 , test loss:  0.08410098837814131\n",
      "Epoch:  96910 , step:  96910 , train loss:  0.040180028923722126 , test loss:  0.08410102590051641\n",
      "Epoch:  96920 , step:  96920 , train loss:  0.04017846240195373 , test loss:  0.08410106354090938\n",
      "Epoch:  96930 , step:  96930 , train loss:  0.040176896116087796 , test loss:  0.08410110129928154\n",
      "Epoch:  96940 , step:  96940 , train loss:  0.0401753300660648 , test loss:  0.0841011391755943\n",
      "Epoch:  96950 , step:  96950 , train loss:  0.04017376425182527 , test loss:  0.0841011771698089\n",
      "Epoch:  96960 , step:  96960 , train loss:  0.04017219867330976 , test loss:  0.08410121528188669\n",
      "Epoch:  96970 , step:  96970 , train loss:  0.04017063333045876 , test loss:  0.08410125351178875\n",
      "Epoch:  96980 , step:  96980 , train loss:  0.04016906822321289 , test loss:  0.08410129185947655\n",
      "Epoch:  96990 , step:  96990 , train loss:  0.040167503351512715 , test loss:  0.0841013303249116\n",
      "Epoch:  97000 , step:  97000 , train loss:  0.04016593871529889 , test loss:  0.08410136890805536\n",
      "Epoch:  97010 , step:  97010 , train loss:  0.040164374314512016 , test loss:  0.08410140760886879\n",
      "Epoch:  97020 , step:  97020 , train loss:  0.04016281014909275 , test loss:  0.08410144642731361\n",
      "Epoch:  97030 , step:  97030 , train loss:  0.04016124621898179 , test loss:  0.08410148536335121\n",
      "Epoch:  97040 , step:  97040 , train loss:  0.04015968252411983 , test loss:  0.08410152441694288\n",
      "Epoch:  97050 , step:  97050 , train loss:  0.04015811906444757 , test loss:  0.08410156358804997\n",
      "Epoch:  97060 , step:  97060 , train loss:  0.0401565558399058 , test loss:  0.08410160287663429\n",
      "Epoch:  97070 , step:  97070 , train loss:  0.040154992850435216 , test loss:  0.0841016422826572\n",
      "Epoch:  97080 , step:  97080 , train loss:  0.04015343009597664 , test loss:  0.0841016818060801\n",
      "Epoch:  97090 , step:  97090 , train loss:  0.04015186757647087 , test loss:  0.08410172144686458\n",
      "Epoch:  97100 , step:  97100 , train loss:  0.04015030529185872 , test loss:  0.0841017612049721\n",
      "Epoch:  97110 , step:  97110 , train loss:  0.04014874324208107 , test loss:  0.08410180108036437\n",
      "Epoch:  97120 , step:  97120 , train loss:  0.04014718142707874 , test loss:  0.08410184107300271\n",
      "Epoch:  97130 , step:  97130 , train loss:  0.04014561984679263 , test loss:  0.0841018811828489\n",
      "Epoch:  97140 , step:  97140 , train loss:  0.04014405850116366 , test loss:  0.08410192140986442\n",
      "Epoch:  97150 , step:  97150 , train loss:  0.04014249739013276 , test loss:  0.08410196175401072\n",
      "Epoch:  97160 , step:  97160 , train loss:  0.04014093651364088 , test loss:  0.08410200221524963\n",
      "Epoch:  97170 , step:  97170 , train loss:  0.04013937587162898 , test loss:  0.08410204279354276\n",
      "Epoch:  97180 , step:  97180 , train loss:  0.04013781546403805 , test loss:  0.0841020834888518\n",
      "Epoch:  97190 , step:  97190 , train loss:  0.0401362552908091 , test loss:  0.08410212430113821\n",
      "Epoch:  97200 , step:  97200 , train loss:  0.040134695351883194 , test loss:  0.08410216523036387\n",
      "Epoch:  97210 , step:  97210 , train loss:  0.04013313564720136 , test loss:  0.08410220627649054\n",
      "Epoch:  97220 , step:  97220 , train loss:  0.04013157617670467 , test loss:  0.08410224743947955\n",
      "Epoch:  97230 , step:  97230 , train loss:  0.04013001694033424 , test loss:  0.08410228871929298\n",
      "Epoch:  97240 , step:  97240 , train loss:  0.040128457938031145 , test loss:  0.0841023301158924\n",
      "Epoch:  97250 , step:  97250 , train loss:  0.04012689916973657 , test loss:  0.08410237162923978\n",
      "Epoch:  97260 , step:  97260 , train loss:  0.040125340635391644 , test loss:  0.08410241325929642\n",
      "Epoch:  97270 , step:  97270 , train loss:  0.04012378233493754 , test loss:  0.0841024550060247\n",
      "Epoch:  97280 , step:  97280 , train loss:  0.04012222426831551 , test loss:  0.08410249686938605\n",
      "Epoch:  97290 , step:  97290 , train loss:  0.040120666435466704 , test loss:  0.08410253884934227\n",
      "Epoch:  97300 , step:  97300 , train loss:  0.040119108836332414 , test loss:  0.08410258094585547\n",
      "Epoch:  97310 , step:  97310 , train loss:  0.040117551470853885 , test loss:  0.08410262315888728\n",
      "Epoch:  97320 , step:  97320 , train loss:  0.040115994338972366 , test loss:  0.08410266548839951\n",
      "Epoch:  97330 , step:  97330 , train loss:  0.04011443744062921 , test loss:  0.0841027079343542\n",
      "Epoch:  97340 , step:  97340 , train loss:  0.04011288077576573 , test loss:  0.08410275049671308\n",
      "Epoch:  97350 , step:  97350 , train loss:  0.04011132434432326 , test loss:  0.08410279317543819\n",
      "Epoch:  97360 , step:  97360 , train loss:  0.040109768146243154 , test loss:  0.0841028359704914\n",
      "Epoch:  97370 , step:  97370 , train loss:  0.04010821218146679 , test loss:  0.08410287888183463\n",
      "Epoch:  97380 , step:  97380 , train loss:  0.040106656449935635 , test loss:  0.08410292190942992\n",
      "Epoch:  97390 , step:  97390 , train loss:  0.04010510095159107 , test loss:  0.08410296505323905\n",
      "Epoch:  97400 , step:  97400 , train loss:  0.04010354568637453 , test loss:  0.0841030083132242\n",
      "Epoch:  97410 , step:  97410 , train loss:  0.040101990654227496 , test loss:  0.08410305168934724\n",
      "Epoch:  97420 , step:  97420 , train loss:  0.04010043585509151 , test loss:  0.08410309518157037\n",
      "Epoch:  97430 , step:  97430 , train loss:  0.040098881288907984 , test loss:  0.08410313878985543\n",
      "Epoch:  97440 , step:  97440 , train loss:  0.040097326955618524 , test loss:  0.08410318251416443\n",
      "Epoch:  97450 , step:  97450 , train loss:  0.04009577285516464 , test loss:  0.08410322635445962\n",
      "Epoch:  97460 , step:  97460 , train loss:  0.040094218987487926 , test loss:  0.084103270310703\n",
      "Epoch:  97470 , step:  97470 , train loss:  0.04009266535252996 , test loss:  0.08410331438285663\n",
      "Epoch:  97480 , step:  97480 , train loss:  0.04009111195023235 , test loss:  0.0841033585708825\n",
      "Epoch:  97490 , step:  97490 , train loss:  0.040089558780536744 , test loss:  0.08410340287474286\n",
      "Epoch:  97500 , step:  97500 , train loss:  0.04008800584338482 , test loss:  0.08410344729439984\n",
      "Epoch:  97510 , step:  97510 , train loss:  0.0400864531387182 , test loss:  0.08410349182981543\n",
      "Epoch:  97520 , step:  97520 , train loss:  0.040084900666478594 , test loss:  0.08410353648095215\n",
      "Epoch:  97530 , step:  97530 , train loss:  0.04008334842660774 , test loss:  0.0841035812477718\n",
      "Epoch:  97540 , step:  97540 , train loss:  0.040081796419047355 , test loss:  0.08410362613023688\n",
      "Epoch:  97550 , step:  97550 , train loss:  0.040080244643739195 , test loss:  0.08410367112830935\n",
      "Epoch:  97560 , step:  97560 , train loss:  0.04007869310062502 , test loss:  0.08410371624195159\n",
      "Epoch:  97570 , step:  97570 , train loss:  0.040077141789646685 , test loss:  0.08410376147112571\n",
      "Epoch:  97580 , step:  97580 , train loss:  0.040075590710745936 , test loss:  0.08410380681579405\n",
      "Epoch:  97590 , step:  97590 , train loss:  0.04007403986386465 , test loss:  0.08410385227591893\n",
      "Epoch:  97600 , step:  97600 , train loss:  0.040072489248944666 , test loss:  0.08410389785146263\n",
      "Epoch:  97610 , step:  97610 , train loss:  0.040070938865927934 , test loss:  0.08410394354238718\n",
      "Epoch:  97620 , step:  97620 , train loss:  0.040069388714756245 , test loss:  0.0841039893486553\n",
      "Epoch:  97630 , step:  97630 , train loss:  0.04006783879537158 , test loss:  0.08410403527022896\n",
      "Epoch:  97640 , step:  97640 , train loss:  0.04006628910771585 , test loss:  0.08410408130707074\n",
      "Epoch:  97650 , step:  97650 , train loss:  0.04006473965173104 , test loss:  0.08410412745914292\n",
      "Epoch:  97660 , step:  97660 , train loss:  0.04006319042735913 , test loss:  0.08410417372640784\n",
      "Epoch:  97670 , step:  97670 , train loss:  0.04006164143454213 , test loss:  0.08410422010882801\n",
      "Epoch:  97680 , step:  97680 , train loss:  0.04006009267322203 , test loss:  0.08410426660636554\n",
      "Epoch:  97690 , step:  97690 , train loss:  0.04005854414334091 , test loss:  0.08410431321898323\n",
      "Epoch:  97700 , step:  97700 , train loss:  0.04005699584484079 , test loss:  0.08410435994664331\n",
      "Epoch:  97710 , step:  97710 , train loss:  0.040055447777663775 , test loss:  0.08410440678930821\n",
      "Epoch:  97720 , step:  97720 , train loss:  0.04005389994175197 , test loss:  0.08410445374694045\n",
      "Epoch:  97730 , step:  97730 , train loss:  0.04005235233704751 , test loss:  0.08410450081950255\n",
      "Epoch:  97740 , step:  97740 , train loss:  0.0400508049634925 , test loss:  0.08410454800695684\n",
      "Epoch:  97750 , step:  97750 , train loss:  0.04004925782102912 , test loss:  0.08410459530926605\n",
      "Epoch:  97760 , step:  97760 , train loss:  0.040047710909599604 , test loss:  0.08410464272639254\n",
      "Epoch:  97770 , step:  97770 , train loss:  0.04004616422914609 , test loss:  0.08410469025829898\n",
      "Epoch:  97780 , step:  97780 , train loss:  0.04004461777961084 , test loss:  0.08410473790494766\n",
      "Epoch:  97790 , step:  97790 , train loss:  0.04004307156093606 , test loss:  0.08410478566630135\n",
      "Epoch:  97800 , step:  97800 , train loss:  0.04004152557306407 , test loss:  0.08410483354232261\n",
      "Epoch:  97810 , step:  97810 , train loss:  0.04003997981593712 , test loss:  0.08410488153297406\n",
      "Epoch:  97820 , step:  97820 , train loss:  0.0400384342894975 , test loss:  0.08410492963821835\n",
      "Epoch:  97830 , step:  97830 , train loss:  0.040036888993687604 , test loss:  0.08410497785801786\n",
      "Epoch:  97840 , step:  97840 , train loss:  0.04003534392844969 , test loss:  0.0841050261923355\n",
      "Epoch:  97850 , step:  97850 , train loss:  0.04003379909372618 , test loss:  0.0841050746411337\n",
      "Epoch:  97860 , step:  97860 , train loss:  0.04003225448945947 , test loss:  0.08410512320437535\n",
      "Epoch:  97870 , step:  97870 , train loss:  0.04003071011559191 , test loss:  0.08410517188202302\n",
      "Epoch:  97880 , step:  97880 , train loss:  0.04002916597206599 , test loss:  0.08410522067403942\n",
      "Epoch:  97890 , step:  97890 , train loss:  0.040027622058824125 , test loss:  0.08410526958038737\n",
      "Epoch:  97900 , step:  97900 , train loss:  0.04002607837580879 , test loss:  0.0841053186010294\n",
      "Epoch:  97910 , step:  97910 , train loss:  0.04002453492296247 , test loss:  0.08410536773592849\n",
      "Epoch:  97920 , step:  97920 , train loss:  0.04002299170022769 , test loss:  0.0841054169850471\n",
      "Epoch:  97930 , step:  97930 , train loss:  0.04002144870754696 , test loss:  0.08410546634834826\n",
      "Epoch:  97940 , step:  97940 , train loss:  0.040019905944862794 , test loss:  0.08410551582579469\n",
      "Epoch:  97950 , step:  97950 , train loss:  0.04001836341211783 , test loss:  0.08410556541734907\n",
      "Epoch:  97960 , step:  97960 , train loss:  0.040016821109254606 , test loss:  0.0841056151229744\n",
      "Epoch:  97970 , step:  97970 , train loss:  0.040015279036215746 , test loss:  0.08410566494263348\n",
      "Epoch:  97980 , step:  97980 , train loss:  0.040013737192943905 , test loss:  0.084105714876289\n",
      "Epoch:  97990 , step:  97990 , train loss:  0.040012195579381675 , test loss:  0.08410576492390388\n",
      "Epoch:  98000 , step:  98000 , train loss:  0.04001065419547177 , test loss:  0.08410581508544113\n",
      "Epoch:  98010 , step:  98010 , train loss:  0.040009113041156855 , test loss:  0.08410586536086362\n",
      "Epoch:  98020 , step:  98020 , train loss:  0.04000757211637965 , test loss:  0.0841059157501341\n",
      "Epoch:  98030 , step:  98030 , train loss:  0.0400060314210829 , test loss:  0.08410596625321574\n",
      "Epoch:  98040 , step:  98040 , train loss:  0.040004490955209325 , test loss:  0.0841060168700712\n",
      "Epoch:  98050 , step:  98050 , train loss:  0.04000295071870172 , test loss:  0.08410606760066341\n",
      "Epoch:  98060 , step:  98060 , train loss:  0.04000141071150285 , test loss:  0.0841061184449556\n",
      "Epoch:  98070 , step:  98070 , train loss:  0.03999987093355552 , test loss:  0.0841061694029107\n",
      "Epoch:  98080 , step:  98080 , train loss:  0.03999833138480257 , test loss:  0.08410622047449157\n",
      "Epoch:  98090 , step:  98090 , train loss:  0.03999679206518686 , test loss:  0.08410627165966128\n",
      "Epoch:  98100 , step:  98100 , train loss:  0.03999525297465127 , test loss:  0.08410632295838287\n",
      "Epoch:  98110 , step:  98110 , train loss:  0.039993714113138644 , test loss:  0.08410637437061932\n",
      "Epoch:  98120 , step:  98120 , train loss:  0.039992175480591925 , test loss:  0.08410642589633353\n",
      "Epoch:  98130 , step:  98130 , train loss:  0.039990637076954064 , test loss:  0.08410647753548885\n",
      "Epoch:  98140 , step:  98140 , train loss:  0.03998909890216794 , test loss:  0.08410652928804845\n",
      "Epoch:  98150 , step:  98150 , train loss:  0.03998756095617656 , test loss:  0.08410658115397499\n",
      "Epoch:  98160 , step:  98160 , train loss:  0.03998602323892293 , test loss:  0.08410663313323202\n",
      "Epoch:  98170 , step:  98170 , train loss:  0.039984485750350025 , test loss:  0.08410668522578246\n",
      "Epoch:  98180 , step:  98180 , train loss:  0.039982948490400905 , test loss:  0.08410673743158925\n",
      "Epoch:  98190 , step:  98190 , train loss:  0.039981411459018616 , test loss:  0.08410678975061592\n",
      "Epoch:  98200 , step:  98200 , train loss:  0.03997987465614621 , test loss:  0.08410684218282541\n",
      "Epoch:  98210 , step:  98210 , train loss:  0.039978338081726766 , test loss:  0.08410689472818093\n",
      "Epoch:  98220 , step:  98220 , train loss:  0.03997680173570338 , test loss:  0.08410694738664574\n",
      "Epoch:  98230 , step:  98230 , train loss:  0.03997526561801927 , test loss:  0.08410700015818318\n",
      "Epoch:  98240 , step:  98240 , train loss:  0.039973729728617485 , test loss:  0.08410705304275615\n",
      "Epoch:  98250 , step:  98250 , train loss:  0.039972194067441257 , test loss:  0.08410710604032812\n",
      "Epoch:  98260 , step:  98260 , train loss:  0.03997065863443374 , test loss:  0.08410715915086234\n",
      "Epoch:  98270 , step:  98270 , train loss:  0.0399691234295381 , test loss:  0.08410721237432192\n",
      "Epoch:  98280 , step:  98280 , train loss:  0.03996758845269766 , test loss:  0.08410726571067037\n",
      "Epoch:  98290 , step:  98290 , train loss:  0.039966053703855614 , test loss:  0.08410731915987076\n",
      "Epoch:  98300 , step:  98300 , train loss:  0.03996451918295521 , test loss:  0.08410737272188652\n",
      "Epoch:  98310 , step:  98310 , train loss:  0.03996298488993978 , test loss:  0.08410742639668102\n",
      "Epoch:  98320 , step:  98320 , train loss:  0.03996145082475258 , test loss:  0.08410748018421751\n",
      "Epoch:  98330 , step:  98330 , train loss:  0.03995991698733699 , test loss:  0.08410753408445935\n",
      "Epoch:  98340 , step:  98340 , train loss:  0.039958383377636325 , test loss:  0.08410758809737018\n",
      "Epoch:  98350 , step:  98350 , train loss:  0.039956849995593964 , test loss:  0.08410764222291296\n",
      "Epoch:  98360 , step:  98360 , train loss:  0.03995531684115329 , test loss:  0.08410769646105137\n",
      "Epoch:  98370 , step:  98370 , train loss:  0.039953783914257676 , test loss:  0.08410775081174868\n",
      "Epoch:  98380 , step:  98380 , train loss:  0.039952251214850604 , test loss:  0.08410780527496847\n",
      "Epoch:  98390 , step:  98390 , train loss:  0.03995071874287547 , test loss:  0.08410785985067401\n",
      "Epoch:  98400 , step:  98400 , train loss:  0.039949186498275766 , test loss:  0.08410791453882893\n",
      "Epoch:  98410 , step:  98410 , train loss:  0.03994765448099499 , test loss:  0.08410796933939652\n",
      "Epoch:  98420 , step:  98420 , train loss:  0.03994612269097659 , test loss:  0.0841080242523403\n",
      "Epoch:  98430 , step:  98430 , train loss:  0.03994459112816413 , test loss:  0.08410807927762395\n",
      "Epoch:  98440 , step:  98440 , train loss:  0.039943059792501186 , test loss:  0.08410813441521088\n",
      "Epoch:  98450 , step:  98450 , train loss:  0.03994152868393124 , test loss:  0.08410818966506463\n",
      "Epoch:  98460 , step:  98460 , train loss:  0.03993999780239793 , test loss:  0.08410824502714859\n",
      "Epoch:  98470 , step:  98470 , train loss:  0.03993846714784485 , test loss:  0.08410830050142663\n",
      "Epoch:  98480 , step:  98480 , train loss:  0.039936936720215616 , test loss:  0.08410835608786198\n",
      "Epoch:  98490 , step:  98490 , train loss:  0.03993540651945387 , test loss:  0.08410841178641841\n",
      "Epoch:  98500 , step:  98500 , train loss:  0.039933876545503275 , test loss:  0.08410846759705953\n",
      "Epoch:  98510 , step:  98510 , train loss:  0.039932346798307504 , test loss:  0.08410852351974887\n",
      "Epoch:  98520 , step:  98520 , train loss:  0.03993081727781028 , test loss:  0.08410857955445016\n",
      "Epoch:  98530 , step:  98530 , train loss:  0.039929287983955314 , test loss:  0.08410863570112713\n",
      "Epoch:  98540 , step:  98540 , train loss:  0.03992775891668631 , test loss:  0.08410869195974317\n",
      "Epoch:  98550 , step:  98550 , train loss:  0.03992623007594708 , test loss:  0.08410874833026206\n",
      "Epoch:  98560 , step:  98560 , train loss:  0.039924701461681394 , test loss:  0.08410880481264765\n",
      "Epoch:  98570 , step:  98570 , train loss:  0.039923173073833 , test loss:  0.08410886140686341\n",
      "Epoch:  98580 , step:  98580 , train loss:  0.039921644912345795 , test loss:  0.08410891811287323\n",
      "Epoch:  98590 , step:  98590 , train loss:  0.03992011697716354 , test loss:  0.08410897493064066\n",
      "Epoch:  98600 , step:  98600 , train loss:  0.039918589268230165 , test loss:  0.08410903186012957\n",
      "Epoch:  98610 , step:  98610 , train loss:  0.03991706178548946 , test loss:  0.08410908890130371\n",
      "Epoch:  98620 , step:  98620 , train loss:  0.03991553452888538 , test loss:  0.08410914605412692\n",
      "Epoch:  98630 , step:  98630 , train loss:  0.03991400749836185 , test loss:  0.08410920331856274\n",
      "Epoch:  98640 , step:  98640 , train loss:  0.03991248069386278 , test loss:  0.08410926069457528\n",
      "Epoch:  98650 , step:  98650 , train loss:  0.03991095411533213 , test loss:  0.08410931818212813\n",
      "Epoch:  98660 , step:  98660 , train loss:  0.039909427762713875 , test loss:  0.08410937578118527\n",
      "Epoch:  98670 , step:  98670 , train loss:  0.03990790163595202 , test loss:  0.08410943349171052\n",
      "Epoch:  98680 , step:  98680 , train loss:  0.039906375734990555 , test loss:  0.08410949131366763\n",
      "Epoch:  98690 , step:  98690 , train loss:  0.03990485005977353 , test loss:  0.08410954924702047\n",
      "Epoch:  98700 , step:  98700 , train loss:  0.03990332461024497 , test loss:  0.08410960729173311\n",
      "Epoch:  98710 , step:  98710 , train loss:  0.039901799386348975 , test loss:  0.08410966544776945\n",
      "Epoch:  98720 , step:  98720 , train loss:  0.03990027438802964 , test loss:  0.08410972371509313\n",
      "Epoch:  98730 , step:  98730 , train loss:  0.039898749615231086 , test loss:  0.08410978209366828\n",
      "Epoch:  98740 , step:  98740 , train loss:  0.03989722506789739 , test loss:  0.08410984058345865\n",
      "Epoch:  98750 , step:  98750 , train loss:  0.039895700745972734 , test loss:  0.08410989918442847\n",
      "Epoch:  98760 , step:  98760 , train loss:  0.03989417664940127 , test loss:  0.0841099578965416\n",
      "Epoch:  98770 , step:  98770 , train loss:  0.039892652778127244 , test loss:  0.08411001671976202\n",
      "Epoch:  98780 , step:  98780 , train loss:  0.03989112913209481 , test loss:  0.08411007565405364\n",
      "Epoch:  98790 , step:  98790 , train loss:  0.039889605711248194 , test loss:  0.08411013469938071\n",
      "Epoch:  98800 , step:  98800 , train loss:  0.03988808251553163 , test loss:  0.08411019385570692\n",
      "Epoch:  98810 , step:  98810 , train loss:  0.03988655954488945 , test loss:  0.0841102531229966\n",
      "Epoch:  98820 , step:  98820 , train loss:  0.03988503679926586 , test loss:  0.08411031250121369\n",
      "Epoch:  98830 , step:  98830 , train loss:  0.039883514278605224 , test loss:  0.08411037199032224\n",
      "Epoch:  98840 , step:  98840 , train loss:  0.039881991982851826 , test loss:  0.08411043159028662\n",
      "Epoch:  98850 , step:  98850 , train loss:  0.039880469911950045 , test loss:  0.08411049130107046\n",
      "Epoch:  98860 , step:  98860 , train loss:  0.039878948065844216 , test loss:  0.08411055112263809\n",
      "Epoch:  98870 , step:  98870 , train loss:  0.039877426444478725 , test loss:  0.0841106110549538\n",
      "Epoch:  98880 , step:  98880 , train loss:  0.03987590504779798 , test loss:  0.08411067109798134\n",
      "Epoch:  98890 , step:  98890 , train loss:  0.0398743838757464 , test loss:  0.08411073125168511\n",
      "Epoch:  98900 , step:  98900 , train loss:  0.039872862928268395 , test loss:  0.08411079151602933\n",
      "Epoch:  98910 , step:  98910 , train loss:  0.03987134220530848 , test loss:  0.0841108518909782\n",
      "Epoch:  98920 , step:  98920 , train loss:  0.03986982170681108 , test loss:  0.08411091237649573\n",
      "Epoch:  98930 , step:  98930 , train loss:  0.03986830143272073 , test loss:  0.08411097297254631\n",
      "Epoch:  98940 , step:  98940 , train loss:  0.03986678138298193 , test loss:  0.08411103367909402\n",
      "Epoch:  98950 , step:  98950 , train loss:  0.0398652615575392 , test loss:  0.08411109449610324\n",
      "Epoch:  98960 , step:  98960 , train loss:  0.03986374195633715 , test loss:  0.08411115542353802\n",
      "Epoch:  98970 , step:  98970 , train loss:  0.039862222579320275 , test loss:  0.08411121646136281\n",
      "Epoch:  98980 , step:  98980 , train loss:  0.03986070342643325 , test loss:  0.08411127760954196\n",
      "Epoch:  98990 , step:  98990 , train loss:  0.039859184497620596 , test loss:  0.08411133886803955\n",
      "Epoch:  99000 , step:  99000 , train loss:  0.039857665792827034 , test loss:  0.0841114002368199\n",
      "Epoch:  99010 , step:  99010 , train loss:  0.03985614731199717 , test loss:  0.08411146171584741\n",
      "Epoch:  99020 , step:  99020 , train loss:  0.03985462905507568 , test loss:  0.08411152330508646\n",
      "Epoch:  99030 , step:  99030 , train loss:  0.03985311102200725 , test loss:  0.08411158500450139\n",
      "Epoch:  99040 , step:  99040 , train loss:  0.03985159321273661 , test loss:  0.08411164681405653\n",
      "Epoch:  99050 , step:  99050 , train loss:  0.03985007562720846 , test loss:  0.08411170873371628\n",
      "Epoch:  99060 , step:  99060 , train loss:  0.03984855826536755 , test loss:  0.08411177076344503\n",
      "Epoch:  99070 , step:  99070 , train loss:  0.039847041127158644 , test loss:  0.08411183290320708\n",
      "Epoch:  99080 , step:  99080 , train loss:  0.03984552421252658 , test loss:  0.08411189515296714\n",
      "Epoch:  99090 , step:  99090 , train loss:  0.039844007521416105 , test loss:  0.08411195751268928\n",
      "Epoch:  99100 , step:  99100 , train loss:  0.039842491053772036 , test loss:  0.08411201998233823\n",
      "Epoch:  99110 , step:  99110 , train loss:  0.039840974809539215 , test loss:  0.08411208256187826\n",
      "Epoch:  99120 , step:  99120 , train loss:  0.03983945878866259 , test loss:  0.08411214525127397\n",
      "Epoch:  99130 , step:  99130 , train loss:  0.03983794299108691 , test loss:  0.08411220805048976\n",
      "Epoch:  99140 , step:  99140 , train loss:  0.03983642741675718 , test loss:  0.0841122709594902\n",
      "Epoch:  99150 , step:  99150 , train loss:  0.039834912065618275 , test loss:  0.08411233397823976\n",
      "Epoch:  99160 , step:  99160 , train loss:  0.039833396937615125 , test loss:  0.08411239710670314\n",
      "Epoch:  99170 , step:  99170 , train loss:  0.0398318820326927 , test loss:  0.08411246034484456\n",
      "Epoch:  99180 , step:  99180 , train loss:  0.03983036735079598 , test loss:  0.08411252369262878\n",
      "Epoch:  99190 , step:  99190 , train loss:  0.03982885289186996 , test loss:  0.08411258715002046\n",
      "Epoch:  99200 , step:  99200 , train loss:  0.039827338655859613 , test loss:  0.084112650716984\n",
      "Epoch:  99210 , step:  99210 , train loss:  0.03982582464271007 , test loss:  0.08411271439348418\n",
      "Epoch:  99220 , step:  99220 , train loss:  0.03982431085236627 , test loss:  0.08411277817948534\n",
      "Epoch:  99230 , step:  99230 , train loss:  0.03982279728477335 , test loss:  0.08411284207495251\n",
      "Epoch:  99240 , step:  99240 , train loss:  0.03982128393987639 , test loss:  0.08411290607985003\n",
      "Epoch:  99250 , step:  99250 , train loss:  0.039819770817620496 , test loss:  0.0841129701941426\n",
      "Epoch:  99260 , step:  99260 , train loss:  0.03981825791795079 , test loss:  0.08411303441779493\n",
      "Epoch:  99270 , step:  99270 , train loss:  0.0398167452408124 , test loss:  0.08411309875077182\n",
      "Epoch:  99280 , step:  99280 , train loss:  0.03981523278615054 , test loss:  0.08411316319303773\n",
      "Epoch:  99290 , step:  99290 , train loss:  0.03981372055391038 , test loss:  0.08411322774455748\n",
      "Epoch:  99300 , step:  99300 , train loss:  0.03981220854403708 , test loss:  0.0841132924052958\n",
      "Epoch:  99310 , step:  99310 , train loss:  0.03981069675647589 , test loss:  0.08411335717521753\n",
      "Epoch:  99320 , step:  99320 , train loss:  0.03980918519117209 , test loss:  0.0841134220542873\n",
      "Epoch:  99330 , step:  99330 , train loss:  0.03980767384807089 , test loss:  0.08411348704246983\n",
      "Epoch:  99340 , step:  99340 , train loss:  0.03980616272711759 , test loss:  0.08411355213973018\n",
      "Epoch:  99350 , step:  99350 , train loss:  0.03980465182825748 , test loss:  0.0841136173460328\n",
      "Epoch:  99360 , step:  99360 , train loss:  0.03980314115143592 , test loss:  0.0841136826613428\n",
      "Epoch:  99370 , step:  99370 , train loss:  0.03980163069659817 , test loss:  0.08411374808562457\n",
      "Epoch:  99380 , step:  99380 , train loss:  0.03980012046368963 , test loss:  0.08411381361884329\n",
      "Epoch:  99390 , step:  99390 , train loss:  0.039798610452655654 , test loss:  0.08411387926096371\n",
      "Epoch:  99400 , step:  99400 , train loss:  0.03979710066344167 , test loss:  0.08411394501195074\n",
      "Epoch:  99410 , step:  99410 , train loss:  0.03979559109599307 , test loss:  0.08411401087176937\n",
      "Epoch:  99420 , step:  99420 , train loss:  0.039794081750255283 , test loss:  0.08411407684038417\n",
      "Epoch:  99430 , step:  99430 , train loss:  0.03979257262617374 , test loss:  0.0841141429177603\n",
      "Epoch:  99440 , step:  99440 , train loss:  0.03979106372369394 , test loss:  0.08411420910386255\n",
      "Epoch:  99450 , step:  99450 , train loss:  0.039789555042761326 , test loss:  0.08411427539865597\n",
      "Epoch:  99460 , step:  99460 , train loss:  0.03978804658332147 , test loss:  0.08411434180210547\n",
      "Epoch:  99470 , step:  99470 , train loss:  0.03978653834531986 , test loss:  0.08411440831417603\n",
      "Epoch:  99480 , step:  99480 , train loss:  0.03978503032870203 , test loss:  0.08411447493483244\n",
      "Epoch:  99490 , step:  99490 , train loss:  0.03978352253341355 , test loss:  0.08411454166403999\n",
      "Epoch:  99500 , step:  99500 , train loss:  0.03978201495939999 , test loss:  0.08411460850176329\n",
      "Epoch:  99510 , step:  99510 , train loss:  0.039780507606606966 , test loss:  0.08411467544796764\n",
      "Epoch:  99520 , step:  99520 , train loss:  0.03977900047498012 , test loss:  0.08411474250261818\n",
      "Epoch:  99530 , step:  99530 , train loss:  0.039777493564465034 , test loss:  0.08411480966567969\n",
      "Epoch:  99540 , step:  99540 , train loss:  0.0397759868750074 , test loss:  0.08411487693711718\n",
      "Epoch:  99550 , step:  99550 , train loss:  0.039774480406552896 , test loss:  0.08411494431689581\n",
      "Epoch:  99560 , step:  99560 , train loss:  0.03977297415904719 , test loss:  0.08411501180498081\n",
      "Epoch:  99570 , step:  99570 , train loss:  0.03977146813243601 , test loss:  0.08411507940133707\n",
      "Epoch:  99580 , step:  99580 , train loss:  0.03976996232666509 , test loss:  0.08411514710592993\n",
      "Epoch:  99590 , step:  99590 , train loss:  0.039768456741680176 , test loss:  0.08411521491872427\n",
      "Epoch:  99600 , step:  99600 , train loss:  0.03976695137742701 , test loss:  0.08411528283968536\n",
      "Epoch:  99610 , step:  99610 , train loss:  0.03976544623385144 , test loss:  0.08411535086877832\n",
      "Epoch:  99620 , step:  99620 , train loss:  0.03976394131089918 , test loss:  0.08411541900596829\n",
      "Epoch:  99630 , step:  99630 , train loss:  0.039762436608516145 , test loss:  0.08411548725122045\n",
      "Epoch:  99640 , step:  99640 , train loss:  0.03976093212664815 , test loss:  0.08411555560449993\n",
      "Epoch:  99650 , step:  99650 , train loss:  0.03975942786524104 , test loss:  0.08411562406577218\n",
      "Epoch:  99660 , step:  99660 , train loss:  0.039757923824240704 , test loss:  0.08411569263500211\n",
      "Epoch:  99670 , step:  99670 , train loss:  0.03975642000359301 , test loss:  0.08411576131215522\n",
      "Epoch:  99680 , step:  99680 , train loss:  0.03975491640324395 , test loss:  0.08411583009719656\n",
      "Epoch:  99690 , step:  99690 , train loss:  0.039753413023139374 , test loss:  0.08411589899009128\n",
      "Epoch:  99700 , step:  99700 , train loss:  0.0397519098632253 , test loss:  0.08411596799080487\n",
      "Epoch:  99710 , step:  99710 , train loss:  0.039750406923447675 , test loss:  0.0841160370993027\n",
      "Epoch:  99720 , step:  99720 , train loss:  0.039748904203752475 , test loss:  0.08411610631554985\n",
      "Epoch:  99730 , step:  99730 , train loss:  0.03974740170408574 , test loss:  0.08411617563951179\n",
      "Epoch:  99740 , step:  99740 , train loss:  0.03974589942439351 , test loss:  0.08411624507115363\n",
      "Epoch:  99750 , step:  99750 , train loss:  0.0397443973646218 , test loss:  0.0841163146104408\n",
      "Epoch:  99760 , step:  99760 , train loss:  0.039742895524716666 , test loss:  0.08411638425733878\n",
      "Epoch:  99770 , step:  99770 , train loss:  0.039741393904624236 , test loss:  0.08411645401181288\n",
      "Epoch:  99780 , step:  99780 , train loss:  0.039739892504290567 , test loss:  0.08411652387382838\n",
      "Epoch:  99790 , step:  99790 , train loss:  0.03973839132366183 , test loss:  0.08411659384335096\n",
      "Epoch:  99800 , step:  99800 , train loss:  0.03973689036268412 , test loss:  0.08411666392034578\n",
      "Epoch:  99810 , step:  99810 , train loss:  0.039735389621303636 , test loss:  0.08411673410477807\n",
      "Epoch:  99820 , step:  99820 , train loss:  0.03973388909946652 , test loss:  0.08411680439661375\n",
      "Epoch:  99830 , step:  99830 , train loss:  0.03973238879711898 , test loss:  0.08411687479581784\n",
      "Epoch:  99840 , step:  99840 , train loss:  0.03973088871420722 , test loss:  0.08411694530235589\n",
      "Epoch:  99850 , step:  99850 , train loss:  0.03972938885067751 , test loss:  0.08411701591619344\n",
      "Epoch:  99860 , step:  99860 , train loss:  0.039727889206476075 , test loss:  0.08411708663729608\n",
      "Epoch:  99870 , step:  99870 , train loss:  0.039726389781549167 , test loss:  0.08411715746562928\n",
      "Epoch:  99880 , step:  99880 , train loss:  0.03972489057584312 , test loss:  0.08411722840115828\n",
      "Epoch:  99890 , step:  99890 , train loss:  0.039723391589304186 , test loss:  0.08411729944384902\n",
      "Epoch:  99900 , step:  99900 , train loss:  0.039721892821878726 , test loss:  0.08411737059366675\n",
      "Epoch:  99910 , step:  99910 , train loss:  0.039720394273513086 , test loss:  0.08411744185057707\n",
      "Epoch:  99920 , step:  99920 , train loss:  0.03971889594415361 , test loss:  0.08411751321454557\n",
      "Epoch:  99930 , step:  99930 , train loss:  0.03971739783374668 , test loss:  0.08411758468553791\n",
      "Epoch:  99940 , step:  99940 , train loss:  0.039715899942238696 , test loss:  0.08411765626351948\n",
      "Epoch:  99950 , step:  99950 , train loss:  0.03971440226957607 , test loss:  0.08411772794845611\n",
      "Epoch:  99960 , step:  99960 , train loss:  0.03971290481570526 , test loss:  0.0841177997403134\n",
      "Epoch:  99970 , step:  99970 , train loss:  0.03971140758057271 , test loss:  0.08411787163905683\n",
      "Epoch:  99980 , step:  99980 , train loss:  0.03970991056412488 , test loss:  0.08411794364465208\n",
      "Epoch:  99990 , step:  99990 , train loss:  0.039708413766308265 , test loss:  0.08411801575706489\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjYsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvq6yFwwAAAAlwSFlzAAAPYQAAD2EBqD+naQAANKpJREFUeJzt3Xt41OWd///XHDKTA8lwiCQEQgweKjWKGqqC0oqHuEjZddfvylYrWrWX7IoIrF1F9quVn23cvVaXtQraKnr5q6XYSl3bL181ri2g0CohsSi0nigJmBjDYRIImcnM3N8/ZjJkcoBMSHKTzPNxXZ9rZu65P5/Pe268mlfvz8lhjDECAACwxGm7AAAAkNoIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsctsuoDcikYg+//xzZWdny+Fw2C4HAAD0gjFGzc3NKigokNPZ8/zHkAgjn3/+uQoLC22XAQAA+qC2tlYTJkzo8fshEUays7MlRX9MTk6O5WoAAEBvNDU1qbCwMP53vCdDIoy0H5rJyckhjAAAMMQc7xQLTmAFAABWEUYAAIBVhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFWEEQAAYBVhBAAAWEUYAQAAVhFGAACAVYQRAABgFWEEAABYNSQelDdgqtdIddXS5DnSqZfargYAgJSU2jMjn7wp/eEpqX677UoAAEhZqR1GAACAdYQRAABgFWEEAABYRRgBAABWJR1GNm7cqDlz5qigoEAOh0OvvPJKr9d955135Ha7dd555yW724FljO0KAABIWUmHkcOHD2vKlCl64oknklrP7/dr3rx5uuKKK5Ld5cBxOGxXAABAykv6PiOzZs3SrFmzkt7RHXfcoRtuuEEulyup2RQAADC8Dco5I88995w+/fRTPfjgg4OxOwAAMIQM+B1YP/74Y913333atGmT3O7e7S4QCCgQCMQ/NzU1DVR5AADAsgGdGQmHw7rhhhv00EMP6cwzz+z1euXl5fL5fPGlsLBwAKsEAAA2DWgYaW5u1tatW7VgwQK53W653W4tX75c77//vtxut956661u11u6dKn8fn98qa2tHcgyJXE1DQAAtgzoYZqcnBxt35743JeVK1fqrbfe0i9/+UsVFxd3u57X65XX6x3I0gAAwEki6TBy6NAhffLJJ/HPu3btUnV1tUaPHq2JEydq6dKl2rt3r1544QU5nU6VlJQkrD927Filp6d3abeDS3sBALAt6TCydetWzZw5M/55yZIlkqSbb75Zzz//vOrq6lRTU9N/FQIAgGHNYczJf/vRpqYm+Xw++f1+5eTk9N+GX/6utP0l6eofStPu7L/tAgCAXv/95tk0AADAKsIIAACwijAi8aA8AAAsSu0wwoPyAACwLrXDCAAAsI4wAgAArCKMAAAAqwgjAADAKsKIJB6UBwCAPYQRAABgVYqHES7tBQDAthQPIwAAwDbCCAAAsIowAgAArCKMSDybBgAAiwgjAADAqtQOIzwoDwAA61I7jAAAAOsIIwAAwCrCCAAAsIowIoln0wAAYA9hBAAAWJXiYYSraQAAsC3FwwgAALCNMAIAAKwijAAAAKsIIwAAwCrCiMSD8gAAsIgwAgAArErtMMKD8gAAsC61wwgAALCOMAIAAKwijAAAAKsII5J4UB4AAPYQRgAAgFUpHka4mgYAANtSPIwAAADbkg4jGzdu1Jw5c1RQUCCHw6FXXnnlmP3XrVunq666SqeccopycnI0bdo0vf76632tFwAADDNJh5HDhw9rypQpeuKJJ3rVf+PGjbrqqqu0fv16VVZWaubMmZozZ46qqqqSLhYAAAw/7mRXmDVrlmbNmtXr/itWrEj4/MMf/lD//d//rV//+tc6//zzk939wODZNAAAWDPo54xEIhE1Nzdr9OjRg71rAABwEkp6ZuREPfroozp8+LCuv/76HvsEAgEFAoH456ampsEoDQAAWDCoMyNr1qzR97//fa1du1Zjx47tsV95ebl8Pl98KSwsHJiCuLIXAADrBi2MrF27VrfddpteeuklXXnllcfsu3TpUvn9/vhSW1s7SFUCAIDBNiiHadasWaNbb71Va9as0ezZs4/b3+v1yuv1DkJlAADAtqTDyKFDh/TJJ5/EP+/atUvV1dUaPXq0Jk6cqKVLl2rv3r164YUXJEWDyLx58/Rf//Vfuvjii1VfXy9JysjIkM/n66efcaK4mgYAAFuSPkyzdetWnX/++fHLcpcsWaLzzz9fDzzwgCSprq5ONTU18f5PP/20QqGQ7rzzTo0bNy6+3H333f30EwAAwFCW9MzIZZddJnOM+3I8//zzCZ9/97vfJbsLAACQQlL82TRcTgMAgG0pHkYAAIBthBEAAGAVYQQAAFhFGJF4UB4AABYRRgAAgFWEEQAAYFVqhxEHl/YCAGBbaocRAABgHWEEAABYRRiRxIPyAACwhzACAACsIowAAACrUjyMcDUNAAC2pXgYAQAAthFGAACAVYQRiYtpAACwiDACAACsIowAAACrUjuM8GwaAACsS+0wAgAArCOMAAAAqwgjkricBgAAewgjAADAKsIIAACwijACAACsSvEwwqW9AADYluJhBAAA2EYYAQAAVhFGJMlwaS8AALYQRgAAgFWEEQAAYFVqhxEelAcAgHWpHUYAAIB1hBEAAGAVYUQSD8oDAMAewggAALCKMAIAAKxKOoxs3LhRc+bMUUFBgRwOh1555ZXjrrNhwwaVlpYqPT1dkyZN0lNPPdWXWgEAwDCUdBg5fPiwpkyZoieeeKJX/Xft2qVrrrlGM2bMUFVVle6//34tXLhQL7/8ctLF9j8u7QUAwDZ3sivMmjVLs2bN6nX/p556ShMnTtSKFSskSZMnT9bWrVv1H//xH7ruuuuS3T0AABhmBvyckS1btqisrCyh7eqrr9bWrVvV1tbW7TqBQEBNTU0Jy4Di2TQAAFgz4GGkvr5eeXl5CW15eXkKhUJqbGzsdp3y8nL5fL74UlhYONBlAgAASwblahpHp9uum9hMROf2dkuXLpXf748vtbW1A14jAACwI+lzRpKVn5+v+vr6hLaGhga53W6NGTOm23W8Xq+8Xu9AlwYAAE4CAz4zMm3aNFVUVCS0vfHGG5o6darS0tIGevfHxoPyAACwLukwcujQIVVXV6u6ulpS9NLd6upq1dTUSIoeYpk3b168//z587V7924tWbJEO3fu1OrVq/Xss8/qnnvu6Z9fAAAAhrSkD9Ns3bpVM2fOjH9esmSJJOnmm2/W888/r7q6ungwkaTi4mKtX79eixcv1pNPPqmCggI9/vjjJ9llvVxNAwCALUmHkcsuuyx+Amp3nn/++S5t3/jGN7Rt27ZkdwUAAFIAz6YBAABWEUYAAIBVhBEAAGBViocRLu0FAMC2FA8jMTybBgAAawgjAADAKsIIAACwijACAACsIowAAACrUjuM8KA8AACsS+0wAgAArCOMSOJBeQAA2EMYAQAAVhFGAACAVYQRAABgVYqHEa6mAQDAthQPIwAAwDbCiMSD8gAAsIgwAgAArCKMAAAAqwgjAADAKsIIAACwKrXDCA/KAwDAutQOI3FcTQMAgC2EEQAAYBVhBAAAWEUYAQAAVhFGAACAVSkeRriaBgAA21I8jMTwbBoAAKwhjAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAq1I7jPCgPAAArEvtMBLHpb0AANjSpzCycuVKFRcXKz09XaWlpdq0adMx+7/44ouaMmWKMjMzNW7cOH3nO9/Rvn37+lQwAAAYXpIOI2vXrtWiRYu0bNkyVVVVacaMGZo1a5Zqamq67f/2229r3rx5uu222/Thhx/qF7/4hd577z3dfvvtJ1w8AAAY+pIOI4899phuu+023X777Zo8ebJWrFihwsJCrVq1qtv+v//973Xqqadq4cKFKi4u1qWXXqo77rhDW7duPeHiAQDA0JdUGAkGg6qsrFRZWVlCe1lZmTZv3tztOtOnT9eePXu0fv16GWP0xRdf6Je//KVmz57d434CgYCampoSFgAAMDwlFUYaGxsVDoeVl5eX0J6Xl6f6+vpu15k+fbpefPFFzZ07Vx6PR/n5+Ro5cqR+9KMf9bif8vJy+Xy++FJYWJhMmUngahoAAGzr0wmsjk6XxBpjurS127FjhxYuXKgHHnhAlZWVeu2117Rr1y7Nnz+/x+0vXbpUfr8/vtTW1valzN7jQXkAAFjjTqZzbm6uXC5Xl1mQhoaGLrMl7crLy3XJJZfoe9/7niTp3HPPVVZWlmbMmKGHH35Y48aN67KO1+uV1+tNpjQAADBEJTUz4vF4VFpaqoqKioT2iooKTZ8+vdt1Wlpa5HQm7sblckmKzqgAAIDUlvRhmiVLluiZZ57R6tWrtXPnTi1evFg1NTXxwy5Lly7VvHnz4v3nzJmjdevWadWqVfrss8/0zjvvaOHChbrwwgtVUFDQf78EAAAMSUkdppGkuXPnat++fVq+fLnq6upUUlKi9evXq6ioSJJUV1eXcM+RW265Rc3NzXriiSf0z//8zxo5cqQuv/xy/du//Vv//QoAADBkOcwQOFbS1NQkn88nv9+vnJyc/tvwa/dLv39SumSRdNVD/bddAADQ67/fqf1smvgVQCd9HgMAYNhK7TACAACsI4wAAACrCCMAAMAqwggAALCKMAIAAKwijEg8mwYAAIsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAqtQOI/Fn0wAAAFtSO4zEcWkvAAC2EEYAAIBVhBEAAGAVYQQAAFhFGAEAAFYRRgAAgFUpHkZil/byoDwAAKxJ8TACAABsI4wAAACrCCMAAMAqwggAALCKMAIAAKxK7TDCg/IAALAutcMIAACwjjACAACsIowAAACrCCMAAMAqwggAALCKMCLxbBoAACxK8TDCpb0AANiW4mEEAADYRhgBAABWEUYAAIBVhBEAAGBVn8LIypUrVVxcrPT0dJWWlmrTpk3H7B8IBLRs2TIVFRXJ6/XqtNNO0+rVq/tUMAAAGF7cya6wdu1aLVq0SCtXrtQll1yip59+WrNmzdKOHTs0ceLEbte5/vrr9cUXX+jZZ5/V6aefroaGBoVCoRMu/oTFH5THpb0AANiSdBh57LHHdNttt+n222+XJK1YsUKvv/66Vq1apfLy8i79X3vtNW3YsEGfffaZRo8eLUk69dRTT6xqAAAwbCR1mCYYDKqyslJlZWUJ7WVlZdq8eXO367z66quaOnWq/v3f/13jx4/XmWeeqXvuuUdHjhzpcT+BQEBNTU0JCwAAGJ6SmhlpbGxUOBxWXl5eQnteXp7q6+u7Xeezzz7T22+/rfT0dP3qV79SY2Oj/umf/kn79+/v8byR8vJyPfTQQ8mUBgAAhqg+ncDqcCTeudQY06WtXSQSkcPh0IsvvqgLL7xQ11xzjR577DE9//zzPc6OLF26VH6/P77U1tb2pUwAADAEJDUzkpubK5fL1WUWpKGhoctsSbtx48Zp/Pjx8vl88bbJkyfLGKM9e/bojDPO6LKO1+uV1+tNpjQAADBEJTUz4vF4VFpaqoqKioT2iooKTZ8+vdt1LrnkEn3++ec6dOhQvO2jjz6S0+nUhAkT+lByf4rN5vCgPAAArEn6MM2SJUv0zDPPaPXq1dq5c6cWL16smpoazZ8/X1L0EMu8efPi/W+44QaNGTNG3/nOd7Rjxw5t3LhR3/ve93TrrbcqIyOj/34JAAAYkpK+tHfu3Lnat2+fli9frrq6OpWUlGj9+vUqKiqSJNXV1ammpibef8SIEaqoqNBdd92lqVOnasyYMbr++uv18MMP99+vAAAAQ5bDmJP/GEVTU5N8Pp/8fr9ycnL6b8NvPiS9/Zh00T9Ksx7pv+0CAIBe//1O6WfTHApE7wLb0nYS3A0WAIAUldJh5N1d+yVJH39x6Dg9AQDAQEnpMNJ+b5QhcKQKAIBhK6XDiNMZDSMRwggAANakdhiJ32aEMAIAgC0pHkbaZ0YsFwIAQApL6TDiIIwAAGBdSocRDtMAAGBfiocRTmAFAMC2lA4jckbvhu+IhC0XAgBA6krtMOJKkyQ5Im2WCwEAIHWldhiJzYw4DbeDBwDAltQOI8yMAABgHWFEzIwAAGATYUSSM0IYAQDAlpQOIw5n+8wIh2kAALAlpcMIMyMAANiX0mHE4eacEQAAbEvtMOLySJJchBEAAKxJ8TDCzAgAALYRRiS5CSMAAFiT0mHE6Y4epnGKMAIAgC2EEUkuw4PyAACwJaXDSPthGk5gBQDAnpQOI67Ypb1uDtMAAGBNSocRR+wwDSewAgBgT0qHEVf7YRpxzggAALakdBiJz4xwmAYAAGtSOoy409rDCDMjAADYQhgR54wAAGBTSoeRNI9XkuRyGCnC7AgAADakdBjxeDPi78NtrRYrAQAgdaV2GEnPir8PHmmxWAkAAKkrpcOI15OmoHFJkoKthyxXAwBAakrpMOJ2OdWq6HkjbQFmRgAAsCGlw4gkBRS9oibUShgBAMAGwogjFkYChy1XAgBAaupTGFm5cqWKi4uVnp6u0tJSbdq0qVfrvfPOO3K73TrvvPP6stsBEXDEDtMwMwIAgBVJh5G1a9dq0aJFWrZsmaqqqjRjxgzNmjVLNTU1x1zP7/dr3rx5uuKKK/pc7EAIxsJIKHjEciUAAKSmpMPIY489pttuu0233367Jk+erBUrVqiwsFCrVq065np33HGHbrjhBk2bNq3PxQ6EtthhmkiQmREAAGxIKowEg0FVVlaqrKwsob2srEybN2/ucb3nnntOn376qR588MFe7ScQCKipqSlhGShtznRJhBEAAGxJKow0NjYqHA4rLy8voT0vL0/19fXdrvPxxx/rvvvu04svvii3292r/ZSXl8vn88WXwsLCZMpMSih2mCbCYRoAAKzo0wmsDocj4bMxpkubJIXDYd1www166KGHdOaZZ/Z6+0uXLpXf748vtbW1fSmzV0KuaBgxbcyMAABgQ++mKmJyc3Plcrm6zII0NDR0mS2RpObmZm3dulVVVVVasGCBJCkSicgYI7fbrTfeeEOXX355l/W8Xq+8Xm8ypfVZyBU9TGOYGQEAwIqkZkY8Ho9KS0tVUVGR0F5RUaHp06d36Z+Tk6Pt27eruro6vsyfP19f+cpXVF1drYsuuujEqu8HEVf0YXmGc0YAALAiqZkRSVqyZIluuukmTZ06VdOmTdOPf/xj1dTUaP78+ZKih1j27t2rF154QU6nUyUlJQnrjx07Vunp6V3abYmkjYi+CTTbLQQAgBSVdBiZO3eu9u3bp+XLl6uurk4lJSVav369ioqKJEl1dXXHvefIySTszZEkOYMDd8UOAADomcMYY2wXcTxNTU3y+Xzy+/3Kycnp122/8cIjKvusXH/yXaqzFv+fft02AACprLd/v1P+2TRKjw6Ou+2Q5UIAAEhNKR9GnOk+SZInRBgBAMCGlA8jroxoGEkPE0YAALAh5cOIO2ukJCkjQhgBAMCGlA8j3qxRkqRM0yKd/OfyAgAw7BBGRoyUJLkUkYLMjgAAMNhSPoxkjchWq0mLfjjcaLcYAABSUMqHkVFZXjUqehJrqLnBcjUAAKSelA8jIzM9ajTRMHJ4f53lagAASD0pH0ZcTof8zpGSpNaDhBEAAAZbyocRSTqUNlqSFDjIYRoAAAYbYURSqycaRiKHvrBcCQAAqYcwIimYnitJchz+0nIlAACkHsKIpFDWOEmS5zDnjAAAMNgII5IivkJJ0ogjn1uuBACA1EMYkZSeWyxJyg7tk9qOWK4GAIDUQhiRlHtKnppNRvSDf4/dYgAASDGEEUnjRmZqj4mexKqDu+0WAwBAiiGMSBo/MkN7zCmSpLbGXZarAQAgtRBGJOVkuFXjHC9Jatn7oeVqAABILYQRSQ6HQ40ZkyRJpmGH5WoAAEgthJGY4OivSJLSD3xkuRIAAFILYSQmveCrihiH0oMHpEPciRUAgMFCGIkpys9VbewkVjVw3ggAAIOFMBJz2ikjtN1EzxvRnq12iwEAIIUQRmJOP2WEtkXOkCSFdv/BcjUAAKQOwkiMLzNNNZlflSSZPVslYyxXBABAaiCMdOAtPE8B41ZaYL+071Pb5QAAkBIIIx18tXCsqkz0UI0++63dYgAASBGEkQ6mTBipDeEp0Q+f/I/dYgAASBGEkQ7OLfRpozlXkhTZtUEKBSxXBADA8EcY6SAnPU3uceeqwYyUs61F2rXJdkkAAAx7hJFOpp9xil4PT41++OCXdosBACAFEEY6ufT0XL0SvkSSZHb+Wmo7YrkiAACGN8JIJ6VFo7TTfZb2mFw5goeknb+xXRIAAMMaYaST9DSXZp6Vr1+Gvx5tePdpuwUBADDMEUa6MeucfL0YulJtckt73pP2VNouCQCAYatPYWTlypUqLi5Wenq6SktLtWlTz1edrFu3TldddZVOOeUU5eTkaNq0aXr99df7XPBguPyssWpOG61XwxdHG95ZYbUeAACGs6TDyNq1a7Vo0SItW7ZMVVVVmjFjhmbNmqWamppu+2/cuFFXXXWV1q9fr8rKSs2cOVNz5sxRVVXVCRc/UDI9bl1zzjg9HZqjiBzSzlelvdtslwUAwLDkMCa5J8JddNFFuuCCC7Rq1ap42+TJk3XttdeqvLy8V9s4++yzNXfuXD3wwAO96t/U1CSfzye/36+cnJxkyu2zyt0HdN2qzfovz0r9jfNtadJl0k2vSA7HoOwfAIChrrd/v5OaGQkGg6qsrFRZWVlCe1lZmTZv3tyrbUQiETU3N2v06NE99gkEAmpqakpYBtsFE0fqrPxs/UfbdQo73NJnv5M+XDfodQAAMNwlFUYaGxsVDoeVl5eX0J6Xl6f6+vpebePRRx/V4cOHdf311/fYp7y8XD6fL74UFhYmU2a/cDgc+u6MSao1eXpGfxtt/L/3Si37B70WAACGsz6dwOrodKjCGNOlrTtr1qzR97//fa1du1Zjx47tsd/SpUvl9/vjS21tbV/KPGF/c16BisZk6tEj39SBzGLp8JfSq3dJyR3ZAgAAx5BUGMnNzZXL5eoyC9LQ0NBltqSztWvX6rbbbtNLL72kK6+88ph9vV6vcnJyEhYb3C6n7px5uoJK0z+1fFfGmSb96TfS71daqQcAgOEoqTDi8XhUWlqqioqKhPaKigpNnz69x/XWrFmjW265RT/72c80e/bsvlVqyXUXTNBXx+VoS+upejV/QbTxjf8tffSG3cIAABgmkj5Ms2TJEj3zzDNavXq1du7cqcWLF6umpkbz58+XFD3EMm/evHj/NWvWaN68eXr00Ud18cUXq76+XvX19fL7/f33KwaQy+nQQ39ztiRp0a6p+vK06yQTll6aJ9W+a7k6AACGvqTDyNy5c7VixQotX75c5513njZu3Kj169erqKhIklRXV5dwz5Gnn35aoVBId955p8aNGxdf7r777v77FQPsa6eO1typhTLGoev2/IPaJl0phY5I///fSX9523Z5AAAMaUnfZ8QGG/cZ6exQIKTZj2/S7n0tuvZsn/4z9EM5dr8judOl656VJn/TSl0AAJysBuQ+I6lshNet/5x7ntJcDr3yoV9PFDwifeUaKdQqrb1ReusHUiRsu0wAAIYcwkgSLpg4Sg9fWyJJevS3tXr59HLpa9+Nfrnx36WfXif591qsEACAoYcwkqS5X5uo784oliTds+5DvVKwWPrbp6OHaz77rbTyYmnbC9yLBACAXiKM9MHSWZN1w0UTZYy05KVqvdg6TbpjozR+qhRoit4Y7dmruNoGAIBeIIz0gdPp0MN/U6JvXVioiJGW/eoDlb8XUeQ7r0tX/X9SWpa0571oIFl7k1T3R9slAwBw0uJqmhNgjNGP3vpEj1V8JEmacUauHr1+isbqoPTWw1LVTyXFhvf0q6RL7pZOvZQn/wIAUkJv/34TRvrBf1fv1X0vb9eRtrByR3j0b9edqysm50lffChteiz6tF8TiXbOPVMqvUWa8i0ps+cnFwMAMNQRRgbZJw3NWvCzKv2pvlmSNPuccXpgzleVl5Mu7ftU2vwj6Y8vSW2Hoyu4PNLpV0pn/630lVmSN9ti9QAA9D/CiAWtbWH9Z8VHeubtXQpHjEZ43brj65N066XFyvK6pdYmafsvpMrnpPrtR1d0eaXTr5DOuEo67QppVJG9HwEAQD8hjFi04/Mm3f+r7aquPShJyh3h1YKZp2nu1yYqw+OKXvbbsFP68FfRQzj7PkncwJgzouGk6BJp4sXSiLGD/yMAADhBhBHLIhGj32yv06Nv/Fm797VIkkZlpummi4t007RTdUq2N9rRGOmLD6SPXpc+eTN6ObDpdCfX0adJE6dJEy+Sxp0nnXKW5PYM7g8CACBJhJGTRDAU0Utba/X0xk9Vu/+IJMnjcqrs7DzN/VqhLjktV05nh6trjhyUdm2QPvudVPMHqWGH4lfktHOmSWMnS+POlfKnSPkl0RNjs3IH62cBAHBchJGTTDhi9PqH9frxxs/ih28kafzIDP3t+eM165x8fXVcjhydL/s9ckCqfU+q2RK9d0n9H6VWf/c7yRgdDSW5Z8Rez5RGnSqNnCh5MgfstwEA0B3CyEnsg71+vbS1Vq9U7VVTayjeXjQmU391dr6uLsnXlAkj5XJ2cz8SY6SDNdFQUvdHqe596cud0sFadZlB6SjrFGlkUTSYjJwYPUnWN1HKGSdlj5MyRnH/EwBAvyKMDAGtbWG9/mG9/s8f67Thoy8VCEXi3/ky0nTp6bmacUauvn7mKSoYmXHsjQVbpP2fSo0fSV9+FH1t/Fg6uDt6i/rjcXmkEflSdvsyLvo6Ii96+CczN3pflKxcyTOC4AIAOC7CyBBzOBDS7/78pf7vB9Fg0txhxkSSTh2TqdKi0Zp66ihNLRql004ZkXiuybEcORCdTTmwO/p6sP21VjpUL7XsS65YlycWTsYcDSiZY6KHidJ9UnpO7LXD4o21OV3J7QsAMGQRRoawUDii9/f4tenjL7Xxoy9VXXtQkU7/SiMz03TBxFEqKcjR2eN9OrsgR+NHZnQ956RXOwxIh76Qmuul5rpOr7Gw0rJfammUQq0n9uM82YmBxZstebKiS1rW0feeEb14n0W4AYCTGGFkGPEfadO2mgOq/MsBbd29X9W1B9XaFunSb2Rmms4uyNHZBT6dMXaEzsjL1mmnZCk7Pa3/igkejoWTfdLhfUfftzRGrwRq9Scugaboa1tL/9XQkTNNSsuQ3OlSWrrkzpDc3g5tx3ntuJ4rLbquKy06++Pq8D7e7o19F+vrTJOcPG8SALpDGBnG2sIR7fi8SdW1B/Xh5359sLdJHzc0qy3c/T9lfk66zsgbodNOGaHTx47QpNwsFY7O1DhfutyuQfpDGgoeDSatB6N3o209GA03wcNS8FAP77tbmo8+6+dk4HQfI7ikHQ0vTvfRxZUWndVxumOBxh39HO/X4XtXh++dHbbj6rC9nvo7XN28OiWHs5vvYu1J9efcIQA9I4ykmEAorI+/OKQP9vq1o65JnzQc0scNh/Rlc6DHddxOhyaMylDh6ExNHJ2pojHR1wmjMlUwMkOjMtP6dthnoBkTPbQUPCyFjkTftx2JHkI65msg2r+ttZvXVincJoWDUjhw9H0o2LUtEjp+jSnD0few0x5mHM7odtrfx1/b252d2h09tHfs7+i07V72PWZ/9bwdqUNdjqNjE29T17Yu6/RmO53b1MvtDOS2j7Gd7vT4vyn91b+H7lZr6q/+fV2nlwbgflW9/fvt7te9whqv26WS8T6VjPcltPtb2vTJl4f0SUNzPKDU7G/Rnv1HFAxH9Jd9LfrLvu4PoXjcTo3zpceWDOV3eD/Ol66xOV6NyfJ2fwnyQHI4oodW0tIHd7/tIuEOwaXDEgr20BaIBphISAqHjr6PtB3dViTUdQnHvo+0dbN++3eh7tdvbzORaD8Tjr1GumkLS5FYe8e2Xs0+mVg4C0nh43YGcDK77lnpnP9lZdeEkWHOl5mm0qJRKi0aldAejhh90dSqmv0t0WVfS/z9ngNH1HgooGAoot37WuK3s++O0yGNzvIod4RXuSO8OiXbq9wRnT97lZvt0ahMj9IG67DQQHLG/p++rTA0WIxJLrwkHXZi21f7fnR0fXXYd8c6ZDp87tzeXX91be+2rznOPju2d9hOpD2BmaPbaB+7HtvUdZ34BHUy2+m8jrrfzglt+1j7O952utND+4D376G71ZqS7N/XfSQzY2Lx6fGEkRTlcjpUMDJDBSMzdPGkMV2+D4Yi+qKpVXX+VtX5j6jO36p6f6s+P3hE9U2t+vxgq/YdDihipMZDQTUeCkpqPu5+s71ujcxK06hMj0ZmejQqs/194mv8fZZHWR7XyXm4aLhzOKKHUxQ7lwUABghhBN3yuJ0qHJ2pwtE930Y+FI5of0tQXzYHooGkOaDGQ4HY51hb7PP+lqCMkZoDITUHQvHn9PSG0yFlp6cpJ8OtbG+astPdysmIvaanKafD5+z0NOWkJ/YZ4XXL63YSaADgJEUYQZ+5XU6NzU7X2OzjH64IR4z8R9p0oCWogy1BHTjc/r5N+7tpa38NhiOKmOjlzf4jbZJ6H2I6cjkdyvK4lOV1K9Pj0givW5ket7K8bmV5o+1ZHpcyPdHwkunt0Ce2XpY3+n2mx6X0NBcBBwD6CWEEg8LldGh0lkejszy9XscYoyNtYTW3htR0pE1NrSE1tbapuTWk5tY2NR2Jvcbbov2aO/Q7FIhe+RKOmNj6/XcljMMhZaS5lJEWDScZHtfRzx6X0t3OeFt337e/z/A4o9+nHQ053vZXt1Net0tpLgfBB8CwRRjBScvhcMRmItzKy+nbyaLhiNHhYEgtgbAOB0M6HAjpcCAcfQ2G1BIMH22Lfd8SDOtQIKSWYEiHAmG1dGprv5+LMVJLMKyW4MBfRuJwSF63Ux5X16DiTevwPhZkov06t3d473bK0+E7j9upNJdTaS6H0lzRzx6XU2nuaJvH5Yy3u50EIwD9izCCYc3ldMTOK+m/EzDbwhG1toV1pC2s1mBER2LvjwTDam0Lx7/r2BZ9H+3bGmtv79Pxc2tbWIFQRIFQRMEOD040Rmpti0TvvNuPszt95WkPLu2hxdUeaBwd3juP9usYcFxOpbm7aYv1dTsdcsffO+WOrR9t79qW5nLK5XT00P/oemlOZ++f5wRgUBFGgCS1/+Hs19vsdyMSMQqGIwqGIwq0RRQIxYJKx/ehiAJt4Q59euoXVjDe/2h7a1tYbWGjtth+gqGI2sKRaFsoEt9/5ysKo+2SBmFWqD85HdFzndI6BRW3MxZ+ehVwjra5nQ65nM7Ya3Rxd3h1OhP7OHv8PrrNY28j2sfpVLxv9/vt2ocQhpMdYQQ4STmdDqU7o+eRyPItTUKxgBIMt4eVo8ElGDIJbcH2MBNrC7QHnNDRbRwNPV3bQhGjUDiiUNioLeF9ROGIUVs41haJ7iMUNgpF2tcz8W2EOz9dUlLERC9bD0pKtbu0HSskuTp953LEXp2SyxE9LOeKt0dnHJ2xNqej/X1i+9H1uvZv35/DEd1+5/boNjutF9umyxk9hOvqpv1oLR1/x9F9dPc74r81tm7n9vZaEt47Or0n7J0wwgiA43K7nHK7pAwNnackRyImHkraIrHQEo7EA05bOPZdhwDU1h5swuZo23ECUcf9hCJGERMNReFYQDr6+Wi/o++775P4OXpFWSgSUTjcYRvGdP3cTQBrF4ptu+cHROBEJISUDsHI0SFUOR1Hg1T8vfNoqOnY19EhkDk6BB9XrF/n/XTX99gBKrHN4XDof5VO6HIX78FCGAEwLDmdDnli/491KIWoE2FMp0ATORpYEj9H4gGmuxAUMUcDTnSb0ZPB29vj30eMjFE8CHVsjxijiImtF9tWJB6ajtbauT3Svp14e7QtsZ6u7fF9dGiPdNpHvJ5e/o5jZLsuIkaKtN+FdohOul1QNIowAgA4MQ5H7JyW1MhegyLSIVhFX4++NxHFAlB0MR3Cjon1aQ81Jh6wdLRvfL3EoNRxXRPfr2KfO9cS3WbYdPou0rFfN7+jw/fttZ0xdoS1cSaMAADQA6fTIeexHwWMfjAMnloGAACGMsIIAACwijACAACs6lMYWblypYqLi5Wenq7S0lJt2rTpmP03bNig0tJSpaena9KkSXrqqaf6VCwAABh+kg4ja9eu1aJFi7Rs2TJVVVVpxowZmjVrlmpqarrtv2vXLl1zzTWaMWOGqqqqdP/992vhwoV6+eWXT7h4AAAw9DmM6Xyj52O76KKLdMEFF2jVqlXxtsmTJ+vaa69VeXl5l/733nuvXn31Ve3cuTPeNn/+fL3//vvasmVLr/bZ1NQkn88nv9+vnJycZMoFAACW9Pbvd1IzI8FgUJWVlSorK0toLysr0+bNm7tdZ8uWLV36X3311dq6dava2tq6XScQCKipqSlhAQAAw1NSYaSxsVHhcFh5eXkJ7Xl5eaqvr+92nfr6+m77h0IhNTY2drtOeXm5fD5ffCksLEymTAAAMIT06QRWhyPxBjDGmC5tx+vfXXu7pUuXyu/3x5fa2tq+lAkAAIaApO7AmpubK5fL1WUWpKGhocvsR7v8/Pxu+7vdbo0ZM6bbdbxer7xebzKlAQCAISqpmRGPx6PS0lJVVFQktFdUVGj69OndrjNt2rQu/d944w1NnTpVaWlpSZYLAACGm6QP0yxZskTPPPOMVq9erZ07d2rx4sWqqanR/PnzJUUPscybNy/ef/78+dq9e7eWLFminTt3avXq1Xr22Wd1zz339N+vAAAAQ1bSD8qbO3eu9u3bp+XLl6uurk4lJSVav369ioqKJEl1dXUJ9xwpLi7W+vXrtXjxYj355JMqKCjQ448/ruuuu67/fgUAABiykr7PiA1+v18jR45UbW0t9xkBAGCIaGpqUmFhoQ4ePCifz9djv6RnRmxobm6WJC7xBQBgCGpubj5mGBkSMyORSESff/65srOzj3kJcbLaExszLgOPsR4cjPPgYJwHB+M8OAZynI0xam5uVkFBgZzOnk9THRIzI06nUxMmTBiw7efk5PAf+iBhrAcH4zw4GOfBwTgPjoEa52PNiLTr003PAAAA+gthBAAAWJXSYcTr9erBBx/kbq+DgLEeHIzz4GCcBwfjPDhOhnEeEiewAgCA4SulZ0YAAIB9hBEAAGAVYQQAAFhFGAEAAFaldBhZuXKliouLlZ6ertLSUm3atMl2SSet8vJyfe1rX1N2drbGjh2ra6+9Vn/+858T+hhj9P3vf18FBQXKyMjQZZddpg8//DChTyAQ0F133aXc3FxlZWXpr//6r7Vnz56EPgcOHNBNN90kn88nn8+nm266SQcPHhzon3hSKi8vl8Ph0KJFi+JtjHP/2Lt3r7797W9rzJgxyszM1HnnnafKysr494zziQuFQvrXf/1XFRcXKyMjQ5MmTdLy5csViUTifRjnvtm4caPmzJmjgoICORwOvfLKKwnfD+a41tTUaM6cOcrKylJubq4WLlyoYDCY3A8yKernP/+5SUtLMz/5yU/Mjh07zN13322ysrLM7t27bZd2Urr66qvNc889Zz744ANTXV1tZs+ebSZOnGgOHToU7/PII4+Y7Oxs8/LLL5vt27ebuXPnmnHjxpmmpqZ4n/nz55vx48ebiooKs23bNjNz5kwzZcoUEwqF4n3+6q/+ypSUlJjNmzebzZs3m5KSEvPNb35zUH/vyeDdd981p556qjn33HPN3XffHW9nnE/c/v37TVFRkbnlllvMH/7wB7Nr1y7z5ptvmk8++STeh3E+cQ8//LAZM2aM+c1vfmN27dplfvGLX5gRI0aYFStWxPswzn2zfv16s2zZMvPyyy8bSeZXv/pVwveDNa6hUMiUlJSYmTNnmm3btpmKigpTUFBgFixYkNTvSdkwcuGFF5r58+cntJ111lnmvvvus1TR0NLQ0GAkmQ0bNhhjjIlEIiY/P9888sgj8T6tra3G5/OZp556yhhjzMGDB01aWpr5+c9/Hu+zd+9e43Q6zWuvvWaMMWbHjh1Gkvn9738f77NlyxYjyfzpT38ajJ92UmhubjZnnHGGqaioMN/4xjfiYYRx7h/33nuvufTSS3v8nnHuH7Nnzza33nprQtvf/d3fmW9/+9vGGMa5v3QOI4M5ruvXrzdOp9Ps3bs33mfNmjXG6/Uav9/f69+QkodpgsGgKisrVVZWltBeVlamzZs3W6pqaPH7/ZKk0aNHS5J27dql+vr6hDH1er36xje+ER/TyspKtbW1JfQpKChQSUlJvM+WLVvk8/l00UUXxftcfPHF8vl8KfVvc+edd2r27Nm68sorE9oZ5/7x6quvaurUqfr7v/97jR07Vueff75+8pOfxL9nnPvHpZdeqv/5n//RRx99JEl6//339fbbb+uaa66RxDgPlMEc1y1btqikpEQFBQXxPldffbUCgUDCYc/jGRIPyutvjY2NCofDysvLS2jPy8tTfX29paqGDmOMlixZoksvvVQlJSWSFB+37sZ09+7d8T4ej0ejRo3q0qd9/fr6eo0dO7bLPseOHZsy/zY///nPtW3bNr333ntdvmOc+8dnn32mVatWacmSJbr//vv17rvvauHChfJ6vZo3bx7j3E/uvfde+f1+nXXWWXK5XAqHw/rBD36gb33rW5L473mgDOa41tfXd9nPqFGj5PF4khr7lAwj7RwOR8JnY0yXNnS1YMEC/fGPf9Tbb7/d5bu+jGnnPt31T5V/m9raWt1999164403lJ6e3mM/xvnERCIRTZ06VT/84Q8lSeeff74+/PBDrVq1SvPmzYv3Y5xPzNq1a/XTn/5UP/vZz3T22WerurpaixYtUkFBgW6++eZ4P8Z5YAzWuPbH2KfkYZrc3Fy5XK4uqa2hoaFLwkOiu+66S6+++qp++9vfasKECfH2/Px8STrmmObn5ysYDOrAgQPH7PPFF1902e+XX36ZEv82lZWVamhoUGlpqdxut9xutzZs2KDHH39cbrc7PgaM84kZN26cvvrVrya0TZ48WTU1NZL477m/fO9739N9992nf/iHf9A555yjm266SYsXL1Z5ebkkxnmgDOa45ufnd9nPgQMH1NbWltTYp2QY8Xg8Ki0tVUVFRUJ7RUWFpk+fbqmqk5sxRgsWLNC6dev01ltvqbi4OOH74uJi5efnJ4xpMBjUhg0b4mNaWlqqtLS0hD51dXX64IMP4n2mTZsmv9+vd999N97nD3/4g/x+f0r821xxxRXavn27qqur48vUqVN14403qrq6WpMmTWKc+8Ell1zS5dL0jz76SEVFRZL477m/tLS0yOlM/DPjcrnil/YyzgNjMMd12rRp+uCDD1RXVxfv88Ybb8jr9aq0tLT3Rff6VNdhpv3S3meffdbs2LHDLFq0yGRlZZm//OUvtks7Kf3jP/6j8fl85ne/+52pq6uLLy0tLfE+jzzyiPH5fGbdunVm+/bt5lvf+la3l5JNmDDBvPnmm2bbtm3m8ssv7/ZSsnPPPdds2bLFbNmyxZxzzjnD+hK94+l4NY0xjHN/ePfdd43b7TY/+MEPzMcff2xefPFFk5mZaX7605/G+zDOJ+7mm28248ePj1/au27dOpObm2v+5V/+Jd6Hce6b5uZmU1VVZaqqqowk89hjj5mqqqr47SkGa1zbL+294oorzLZt28ybb75pJkyYwKW9yXjyySdNUVGR8Xg85oILLohfpoquJHW7PPfcc/E+kUjEPPjggyY/P994vV7z9a9/3Wzfvj1hO0eOHDELFiwwo0ePNhkZGeab3/ymqampSeizb98+c+ONN5rs7GyTnZ1tbrzxRnPgwIFB+JUnp85hhHHuH7/+9a9NSUmJ8Xq95qyzzjI//vGPE75nnE9cU1OTufvuu83EiRNNenq6mTRpklm2bJkJBALxPoxz3/z2t7/t9n+Tb775ZmPM4I7r7t27zezZs01GRoYZPXq0WbBggWltbU3q9ziMMab38ygAAAD9KyXPGQEAACcPwggAALCKMAIAAKwijAAAAKsIIwAAwCrCCAAAsIowAgAArCKMAAAAqwgjAADAKsIIAACwijACAACsIowAAACr/h9OHSoZQvWIzAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "trainX = np.concatenate((\n",
    "                         corpusEmbedding(trainEcoCorpus,X_normalized_100d,word2ind),\n",
    "                         corpusEmbedding(trainMilCorpus,X_normalized_100d,word2ind)\n",
    "                         ))\n",
    "trainY = np.concatenate((\n",
    "                         np.ones(len(trainEcoCorpus),dtype='int32')*0,\n",
    "                         np.ones(len(trainMilCorpus),dtype='int32')*1\n",
    "                         ))\n",
    "testX = np.concatenate((\n",
    "                        corpusEmbedding(testEcoCorpus,X_normalized_100d,word2ind),\n",
    "                        corpusEmbedding(testMilCorpus,X_normalized_100d,word2ind)\n",
    "                        ))\n",
    "testY = np.concatenate((\n",
    "                        np.ones(len(testEcoCorpus),dtype='int32')*0,\n",
    "                        np.ones(len(testMilCorpus),dtype='int32')*1\n",
    "                        ))\n",
    "\n",
    "w0 = np.random.normal(0.,1.,100)\n",
    "b0 = np.random.normal(0., 1., 1)\n",
    "\n",
    "ceList,tceList = logistic.batchedStochasticGradient([np.copy(w0),np.copy(b0)], [trainX,trainY], [testX,testY], trainX.shape[0], maxStep = 100000, printInterval = 10)\n",
    "w1 = np.copy(w.value)\n",
    "b1 = np.copy(b.value)\n",
    "plt.plot([*range(len(ceList))],ceList)\n",
    "plt.plot([*range(len(tceList))],tceList)\n",
    "plt.show()\n",
    "\n",
    "gamma = lambda text : 1 if sigmoid(np.dot(w1,docVector(text, X_normalized_100d, word2ind))+b1)>0.5 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[--------------------------------------------------]----]\n",
      "=================================================================\n",
      "Матрица на обърквания: \n",
      "  65   0\n",
      "   4 154\n",
      "Прецизност: [0.9420289855072463, 1.0]\n",
      "Обхват: [1.0, 0.9746835443037974]\n",
      "F-оценка: [0.9701492537313433, 0.9871794871794872]\n",
      "Обща презизност: 0.9831026190940404\n",
      "Общ обхват: 0.9820627802690582\n",
      "Обща F-оценка: 0.9825824245737181\n",
      "=================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testClassifier([testEcoCorpus,testMilCorpus], gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Втори, подобрен вариант"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class compNode:\n",
    "    def __init__(self, predecessors, trainable = True):\n",
    "        self.predecessors = predecessors\n",
    "        self.trainable = trainable\n",
    "        self.value = None\n",
    "        self.grad = None\n",
    "    \n",
    "    def calcValue(self): ## трябва да се дефинира за конкретния връх като се извика setValue\n",
    "        return\n",
    "    \n",
    "    def propagateGrad(self, grad):\n",
    "        if not self.grad:\n",
    "            self.grad = grad\n",
    "        else:\n",
    "            self.grad += grad\n",
    "\n",
    "    def derivativeMult(self,i): ## трябва да се дефинира за конкретния връх\n",
    "        return\n",
    "    \n",
    "    def propagateBack(self):\n",
    "        if not self.predecessors: return\n",
    "        for i,p in enumerate(self.predecessors):\n",
    "            if p.trainable:\n",
    "                partialGrad = self.derivativeMult(i)\n",
    "                p.propagateGrad(partialGrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class crossEntropyNode(compNode):\n",
    "    def calcValue(self):\n",
    "        t = self.predecessors[0].value\n",
    "        y = self.predecessors[1].value\n",
    "        self.v = sigmoid(t)\n",
    "        p = (1-y) + (2*y-1) * self.v\n",
    "        self.value = -np.mean(np.log(p))\n",
    "    def derivativeMult(self,i):\n",
    "        y = self.predecessors[1].value\n",
    "        S = y.shape[0]\n",
    "        return self.grad * (-(y-self.v)/S)\n",
    "\n",
    "class plusVectorsNode(compNode):\n",
    "    def calcValue(self):\n",
    "        x = self.predecessors[0].value\n",
    "        y = self.predecessors[1].value\n",
    "        self.value = x+y\n",
    "    def derivativeMult(self,i):\n",
    "        return self.grad\n",
    "\n",
    "class mulMatrixVectorNode(compNode):\n",
    "    def calcValue(self):\n",
    "        x = self.predecessors[0].value\n",
    "        y = self.predecessors[1].value\n",
    "        self.value = np.dot(x,y)\n",
    "    def derivativeMult(self,i):\n",
    "        assert i==1\n",
    "        x = self.predecessors[0].value\n",
    "        return np.dot(self.grad,x)\n",
    "\n",
    "class copyNode(compNode):\n",
    "    def calcValue(self):\n",
    "        self.value = self.predecessors[0].value\n",
    "    def derivativeMult(self,i):\n",
    "        return np.sum(self.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = compNode(None,trainable=False)\n",
    "y = compNode(None,trainable=False)\n",
    "w = compNode(None)\n",
    "b = compNode(None)\n",
    "u = mulMatrixVectorNode([x,w])\n",
    "bS = copyNode([b])\n",
    "t = plusVectorsNode([u,bS])\n",
    "h = crossEntropyNode([t,y])\n",
    "\n",
    "logisticBetter = model(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 , step:  0 , train loss:  None , test loss:  1.4377669906382167\n",
      "Epoch:  1000 , step:  1000 , train loss:  0.245556567300468 , test loss:  0.25877217781556927\n",
      "Epoch:  2000 , step:  2000 , train loss:  0.19236957282423572 , test loss:  0.2068752716480684\n",
      "Epoch:  3000 , step:  3000 , train loss:  0.16457307306947094 , test loss:  0.1802326976483899\n",
      "Epoch:  4000 , step:  4000 , train loss:  0.146653273963851 , test loss:  0.16322318158521096\n",
      "Epoch:  5000 , step:  5000 , train loss:  0.13385297587511988 , test loss:  0.15116359857612985\n",
      "Epoch:  6000 , step:  6000 , train loss:  0.12411826666532255 , test loss:  0.142070709363387\n",
      "Epoch:  7000 , step:  7000 , train loss:  0.11639273950502284 , test loss:  0.13493083581831716\n",
      "Epoch:  8000 , step:  8000 , train loss:  0.11006881352653793 , test loss:  0.12915964352277493\n",
      "Epoch:  9000 , step:  9000 , train loss:  0.10476855212772533 , test loss:  0.12439136997167316\n",
      "Epoch:  10000 , step:  10000 , train loss:  0.10024276061564921 , test loss:  0.12038305294593643\n",
      "Epoch:  11000 , step:  11000 , train loss:  0.09631954732900498 , test loss:  0.11696591489416426\n",
      "Epoch:  12000 , step:  12000 , train loss:  0.09287599211097365 , test loss:  0.11401853892150635\n",
      "Epoch:  13000 , step:  13000 , train loss:  0.0898215631413263 , test loss:  0.11145108299948142\n",
      "Epoch:  14000 , step:  14000 , train loss:  0.08708792112653883 , test loss:  0.10919550364574711\n",
      "Epoch:  15000 , step:  15000 , train loss:  0.08462239073675953 , test loss:  0.10719924601993801\n",
      "Epoch:  16000 , step:  16000 , train loss:  0.0823836350112228 , test loss:  0.10542103008789136\n",
      "Epoch:  17000 , step:  17000 , train loss:  0.08033870462737717 , test loss:  0.10382795477624794\n",
      "Epoch:  18000 , step:  18000 , train loss:  0.07846097372158704 , test loss:  0.10239345860676521\n",
      "Epoch:  19000 , step:  19000 , train loss:  0.07672866379664955 , test loss:  0.10109585277144183\n",
      "Epoch:  20000 , step:  20000 , train loss:  0.0751237675156903 , test loss:  0.09991724622243818\n",
      "Epoch:  21000 , step:  21000 , train loss:  0.07363125042302944 , test loss:  0.09884274498900808\n",
      "Epoch:  22000 , step:  22000 , train loss:  0.07223844962518557 , test loss:  0.09785984696399379\n",
      "Epoch:  23000 , step:  23000 , train loss:  0.07093461450888694 , test loss:  0.09695797837532298\n",
      "Epoch:  24000 , step:  24000 , train loss:  0.06971055151221701 , test loss:  0.09612813451466092\n",
      "Epoch:  25000 , step:  25000 , train loss:  0.0685583462174352 , test loss:  0.09536259823420191\n",
      "Epoch:  26000 , step:  26000 , train loss:  0.06747114365262237 , test loss:  0.09465471717608685\n",
      "Epoch:  27000 , step:  27000 , train loss:  0.06644297293770698 , test loss:  0.09399872586403114\n",
      "Epoch:  28000 , step:  28000 , train loss:  0.06546860608363987 , test loss:  0.09338960242132247\n",
      "Epoch:  29000 , step:  29000 , train loss:  0.06454344336179878 , test loss:  0.0928229522729894\n",
      "Epoch:  30000 , step:  30000 , train loss:  0.06366341953772339 , test loss:  0.09229491306471839\n",
      "Epoch:  31000 , step:  31000 , train loss:  0.06282492663079013 , test loss:  0.09180207640239914\n",
      "Epoch:  32000 , step:  32000 , train loss:  0.06202474986918344 , test loss:  0.09134142303028485\n",
      "Epoch:  33000 , step:  33000 , train loss:  0.06126001426007834 , test loss:  0.09091026882339254\n",
      "Epoch:  34000 , step:  34000 , train loss:  0.06052813975951442 , test loss:  0.09050621954118514\n",
      "Epoch:  35000 , step:  35000 , train loss:  0.05982680345504851 , test loss:  0.09012713272440623\n",
      "Epoch:  36000 , step:  36000 , train loss:  0.05915390750249223 , test loss:  0.08977108545057888\n",
      "Epoch:  37000 , step:  37000 , train loss:  0.05850755181143904 , test loss:  0.08943634692169483\n",
      "Epoch:  38000 , step:  38000 , train loss:  0.05788601067141367 , test loss:  0.08912135505862318\n",
      "Epoch:  39000 , step:  39000 , train loss:  0.05728771266494749 , test loss:  0.08882469643444695\n",
      "Epoch:  40000 , step:  40000 , train loss:  0.05671122333574339 , test loss:  0.08854508900344478\n",
      "Epoch:  41000 , step:  41000 , train loss:  0.05615523017685694 , test loss:  0.0882813671813655\n",
      "Epoch:  42000 , step:  42000 , train loss:  0.055618529581115406 , test loss:  0.08803246891170982\n",
      "Epoch:  43000 , step:  43000 , train loss:  0.0551000154581046 , test loss:  0.08779742441628684\n",
      "Epoch:  44000 , step:  44000 , train loss:  0.05459866927222649 , test loss:  0.0875753463796622\n",
      "Epoch:  45000 , step:  45000 , train loss:  0.05411355129708075 , test loss:  0.08736542135881938\n",
      "Epoch:  46000 , step:  46000 , train loss:  0.05364379291467631 , test loss:  0.08716690224338929\n",
      "Epoch:  47000 , step:  47000 , train loss:  0.05318858981525136 , test loss:  0.08697910161970496\n",
      "Epoch:  48000 , step:  48000 , train loss:  0.05274719597594082 , test loss:  0.08680138591491758\n",
      "Epoch:  49000 , step:  49000 , train loss:  0.05231891831511424 , test loss:  0.08663317021640422\n",
      "Epoch:  50000 , step:  50000 , train loss:  0.05190311193464181 , test loss:  0.08647391367748238\n",
      "Epoch:  51000 , step:  51000 , train loss:  0.05149917587521987 , test loss:  0.08632311543358863\n",
      "Epoch:  52000 , step:  52000 , train loss:  0.0511065493206625 , test loss:  0.08618031096408264\n",
      "Epoch:  53000 , step:  53000 , train loss:  0.0507247081961198 , test loss:  0.08604506884407451\n",
      "Epoch:  54000 , step:  54000 , train loss:  0.05035316211281533 , test loss:  0.0859169878384534\n",
      "Epoch:  55000 , step:  55000 , train loss:  0.04999145161835148 , test loss:  0.08579569429686801\n",
      "Epoch:  56000 , step:  56000 , train loss:  0.04963914571711018 , test loss:  0.08568083981399664\n",
      "Epoch:  57000 , step:  57000 , train loss:  0.049295839629940136 , test loss:  0.0855720991241718\n",
      "Epoch:  58000 , step:  58000 , train loss:  0.048961152766302196 , test loss:  0.08546916820348274\n",
      "Epoch:  59000 , step:  59000 , train loss:  0.04863472688545468 , test loss:  0.08537176255592736\n",
      "Epoch:  60000 , step:  60000 , train loss:  0.048316224426185835 , test loss:  0.08527961566316324\n",
      "Epoch:  61000 , step:  61000 , train loss:  0.04800532698712134 , test loss:  0.08519247757994851\n",
      "Epoch:  62000 , step:  62000 , train loss:  0.04770173394180776 , test loss:  0.08511011365956876\n",
      "Epoch:  63000 , step:  63000 , train loss:  0.047405161174655534 , test loss:  0.08503230339544553\n",
      "Epoch:  64000 , step:  64000 , train loss:  0.04711533992545578 , test loss:  0.08495883936676661\n",
      "Epoch:  65000 , step:  65000 , train loss:  0.04683201573160375 , test loss:  0.08488952627740683\n",
      "Epoch:  66000 , step:  66000 , train loss:  0.046554947458398006 , test loss:  0.0848241800786579\n",
      "Epoch:  67000 , step:  67000 , train loss:  0.046283906408862825 , test loss:  0.08476262716735483\n",
      "Epoch:  68000 , step:  68000 , train loss:  0.04601867550548591 , test loss:  0.08470470365195368\n",
      "Epoch:  69000 , step:  69000 , train loss:  0.04575904853709112 , test loss:  0.08465025467992524\n",
      "Epoch:  70000 , step:  70000 , train loss:  0.0455048294647932 , test loss:  0.08459913382056664\n",
      "Epoch:  71000 , step:  71000 , train loss:  0.04525583178162132 , test loss:  0.08455120249797116\n",
      "Epoch:  72000 , step:  72000 , train loss:  0.04501187792096398 , test loss:  0.08450632946945356\n",
      "Epoch:  73000 , step:  73000 , train loss:  0.044772798709484925 , test loss:  0.08446439034522978\n",
      "Epoch:  74000 , step:  74000 , train loss:  0.04453843286060219 , test loss:  0.08442526714558209\n",
      "Epoch:  75000 , step:  75000 , train loss:  0.0443086265050138 , test loss:  0.08438884789213319\n",
      "Epoch:  76000 , step:  76000 , train loss:  0.0440832327551003 , test loss:  0.0843550262301934\n",
      "Epoch:  77000 , step:  77000 , train loss:  0.04386211130034462 , test loss:  0.0843237010794494\n",
      "Epoch:  78000 , step:  78000 , train loss:  0.04364512803118373 , test loss:  0.08429477631053407\n",
      "Epoch:  79000 , step:  79000 , train loss:  0.04343215468895401 , test loss:  0.08426816044525991\n",
      "Epoch:  80000 , step:  80000 , train loss:  0.043223068539810054 , test loss:  0.08424376637850922\n",
      "Epoch:  81000 , step:  81000 , train loss:  0.04301775207069425 , test loss:  0.08422151111997303\n",
      "Epoch:  82000 , step:  82000 , train loss:  0.04281609270561054 , test loss:  0.08420131555409048\n",
      "Epoch:  83000 , step:  83000 , train loss:  0.0426179825406128 , test loss:  0.08418310421670959\n",
      "Epoch:  84000 , step:  84000 , train loss:  0.04242331809606193 , test loss:  0.08416680508711205\n",
      "Epoch:  85000 , step:  85000 , train loss:  0.0422320000848319 , test loss:  0.08415234939417965\n",
      "Epoch:  86000 , step:  86000 , train loss:  0.04204393319526132 , test loss:  0.08413967143558522\n",
      "Epoch:  87000 , step:  87000 , train loss:  0.04185902588775013 , test loss:  0.08412870840899132\n",
      "Epoch:  88000 , step:  88000 , train loss:  0.04167719020399576 , test loss:  0.08411940025433204\n",
      "Epoch:  89000 , step:  89000 , train loss:  0.04149834158794686 , test loss:  0.08411168950633177\n",
      "Epoch:  90000 , step:  90000 , train loss:  0.04132239871763027 , test loss:  0.08410552115648857\n",
      "Epoch:  91000 , step:  91000 , train loss:  0.04114928334707663 , test loss:  0.0841008425238169\n",
      "Epoch:  92000 , step:  92000 , train loss:  0.04097892015763265 , test loss:  0.08409760313370306\n",
      "Epoch:  93000 , step:  93000 , train loss:  0.040811236618005536 , test loss:  0.08409575460428163\n",
      "Epoch:  94000 , step:  94000 , train loss:  0.04064616285243809 , test loss:  0.0840952505397906\n",
      "Epoch:  95000 , step:  95000 , train loss:  0.040483631516459284 , test loss:  0.08409604643040344\n",
      "Epoch:  96000 , step:  96000 , train loss:  0.040323577679699574 , test loss:  0.08409809955808673\n",
      "Epoch:  97000 , step:  97000 , train loss:  0.04016593871529889 , test loss:  0.08410136890805538\n",
      "Epoch:  98000 , step:  98000 , train loss:  0.04001065419547176 , test loss:  0.0841058150854412\n",
      "Epoch:  99000 , step:  99000 , train loss:  0.039857665792827034 , test loss:  0.08411140023681993\n",
      "[--------------------------------------------------]----]\n",
      "=================================================================\n",
      "Матрица на обърквания: \n",
      "  65   0\n",
      "   4 154\n",
      "Прецизност: [0.9420289855072463, 1.0]\n",
      "Обхват: [1.0, 0.9746835443037974]\n",
      "F-оценка: [0.9701492537313433, 0.9871794871794872]\n",
      "Обща презизност: 0.9831026190940404\n",
      "Общ обхват: 0.9820627802690582\n",
      "Обща F-оценка: 0.9825824245737181\n",
      "=================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cefList, tcefList = logisticBetter.batchedStochasticGradient([np.copy(w0),np.copy(b0)], [trainX,trainY], [testX,testY], trainX.shape[0], maxStep = 100000, printInterval = 1000)\n",
    "w1 = np.copy(w.value)\n",
    "b1 = np.copy(b.value)\n",
    "gamma = lambda text : 1 if sigmoid(np.dot(w1,docVector(text, X_normalized_100d, word2ind))+b1)>0.5 else 0\n",
    "testClassifier([testEcoCorpus,testMilCorpus], gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 , step:  0 , train loss:  None , test loss:  1.4377669906382167\n",
      "Epoch:  47 , step:  1000 , train loss:  0.21435465750057292 , test loss:  0.2581316974738821\n",
      "Epoch:  95 , step:  2000 , train loss:  0.17219541312036296 , test loss:  0.20640569218732951\n",
      "Epoch:  142 , step:  3000 , train loss:  0.12133460577873606 , test loss:  0.18217302480836145\n",
      "Epoch:  190 , step:  4000 , train loss:  0.13602834525378077 , test loss:  0.16334668830337423\n",
      "Epoch:  238 , step:  5000 , train loss:  0.11569330394073814 , test loss:  0.14950823265518284\n",
      "Epoch:  285 , step:  6000 , train loss:  0.10220363070873394 , test loss:  0.1416245649692069\n",
      "Epoch:  333 , step:  7000 , train loss:  0.1451472808321077 , test loss:  0.13490937315135065\n",
      "Epoch:  380 , step:  8000 , train loss:  0.1280617291172868 , test loss:  0.12858614156467676\n",
      "Epoch:  428 , step:  9000 , train loss:  0.12947318661295404 , test loss:  0.12416171421473556\n",
      "Epoch:  476 , step:  10000 , train loss:  0.08380334672702547 , test loss:  0.12147661261554447\n",
      "Epoch:  523 , step:  11000 , train loss:  0.10718573173667945 , test loss:  0.11718148286605813\n",
      "Epoch:  571 , step:  12000 , train loss:  0.08665458700125347 , test loss:  0.11293931662919529\n",
      "Epoch:  619 , step:  13000 , train loss:  0.0900858626674828 , test loss:  0.11374171717968709\n",
      "Epoch:  666 , step:  14000 , train loss:  0.08634003644014633 , test loss:  0.10813500312766842\n",
      "Epoch:  714 , step:  15000 , train loss:  0.10834802201553945 , test loss:  0.10852147498187369\n",
      "Epoch:  761 , step:  16000 , train loss:  0.08725770307097115 , test loss:  0.10526093658353117\n",
      "Epoch:  809 , step:  17000 , train loss:  0.10169541140803118 , test loss:  0.10369011529391237\n",
      "Epoch:  857 , step:  18000 , train loss:  0.06563439276438746 , test loss:  0.10297169065791346\n",
      "Epoch:  904 , step:  19000 , train loss:  0.06337817273731307 , test loss:  0.09929967782274947\n",
      "Epoch:  952 , step:  20000 , train loss:  0.10027379923562407 , test loss:  0.10035214969796015\n",
      "Epoch:  1000 , step:  21000 , train loss:  0.07673864022380927 , test loss:  0.09978286091141944\n",
      "Epoch:  1047 , step:  22000 , train loss:  0.08143407593117555 , test loss:  0.09830794977988055\n",
      "Epoch:  1095 , step:  23000 , train loss:  0.045127002580200914 , test loss:  0.09753452141155895\n",
      "Epoch:  1142 , step:  24000 , train loss:  0.046140806227084316 , test loss:  0.09602090414214964\n",
      "Epoch:  1190 , step:  25000 , train loss:  0.06730078493014348 , test loss:  0.09472981727303723\n",
      "Epoch:  1238 , step:  26000 , train loss:  0.07837301762902667 , test loss:  0.09386109490161963\n",
      "Epoch:  1285 , step:  27000 , train loss:  0.050932656581155325 , test loss:  0.09357008239564253\n",
      "Epoch:  1333 , step:  28000 , train loss:  0.10172291299803052 , test loss:  0.09352078589988694\n",
      "Epoch:  1380 , step:  29000 , train loss:  0.06420886002436982 , test loss:  0.09173003687670812\n",
      "Epoch:  1428 , step:  30000 , train loss:  0.060573177762430354 , test loss:  0.09286191570450004\n",
      "Epoch:  1476 , step:  31000 , train loss:  0.058777309669130175 , test loss:  0.09119121762733455\n",
      "Epoch:  1523 , step:  32000 , train loss:  0.051798356140477154 , test loss:  0.09226107287748385\n",
      "Epoch:  1571 , step:  33000 , train loss:  0.0565562424310769 , test loss:  0.09076107662118522\n",
      "Epoch:  1619 , step:  34000 , train loss:  0.12929112452124922 , test loss:  0.09034860913924272\n",
      "Epoch:  1666 , step:  35000 , train loss:  0.06610831245327778 , test loss:  0.08974090101110535\n",
      "Epoch:  1714 , step:  36000 , train loss:  0.06121591199446423 , test loss:  0.08873425392184596\n",
      "Epoch:  1761 , step:  37000 , train loss:  0.08738898314767006 , test loss:  0.0886977842217327\n",
      "Epoch:  1809 , step:  38000 , train loss:  0.038413988358947534 , test loss:  0.0875468929709811\n",
      "Epoch:  1857 , step:  39000 , train loss:  0.05236654732560992 , test loss:  0.08909226391854519\n",
      "Epoch:  1904 , step:  40000 , train loss:  0.03323272745527193 , test loss:  0.08790659942063044\n",
      "Epoch:  1952 , step:  41000 , train loss:  0.04083269587052257 , test loss:  0.08721425479777777\n",
      "Epoch:  2000 , step:  42000 , train loss:  0.03162658357390044 , test loss:  0.08727439581584837\n",
      "Epoch:  2047 , step:  43000 , train loss:  0.03605650832968041 , test loss:  0.08678919281137903\n",
      "Epoch:  2095 , step:  44000 , train loss:  0.04270428410204101 , test loss:  0.08674025552541566\n",
      "Epoch:  2142 , step:  45000 , train loss:  0.04766626904398556 , test loss:  0.08782282454186226\n",
      "Epoch:  2190 , step:  46000 , train loss:  0.05490324466761889 , test loss:  0.08834407416079627\n",
      "Epoch:  2238 , step:  47000 , train loss:  0.04258024144414914 , test loss:  0.08870524544861923\n",
      "Epoch:  2285 , step:  48000 , train loss:  0.02487670418796895 , test loss:  0.08622290885268666\n",
      "Epoch:  2333 , step:  49000 , train loss:  0.06264887269045913 , test loss:  0.0868090703774859\n",
      "Epoch:  2380 , step:  50000 , train loss:  0.06678428098774863 , test loss:  0.08737494388075426\n",
      "Epoch:  2428 , step:  51000 , train loss:  0.0298307608399249 , test loss:  0.08729400767035313\n",
      "Epoch:  2476 , step:  52000 , train loss:  0.046540669401524584 , test loss:  0.08554743820899345\n",
      "Epoch:  2523 , step:  53000 , train loss:  0.04949908576789065 , test loss:  0.08578020700883673\n",
      "Epoch:  2571 , step:  54000 , train loss:  0.06036884390283733 , test loss:  0.08471062983478918\n",
      "Epoch:  2619 , step:  55000 , train loss:  0.03455215113054362 , test loss:  0.08559402854221568\n",
      "Epoch:  2666 , step:  56000 , train loss:  0.029764751074203864 , test loss:  0.08468807820098373\n",
      "Epoch:  2714 , step:  57000 , train loss:  0.04148232889443934 , test loss:  0.08721831269315103\n",
      "Epoch:  2761 , step:  58000 , train loss:  0.017566824723225963 , test loss:  0.08560271160301061\n",
      "Epoch:  2809 , step:  59000 , train loss:  0.0318728787613978 , test loss:  0.08501214605918252\n",
      "Epoch:  2857 , step:  60000 , train loss:  0.04219430825935619 , test loss:  0.0849317181696964\n",
      "Epoch:  2904 , step:  61000 , train loss:  0.026065769476235588 , test loss:  0.0855257791077264\n",
      "Epoch:  2952 , step:  62000 , train loss:  0.05000036727122756 , test loss:  0.08502386703749819\n",
      "Epoch:  3000 , step:  63000 , train loss:  0.12137963186875655 , test loss:  0.08734693965501425\n",
      "Epoch:  3047 , step:  64000 , train loss:  0.024090571061382526 , test loss:  0.08302339727881095\n",
      "Epoch:  3095 , step:  65000 , train loss:  0.07202795554061 , test loss:  0.08471292310361589\n",
      "Epoch:  3142 , step:  66000 , train loss:  0.03163153270843527 , test loss:  0.08496718193385594\n",
      "Epoch:  3190 , step:  67000 , train loss:  0.064230018761808 , test loss:  0.08451851505952533\n",
      "Epoch:  3238 , step:  68000 , train loss:  0.06265804026822601 , test loss:  0.08484714636409818\n",
      "Epoch:  3285 , step:  69000 , train loss:  0.0502517061187468 , test loss:  0.08394820805358574\n",
      "Epoch:  3333 , step:  70000 , train loss:  0.07111656174814626 , test loss:  0.08525663903720394\n",
      "Epoch:  3380 , step:  71000 , train loss:  0.0330720607206295 , test loss:  0.08572712462806802\n",
      "Epoch:  3428 , step:  72000 , train loss:  0.023707299289979847 , test loss:  0.08399268636017071\n",
      "Epoch:  3476 , step:  73000 , train loss:  0.04658834363644662 , test loss:  0.08189208986100815\n",
      "Epoch:  3523 , step:  74000 , train loss:  0.04051039381587418 , test loss:  0.08471975328857952\n",
      "Epoch:  3571 , step:  75000 , train loss:  0.03193049701755546 , test loss:  0.08434066529769595\n",
      "Epoch:  3619 , step:  76000 , train loss:  0.02820836569396856 , test loss:  0.08521484613831118\n",
      "Epoch:  3666 , step:  77000 , train loss:  0.040253245708851325 , test loss:  0.08211334078192187\n",
      "Epoch:  3714 , step:  78000 , train loss:  0.04143580680331129 , test loss:  0.08527895284683\n",
      "Epoch:  3761 , step:  79000 , train loss:  0.03172431936151872 , test loss:  0.08434964862543819\n",
      "Epoch:  3809 , step:  80000 , train loss:  0.03216890710938805 , test loss:  0.08266426346355735\n",
      "Epoch:  3857 , step:  81000 , train loss:  0.05127347347969615 , test loss:  0.08331005914898808\n",
      "Epoch:  3904 , step:  82000 , train loss:  0.054127762856131224 , test loss:  0.08453993495695956\n",
      "Epoch:  3952 , step:  83000 , train loss:  0.01568336009226593 , test loss:  0.08298637941899811\n",
      "Epoch:  4000 , step:  84000 , train loss:  0.023027209079876616 , test loss:  0.08360190447654833\n",
      "Epoch:  4047 , step:  85000 , train loss:  0.018966282750393988 , test loss:  0.08395787484641853\n",
      "Epoch:  4095 , step:  86000 , train loss:  0.018187861776640103 , test loss:  0.08342915877363659\n",
      "Epoch:  4142 , step:  87000 , train loss:  0.038149829889504 , test loss:  0.0836564078120708\n",
      "Epoch:  4190 , step:  88000 , train loss:  0.03989013709585053 , test loss:  0.08405277168758787\n",
      "Epoch:  4238 , step:  89000 , train loss:  0.03060489739936957 , test loss:  0.08371656256859245\n",
      "Epoch:  4285 , step:  90000 , train loss:  0.0822818101582132 , test loss:  0.08484791234625891\n",
      "Epoch:  4333 , step:  91000 , train loss:  0.06931455908365805 , test loss:  0.08620648018291104\n",
      "Epoch:  4380 , step:  92000 , train loss:  0.038161190982428914 , test loss:  0.08466220030451253\n",
      "Epoch:  4428 , step:  93000 , train loss:  0.03456029935382655 , test loss:  0.08472240933597171\n",
      "Epoch:  4476 , step:  94000 , train loss:  0.04403103601482927 , test loss:  0.08525191860755305\n",
      "Epoch:  4523 , step:  95000 , train loss:  0.03821119948326654 , test loss:  0.083519351186565\n",
      "Epoch:  4571 , step:  96000 , train loss:  0.02639085532171824 , test loss:  0.08409000778293056\n",
      "Epoch:  4619 , step:  97000 , train loss:  0.04525127460424087 , test loss:  0.08576975215271924\n",
      "Epoch:  4666 , step:  98000 , train loss:  0.03875196948751693 , test loss:  0.08246776809285367\n",
      "Epoch:  4714 , step:  99000 , train loss:  0.04013061743568089 , test loss:  0.08491272998080227\n",
      "[--------------------------------------------------]----]\n",
      "=================================================================\n",
      "Матрица на обърквания: \n",
      "  65   0\n",
      "   4 154\n",
      "Прецизност: [0.9420289855072463, 1.0]\n",
      "Обхват: [1.0, 0.9746835443037974]\n",
      "F-оценка: [0.9701492537313433, 0.9871794871794872]\n",
      "Обща презизност: 0.9831026190940404\n",
      "Общ обхват: 0.9820627802690582\n",
      "Обща F-оценка: 0.9825824245737181\n",
      "=================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cebList, tcebList = logisticBetter.batchedStochasticGradient([np.copy(w0),np.copy(b0)], [trainX,trainY], [testX,testY], 100, maxStep = 100000, printInterval = 1000)\n",
    "w1 = np.copy(w.value)\n",
    "b1 = np.copy(b.value)\n",
    "gamma = lambda text : 1 if sigmoid(np.dot(w1,docVector(text, X_normalized_100d, word2ind))+b1)>0.5 else 0\n",
    "testClassifier([testEcoCorpus,testMilCorpus], gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot([*range(len(cefList))],cefList)\n",
    "plt.plot([*range(len(tcefList))],tcefList)\n",
    "plt.plot([*range(len(cebList))],cebList)\n",
    "plt.plot([*range(len(tcebList))],tcebList)\n",
    "plt.legend(['Full gradient train','Full gradient test','Stochastic gradient train','Stochastic gradient test'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 , step:  0 , train loss:  None , test loss:  1.4377669906382167\n",
      "Epoch:  0 , step:  1000 , train loss:  0.05929300613257667 , test loss:  0.2495689619062734\n",
      "Epoch:  0 , step:  2000 , train loss:  0.029544603149000974 , test loss:  0.21697514160998915\n",
      "Epoch:  1 , step:  3000 , train loss:  0.2006552812309665 , test loss:  0.17285371393951862\n",
      "Epoch:  1 , step:  4000 , train loss:  1.551667121969771 , test loss:  0.21532491863513165\n",
      "Epoch:  2 , step:  5000 , train loss:  0.031104213775585317 , test loss:  0.21259664502188694\n",
      "Epoch:  2 , step:  6000 , train loss:  0.0023366874442115647 , test loss:  0.14426707018283394\n",
      "Epoch:  3 , step:  7000 , train loss:  0.08443828367331692 , test loss:  0.1337075633271774\n",
      "Epoch:  3 , step:  8000 , train loss:  0.457025488352349 , test loss:  0.14146754356287836\n",
      "Epoch:  4 , step:  9000 , train loss:  0.007859021564127214 , test loss:  0.12040165305020238\n",
      "Epoch:  4 , step:  10000 , train loss:  0.014411337974422558 , test loss:  0.12389738163167383\n",
      "Epoch:  5 , step:  11000 , train loss:  0.005079519459322326 , test loss:  0.11391205724080673\n",
      "Epoch:  5 , step:  12000 , train loss:  0.16238346706241982 , test loss:  0.1086274164676859\n",
      "Epoch:  6 , step:  13000 , train loss:  0.009050176975950916 , test loss:  0.1446441376588576\n",
      "Epoch:  6 , step:  14000 , train loss:  0.012224240363622478 , test loss:  0.10468576777609437\n",
      "Epoch:  7 , step:  15000 , train loss:  0.013967438640125568 , test loss:  0.10386570552143241\n",
      "Epoch:  7 , step:  16000 , train loss:  0.015713274415223293 , test loss:  0.13306737013236328\n",
      "Epoch:  8 , step:  17000 , train loss:  0.00672051467307654 , test loss:  0.09972294102423081\n",
      "Epoch:  8 , step:  18000 , train loss:  0.00043572080069837675 , test loss:  0.10582955678803992\n",
      "Epoch:  9 , step:  19000 , train loss:  9.734241849188304e-05 , test loss:  0.1241444345259396\n",
      "Epoch:  9 , step:  20000 , train loss:  0.018656163616004553 , test loss:  0.09618697739825985\n",
      "Epoch:  10 , step:  21000 , train loss:  0.010128918582937548 , test loss:  0.11481282147535328\n",
      "Epoch:  10 , step:  22000 , train loss:  0.07030514718280198 , test loss:  0.10648112078117537\n",
      "Epoch:  11 , step:  23000 , train loss:  0.004949867795055653 , test loss:  0.09303536346644033\n",
      "Epoch:  11 , step:  24000 , train loss:  0.009670707313485741 , test loss:  0.08999675620403388\n",
      "Epoch:  12 , step:  25000 , train loss:  0.0008949271796718269 , test loss:  0.08996971873061545\n",
      "Epoch:  12 , step:  26000 , train loss:  0.0007673786680013156 , test loss:  0.08898636448404687\n",
      "Epoch:  13 , step:  27000 , train loss:  0.005456842889790872 , test loss:  0.09029572317701842\n",
      "Epoch:  13 , step:  28000 , train loss:  0.02154872812194501 , test loss:  0.08707185717799537\n",
      "Epoch:  14 , step:  29000 , train loss:  0.8222199893731055 , test loss:  0.14308826171706818\n",
      "Epoch:  14 , step:  30000 , train loss:  0.10981149035803757 , test loss:  0.08695376411030588\n",
      "Epoch:  15 , step:  31000 , train loss:  0.1672211189455631 , test loss:  0.08483477632875015\n",
      "Epoch:  15 , step:  32000 , train loss:  4.1966113096310885e-05 , test loss:  0.0853240807024179\n",
      "Epoch:  16 , step:  33000 , train loss:  0.0033729477536503013 , test loss:  0.09474588573761536\n",
      "Epoch:  16 , step:  34000 , train loss:  0.000746817898096543 , test loss:  0.0866852707001255\n",
      "Epoch:  17 , step:  35000 , train loss:  0.02504022766851686 , test loss:  0.08295916661366606\n",
      "Epoch:  17 , step:  36000 , train loss:  0.00046154338403386354 , test loss:  0.08251112330369909\n",
      "Epoch:  18 , step:  37000 , train loss:  0.010953479919515218 , test loss:  0.08442791592649646\n",
      "Epoch:  18 , step:  38000 , train loss:  0.02392075593319391 , test loss:  0.08191944784851442\n",
      "Epoch:  19 , step:  39000 , train loss:  0.03597568414312396 , test loss:  0.08298504324493688\n",
      "Epoch:  19 , step:  40000 , train loss:  0.005021436475814506 , test loss:  0.0838240484300613\n",
      "Epoch:  20 , step:  41000 , train loss:  0.031063670004208767 , test loss:  0.09156860050240191\n",
      "Epoch:  20 , step:  42000 , train loss:  8.361417770140187e-05 , test loss:  0.08339121335408538\n",
      "Epoch:  21 , step:  43000 , train loss:  0.00538555645810184 , test loss:  0.08135206018006537\n",
      "Epoch:  21 , step:  44000 , train loss:  0.0003322730520002953 , test loss:  0.08191834459615206\n",
      "Epoch:  22 , step:  45000 , train loss:  0.0054084668842088865 , test loss:  0.080630284982899\n",
      "Epoch:  22 , step:  46000 , train loss:  0.0001420136709563124 , test loss:  0.08063440729426333\n",
      "Epoch:  23 , step:  47000 , train loss:  0.0005891323777846472 , test loss:  0.07974204433019254\n",
      "Epoch:  23 , step:  48000 , train loss:  0.023577389707657576 , test loss:  0.08994804311277108\n",
      "Epoch:  24 , step:  49000 , train loss:  0.0939969282917343 , test loss:  0.09994033448721176\n",
      "Epoch:  24 , step:  50000 , train loss:  0.14977107140538334 , test loss:  0.10254655811308659\n",
      "Epoch:  25 , step:  51000 , train loss:  0.0008351497865723996 , test loss:  0.07953091562521972\n",
      "Epoch:  25 , step:  52000 , train loss:  0.00029992407409983534 , test loss:  0.10832433816277406\n",
      "Epoch:  26 , step:  53000 , train loss:  0.003880484756966012 , test loss:  0.08629443912749826\n",
      "Epoch:  26 , step:  54000 , train loss:  0.002190921561084214 , test loss:  0.08280358271009808\n",
      "Epoch:  27 , step:  55000 , train loss:  0.0013320914207053045 , test loss:  0.08173958602998554\n",
      "Epoch:  27 , step:  56000 , train loss:  0.020345501684080256 , test loss:  0.0819057737262331\n",
      "Epoch:  28 , step:  57000 , train loss:  0.0005896913773006764 , test loss:  0.07987697479610871\n",
      "Epoch:  28 , step:  58000 , train loss:  0.0003696290184337616 , test loss:  0.11583302699279065\n",
      "Epoch:  29 , step:  59000 , train loss:  0.0010168888758535478 , test loss:  0.11868367848408641\n",
      "Epoch:  29 , step:  60000 , train loss:  0.0013225620871068387 , test loss:  0.0882975221177332\n",
      "Epoch:  30 , step:  61000 , train loss:  0.004815877025218932 , test loss:  0.07832889569864382\n",
      "Epoch:  30 , step:  62000 , train loss:  0.0014963933547020804 , test loss:  0.08829246628767959\n",
      "Epoch:  31 , step:  63000 , train loss:  0.03239921754197441 , test loss:  0.10245321358466604\n",
      "Epoch:  31 , step:  64000 , train loss:  5.1734210353607996e-05 , test loss:  0.08375545760151287\n",
      "Epoch:  32 , step:  65000 , train loss:  0.009004112880264334 , test loss:  0.0825563546910283\n",
      "Epoch:  32 , step:  66000 , train loss:  0.0003581088446558371 , test loss:  0.09256496413142928\n",
      "Epoch:  33 , step:  67000 , train loss:  0.0006726570832995184 , test loss:  0.0768666379491728\n",
      "Epoch:  33 , step:  68000 , train loss:  0.0002582441538640357 , test loss:  0.08224833921640146\n",
      "Epoch:  34 , step:  69000 , train loss:  0.029266556096256748 , test loss:  0.09710645398950318\n",
      "Epoch:  34 , step:  70000 , train loss:  0.004320322535620266 , test loss:  0.08682359324850629\n",
      "Epoch:  35 , step:  71000 , train loss:  0.00416803736167226 , test loss:  0.11102084837333422\n",
      "Epoch:  35 , step:  72000 , train loss:  0.0015082618012008076 , test loss:  0.07657129648828497\n",
      "Epoch:  36 , step:  73000 , train loss:  5.2075031104425884e-05 , test loss:  0.10773357307945944\n",
      "Epoch:  36 , step:  74000 , train loss:  0.0005790585199114031 , test loss:  0.08329489241707351\n",
      "Epoch:  37 , step:  75000 , train loss:  0.000608197230199163 , test loss:  0.11125137978684277\n",
      "Epoch:  37 , step:  76000 , train loss:  0.0008641718027767874 , test loss:  0.16003255775573297\n",
      "Epoch:  38 , step:  77000 , train loss:  0.021180706588497238 , test loss:  0.08834056529089032\n",
      "Epoch:  38 , step:  78000 , train loss:  0.0001295895020157905 , test loss:  0.09568120091613644\n",
      "Epoch:  39 , step:  79000 , train loss:  0.01762337316935898 , test loss:  0.10980141692605294\n",
      "Epoch:  39 , step:  80000 , train loss:  0.0003050487866317686 , test loss:  0.09011621289918002\n",
      "Epoch:  40 , step:  81000 , train loss:  0.001593975908717412 , test loss:  0.07735172808723435\n",
      "Epoch:  40 , step:  82000 , train loss:  1.9486570529492762e-05 , test loss:  0.07547326933251461\n",
      "Epoch:  41 , step:  83000 , train loss:  0.0001821751518694293 , test loss:  0.07988047208130991\n",
      "Epoch:  41 , step:  84000 , train loss:  0.03150350201353853 , test loss:  0.07717257477099176\n",
      "Epoch:  42 , step:  85000 , train loss:  0.00021617254106635398 , test loss:  0.0772004048257339\n",
      "Epoch:  42 , step:  86000 , train loss:  0.0004674100265149079 , test loss:  0.10587493343698282\n",
      "Epoch:  43 , step:  87000 , train loss:  0.0067251077435586 , test loss:  0.10763516408948992\n",
      "Epoch:  43 , step:  88000 , train loss:  0.007839290094110425 , test loss:  0.07960245016589847\n",
      "Epoch:  44 , step:  89000 , train loss:  0.020252903630426672 , test loss:  0.07485930326236476\n",
      "Epoch:  44 , step:  90000 , train loss:  0.000602592416846232 , test loss:  0.09654570492301286\n",
      "Epoch:  45 , step:  91000 , train loss:  4.966601176787893e-05 , test loss:  0.08287600363604959\n",
      "Epoch:  45 , step:  92000 , train loss:  0.00014555804058620855 , test loss:  0.07572361469465304\n",
      "Epoch:  46 , step:  93000 , train loss:  0.06861468352881138 , test loss:  0.13489929793270802\n",
      "Epoch:  46 , step:  94000 , train loss:  0.0002560435420747255 , test loss:  0.0924898838179039\n",
      "Epoch:  47 , step:  95000 , train loss:  0.0033626572546718064 , test loss:  0.08123808723251263\n",
      "Epoch:  47 , step:  96000 , train loss:  0.0004214904928203211 , test loss:  0.10660439675527138\n",
      "Epoch:  48 , step:  97000 , train loss:  1.2614575016173147e-05 , test loss:  0.07642078646578535\n",
      "Epoch:  48 , step:  98000 , train loss:  0.5260733482267675 , test loss:  0.09831199455722749\n",
      "Epoch:  49 , step:  99000 , train loss:  0.007928966370646202 , test loss:  0.08689916485704012\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cesList, tcesList = logistic.batchedStochasticGradient([np.copy(w0),np.copy(b0)], [trainX,trainY], [testX,testY], 1, maxStep = 100000, printInterval = 1000)\n",
    "\n",
    "plt.plot([*range(len(cefList))],cefList)\n",
    "plt.plot([*range(len(tcefList))],tcefList)\n",
    "plt.plot([*range(len(cebList))],cebList)\n",
    "plt.plot([*range(len(tcebList))],tcebList)\n",
    "plt.plot([*range(len(cesList))],cesList)\n",
    "plt.plot([*range(len(tcesList))],tcesList)\n",
    "plt.legend(['Full gradient train','Full gradient test','Batched Stochastic gradient train','Batched Stochastic gradient test','Standard Stochastic gradient train','Standard Stochastic gradient test'])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tii",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
